#!/usr/bin/env python3
"""
PINNs çµ±ä¸€è©•ä¼°è…³æœ¬
æ”¯æ´é…ç½®é©…å‹•çš„æ¨¡çµ„åŒ–è©•ä¼°æµç¨‹ï¼Œä¿®å¾©åº§æ¨™æ¨™æº–åŒ–å•é¡Œ

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python scripts/evaluate.py --checkpoint checkpoints/model.pth --config configs/model.yml
    python scripts/evaluate.py --checkpoint checkpoints/model.pth --reference data/jhtdb/full_field.npz --output results/eval
"""

import argparse
import datetime
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

import numpy as np
import torch
import yaml

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from pinnx.evals.metrics import relative_L2, rmse_metrics, conservation_error
from pinnx.evals.visualizer import Visualizer
from pinnx.models.wrappers import ManualScalingWrapper

logger = logging.getLogger(__name__)


def setup_logging(level: str = "INFO") -> None:
    """è¨­ç½®æ—¥èªŒç³»çµ±"""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )


def load_config(config_path: str) -> Dict[str, Any]:
    """è¼‰å…¥é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    logger.info(f"âœ… é…ç½®è¼‰å…¥æˆåŠŸ: {config_path}")
    return config


def load_checkpoint(checkpoint_path: str, device: str = 'cpu') -> Tuple[torch.nn.Module, Dict]:
    """è¼‰å…¥æ¨¡å‹æª¢æŸ¥é»ï¼ˆç›¸å®¹å¤šç¨® state_dict çµæ§‹ï¼‰"""
    checkpoint = torch.load(checkpoint_path, map_location=device)

    # å¾æª¢æŸ¥é»ä¸­æ¢å¾©é…ç½®
    config = checkpoint.get('config', {})

    # é‡å»ºæ¨¡å‹ï¼ˆéœ€è¦å¾é…ç½®ä¸­ç²å–æ¶æ§‹è³‡è¨Šï¼‰
    from pinnx.models.fourier_mlp import PINNNet

    model_cfg = config.get('model', {})
    in_dim = model_cfg.get('in_dim', 2)  # (x, y) for 2D channel flow
    out_dim = model_cfg.get('out_dim', 3)  # (u, v, p)
    width = model_cfg.get('width', 128)
    depth = model_cfg.get('depth', 6)
    fourier_m = model_cfg.get('fourier_m', 32)
    fourier_sigma = model_cfg.get('fourier_sigma', 1.0)

    model = PINNNet(
        in_dim=in_dim,
        out_dim=out_dim,
        width=width,
        depth=depth,
        fourier_m=fourier_m,
        fourier_sigma=fourier_sigma
    ).to(device)

    # å–å¾—åŸå§‹ state_dictï¼ˆå¯èƒ½åŒ…å«åŒ…è£å™¨èˆ‡æ¨™æº–åŒ–ç·©è¡å€ï¼‰
    raw_state = checkpoint.get('model_state_dict', {})
    if not raw_state:
        # fallback: ä¸€äº›èˆŠæª”å¯èƒ½ç›´æ¥å­˜æ”¾åœ¨ 'state_dict' æˆ–æ ¹å±¤
        raw_state = checkpoint.get('state_dict', checkpoint)

    # æª¢æ¸¬æ˜¯å¦åŒ…å« ManualScalingWrapper çš„ç·©è¡å€
    has_scaling_buffers = any(k in raw_state for k in ['input_min', 'input_max', 'output_min', 'output_max'])
    has_base_prefix = any(k.startswith('base_model.') for k in raw_state.keys())

    if has_scaling_buffers:
        # ä½¿ç”¨ ManualScalingWrapper é‚„åŸå®Œæ•´æ¨¡å‹ï¼ˆä»¥ä½”ä½ç¯„åœåˆå§‹åŒ–ï¼Œéš¨å¾Œç”± state_dict è¦†è“‹ï¼‰
        try:
            # å»ºç«‹ä½”ä½ç¯„åœï¼ˆåç¨±ç„¡é—œï¼Œåƒ…éœ€ç¶­åº¦æ­£ç¢ºï¼‰
            in_ranges = {f'in_{i}': (0.0, 1.0) for i in range(in_dim)}
            out_ranges = {name: (0.0, 1.0) for name in (['u', 'v', 'p'][:out_dim] + [f'out_{i}' for i in range(max(0, out_dim-3))])}
            # è‹¥ out_dim > 3ï¼Œä¸Šé¢æœƒç”¢ç”Ÿå¤šé¤˜åç¨±ï¼Œçµ±ä¸€é‡å»ºç‚ºé€£çºŒéµ
            out_ranges = {f'out_{i}': (0.0, 1.0) for i in range(out_dim)}

            wrapper = ManualScalingWrapper(base_model=model, input_ranges=in_ranges, output_ranges=out_ranges).to(device)

            # å¦‚æœæ¬Šé‡éµæ²’æœ‰ 'base_model.' å‰ç¶´ï¼Œä¸”æ˜¯è£¸æ¨¡å‹æ¬Šé‡ï¼Œå‰‡éœ€è¦å°‡éµæ˜ å°„åˆ° wrapper.base_model ä¸‹
            if not has_base_prefix:
                mapped_state = {}
                for k, v in raw_state.items():
                    if k in ['input_min', 'input_max', 'output_min', 'output_max']:
                        mapped_state[k] = v
                    else:
                        mapped_state[f'base_model.{k}'] = v
                raw_state_to_load = mapped_state
            else:
                raw_state_to_load = raw_state

            missing, unexpected = wrapper.load_state_dict(raw_state_to_load, strict=False)
            if missing:
                logger.warning(f"âš ï¸ è¼‰å…¥åŒ…è£æ¨¡å‹æ™‚ç¼ºå°‘éµ: {missing}")
            if unexpected:
                logger.warning(f"âš ï¸ è¼‰å…¥åŒ…è£æ¨¡å‹æ™‚å­˜åœ¨æœªä½¿ç”¨éµ: {unexpected}")

            model = wrapper
            logger.info("âœ… åµæ¸¬åˆ°å°ºåº¦åŒ–ç·©è¡å€ï¼Œå·²ä½¿ç”¨ ManualScalingWrapper é‚„åŸæ¨¡å‹")
        except Exception as e:
            logger.warning(f"âš ï¸ ManualScalingWrapper é‚„åŸå¤±æ•—ï¼Œå›é€€è‡³è£¸æ¨¡å‹è¼‰å…¥ï¼š{e}")
            # å›é€€ï¼šæŒ‰è£¸æ¨¡å‹æµç¨‹éæ¿¾ä¸¦è¼‰å…¥
            state_no_buffers = {k: v for k, v in raw_state.items() if k not in ['input_min', 'input_max', 'output_min', 'output_max']}
            if any(k.startswith('base_model.') for k in state_no_buffers):
                state_no_buffers = {k.replace('base_model.', '', 1): v for k, v in state_no_buffers.items()}
            model_keys = set(model.state_dict().keys())
            filtered_state = {k: v for k, v in state_no_buffers.items() if k in model_keys}
            missing, unexpected = model.load_state_dict(filtered_state, strict=False)
            if missing:
                logger.warning(f"âš ï¸ è¼‰å…¥æ¬Šé‡æ™‚ç¼ºå°‘éµ: {missing}")
            if unexpected:
                logger.warning(f"âš ï¸ è¼‰å…¥æ¬Šé‡æ™‚å­˜åœ¨æœªä½¿ç”¨éµ: {unexpected}")
    else:
        # è£¸æ¨¡å‹ï¼šå»é™¤åŒ…è£å‰ç¶´èˆ‡éåƒæ•¸éµï¼Œä¸¦éæ¿¾åˆ°åŒ¹é…çš„éµ
        state_no_buffers = {k: v for k, v in raw_state.items() if k not in ['input_min', 'input_max', 'output_min', 'output_max']}
        if has_base_prefix:
            state_no_buffers = {k.replace('base_model.', '', 1): v for k, v in state_no_buffers.items()}
        model_keys = set(model.state_dict().keys())
        filtered_state = {k: v for k, v in state_no_buffers.items() if k in model_keys}
        missing, unexpected = model.load_state_dict(filtered_state, strict=False)
        if missing:
            logger.warning(f"âš ï¸ è¼‰å…¥æ¬Šé‡æ™‚ç¼ºå°‘éµ: {missing}")
        if unexpected:
            logger.warning(f"âš ï¸ è¼‰å…¥æ¬Šé‡æ™‚å­˜åœ¨æœªä½¿ç”¨éµ: {unexpected}")

    model.eval()

    logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ: {checkpoint_path}")
    logger.info(f"   Epoch: {checkpoint.get('epoch', 'N/A')}")
    loss_val = checkpoint.get('loss', None)
    if isinstance(loss_val, (int, float)):
        logger.info(f"   Loss: {loss_val:.6e}")
    else:
        logger.info("   Loss: N/A")
    logger.info(f"   æ¶æ§‹: {width}Ã—{depth}, Fourier M={fourier_m}")

    return model, config


def load_reference_data(ref_path: str) -> Dict[str, np.ndarray]:
    """è¼‰å…¥åƒè€ƒæ•¸æ“šï¼ˆçœŸå€¼ï¼‰"""
    data = np.load(ref_path, allow_pickle=True)
    
    # è™•ç†å…©ç¨®å¯èƒ½çš„æ•¸æ“šæ ¼å¼
    if 'coordinates' in data.keys():
        # æ ¼å¼ 1: coordinates (dict) + (u, v, p) ç¶²æ ¼
        coords = data['coordinates'].item()  # å¾ object array è§£åŒ…
        x_1d = coords['x']
        y_1d = coords['y']
        
        # ç”Ÿæˆ 2D ç¶²æ ¼
        X, Y = np.meshgrid(x_1d, y_1d, indexing='ij')
        
        ref_data = {
            'x': X,
            'y': Y,
            'u': data['u'],
            'v': data['v'],
            'p': data['p']
        }
    else:
        # æ ¼å¼ 2: ç›´æ¥çš„ x, y é™£åˆ—ï¼ˆå·²ç¶“æ˜¯ç¶²æ ¼ï¼‰
        ref_data = {
            'x': data['x'],
            'y': data['y'],
            'u': data['u'],
            'v': data['v'],
            'p': data['p']
        }
    
    logger.info(f"âœ… åƒè€ƒæ•¸æ“šè¼‰å…¥æˆåŠŸ: {ref_path}")
    logger.info(f"   æ•¸æ“šå½¢ç‹€: {ref_data['u'].shape}")
    logger.info(f"   x ç¯„åœ: [{ref_data['x'].min():.4f}, {ref_data['x'].max():.4f}]")
    logger.info(f"   y ç¯„åœ: [{ref_data['y'].min():.4f}, {ref_data['y'].max():.4f}]")
    
    return ref_data


def normalize_coords(x: np.ndarray, y: np.ndarray, 
                     x_range: Tuple[float, float], 
                     y_range: Tuple[float, float]) -> Tuple[np.ndarray, np.ndarray]:
    """
    åº§æ¨™æ¨™æº–åŒ–åˆ° [-1, 1]ï¼ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    
    Args:
        x, y: åŸå§‹åº§æ¨™
        x_range, y_range: åº§æ¨™ç¯„åœ (min, max)
        
    Returns:
        æ¨™æº–åŒ–å¾Œçš„åº§æ¨™
    """
    x_min, x_max = x_range
    y_min, y_max = y_range
    
    x_norm = 2.0 * (x - x_min) / (x_max - x_min) - 1.0
    y_norm = 2.0 * (y - y_min) / (y_max - y_min) - 1.0
    
    return x_norm, y_norm


def predict_on_grid(model: torch.nn.Module,
                    ref_data: Dict[str, np.ndarray],
                    config: Dict[str, Any],
                    device: str = 'cpu') -> Dict[str, torch.Tensor]:
    """
    åœ¨åƒè€ƒæ•¸æ“šç¶²æ ¼ä¸Šé€²è¡Œé æ¸¬
    
    Args:
        model: PINNs æ¨¡å‹
        ref_data: åƒè€ƒæ•¸æ“šï¼ˆåŒ…å« x, y åº§æ¨™ï¼‰
        config: é…ç½®å­—å…¸
        device: è¨ˆç®—è¨­å‚™
        
    Returns:
        é æ¸¬çµæœå­—å…¸ {u, v, p}
    """
    # æå–åº§æ¨™
    x_raw = ref_data['x'].flatten()
    y_raw = ref_data['y'].flatten()
    
    # å¾é…ç½®ä¸­ç²å–åŸŸç¯„åœ
    domain_cfg = config.get('domain', {})
    x_range = domain_cfg.get('x_range', [x_raw.min(), x_raw.max()])
    y_range = domain_cfg.get('y_range', [y_raw.min(), y_raw.max()])
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¨™æº–åŒ–
    normalize = config.get('normalize', True)
    
    if normalize:
        logger.info("ğŸ”„ æ‡‰ç”¨åº§æ¨™æ¨™æº–åŒ–ï¼ˆèˆ‡è¨“ç·´ä¸€è‡´ï¼‰")
        x_norm, y_norm = normalize_coords(x_raw, y_raw, x_range, y_range)
    else:
        logger.info("âš ï¸  æœªé€²è¡Œåº§æ¨™æ¨™æº–åŒ–")
        x_norm, y_norm = x_raw, y_raw
    
    # æ§‹å»ºè¼¸å…¥å¼µé‡ (x, y) - 2D é€šé“æµ
    coords = np.stack([x_norm, y_norm], axis=1)  # (N, 2)
    coords_tensor = torch.from_numpy(coords).float().to(device)
    
    logger.info(f"ğŸ“Š è¼¸å…¥åº§æ¨™ç¯„åœæª¢æŸ¥:")
    logger.info(f"   x: [{coords_tensor[:, 0].min():.4f}, {coords_tensor[:, 0].max():.4f}]")
    logger.info(f"   y: [{coords_tensor[:, 1].min():.4f}, {coords_tensor[:, 1].max():.4f}]")
    
    # æ‰¹æ¬¡é æ¸¬ï¼ˆé¿å…è¨˜æ†¶é«”æº¢å‡ºï¼‰
    batch_size = 4096
    n_points = len(coords_tensor)
    predictions = []
    
    with torch.no_grad():
        for i in range(0, n_points, batch_size):
            batch = coords_tensor[i:i+batch_size]
            pred = model(batch)
            predictions.append(pred)
    
    # æ‹¼æ¥çµæœ
    pred_full = torch.cat(predictions, dim=0)  # (N, 3) -> [u, v, p]
    
    pred_data = {
        'u': pred_full[:, 0],
        'v': pred_full[:, 1],
        'p': pred_full[:, 2]
    }
    
    logger.info(f"âœ… é æ¸¬å®Œæˆï¼Œæ•¸æ“šé»æ•¸: {n_points}")
    
    return pred_data


def calibrate_scale_from_sensors(
    model: torch.nn.Module,
    pred_data: Dict[str, torch.Tensor],
    sensor_file: str,
    device: str = 'cpu'
) -> Dict[str, torch.Tensor]:
    """
    åŸºæ–¼æ„Ÿæ¸¬é»æ•¸æ“šæ ¡æ­£é æ¸¬å ´çš„å°ºåº¦
    
    ç­–ç•¥ï¼š
        1. è¼‰å…¥æ„Ÿæ¸¬é»åº§æ¨™èˆ‡çœŸå¯¦å€¼
        2. åœ¨æ„Ÿæ¸¬é»ä½ç½®é æ¸¬æ¨¡å‹è¼¸å‡º
        3. è¨ˆç®— affine è®Šæ›ï¼špred_corrected = pred * scale + shift
        4. æ‡‰ç”¨åˆ°æ•´å€‹å ´
    
    Args:
        model: è¨“ç·´å¥½çš„ PINN æ¨¡å‹
        pred_data: åŸå§‹é æ¸¬æ•¸æ“š {'u', 'v', 'p'}
        sensor_file: æ„Ÿæ¸¬é»æ•¸æ“šè·¯å¾‘ï¼ˆ.npzï¼‰
        device: è¨ˆç®—è¨­å‚™
    
    Returns:
        æ ¡æ­£å¾Œçš„é æ¸¬æ•¸æ“š
    """
    logger.info(f"ğŸ”§ é–‹å§‹åŸºæ–¼æ„Ÿæ¸¬é»çš„å°ºåº¦æ ¡æ­£...")
    
    # 1. è¼‰å…¥æ„Ÿæ¸¬é»æ•¸æ“š
    sensor_data = np.load(sensor_file, allow_pickle=True)
    sensor_coords = sensor_data['sensor_points']  # (K, 2)
    sensor_true = sensor_data['sensor_data'].item()  # dict: {'u', 'v', 'p'}
    
    K = sensor_coords.shape[0]
    logger.info(f"   æ„Ÿæ¸¬é»æ•¸é‡: {K}")
    
    # 2. åœ¨æ„Ÿæ¸¬é»ä½ç½®é æ¸¬
    with torch.no_grad():
        coords_tensor = torch.from_numpy(sensor_coords).float().to(device)
        sensor_pred = model(coords_tensor)  # (K, 3)
    
    # 3. è¨ˆç®—æ¯å€‹å ´çš„å°ºåº¦è®Šæ›åƒæ•¸
    corrected_data = {}
    
    for i, field in enumerate(['u', 'v', 'p']):
        # æ„Ÿæ¸¬é»è™•çš„é æ¸¬èˆ‡çœŸå€¼
        pred_at_sensors = sensor_pred[:, i].cpu().numpy()
        true_at_sensors = sensor_true[field]
        
        # è¨ˆç®— affine è®Šæ›ï¼ˆæœ€å°äºŒä¹˜æ³•æ“¬åˆï¼‰
        # pred_corrected = a * pred + b
        # ä½¿ç”¨çµ±è¨ˆæ–¹æ³•ï¼šåŒ¹é…å‡å€¼èˆ‡æ¨™æº–å·®
        pred_mean = pred_at_sensors.mean()
        pred_std = pred_at_sensors.std() + 1e-10
        true_mean = true_at_sensors.mean()
        true_std = true_at_sensors.std()
        
        scale = true_std / pred_std
        shift = true_mean - pred_mean * scale
        
        # æ‡‰ç”¨åˆ°æ•´å€‹å ´
        corrected_field = pred_data[field] * scale + shift
        corrected_data[field] = corrected_field
        
        logger.info(f"   {field.upper()}: scale={scale:.4f}, shift={shift:.4f}")
        logger.info(f"      åŸå§‹ç¯„åœ: [{pred_data[field].min():.3f}, {pred_data[field].max():.3f}]")
        logger.info(f"      æ ¡æ­£ç¯„åœ: [{corrected_field.min():.3f}, {corrected_field.max():.3f}]")
        logger.info(f"      çœŸå¯¦ç¯„åœ: [{true_at_sensors.min():.3f}, {true_at_sensors.max():.3f}]")
    
    logger.info("âœ… å°ºåº¦æ ¡æ­£å®Œæˆï¼")
    return corrected_data


def compute_metrics(pred_data: Dict[str, torch.Tensor],
                    ref_data: Dict[str, np.ndarray],
                    coords: Optional[torch.Tensor] = None) -> Dict[str, float]:
    """
    è¨ˆç®—è©•ä¼°æŒ‡æ¨™
    
    Returns:
        æŒ‡æ¨™å­—å…¸
    """
    metrics = {}
    
    # è½‰æ›åƒè€ƒæ•¸æ“šç‚º Tensor
    ref_tensors = {
        'u': torch.from_numpy(ref_data['u'].flatten()).float(),
        'v': torch.from_numpy(ref_data['v'].flatten()).float(),
        'p': torch.from_numpy(ref_data['p'].flatten()).float()
    }
    
    # è¨ˆç®—ç›¸å° L2 èª¤å·®ï¼ˆåœ¨CPUä¸Šé¿å…è£ç½®ä¸ä¸€è‡´å•é¡Œï¼Œä¸”ä¸å½±éŸ¿è‡ªå‹•å¾®åˆ†åœ–ï¼‰
    for field in ['u', 'v', 'p']:
        pred_cpu = pred_data[field].detach().cpu()
        ref_cpu = ref_tensors[field].detach().cpu()
        rel_l2 = relative_L2(pred_cpu, ref_cpu).item()
        metrics[f'rel_L2_{field}'] = rel_l2
        logger.info(f"   {field.upper()} ç›¸å° L2 èª¤å·®: {rel_l2:.6f} {'âœ…' if rel_l2 <= 0.15 else 'âŒ'}")

    # è¨ˆç®— RMSEï¼ˆè½‰ç‚º numpyï¼‰
    for field in ['u', 'v', 'p']:
        pred = pred_data[field].detach().cpu().numpy()
        ref = ref_tensors[field].detach().cpu().numpy()
        rmse = np.sqrt(np.mean((pred - ref)**2))
        ref_std = ref.std()
        rel_rmse = rmse / (ref_std + 1e-10)
        metrics[f'rmse_{field}'] = float(rmse)
        metrics[f'rel_rmse_{field}'] = float(rel_rmse)
    
    # è³ªé‡å®ˆæ†æª¢æŸ¥ï¼ˆå¦‚æœæä¾›åº§æ¨™ï¼‰
    if coords is not None:
        try:
            div = conservation_error(pred_data['u'], pred_data['v'], coords)
            metrics['divergence_mean'] = div
            logger.info(f"   æ•£åº¦èª¤å·®: {div:.2e}")
        except Exception as e:
            logger.warning(f"âš ï¸  ç„¡æ³•è¨ˆç®—æ•£åº¦: {e}")
    
    return metrics


def generate_evaluation(checkpoint_path: str,
                       reference_path: str,
                       config_path: Optional[str] = None,
                       output_dir: str = "evaluation_results",
                       device: str = "cpu",
                       sensor_file: Optional[str] = None,
                       apply_scale_calibration: bool = False) -> None:
    """Generate comprehensive evaluation report for a trained model."""
    # é€™å€‹å‡½æ•¸å°šæœªå¯¦ç¾ï¼Œä½¿ç”¨mainå‡½æ•¸æ›¿ä»£
    print("âš ï¸  generate_evaluation function not implemented. Use main() instead.")
    pass
