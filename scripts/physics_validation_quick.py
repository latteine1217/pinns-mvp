#!/usr/bin/env python3
"""
å¿«é€Ÿç‰©ç†é©—è­‰è…³æœ¬ - æª¢æŸ¥æª¢æŸ¥é»çš„ç‰©ç†ä¸€è‡´æ€§

é©—è­‰é …ç›®ï¼š
1. å£é¢é‚Šç•Œæ¢ä»¶ (u,v,w â‰ˆ 0 at y=-1, y=1)
2. å£é¢å‰ªæ‡‰åŠ› Ï„_w (ç›®æ¨™ >5.0, ç†æƒ³ >15.0)
3. è³ªé‡å®ˆæ† âˆ‚u/âˆ‚x + âˆ‚v/âˆ‚y + âˆ‚w/âˆ‚z â‰ˆ 0
4. é€Ÿåº¦å ´çµ±è¨ˆåˆ†ä½ˆ
"""

import torch
import numpy as np
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from pinnx.models.fourier_mlp import create_enhanced_pinn

def load_checkpoint(checkpoint_path):
    """è¼‰å…¥æª¢æŸ¥é»"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    return checkpoint

def create_model_from_checkpoint(checkpoint):
    """å¾æª¢æŸ¥é»é‡å»ºæ¨¡å‹"""
    config = checkpoint['config']
    model_config = config['model']
    
    # æå– Fourier åƒæ•¸
    fourier_cfg = model_config.get('fourier_features', {})
    fourier_m = fourier_cfg.get('fourier_m', model_config.get('fourier_m', 32))
    
    # å¾æª¢æŸ¥é»çš„ state_dict ä¸­æå–å°ºåº¦å› å­
    input_scale_factors = None
    if 'input_scale_factors' in checkpoint['model_state_dict']:
        input_scale_factors = checkpoint['model_state_dict']['input_scale_factors']
    elif 'scaling' in config and 'input_scale_factors' in config['scaling']:
        input_scale_factors = config['scaling']['input_scale_factors']
    
    # ä½¿ç”¨ factory å‡½æ•¸å‰µå»ºæ¨¡å‹
    model = create_enhanced_pinn(
        in_dim=model_config.get('in_dim', 3),
        out_dim=model_config.get('out_dim', 4),
        width=model_config.get('width', 256),
        depth=model_config.get('depth', 6),
        activation=model_config.get('activation', 'sine'),
        use_fourier=model_config.get('use_fourier', True),
        fourier_m=fourier_m,
        fourier_sigma=fourier_cfg.get('fourier_sigma', 1.0),
        use_rwf=model_config.get('use_rwf', False),
        rwf_scale_std=model_config.get('rwf_scale_std', 0.1),
        fourier_normalize_input=fourier_cfg.get('normalize_input', True),
        input_scale_factors=input_scale_factors
    )
    
    # è¼‰å…¥ state_dictï¼ˆstrict=False é¿å… input_scale_factors çš„å•é¡Œï¼‰
    model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    model.eval()
    
    # æå–æ­¸ä¸€åŒ–åƒæ•¸ï¼ˆç”¨æ–¼åæ­¸ä¸€åŒ–ï¼‰
    normalization = checkpoint.get('normalization', None)
    
    return model, config, normalization

def denormalize_output(output, normalization):
    """åæ­¸ä¸€åŒ–è¼¸å‡º"""
    if normalization is None:
        return output
    
    means = normalization['means']
    scales = normalization['scales']
    
    # output shape: [N, 4] -> [u, v, w, p]
    u = output[:, 0] * scales['u'] + means['u']
    v = output[:, 1] * scales['v'] + means['v']
    w = output[:, 2] * scales['w'] + means['w']
    p = output[:, 3] * scales['p'] + means['p']
    
    return torch.stack([u, v, w, p], dim=1)

def compute_gradients(model, normalization, x, y, z):
    """è¨ˆç®—ä¸€éšå°æ•¸ (ç”¨æ–¼å‰ªæ‡‰åŠ›å’Œè³ªé‡å®ˆæ†)ï¼Œè‡ªå‹•è™•ç†æ­¸ä¸€åŒ–"""
    xyz = torch.stack([x, y, z], dim=1)
    xyz.requires_grad_(True)
    
    output_norm = model(xyz)
    output = denormalize_output(output_norm, normalization)
    
    u, v, w, p = output[:, 0], output[:, 1], output[:, 2], output[:, 3]
    
    # è¨ˆç®—æ¢¯åº¦
    du_dy = torch.autograd.grad(u.sum(), xyz, create_graph=True)[0][:, 1]
    dv_dy = torch.autograd.grad(v.sum(), xyz, create_graph=True)[0][:, 1]
    dw_dy = torch.autograd.grad(w.sum(), xyz, create_graph=True)[0][:, 1]
    
    # è³ªé‡å®ˆæ†é …
    du_dx = torch.autograd.grad(u.sum(), xyz, create_graph=True)[0][:, 0]
    dv_dy_cont = torch.autograd.grad(v.sum(), xyz, create_graph=True)[0][:, 1]
    dw_dz = torch.autograd.grad(w.sum(), xyz, create_graph=True)[0][:, 2]
    
    return {
        'u': u, 'v': v, 'w': w, 'p': p,
        'du_dy': du_dy, 'dv_dy': dv_dy, 'dw_dy': dw_dy,
        'du_dx': du_dx, 'dv_dy_cont': dv_dy_cont, 'dw_dz': dw_dz
    }

def validate_wall_boundary(model, normalization, n_points=100):
    """é©—è­‰å£é¢é‚Šç•Œæ¢ä»¶"""
    print("\n" + "="*70)
    print("ğŸ§± å£é¢é‚Šç•Œæ¢ä»¶é©—è­‰")
    print("="*70)
    
    # åœ¨å£é¢ç”Ÿæˆæ¸¬è©¦é» (y=-1 å’Œ y=1)
    x = torch.linspace(0, 8*np.pi, n_points)
    z = torch.linspace(0, 3*np.pi, n_points)
    x_wall, z_wall = torch.meshgrid(x, z, indexing='ij')
    x_wall, z_wall = x_wall.flatten(), z_wall.flatten()
    
    results = {}
    
    for wall_name, y_val in [("ä¸‹å£é¢ (y=-1)", -1.0), ("ä¸Šå£é¢ (y=+1)", 1.0)]:
        y_wall = torch.full_like(x_wall, y_val)
        
        with torch.no_grad():
            xyz_wall = torch.stack([x_wall, y_wall, z_wall], dim=1)
            output_norm = model(xyz_wall)
            output = denormalize_output(output_norm, normalization)
            
            u_wall = output[:, 0]
            v_wall = output[:, 1]
            w_wall = output[:, 2]
        
        results[wall_name] = {
            'u': u_wall.numpy(),
            'v': v_wall.numpy(),
            'w': w_wall.numpy()
        }
        
        print(f"\n{wall_name}:")
        print(f"  u: mean={u_wall.mean():.6f}, std={u_wall.std():.6f}, max_abs={u_wall.abs().max():.6f}")
        print(f"  v: mean={v_wall.mean():.6f}, std={v_wall.std():.6f}, max_abs={v_wall.abs().max():.6f}")
        print(f"  w: mean={w_wall.mean():.6f}, std={w_wall.std():.6f}, max_abs={w_wall.abs().max():.6f}")
        
        # åˆ¤æ–·æ˜¯å¦æ»¿è¶³ no-slip
        u_violation = u_wall.abs().max().item()
        v_violation = v_wall.abs().max().item()
        w_violation = w_wall.abs().max().item()
        
        if max(u_violation, v_violation, w_violation) < 0.01:
            print(f"  âœ… No-slip æ¢ä»¶è‰¯å¥½ (max violation < 0.01)")
        elif max(u_violation, v_violation, w_violation) < 0.1:
            print(f"  âš ï¸  No-slip æ¢ä»¶å°šå¯ (max violation < 0.1)")
        else:
            print(f"  âŒ No-slip æ¢ä»¶ä¸ä½³ (max violation >= 0.1)")
    
    return results

def validate_wall_shear_stress(model, normalization, physics_config, n_points=100):
    """é©—è­‰å£é¢å‰ªæ‡‰åŠ›"""
    print("\n" + "="*70)
    print("ğŸ“ å£é¢å‰ªæ‡‰åŠ›é©—è­‰")
    print("="*70)
    
    nu = physics_config['nu']
    
    # æ”¯æ´å…©ç¨®é…ç½®æ ¼å¼
    if 'Re_tau' in physics_config:
        Re_tau = physics_config['Re_tau']
    elif 'channel_flow' in physics_config and 'Re_tau' in physics_config['channel_flow']:
        Re_tau = physics_config['channel_flow']['Re_tau']
    else:
        print("âš ï¸  ç„¡æ³•æ‰¾åˆ° Re_tauï¼Œè·³éå‰ªæ‡‰åŠ›é©—è­‰")
        return
    
    # ç†è«–å€¼: Ï„_w = Ï u_Ï„Â² (Ï=1, u_Ï„=0.04997)
    u_tau_theoretical = nu * Re_tau  # u_Ï„ = Î½ Re_Ï„ / Î´_Î½, Î´_Î½=1
    tau_w_theoretical = u_tau_theoretical ** 2
    
    print(f"\nç†è«–å€¼:")
    print(f"  Î½ = {nu}")
    print(f"  Re_Ï„ = {Re_tau}")
    print(f"  u_Ï„ (ç†è«–) = {u_tau_theoretical:.6f}")
    print(f"  Ï„_w (ç†è«–) = ÏÂ·u_Ï„Â² = {tau_w_theoretical:.6f}")
    
    # åœ¨å£é¢ç”Ÿæˆæ¸¬è©¦é»
    x = torch.linspace(0, 8*np.pi, n_points)
    z = torch.linspace(0, 3*np.pi, n_points)
    x_wall, z_wall = torch.meshgrid(x, z, indexing='ij')
    x_wall, z_wall = x_wall.flatten(), z_wall.flatten()
    
    for wall_name, y_val in [("ä¸‹å£é¢ (y=-1)", -1.0), ("ä¸Šå£é¢ (y=+1)", 1.0)]:
        y_wall = torch.full_like(x_wall, y_val)
        
        grads = compute_gradients(model, normalization, x_wall, y_wall, z_wall)
        du_dy = grads['du_dy'].detach().numpy()
        
        # Ï„_w = Î¼ (âˆ‚u/âˆ‚y)|_wall (Î¼ = ÏÂ·Î½, Ï=1)
        tau_w = nu * du_dy
        
        print(f"\n{wall_name}:")
        print(f"  âˆ‚u/âˆ‚y: mean={du_dy.mean():.3f}, std={du_dy.std():.3f}")
        print(f"  Ï„_w = Î½Â·âˆ‚u/âˆ‚y: mean={tau_w.mean():.6f}, std={tau_w.std():.6f}")
        print(f"  Ï„_w / Ï„_w(ç†è«–): {tau_w.mean()/tau_w_theoretical:.3f}")
        
        # åˆ¤æ–·æ¨™æº–
        tau_w_mean = abs(tau_w.mean())
        if tau_w_mean > 15.0 * tau_w_theoretical:
            print(f"  âœ… å‰ªæ‡‰åŠ›å„ªç§€ (>{15*tau_w_theoretical:.4f})")
        elif tau_w_mean > 5.0 * tau_w_theoretical:
            print(f"  âš ï¸  å‰ªæ‡‰åŠ›å°šå¯ (>{5*tau_w_theoretical:.4f})")
        else:
            print(f"  âŒ å‰ªæ‡‰åŠ›ä¸è¶³ (<{5*tau_w_theoretical:.4f})")

def validate_mass_conservation(model, normalization, n_points=50):
    """é©—è­‰è³ªé‡å®ˆæ†"""
    print("\n" + "="*70)
    print("ğŸŒŠ è³ªé‡å®ˆæ†é©—è­‰ (é€£çºŒæ€§æ–¹ç¨‹)")
    print("="*70)
    
    # åœ¨å…§éƒ¨å€åŸŸç”Ÿæˆæ¸¬è©¦é»
    x_test = torch.rand(n_points**2) * 8 * np.pi
    y_test = torch.rand(n_points**2) * 1.8 - 0.9  # é¿é–‹é‚Šç•Œ
    z_test = torch.rand(n_points**2) * 3 * np.pi
    
    grads = compute_gradients(model, normalization, x_test, y_test, z_test)
    
    # âˆ‡Â·u = âˆ‚u/âˆ‚x + âˆ‚v/âˆ‚y + âˆ‚w/âˆ‚z
    div_u = grads['du_dx'] + grads['dv_dy_cont'] + grads['dw_dz']
    div_u_np = div_u.detach().numpy()
    
    print(f"\næ•£åº¦ âˆ‡Â·u (ä¸å¯å£“ç¸®è¦æ±‚: â‰ˆ 0):")
    print(f"  mean = {div_u_np.mean():.6e}")
    print(f"  std  = {div_u_np.std():.6e}")
    print(f"  max  = {div_u_np.max():.6e}")
    print(f"  min  = {div_u_np.min():.6e}")
    print(f"  |âˆ‡Â·u| max = {np.abs(div_u_np).max():.6e}")
    
    # åˆ¤æ–·æ¨™æº–
    max_div = np.abs(div_u_np).max()
    if max_div < 1e-3:
        print(f"  âœ… è³ªé‡å®ˆæ†å„ªç§€ (|âˆ‡Â·u| < 1e-3)")
    elif max_div < 1e-2:
        print(f"  âš ï¸  è³ªé‡å®ˆæ†å°šå¯ (|âˆ‡Â·u| < 1e-2)")
    else:
        print(f"  âŒ è³ªé‡å®ˆæ†ä¸ä½³ (|âˆ‡Â·u| >= 1e-2)")
    
    return div_u_np

def validate_velocity_statistics(model, normalization, n_points=100):
    """é©—è­‰é€Ÿåº¦å ´çµ±è¨ˆç‰¹æ€§"""
    print("\n" + "="*70)
    print("ğŸ“Š é€Ÿåº¦å ´çµ±è¨ˆé©—è­‰")
    print("="*70)
    
    # åœ¨æ•´å€‹å€åŸŸç”Ÿæˆæ¸¬è©¦é»
    x = torch.rand(n_points**2) * 8 * np.pi
    y = torch.rand(n_points**2) * 2 - 1  # [-1, 1]
    z = torch.rand(n_points**2) * 3 * np.pi
    
    with torch.no_grad():
        xyz = torch.stack([x, y, z], dim=1)
        output_norm = model(xyz)
        output = denormalize_output(output_norm, normalization)
        
        u = output[:, 0].numpy()
        v = output[:, 1].numpy()
        w = output[:, 2].numpy()
        p = output[:, 3].numpy()
    
    print(f"\né€Ÿåº¦åˆ†é‡çµ±è¨ˆ:")
    print(f"  u (æµå‘): mean={u.mean():.4f}, std={u.std():.4f}, range=[{u.min():.4f}, {u.max():.4f}]")
    print(f"  v (å£é¢): mean={v.mean():.4f}, std={v.std():.4f}, range=[{v.min():.4f}, {v.max():.4f}]")
    print(f"  w (å±•å‘): mean={w.mean():.4f}, std={w.std():.4f}, range=[{w.min():.4f}, {w.max():.4f}]")
    print(f"  p (å£“åŠ›): mean={p.mean():.4f}, std={p.std():.4f}, range=[{p.min():.4f}, {p.max():.4f}]")
    
    # é€šé“æµç‰¹æ€§æª¢æŸ¥
    print(f"\né€šé“æµç‰¹æ€§æª¢æŸ¥:")
    if u.mean() > 0.5:
        print(f"  âœ… æµå‘é€Ÿåº¦ u å¹³å‡å€¼åˆç† (>{0.5})")
    else:
        print(f"  âš ï¸  æµå‘é€Ÿåº¦ u å¹³å‡å€¼åä½ (<{0.5})")
    
    if abs(v.mean()) < 0.1:
        print(f"  âœ… å£é¢æ³•å‘é€Ÿåº¦ v å¹³å‡æ¥è¿‘é›¶")
    else:
        print(f"  âš ï¸  å£é¢æ³•å‘é€Ÿåº¦ v å¹³å‡å€¼ç•°å¸¸ (|mean|={abs(v.mean()):.4f})")
    
    if abs(w.mean()) < 0.1:
        print(f"  âœ… å±•å‘é€Ÿåº¦ w å¹³å‡æ¥è¿‘é›¶")
    else:
        print(f"  âš ï¸  å±•å‘é€Ÿåº¦ w å¹³å‡å€¼ç•°å¸¸ (|mean|={abs(w.mean()):.4f})")

def main():
    # è¼‰å…¥æª¢æŸ¥é»
    checkpoint_path = "checkpoints/test_physics_fix_1k_v2/best_model.pth"
    
    print("="*70)
    print("ğŸ”¬ ç‰©ç†é©—è­‰é–‹å§‹")
    print("="*70)
    print(f"æª¢æŸ¥é»: {checkpoint_path}")
    
    checkpoint = load_checkpoint(checkpoint_path)
    model, config, normalization = create_model_from_checkpoint(checkpoint)
    
    print(f"Epoch: {checkpoint['epoch']}")
    print(f"é©—è­‰é›†æå¤±: {checkpoint['metrics'].get('val_loss', 'N/A'):.6f}")
    
    if normalization:
        print(f"\nâš ï¸  æª¢æ¸¬åˆ°è¼¸å‡ºæ­¸ä¸€åŒ–:")
        print(f"  Means: u={normalization['means']['u']:.3f}, v={normalization['means']['v']:.3f}, w={normalization['means']['w']:.3f}, p={normalization['means']['p']:.3f}")
        print(f"  Scales: u={normalization['scales']['u']:.3f}, v={normalization['scales']['v']:.3f}, w={normalization['scales']['w']:.3f}, p={normalization['scales']['p']:.3f}")
    
    # åŸ·è¡Œé©—è­‰ï¼ˆå‚³éæ­¸ä¸€åŒ–åƒæ•¸ï¼‰
    validate_wall_boundary(model, normalization, n_points=100)
    validate_wall_shear_stress(model, normalization, config['physics'], n_points=100)
    validate_mass_conservation(model, normalization, n_points=50)
    validate_velocity_statistics(model, normalization, n_points=100)
    
    print("\n" + "="*70)
    print("âœ… ç‰©ç†é©—è­‰å®Œæˆ")
    print("="*70)

if __name__ == "__main__":
    main()
