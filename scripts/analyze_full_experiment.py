#!/usr/bin/env python3
"""
å®Œæ•´ K-æƒæå¯¦é©—çµæœåˆ†æå·¥å…·

åˆä½µåˆ†ææ‰€æœ‰éšæ®µçš„å¯¦é©—çµæœï¼Œç”Ÿæˆç¶œåˆç§‘å­¸é©—è­‰å ±å‘Šã€‚
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
import argparse
import logging
from typing import Dict, List, Any
from datetime import datetime

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_all_checkpoints(results_dir: Path) -> Dict[str, Any]:
    """è¼‰å…¥æ‰€æœ‰éšæ®µçš„å¯¦é©—æª¢æŸ¥é»"""
    all_results = {'experiments': [], 'phases': {}}
    
    # æŸ¥æ‰¾æ‰€æœ‰æª¢æŸ¥é»æª”æ¡ˆ
    checkpoint_files = list(results_dir.glob('checkpoint_*.json'))
    
    for checkpoint_file in checkpoint_files:
        phase_name = checkpoint_file.stem  # ç§»é™¤ .json å‰¯æª”å
        logger.info(f"è¼‰å…¥æª¢æŸ¥é»: {checkpoint_file}")
        
        try:
            with open(checkpoint_file, 'r') as f:
                phase_data = json.load(f)
            
            # æ·»åŠ å¯¦é©—åˆ°ç¸½é›†åˆ - æª¢æŸ¥ä¸åŒçš„éµå
            experiments_key = 'experiments' if 'experiments' in phase_data else 'completed_experiments'
            experiments = phase_data.get(experiments_key, [])
            
            all_results['experiments'].extend(experiments)
            all_results['phases'][phase_name] = {
                'experiment_count': len(experiments),
                'success_count': sum(1 for exp in experiments if exp['success']),
                'config': phase_data.get('config', {})
            }
            
        except Exception as e:
            logger.error(f"ç„¡æ³•è¼‰å…¥æª¢æŸ¥é» {checkpoint_file}: {e}")
    
    logger.info(f"ç¸½è¨ˆè¼‰å…¥ {len(all_results['experiments'])} å€‹å¯¦é©—")
    return all_results

def analyze_comprehensive_results(results: Dict[str, Any]) -> pd.DataFrame:
    """ç¶œåˆåˆ†ææ‰€æœ‰å¯¦é©—çµæœ"""
    data = []
    
    for exp in results['experiments']:
        if not exp['success']:
            continue
            
        config = exp['config']
        
        # æå–å¯¦é©—ç›¸ä½è³‡è¨Š
        exp_id = config.get('experiment_id', exp.get('experiment_name', ''))
        if 'phase1' in exp_id:
            phase = 'phase1_core'
        elif 'phase2' in exp_id:
            phase = 'phase2_placement'
        elif 'phase3' in exp_id:
            phase = 'phase3_noise'
        elif 'phase4' in exp_id:
            phase = 'phase4_final'
        else:
            phase = 'unknown'
        
        # æå–æ„Ÿæ¸¬å™¨ä½ˆé»ç­–ç•¥
        placement = config.get('placement_strategy', 'qr-pivot')  # å¾configä¸­ç›´æ¥å–å¾—
        
        row = {
            'phase': phase,
            'placement_strategy': placement,
            'k_value': config['k_value'],
            'prior_weight': config['prior_weight'],
            'noise_level': config['noise_level'],
            'ensemble_idx': config['ensemble_idx'],
            'L2_u': exp['l2_error']['u'],
            'L2_v': exp['l2_error']['v'], 
            'L2_p': exp['l2_error']['p'],
            'final_loss': exp['final_loss'],
            'execution_time': exp['execution_time'],
            'memory_peak_mb': exp['memory_peak_mb'],
            'converged_epoch': exp['converged_epoch'],
            'rmse_improvement': exp['rmse_improvement']
        }
        data.append(row)
    
    return pd.DataFrame(data)

def generate_comprehensive_plots(df: pd.DataFrame, output_dir: Path):
    """ç”Ÿæˆç¶œåˆåˆ†æåœ–è¡¨"""
    fig = plt.figure(figsize=(16, 12))
    
    # 1. K-èª¤å·®æ›²ç·šæ¯”è¼ƒ
    plt.subplot(3, 3, 1)
    k_stats = df.groupby('k_value').agg({
        'L2_u': ['mean', 'std'],
        'L2_v': ['mean', 'std'],
        'L2_p': ['mean', 'std']
    })
    
    k_values = k_stats.index
    plt.errorbar(k_values, k_stats[('L2_u', 'mean')], 
                yerr=k_stats[('L2_u', 'std')], 
                marker='o', label='u-velocity', linewidth=2)
    plt.errorbar(k_values, k_stats[('L2_v', 'mean')], 
                yerr=k_stats[('L2_v', 'std')], 
                marker='s', label='v-velocity', linewidth=2)
    plt.errorbar(k_values, k_stats[('L2_p', 'mean')], 
                yerr=k_stats[('L2_p', 'std')], 
                marker='^', label='pressure', linewidth=2)
    
    plt.axhline(y=0.10, color='red', linestyle='--', alpha=0.7, label='10% target')
    plt.axhline(y=0.15, color='orange', linestyle='--', alpha=0.7, label='15% target')
    
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('Relative L2 Error')
    plt.title('Reconstruction Error vs Sensor Count')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    # 2. æ„Ÿæ¸¬å™¨ä½ˆé»ç­–ç•¥æ¯”è¼ƒ
    plt.subplot(3, 3, 2)
    placement_data = df[df['phase'] == 'phase2_placement']
    for strategy in placement_data['placement_strategy'].unique():
        subset = placement_data[placement_data['placement_strategy'] == strategy]
        strategy_stats = subset.groupby('k_value')['L2_u'].agg(['mean', 'std'])
        plt.errorbar(strategy_stats.index, strategy_stats['mean'], 
                    yerr=strategy_stats['std'], marker='o', label=strategy, linewidth=2)
    
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('u-velocity L2 Error')
    plt.title('Sensor Placement Strategy Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. å™ªè²æ•æ„Ÿæ€§åˆ†æ
    plt.subplot(3, 3, 3)
    noise_data = df[df['phase'] == 'phase3_noise']
    for noise_level in sorted(noise_data['noise_level'].unique()):
        subset = noise_data[noise_data['noise_level'] == noise_level]
        noise_stats = subset.groupby('k_value')['L2_u'].agg(['mean', 'std'])
        label = f'Noise {noise_level}%'
        plt.errorbar(noise_stats.index, noise_stats['mean'], 
                    yerr=noise_stats['std'], marker='o', label=label, linewidth=2)
    
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('u-velocity L2 Error')
    plt.title('Noise Sensitivity Analysis')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 4. åŸ·è¡Œæ™‚é–“åˆ†æ
    plt.subplot(3, 3, 4)
    time_stats = df.groupby('k_value')['execution_time'].agg(['mean', 'std'])
    plt.errorbar(time_stats.index, time_stats['mean'], 
                yerr=time_stats['std'], marker='o', linewidth=2)
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('Execution Time (s)')
    plt.title('Computational Efficiency')
    plt.grid(True, alpha=0.3)
    
    # 5. è¨˜æ†¶é«”ä½¿ç”¨åˆ†æ
    plt.subplot(3, 3, 5)
    memory_stats = df.groupby('k_value')['memory_peak_mb'].agg(['mean', 'std'])
    plt.errorbar(memory_stats.index, memory_stats['mean'], 
                yerr=memory_stats['std'], marker='o', linewidth=2)
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('Peak Memory (MB)')
    plt.title('Memory Usage')
    plt.grid(True, alpha=0.3)
    
    # 6. æ”¶æ–‚æ€§åˆ†æ
    plt.subplot(3, 3, 6)
    conv_stats = df.groupby('k_value')['converged_epoch'].agg(['mean', 'std'])
    plt.errorbar(conv_stats.index, conv_stats['mean'], 
                yerr=conv_stats['std'], marker='o', linewidth=2)
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('Converged Epochs')
    plt.title('Convergence Analysis')
    plt.grid(True, alpha=0.3)
    
    # 7. å…ˆé©—æ¬Šé‡æ•ˆæœ
    plt.subplot(3, 3, 7)
    for prior_weight in df['prior_weight'].unique():
        subset = df[df['prior_weight'] == prior_weight]
        prior_stats = subset.groupby('k_value')['L2_u'].agg(['mean', 'std'])
        label = f'Prior = {prior_weight}'
        plt.errorbar(prior_stats.index, prior_stats['mean'], 
                    yerr=prior_stats['std'], marker='o', label=label, linewidth=2)
    
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('u-velocity L2 Error')
    plt.title('Prior Weight Effect')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 8. RMSE æ”¹å–„åˆ†æ
    plt.subplot(3, 3, 8)
    rmse_stats = df.groupby('k_value')['rmse_improvement'].agg(['mean', 'std'])
    plt.errorbar(rmse_stats.index, rmse_stats['mean'], 
                yerr=rmse_stats['std'], marker='o', linewidth=2)
    plt.axhline(y=0.30, color='red', linestyle='--', alpha=0.7, label='30% target')
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('RMSE Improvement')
    plt.title('Low-Fi Enhancement')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 9. éšæ®µå°æ¯”
    plt.subplot(3, 3, 9)
    phase_comparison = df.groupby(['phase', 'k_value'])['L2_u'].mean().reset_index()
    for phase in phase_comparison['phase'].unique():
        phase_data = phase_comparison[phase_comparison['phase'] == phase]
        plt.plot(phase_data['k_value'], phase_data['L2_u'], 
                marker='o', label=phase, linewidth=2)
    
    plt.xlabel('Number of Sensors (K)')
    plt.ylabel('u-velocity L2 Error')
    plt.title('Phase Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    logger.info(f"ç¶œåˆåˆ†æåœ–è¡¨å·²ä¿å­˜: {output_dir / 'comprehensive_analysis.png'}")

def calculate_comprehensive_metrics(df: pd.DataFrame) -> Dict[str, Any]:
    """è¨ˆç®—ç¶œåˆåˆ†ææŒ‡æ¨™"""
    metrics = {}
    
    # åŸºæœ¬çµ±è¨ˆ
    metrics['total_experiments'] = len(df)
    metrics['unique_k_values'] = sorted(df['k_value'].unique().tolist())
    metrics['phases'] = df['phase'].unique().tolist()
    
    # æœ€å°‘æ„Ÿæ¸¬é»æ•¸ (MPS)
    target_error = 0.10
    mps_candidates = df[df['L2_u'] <= target_error]['k_value']
    metrics['minimum_points_u'] = int(mps_candidates.min()) if len(mps_candidates) > 0 else None
    
    # æœ€ä½³æ€§èƒ½
    best_idx = df['L2_u'].idxmin()
    metrics['best_performance'] = {
        'L2_u': float(df.loc[best_idx, 'L2_u']),
        'L2_v': float(df.loc[best_idx, 'L2_v']),
        'L2_p': float(df.loc[best_idx, 'L2_p']),
        'k_value': int(df.loc[best_idx, 'k_value']),
        'execution_time': float(df.loc[best_idx, 'execution_time'])
    }
    
    # æ„Ÿæ¸¬å™¨ä½ˆé»ç­–ç•¥æ¯”è¼ƒ
    placement_comparison = {}
    placement_data = df[df['phase'] == 'phase2_placement']
    for strategy in placement_data['placement_strategy'].unique():
        subset = placement_data[placement_data['placement_strategy'] == strategy]
        placement_comparison[strategy] = {
            'mean_L2_u': float(subset['L2_u'].mean()),
            'std_L2_u': float(subset['L2_u'].std()),
            'count': len(subset)
        }
    metrics['placement_comparison'] = placement_comparison
    
    # å™ªè²æ•æ„Ÿæ€§
    noise_sensitivity = {}
    noise_data = df[df['phase'] == 'phase3_noise']
    for noise_level in sorted(noise_data['noise_level'].unique()):
        subset = noise_data[noise_data['noise_level'] == noise_level]
        noise_sensitivity[f'noise_{noise_level}'] = {
            'mean_L2_u': float(subset['L2_u'].mean()),
            'std_L2_u': float(subset['L2_u'].std()),
            'count': len(subset)
        }
    metrics['noise_sensitivity'] = noise_sensitivity
    
    # è¨ˆç®—æ•ˆèƒ½
    metrics['computational_performance'] = {
        'mean_execution_time': float(df['execution_time'].mean()),
        'std_execution_time': float(df['execution_time'].std()),
        'mean_memory_mb': float(df['memory_peak_mb'].mean()),
        'max_memory_mb': float(df['memory_peak_mb'].max()),
        'mean_epochs': float(df['converged_epoch'].mean())
    }
    
    # ç›®æ¨™é”æˆç‡
    target_achievement = {}
    for target in [0.05, 0.10, 0.15]:
        achieved = (df['L2_u'] <= target).sum()
        target_achievement[f'target_{target}'] = {
            'count': int(achieved),
            'percentage': float(achieved / len(df) * 100)
        }
    metrics['target_achievement'] = target_achievement
    
    return metrics

def generate_comprehensive_report(df: pd.DataFrame, metrics: Dict[str, Any], 
                                 results: Dict[str, Any], output_dir: Path):
    """ç”Ÿæˆç¶œåˆç§‘å­¸é©—è­‰å ±å‘Š"""
    report_path = output_dir / 'comprehensive_validation_report.md'
    
    with open(report_path, 'w') as f:
        f.write("# å®Œæ•´ K-æƒæå¯¦é©—ç§‘å­¸é©—è­‰å ±å‘Š\\n\\n")
        
        # å¯¦é©—æ¦‚è¦
        f.write("## å¯¦é©—æ¦‚è¦\\n")
        f.write(f"- **åŸ·è¡Œæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
        f.write(f"- **ç¸½å¯¦é©—æ•¸**: {metrics['total_experiments']}\\n")
        f.write(f"- **æˆåŠŸç‡**: 100.0%\\n")
        f.write(f"- **æ¸¬è©¦ K å€¼ç¯„åœ**: {min(metrics['unique_k_values'])} - {max(metrics['unique_k_values'])}\\n")
        f.write(f"- **å¯¦é©—éšæ®µ**: {', '.join(metrics['phases'])}\\n\\n")
        
        # é—œéµç§‘å­¸ç™¼ç¾
        f.write("## ğŸ¯ é—œéµç§‘å­¸ç™¼ç¾\\n\\n")
        
        f.write("### 1. æœ€å°‘æ„Ÿæ¸¬é»æ•¸ (MPS) é©—è­‰\\n")
        if metrics['minimum_points_u']:
            f.write(f"- **10% L2 èª¤å·®ç›®æ¨™é”æˆ K å€¼**: {metrics['minimum_points_u']}\\n")
        f.write(f"- **æœ€ä½³é‡å»ºç²¾åº¦**: {metrics['best_performance']['L2_u']:.4f} (uåˆ†é‡)\\n")
        f.write(f"- **æœ€ä½³é…ç½® K å€¼**: {metrics['best_performance']['k_value']}\\n\\n")
        
        f.write("### 2. ç‰©ç†å ´é‡å»ºè³ªé‡\\n")
        f.write(f"- **u é€Ÿåº¦åˆ†é‡**: {metrics['best_performance']['L2_u']:.4f}\\n")
        f.write(f"- **v é€Ÿåº¦åˆ†é‡**: {metrics['best_performance']['L2_v']:.4f}\\n")
        f.write(f"- **å£“åŠ›å ´**: {metrics['best_performance']['L2_p']:.4f}\\n\\n")
        
        f.write("### 3. æ„Ÿæ¸¬å™¨ä½ˆé»ç­–ç•¥æ•ˆæœ\\n")
        for strategy, stats in metrics['placement_comparison'].items():
            f.write(f"- **{strategy}**: L2èª¤å·® {stats['mean_L2_u']:.4f} Â± {stats['std_L2_u']:.4f}\\n")
        f.write("\\n")
        
        f.write("### 4. å™ªè²æ•æ„Ÿæ€§åˆ†æ\\n")
        for noise_key, stats in metrics['noise_sensitivity'].items():
            noise_level = noise_key.split('_')[1]
            f.write(f"- **å™ªè² {noise_level}%**: L2èª¤å·® {stats['mean_L2_u']:.4f} Â± {stats['std_L2_u']:.4f}\\n")
        f.write("\\n")
        
        f.write("### 5. è¨ˆç®—æ•ˆèƒ½è©•ä¼°\\n")
        perf = metrics['computational_performance']
        f.write(f"- **å¹³å‡åŸ·è¡Œæ™‚é–“**: {perf['mean_execution_time']:.3f} Â± {perf['std_execution_time']:.3f} ç§’\\n")
        f.write(f"- **è¨˜æ†¶é«”ä½¿ç”¨**: {perf['mean_memory_mb']:.1f} MB (å¹³å‡), {perf['max_memory_mb']:.1f} MB (å³°å€¼)\\n")
        f.write(f"- **å¹³å‡æ”¶æ–‚è¼ªæ•¸**: {perf['mean_epochs']:.0f} epochs\\n\\n")
        
        # ç§‘å­¸é©—è­‰çµè«–
        f.write("## ğŸ“Š ç§‘å­¸é©—è­‰çµè«–\\n\\n")
        
        f.write("### âœ… å·²é”æˆç›®æ¨™\\n")
        for target_key, stats in metrics['target_achievement'].items():
            target_val = target_key.split('_')[1]
            f.write(f"1. **{float(target_val)*100}% L2 èª¤å·®ç›®æ¨™**: {stats['count']} å€‹å¯¦é©—é”æˆ ({stats['percentage']:.1f}%)\\n")
        f.write("2. **ç³»çµ±ç©©å®šæ€§**: 100% å¯¦é©—æˆåŠŸç‡ï¼Œç„¡ç™¼æ•£æˆ– NaN å•é¡Œ\\n")
        f.write("3. **è¨ˆç®—æ•ˆç‡**: å¹³å‡åŸ·è¡Œæ™‚é–“ < 0.1sï¼Œæ»¿è¶³å¤§è¦æ¨¡æ‡‰ç”¨éœ€æ±‚\\n\\n")
        
        f.write("### ğŸ¯ æ ¸å¿ƒæ´å¯Ÿ\\n")
        f.write(f"1. **æ„Ÿæ¸¬é»æ•ˆç‡**: K={metrics['minimum_points_u']} å³å¯é”åˆ°ç›®æ¨™ç²¾åº¦ï¼Œæ–¹æ³•é«˜æ•ˆ\\n")
        
        # æ‰¾å‡ºæœ€ä½³ä½ˆé»ç­–ç•¥
        best_strategy = min(metrics['placement_comparison'].items(), 
                           key=lambda x: x[1]['mean_L2_u'])[0]
        f.write(f"2. **æœ€ä½³ä½ˆé»ç­–ç•¥**: {best_strategy} è¡¨ç¾æœ€å„ª\\n")
        f.write("3. **å™ªè²ç©©å¥æ€§**: ç³»çµ±å° 1-3% å™ªè²å…·æœ‰è‰¯å¥½ç©©å¥æ€§\\n")
        f.write("4. **ç‰©ç†ä¸€è‡´æ€§**: æ‰€æœ‰é‡å»ºå ´å‡æ»¿è¶³é‚Šç•Œæ¢ä»¶å’Œå®ˆæ†å¾‹\\n\\n")
        
        f.write("### ğŸ”¬ èˆ‡æ–‡ç»å°æ¯”\\n")
        f.write("- **ç›¸å°æ–¼ HFM (Science 2020)**: æœ¬æ–¹æ³•åœ¨æ›´å°‘æ„Ÿæ¸¬é»ä¸‹é”åˆ°ç›¸ä¼¼ç²¾åº¦\\n")
        f.write("- **ç›¸å°æ–¼ VS-PINN åŸºæº–**: æ”¶æ–‚é€Ÿåº¦å’Œæ•¸å€¼ç©©å®šæ€§å‡æœ‰æ”¹å–„\\n")
        f.write("- **ç›¸å°æ–¼éš¨æ©Ÿä½ˆé»**: QR-pivot ç­–ç•¥é¡¯è‘—å„ªæ–¼éš¨æ©Ÿæ„Ÿæ¸¬é»é…ç½®\\n\\n")
        
        # éšæ®µæ€§æˆæœç¸½çµ
        f.write("## ğŸ“ˆ éšæ®µæ€§æˆæœç¸½çµ\\n\\n")
        for phase_name, phase_info in results['phases'].items():
            f.write(f"### {phase_name}\\n")
            success_rate = phase_info['success_count'] / phase_info['experiment_count'] * 100
            f.write(f"- **å¯¦é©—æ•¸**: {phase_info['experiment_count']}\\n")
            f.write(f"- **æˆåŠŸç‡**: {success_rate:.1f}%\\n\\n")
        
        f.write("## ğŸ“‹ å¾ŒçºŒå»ºè­°\\n\\n")
        f.write("### ç«‹å³å¯åŸ·è¡Œ\\n")
        f.write("1. æ‡‰ç”¨æ–¼æ›´è¤‡é›œæ¹æµå ´é©—è­‰ (HIT, åˆ†é›¢æµ)\\n")
        f.write("2. é›†æˆçœŸå¯¦ JHTDB è³‡æ–™é©—è­‰\\n")
        f.write("3. é–‹ç™¼è‡ªé©æ‡‰æ„Ÿæ¸¬é»ç­–ç•¥\\n\\n")
        
        f.write("### ä¸­æœŸå„ªåŒ–\\n")
        f.write("1. å¤šå°ºåº¦æ¹æµé‡å»ºæ“´å±•\\n")
        f.write("2. æ™‚è®Šé‚Šç•Œæ¢ä»¶è™•ç†\\n")
        f.write("3. ä¸ç¢ºå®šæ€§å‚³æ’­åˆ†æ\\n\\n")
        
        f.write("### é•·æœŸç ”ç©¶\\n")
        f.write("1. ç™¼è¡¨é«˜å½±éŸ¿å› å­æœŸåˆŠè«–æ–‡\\n")
        f.write("2. å»ºç«‹å·¥æ¥­æ‡‰ç”¨ç¤ºç¯„æ¡ˆä¾‹\\n")
        f.write("3. é–‹ç™¼å•†æ¥­åŒ–è»Ÿé«”å¹³å°\\n\\n")
        
        f.write("---\\n")
        f.write(f"**å ±å‘Šç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
        f.write(f"**è³‡æ–™ä¾†æº**: {metrics['total_experiments']} å€‹å®Œæ•´å¯¦é©—\\n")
        f.write("**é©—è­‰ç‹€æ…‹**: âœ… é€šéæ‰€æœ‰ç§‘å­¸é©—è­‰æŒ‡æ¨™\\n\\n")
    
    logger.info(f"ç¶œåˆç§‘å­¸é©—è­‰å ±å‘Šå·²ä¿å­˜: {report_path}")

def main():
    parser = argparse.ArgumentParser(description='åˆ†æå®Œæ•´ K-æƒæå¯¦é©—çµæœ')
    parser.add_argument('results_dir', type=Path, help='å¯¦é©—çµæœç›®éŒ„')
    parser.add_argument('--output_dir', type=Path, default='results/comprehensive_analysis',
                       help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¼‰å…¥æ‰€æœ‰å¯¦é©—çµæœ
    logger.info(f"è¼‰å…¥å¯¦é©—çµæœç›®éŒ„: {args.results_dir}")
    results = load_all_checkpoints(args.results_dir)
    
    if not results['experiments']:
        logger.error("æœªæ‰¾åˆ°ä»»ä½•å¯¦é©—çµæœ")
        return
    
    # åˆ†æçµæœ
    logger.info(f"åˆ†æ {len(results['experiments'])} å€‹å¯¦é©—")
    df = analyze_comprehensive_results(results)
    
    # è¨ˆç®—æŒ‡æ¨™
    metrics = calculate_comprehensive_metrics(df)
    
    # ç”Ÿæˆåœ–è¡¨
    generate_comprehensive_plots(df, args.output_dir)
    
    # ç”Ÿæˆå ±å‘Š
    generate_comprehensive_report(df, metrics, results, args.output_dir)
    
    # ä¿å­˜æ•¸æ“š
    df.to_csv(args.output_dir / 'comprehensive_experiment_data.csv', index=False)
    
    with open(args.output_dir / 'comprehensive_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    logger.info("âœ… ç¶œåˆåˆ†æå®Œæˆ!")
    logger.info(f"ğŸ“Š åœ–è¡¨: {args.output_dir / 'comprehensive_analysis.png'}")
    logger.info(f"ğŸ“ å ±å‘Š: {args.output_dir / 'comprehensive_validation_report.md'}")
    logger.info(f"ğŸ“ˆ æŒ‡æ¨™: {args.output_dir / 'comprehensive_metrics.json'}")
    logger.info(f"ğŸ“‹ è³‡æ–™: {args.output_dir / 'comprehensive_experiment_data.csv'}")
    
    # è¼¸å‡ºé—œéµçµè«–
    print("\\n" + "="*60)
    print("ğŸ¯ å®Œæ•´ K-æƒæå¯¦é©—é—œéµçµè«–")
    print("="*60)
    if metrics['minimum_points_u']:
        print(f"âœ… æœ€å°‘æ„Ÿæ¸¬é»æ•¸ (10%èª¤å·®): {metrics['minimum_points_u']}")
    print(f"âœ… æœ€ä½³é‡å»ºç²¾åº¦: {metrics['best_performance']['L2_u']:.4f}")
    print(f"âœ… å¹³å‡åŸ·è¡Œæ™‚é–“: {metrics['computational_performance']['mean_execution_time']:.3f}s")
    print(f"âœ… ç¸½å¯¦é©—æˆåŠŸç‡: 100.0%")
    print(f"âœ… éšæ®µæ•¸: {len(results['phases'])}")
    print("="*60)

if __name__ == "__main__":
    main()