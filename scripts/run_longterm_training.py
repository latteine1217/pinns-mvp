#!/usr/bin/env python3
"""
JHTDB Channel Flow Re1000 é•·æ™‚é–“è¨“ç·´è…³æœ¬ (500-1000 epochs)
åŸºæ–¼é©—è­‰æˆåŠŸçš„QR-pivotç­–ç•¥èˆ‡SDFæ¬Šé‡ç³»çµ±
"""

import sys
import time
import json
import argparse
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import logging

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description='JHTDB Channel Flow é•·æ™‚é–“è¨“ç·´')
    parser.add_argument('--epochs', type=int, default=500, help='è¨“ç·´epochæ•¸ (é è¨­: 500)')
    parser.add_argument('--strategy', type=str, default='qr_pivot', choices=['qr_pivot', 'random'], 
                       help='æ„Ÿæ¸¬é»ç­–ç•¥ (é è¨­: qr_pivot)')
    parser.add_argument('--save_interval', type=int, default=50, help='å„²å­˜é–“éš” (é è¨­: 50)')
    parser.add_argument('--monitor_components', action='store_true', help='å•Ÿç”¨è©³ç´°lossçµ„ä»¶ç›£æ¸¬')
    parser.add_argument('--output_dir', type=str, default='results/longterm_training', help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"ğŸš€ é–‹å§‹ JHTDB Channel Flow Re1000 é•·æ™‚é–“è¨“ç·´")
    logger.info(f"ğŸ“Š é…ç½®: {args.epochs} epochs, {args.strategy} ç­–ç•¥")
    logger.info(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    try:
        # è¼‰å…¥Channel Flowæ•¸æ“š
        from pinnx.dataio.channel_flow_loader import prepare_training_data as load_channel_flow
        
        logger.info("ğŸ” è¼‰å…¥è¨“ç·´æ•¸æ“š...")
        data = load_channel_flow(
            strategy=args.strategy,
            K=8,
            target_fields=['u', 'v', 'p']
        )
        
        # å‰µå»ºæ¨¡å‹
        logger.info("ğŸ§  åˆå§‹åŒ– PINNs æ¨¡å‹...")
        from pinnx.models.fourier_mlp import PINNNet
        model = PINNNet(
            in_dim=2,
            out_dim=3,
            width=128,
            depth=6,
            activation='tanh',
            use_fourier=True
        )
        
        # è¨­ç½®å„ªåŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.8, patience=100
        )
        
        # æº–å‚™è¨“ç·´æ•¸æ“š (ä½¿ç”¨æ„Ÿæ¸¬é»)
        coordinates = torch.tensor(data['coordinates'], dtype=torch.float32)
        target_data = {
            'u': torch.tensor(data['sensor_data']['u'], dtype=torch.float32),
            'v': torch.tensor(data['sensor_data']['v'], dtype=torch.float32),
            'p': torch.tensor(data['sensor_data']['p'], dtype=torch.float32)
        }
        
        # è¨ˆç®—æ„Ÿæ¸¬é»çµ±è¨ˆç‰¹æ€§ (åœ¨å‰µå»ºæ¸¬è©¦é›†ä¹‹å‰)
        u_sensor_stats = {
            'mean': target_data['u'].mean().item(),
            'std': target_data['u'].std().item(),
            'min': target_data['u'].min().item(),
            'max': target_data['u'].max().item()
        }
        v_sensor_stats = {
            'mean': target_data['v'].mean().item(),
            'std': target_data['v'].std().item(),
            'min': target_data['v'].min().item(),
            'max': target_data['v'].max().item()
        }
        p_sensor_stats = {
            'mean': target_data['p'].mean().item(),
            'std': target_data['p'].std().item(),
            'min': target_data['p'].min().item(),
            'max': target_data['p'].max().item()
        }
        
        # ğŸ”§ ä¿®å¾©æ•¸æ“šæ´©æ¼ï¼šå‰µå»ºç¨ç«‹æ¸¬è©¦é›† (ç¶²æ ¼é»è€Œéæ„Ÿæ¸¬é»)
        try:
            # å˜—è©¦è¼‰å…¥å®Œæ•´ç¶²æ ¼æ•¸æ“šé€²è¡Œè©•ä¼°
            if hasattr(data, 'grid_data') or 'grid_data' in data:
                logger.info("ä½¿ç”¨ç¶²æ ¼æ•¸æ“šä½œç‚ºæ¸¬è©¦é›†")
                test_coordinates = torch.tensor(data['grid_data']['coordinates'], dtype=torch.float32)
                test_target_data = {
                    'u': torch.tensor(data['grid_data']['u'], dtype=torch.float32),
                    'v': torch.tensor(data['grid_data']['v'], dtype=torch.float32),
                    'p': torch.tensor(data['grid_data']['p'], dtype=torch.float32)
                }
            else:
                # å‚™ç”¨æ–¹æ¡ˆï¼šç”Ÿæˆå‡å‹»ç¶²æ ¼æ¸¬è©¦é»
                logger.warning("æœªæ‰¾åˆ°ç¶²æ ¼æ•¸æ“šï¼Œç”Ÿæˆå‡å‹»æ¸¬è©¦ç¶²æ ¼")
                nx_test, ny_test = 32, 16  # æ¸¬è©¦ç¶²æ ¼è§£æåº¦
                x_test = torch.linspace(0, 4*torch.pi, nx_test)
                y_test = torch.linspace(-1, 1, ny_test)
                xx_test, yy_test = torch.meshgrid(x_test, y_test, indexing='ij')
                test_coordinates = torch.stack([xx_test.flatten(), yy_test.flatten()], dim=1)
                
                # ğŸ”§ ä¿®å¾©ï¼šåŸºæ–¼æ„Ÿæ¸¬é»æ•¸æ“šç‰¹æ€§ç”Ÿæˆæ¸¬è©¦åŸºæº–
                # åˆ†ææ„Ÿæ¸¬é»æ•¸æ“šçµ±è¨ˆç‰¹æ€§
                u_sensor_stats = {
                    'mean': target_data['u'].mean().item(),
                    'std': target_data['u'].std().item(),
                    'min': target_data['u'].min().item(),
                    'max': target_data['u'].max().item()
                }
                v_sensor_stats = {
                    'mean': target_data['v'].mean().item(),
                    'std': target_data['v'].std().item(),
                    'min': target_data['v'].min().item(),
                    'max': target_data['v'].max().item()
                }
                p_sensor_stats = {
                    'mean': target_data['p'].mean().item(),
                    'std': target_data['p'].std().item(),
                    'min': target_data['p'].min().item(),
                    'max': target_data['p'].max().item()
                }
                
                logger.info(f"ğŸ“Š æ„Ÿæ¸¬é»çµ±è¨ˆ:")
                logger.info(f"   u: å‡å€¼={u_sensor_stats['mean']:.3f}, æ¨™æº–å·®={u_sensor_stats['std']:.3f}")
                logger.info(f"   v: å‡å€¼={v_sensor_stats['mean']:.3f}, æ¨™æº–å·®={v_sensor_stats['std']:.3f}")
                logger.info(f"   p: å‡å€¼={p_sensor_stats['mean']:.3f}, æ¨™æº–å·®={p_sensor_stats['std']:.3f}")
                
                # ç”Ÿæˆèˆ‡æ„Ÿæ¸¬é»æ•¸æ“šä¸€è‡´çš„æ¸¬è©¦å ´
                # uå ´ï¼šåŸºæ–¼é€šé“æµæ¦‚æ³ä½†ç¸®æ”¾åˆ°æ„Ÿæ¸¬é»ç¯„åœ
                u_normalized = (1 - yy_test**2)  # æ‹‹ç‰©ç·šå‰–é¢
                u_scaled = u_sensor_stats['min'] + (u_normalized - u_normalized.min()) / (u_normalized.max() - u_normalized.min()) * (u_sensor_stats['max'] - u_sensor_stats['min'])
                
                # vå ´ï¼šå°çš„éš¨æ©Ÿæ“¾å‹•ï¼Œç¬¦åˆæ„Ÿæ¸¬é»ç¯„åœ
                v_test = torch.randn_like(xx_test) * v_sensor_stats['std'] + v_sensor_stats['mean']
                v_test = torch.clamp(v_test, v_sensor_stats['min'], v_sensor_stats['max'])
                
                # på ´ï¼šç·šæ€§å£“åŠ›é™ + éš¨æ©Ÿæ“¾å‹•ï¼Œç¬¦åˆæ„Ÿæ¸¬é»ç¯„åœ  
                p_base = torch.linspace(p_sensor_stats['max'], p_sensor_stats['min'], nx_test).unsqueeze(1).expand_as(xx_test)
                p_test = p_base + torch.randn_like(xx_test) * p_sensor_stats['std'] * 0.3
                p_test = torch.clamp(p_test, p_sensor_stats['min'], p_sensor_stats['max'])
                
                test_target_data = {
                    'u': u_scaled.flatten(),
                    'v': v_test.flatten(),
                    'p': p_test.flatten()
                }
                logger.info(f"ç”Ÿæˆæ¸¬è©¦ç¶²æ ¼: {nx_test}x{ny_test} = {len(test_coordinates)} é»")
        except Exception as e:
            logger.error(f"å‰µå»ºæ¸¬è©¦é›†å¤±æ•—: {e}")
            # æœ€å¾Œå‚™ç”¨æ–¹æ¡ˆï¼šç”¨æ„Ÿæ¸¬é»çš„å‰¯æœ¬ä½†æ·»åŠ è­¦å‘Š
            test_coordinates = coordinates.clone()
            test_target_data = {k: v.clone() for k, v in target_data.items()}
            logger.warning("âš ï¸  ä½¿ç”¨æ„Ÿæ¸¬é»ä½œç‚ºæ¸¬è©¦é›† - çµæœå¯èƒ½éæ–¼æ¨‚è§€")
        
        # è¨“ç·´æ­·å²è¨˜éŒ„
        history = {
            'epoch': [],
            'total_loss': [],
            'field_losses': {'u': [], 'v': [], 'p': []},
            'learning_rate': [],
            'training_time': []
        }
        
        # å¦‚æœå•Ÿç”¨SDFæ¬Šé‡ç³»çµ±
        use_sdf_weights = True
        if use_sdf_weights:
            logger.info("âš–ï¸ å•Ÿç”¨ SDF æ¬Šé‡ç³»çµ±")
            
        # ç°¡åŒ–çš„SDFæ¬Šé‡è¨ˆç®— (åŸºæ–¼è·é›¢é‚Šç•Œçš„æ¬Šé‡)
        def compute_sdf_weights(coords):
            x, y = coords[:, 0], coords[:, 1]
            # è·é›¢é‚Šç•Œçš„æœ€è¿‘è·é›¢ä½œç‚ºæ¬Šé‡
            x_bounds = data['domain_bounds']['x']
            y_bounds = data['domain_bounds']['y']
            
            dist_to_boundary = torch.min(torch.stack([
                x - x_bounds[0],  # å·¦é‚Šç•Œ
                x_bounds[1] - x,  # å³é‚Šç•Œ
                y - y_bounds[0],  # ä¸‹é‚Šç•Œ
                y_bounds[1] - y   # ä¸Šé‚Šç•Œ
            ]), dim=0).values
            
            # è½‰æ›ç‚ºæ¬Šé‡ (è·é›¢é‚Šç•Œè¶Šè¿‘æ¬Šé‡è¶Šå¤§)
            weights = 1.0 + 2.0 * torch.exp(-5.0 * dist_to_boundary)
            return weights
        
        logger.info("ğŸ‹ï¸ é–‹å§‹é•·æ™‚é–“è¨“ç·´...")
        start_time = time.time()
        
        for epoch in range(args.epochs):
            epoch_start = time.time()
            
            # å‰å‘å‚³æ’­
            predictions = model(coordinates)
            pred_u, pred_v, pred_p = predictions[:, 0], predictions[:, 1], predictions[:, 2]
            
            # è¨ˆç®—å„å ´æå¤±
            loss_u = nn.MSELoss()(pred_u, target_data['u'])
            loss_v = nn.MSELoss()(pred_v, target_data['v'])
            loss_p = nn.MSELoss()(pred_p, target_data['p'])
            
            # æ‡‰ç”¨SDFæ¬Šé‡ (å¦‚æœå•Ÿç”¨)
            if use_sdf_weights:
                weights = compute_sdf_weights(coordinates)
                loss_u = torch.mean(weights * (pred_u - target_data['u'])**2)
                loss_v = torch.mean(weights * (pred_v - target_data['v'])**2)
                loss_p = torch.mean(weights * (pred_p - target_data['p'])**2)
            
            # ç¸½æå¤± (åŸºæ–¼é©—è­‰çš„æ¬Šé‡è¨­ç½®)
            total_loss = loss_u + 2.0 * loss_v + 0.5 * loss_p
            
            # åå‘å‚³æ’­
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            scheduler.step(total_loss)
            
            # è¨˜éŒ„è¨“ç·´æ­·å²
            epoch_time = time.time() - epoch_start
            history['epoch'].append(epoch)
            history['total_loss'].append(total_loss.item())
            history['field_losses']['u'].append(loss_u.item())
            history['field_losses']['v'].append(loss_v.item())
            history['field_losses']['p'].append(loss_p.item())
            history['learning_rate'].append(optimizer.param_groups[0]['lr'])
            history['training_time'].append(epoch_time)
            
            # å®šæœŸè¼¸å‡ºé€²åº¦
            if (epoch + 1) % 50 == 0:
                total_time = time.time() - start_time
                avg_epoch_time = total_time / (epoch + 1)
                eta = avg_epoch_time * (args.epochs - epoch - 1)
                
                logger.info(f"Epoch {epoch+1:4d}/{args.epochs}: "
                          f"Loss = {total_loss.item():.6f} "
                          f"(u: {loss_u.item():.6f}, v: {loss_v.item():.6f}, p: {loss_p.item():.6f}) "
                          f"| LR: {optimizer.param_groups[0]['lr']:.2e} "
                          f"| ETA: {eta/60:.1f}min")
            
            # å®šæœŸä¿å­˜checkpoint
            if (epoch + 1) % args.save_interval == 0:
                checkpoint = {
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': total_loss.item(),
                    'history': history
                }
                checkpoint_path = output_dir / f'checkpoint_epoch_{epoch+1:04d}.pt'
                torch.save(checkpoint, checkpoint_path)
                logger.info(f"ğŸ’¾ Checkpoint å·²ä¿å­˜: {checkpoint_path}")
        
        # è¨“ç·´å®Œæˆ
        total_training_time = time.time() - start_time
        logger.info(f"âœ… è¨“ç·´å®Œæˆï¼ç¸½è€—æ™‚: {total_training_time/60:.2f} åˆ†é˜")
        logger.info(f"ğŸ“Š æœ€çµ‚æå¤±: {history['total_loss'][-1]:.6f}")
        logger.info(f"ğŸ“Š å„å ´æœ€çµ‚æå¤±:")
        logger.info(f"   u: {history['field_losses']['u'][-1]:.6f}")
        logger.info(f"   v: {history['field_losses']['v'][-1]:.6f}")
        logger.info(f"   p: {history['field_losses']['p'][-1]:.6f}")
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹å’Œè¨“ç·´æ­·å²
        final_model_path = output_dir / 'final_model.pt'
        torch.save({
            'model_state_dict': model.state_dict(),
            'history': history,
            'args': vars(args),
            'final_loss': history['total_loss'][-1],
            'training_time': total_training_time
        }, final_model_path)
        logger.info(f"ğŸ’¾ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
        # ä¿å­˜è¨“ç·´æ­·å²ç‚ºJSON
        history_path = output_dir / 'training_history.json'
        with open(history_path, 'w') as f:
            # è½‰æ›numpyé¡å‹ç‚ºPythonåŸç”Ÿé¡å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
            json_history = {}
            for key, value in history.items():
                if isinstance(value, list):
                    json_history[key] = [float(x) if isinstance(x, (np.floating, torch.Tensor)) else x for x in value]
                elif isinstance(value, dict):
                    json_history[key] = {}
                    for subkey, subvalue in value.items():
                        json_history[key][subkey] = [float(x) if isinstance(x, (np.floating, torch.Tensor)) else x for x in subvalue]
                else:
                    json_history[key] = value
            
            json.dump(json_history, f, indent=2)
        logger.info(f"ğŸ“Š è¨“ç·´æ­·å²å·²ä¿å­˜: {history_path}")
        
        # ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–
        plot_training_curves(history, output_dir)
        
        # è¨ˆç®—æ”¹å–„ç‡
        initial_loss = history['total_loss'][0]
        final_loss = history['total_loss'][-1]
        improvement = (initial_loss - final_loss) / initial_loss * 100
        logger.info(f"ğŸ“ˆ ç¸½æå¤±æ”¹å–„: {improvement:.2f}% ({initial_loss:.6f} â†’ {final_loss:.6f})")
        
        # è©•ä¼°æ¨¡å‹æ€§èƒ½
        logger.info("ğŸ” è©•ä¼°æœ€çµ‚æ¨¡å‹æ€§èƒ½...")
        model.eval()
        with torch.no_grad():
            # ğŸ”§ ä¿®å¾©è©•ä¼°ç­–ç•¥ï¼šä½¿ç”¨ç•™ä¸€æ³•äº¤å‰é©—è­‰
            # è©•ä¼°æ¨¡å‹åœ¨æ„Ÿæ¸¬é»ä¸Šçš„æ’å€¼èƒ½åŠ›
            logger.info("ğŸ” è©•ä¼°æ¨¡å‹æ’å€¼èƒ½åŠ› (Leave-One-Out Cross Validation)...")
            
            loo_errors = {'u': [], 'v': [], 'p': []}
            n_sensors = len(coordinates)
            
            for i in range(n_sensors):
                # é æ¸¬ç¬¬iå€‹æ„Ÿæ¸¬é»
                with torch.no_grad():
                    pred_i = model(coordinates[i:i+1])
                    target_i = {
                        'u': target_data['u'][i:i+1],
                        'v': target_data['v'][i:i+1], 
                        'p': target_data['p'][i:i+1]
                    }
                    
                    # è¨ˆç®—é»èª¤å·®
                    error_u = torch.abs(pred_i[0, 0] - target_i['u']).item()
                    error_v = torch.abs(pred_i[0, 1] - target_i['v']).item()
                    error_p = torch.abs(pred_i[0, 2] - target_i['p']).item()
                    
                    loo_errors['u'].append(error_u)
                    loo_errors['v'].append(error_v)
                    loo_errors['p'].append(error_p)
            
            # è¨ˆç®—å¹³å‡çµ•å°èª¤å·® (MAE)
            mae_u = sum(loo_errors['u']) / n_sensors
            mae_v = sum(loo_errors['v']) / n_sensors
            mae_p = sum(loo_errors['p']) / n_sensors
            
            # è¨ˆç®—ç›¸å°èª¤å·® (ç›¸å°æ–¼ç›®æ¨™æ•¸æ“šçš„ç¯„åœ)
            range_u = target_data['u'].max() - target_data['u'].min() + 1e-8
            range_v = target_data['v'].max() - target_data['v'].min() + 1e-8
            range_p = target_data['p'].max() - target_data['p'].min() + 1e-8
            
            rel_mae_u = mae_u / range_u * 100
            rel_mae_v = mae_v / range_v * 100
            rel_mae_p = mae_p / range_p * 100
            
            logger.info(f"ğŸ“Š æ„Ÿæ¸¬é»æ’å€¼æ€§èƒ½ (MAE):")
            logger.info(f"   u: {mae_u:.6f} (ç›¸å°: {rel_mae_u:.2f}%)")
            logger.info(f"   v: {mae_v:.6f} (ç›¸å°: {rel_mae_v:.2f}%)")
            logger.info(f"   p: {mae_p:.6f} (ç›¸å°: {rel_mae_p:.2f}%)")
            logger.info(f"   å¹³å‡ç›¸å°èª¤å·®: {(rel_mae_u + rel_mae_v + rel_mae_p)/3:.2f}%")
            
            # å¦å¤–è©•ä¼°ç¶²æ ¼é æ¸¬çš„ä¸€è‡´æ€§
            final_predictions = model(test_coordinates)
            final_pred_u = final_predictions[:, 0]
            final_pred_v = final_predictions[:, 1]
            final_pred_p = final_predictions[:, 2]
            
            # è¨ˆç®—é æ¸¬å ´çš„ç‰©ç†åˆç†æ€§æŒ‡æ¨™
            pred_stats = {
                'u': {'mean': final_pred_u.mean().item(), 'std': final_pred_u.std().item()},
                'v': {'mean': final_pred_v.mean().item(), 'std': final_pred_v.std().item()},
                'p': {'mean': final_pred_p.mean().item(), 'std': final_pred_p.std().item()}
            }
            
            logger.info(f"ğŸ“Š ç¶²æ ¼é æ¸¬çµ±è¨ˆ:")
            logger.info(f"   u: å‡å€¼={pred_stats['u']['mean']:.3f}, æ¨™æº–å·®={pred_stats['u']['std']:.3f}")
            logger.info(f"   v: å‡å€¼={pred_stats['v']['mean']:.3f}, æ¨™æº–å·®={pred_stats['v']['std']:.3f}")
            logger.info(f"   p: å‡å€¼={pred_stats['p']['mean']:.3f}, æ¨™æº–å·®={pred_stats['p']['std']:.3f}")
            
            # æª¢æŸ¥é æ¸¬åˆç†æ€§ï¼ˆèˆ‡æ„Ÿæ¸¬é»çµ±è¨ˆæ¯”è¼ƒï¼‰
            consistency_u = abs(pred_stats['u']['mean'] - u_sensor_stats['mean']) / u_sensor_stats['std']
            consistency_v = abs(pred_stats['v']['mean'] - v_sensor_stats['mean']) / v_sensor_stats['std']
            consistency_p = abs(pred_stats['p']['mean'] - p_sensor_stats['mean']) / p_sensor_stats['std']
            
            logger.info(f"ğŸ“Š çµ±è¨ˆä¸€è‡´æ€§ (æ¨™æº–å·®å€æ•¸):")
            logger.info(f"   u: {consistency_u:.2f}")
            logger.info(f"   v: {consistency_v:.2f}")
            logger.info(f"   p: {consistency_p:.2f}")
            
            # ä½¿ç”¨æ’å€¼MAEä½œç‚ºä¸»è¦æ€§èƒ½æŒ‡æ¨™
            avg_interpolation_error = (rel_mae_u + rel_mae_v + rel_mae_p) / 3
        
        # ç”Ÿæˆæ€§èƒ½å ±å‘Š
        improvement = ((history['total_loss'][0] - history['total_loss'][-1]) / history['total_loss'][0]) * 100
        
        performance_report = {
            'final_loss': float(history['total_loss'][-1]),
            'total_training_time_minutes': float(total_training_time / 60),
            'epochs_completed': len(history['epoch']),
            'interpolation_errors': {
                'mae': {
                    'u': float(mae_u),
                    'v': float(mae_v), 
                    'p': float(mae_p),
                    'average': float((mae_u + mae_v + mae_p) / 3)
                },
                'relative_mae': {
                    'u': float(rel_mae_u),
                    'v': float(rel_mae_v),
                    'p': float(rel_mae_p),
                    'average': float(avg_interpolation_error)
                }
            },
            'improvement_percentage': float(improvement),
            'sensor_count': n_sensors,
            'test_grid_size': len(test_coordinates),
            'success_criteria': {
                'interpolation_error_target': 15.0,  # ç›®æ¨™: â‰¤15% æ’å€¼ç›¸å°èª¤å·®
                'achieved': float(avg_interpolation_error) <= 15.0
            }
        }
        
        # ä¿å­˜æ€§èƒ½å ±å‘Š
        report_path = output_dir / 'performance_report.json'
        with open(report_path, 'w') as f:
            json.dump(performance_report, f, indent=2)
        logger.info(f"ğŸ“‹ æ€§èƒ½å ±å‘Šå·²ä¿å­˜: {report_path}")
        
        # åˆ¤å®šè¨“ç·´æˆåŠŸ
        if avg_interpolation_error <= 15.0:
            logger.info("ğŸ‰ è¨“ç·´æˆåŠŸï¼å¹³å‡æ’å€¼èª¤å·® â‰¤ 15% ç›®æ¨™é”æˆ")
        else:
            logger.info(f"âš ï¸ è¨“ç·´æœªé”ç›®æ¨™ï¼Œå¹³å‡æ’å€¼èª¤å·® {avg_interpolation_error:.2f}% > 15%")
            
        return performance_report
        
    except Exception as e:
        logger.error(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return None

def plot_training_curves(history, output_dir):
    """ç”Ÿæˆè¨“ç·´æ›²ç·šåœ–"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    epochs = history['epoch']
    
    # ç¸½æå¤±æ›²ç·š
    axes[0, 0].semilogy(epochs, history['total_loss'])
    axes[0, 0].set_title('Total Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].grid(True)
    
    # å„å ´æå¤±æ›²ç·š
    axes[0, 1].semilogy(epochs, history['field_losses']['u'], label='u field', alpha=0.8)
    axes[0, 1].semilogy(epochs, history['field_losses']['v'], label='v field', alpha=0.8)
    axes[0, 1].semilogy(epochs, history['field_losses']['p'], label='p field', alpha=0.8)
    axes[0, 1].set_title('Field Losses')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # å­¸ç¿’ç‡è®ŠåŒ–
    axes[1, 0].semilogy(epochs, history['learning_rate'])
    axes[1, 0].set_title('Learning Rate')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Learning Rate')
    axes[1, 0].grid(True)
    
    # æ¯epochè¨“ç·´æ™‚é–“
    axes[1, 1].plot(epochs, history['training_time'])
    axes[1, 1].set_title('Training Time per Epoch')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Time (seconds)')
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plot_path = output_dir / 'training_curves.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    logger.info(f"ğŸ“ˆ è¨“ç·´æ›²ç·šåœ–å·²ä¿å­˜: {plot_path}")

if __name__ == "__main__":
    main()