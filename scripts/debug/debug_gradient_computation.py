#!/usr/bin/env python3
"""
æ¢¯åº¦è¨ˆç®—è¨ºæ–·è…³æœ¬
è¨ºæ–·ç‚ºä»€éº¼ compute_derivatives_3d_temporal è¿”å› None
"""

import torch
import torch.autograd as autograd
import numpy as np
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def debug_simple_gradient():
    """æ¸¬è©¦æœ€åŸºæœ¬çš„æ¢¯åº¦è¨ˆç®—"""
    logger.info("ğŸ” æ¸¬è©¦æœ€åŸºæœ¬çš„æ¢¯åº¦è¨ˆç®—...")
    
    # å‰µå»ºç°¡å–®æ¸¬è©¦æ•¸æ“š
    x = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    f = x[:, 0:1] * x[:, 1:2]  # f = x * y
    
    logger.info(f"  è¼¸å…¥ x: shape={x.shape}, requires_grad={x.requires_grad}")
    logger.info(f"  å‡½æ•¸ f: shape={f.shape}, requires_grad={f.requires_grad}")
    
    # è¨ˆç®—æ¢¯åº¦
    grad_outputs = torch.ones_like(f)
    grads = autograd.grad(
        outputs=f,
        inputs=x,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    logger.info(f"  æ¢¯åº¦çµæœ: {grads[0]}")
    logger.info(f"  æ¢¯åº¦æ˜¯å¦ç‚ºNone: {grads[0] is None}")
    
    return grads[0] is not None

def debug_4d_gradient():
    """æ¸¬è©¦4Dåº§æ¨™çš„æ¢¯åº¦è¨ˆç®—"""
    logger.info("ğŸ” æ¸¬è©¦4Dåº§æ¨™æ¢¯åº¦è¨ˆç®—...")
    
    # å‰µå»º4Dæ¸¬è©¦æ•¸æ“š [t, x, y, z]
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.0],
        [0.1, 1.5, 0.6, 0.1],
        [0.2, 2.0, 0.7, 0.2],
        [0.3, 2.5, 0.8, 0.3]
    ], requires_grad=True)
    
    # å‰µå»ºç°¡å–®å‡½æ•¸ f = x + y (åªç”¨ç©ºé–“åº§æ¨™)
    f = coords[:, 1:2] + coords[:, 2:3]  # f = x + y
    
    logger.info(f"  4Dåº§æ¨™: shape={coords.shape}, requires_grad={coords.requires_grad}")
    logger.info(f"  å‡½æ•¸ f: shape={f.shape}, requires_grad={f.requires_grad}")
    
    # è¨ˆç®—æ¢¯åº¦
    grad_outputs = torch.ones_like(f)
    grads = autograd.grad(
        outputs=f,
        inputs=coords,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    logger.info(f"  æ¢¯åº¦çµæœ: {grads[0]}")
    logger.info(f"  æ¢¯åº¦æ˜¯å¦ç‚ºNone: {grads[0] is None}")
    
    if grads[0] is not None:
        logger.info(f"  âˆ‚f/âˆ‚t: {grads[0][:, 0]}")  # æ‡‰è©²æ˜¯ 0
        logger.info(f"  âˆ‚f/âˆ‚x: {grads[0][:, 1]}")  # æ‡‰è©²æ˜¯ 1
        logger.info(f"  âˆ‚f/âˆ‚y: {grads[0][:, 2]}")  # æ‡‰è©²æ˜¯ 1
        logger.info(f"  âˆ‚f/âˆ‚z: {grads[0][:, 3]}")  # æ‡‰è©²æ˜¯ 0
    
    return grads[0] is not None

def debug_neural_network_gradient():
    """æ¸¬è©¦ç¥ç¶“ç¶²è·¯è¼¸å‡ºçš„æ¢¯åº¦è¨ˆç®—"""
    logger.info("ğŸ” æ¸¬è©¦ç¥ç¶“ç¶²è·¯è¼¸å‡ºæ¢¯åº¦è¨ˆç®—...")
    
    # å‰µå»ºç°¡å–®ç¥ç¶“ç¶²è·¯
    net = torch.nn.Sequential(
        torch.nn.Linear(4, 8),
        torch.nn.Tanh(),
        torch.nn.Linear(8, 3)  # è¼¸å‡º [u, v, p]
    )
    
    # 4Dè¼¸å…¥
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.0],
        [0.1, 1.5, 0.6, 0.1],
        [0.2, 2.0, 0.7, 0.2],
        [0.3, 2.5, 0.8, 0.3]
    ], requires_grad=True)
    
    # ç¥ç¶“ç¶²è·¯é æ¸¬
    predictions = net(coords)
    u = predictions[:, 0:1]
    
    logger.info(f"  4Dåº§æ¨™: shape={coords.shape}, requires_grad={coords.requires_grad}")
    logger.info(f"  ç¥ç¶“ç¶²è·¯è¼¸å‡º: shape={predictions.shape}, requires_grad={predictions.requires_grad}")
    logger.info(f"  uåˆ†é‡: shape={u.shape}, requires_grad={u.requires_grad}")
    
    # è¨ˆç®— u å°åº§æ¨™çš„æ¢¯åº¦
    grad_outputs = torch.ones_like(u)
    grads = autograd.grad(
        outputs=u,
        inputs=coords,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    logger.info(f"  æ¢¯åº¦çµæœ: {grads[0]}")
    logger.info(f"  æ¢¯åº¦æ˜¯å¦ç‚ºNone: {grads[0] is None}")
    
    if grads[0] is not None:
        logger.info(f"  âˆ‚u/âˆ‚t: {grads[0][:, 0]}")
        logger.info(f"  âˆ‚u/âˆ‚x: {grads[0][:, 1]}")
        logger.info(f"  âˆ‚u/âˆ‚y: {grads[0][:, 2]}")
        logger.info(f"  âˆ‚u/âˆ‚z: {grads[0][:, 3]}")
    
    return grads[0] is not None

def test_gradient_computation_issue():
    """æ¨¡æ“¬ç‰©ç†æ®˜å·®è¨ˆç®—ä¸­çš„æ¢¯åº¦å•é¡Œ"""
    logger.info("ğŸ” æ¨¡æ“¬ç‰©ç†æ®˜å·®è¨ˆç®—ä¸­çš„æ¢¯åº¦å•é¡Œ...")
    
    # ä½¿ç”¨èˆ‡å¯¦éš›ç¨‹å¼ç›¸åŒçš„è¨­å®š
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.0],
        [0.1, 1.5, 0.6, 0.1],
        [0.2, 2.0, 0.7, 0.2],
        [0.3, 2.5, 0.8, 0.3]
    ], requires_grad=True)
    
    # å‰µå»ºæ‰‹å‹•è¨­å®šçš„é€Ÿåº¦å ´
    u = coords[:, 1:2] * 0.5  # u = 0.5 * x
    v = coords[:, 2:3] * 0.1  # v = 0.1 * y
    
    logger.info(f"  åº§æ¨™: shape={coords.shape}, requires_grad={coords.requires_grad}")
    logger.info(f"  u: shape={u.shape}, requires_grad={u.requires_grad}")
    logger.info(f"  v: shape={v.shape}, requires_grad={v.requires_grad}")
    
    # æ¸¬è©¦ u å° x çš„å°æ•¸ (æ‡‰è©²æ˜¯ 0.5)
    grad_outputs = torch.ones_like(u)
    u_grads = autograd.grad(
        outputs=u,
        inputs=coords,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    logger.info(f"  uçš„æ¢¯åº¦: {u_grads[0]}")
    logger.info(f"  uæ¢¯åº¦æ˜¯å¦ç‚ºNone: {u_grads[0] is None}")
    
    if u_grads[0] is not None:
        logger.info(f"  âˆ‚u/âˆ‚x å¯¦éš›å€¼: {u_grads[0][:, 1]} (é æœŸ: 0.5)")
        logger.info(f"  âˆ‚u/âˆ‚y å¯¦éš›å€¼: {u_grads[0][:, 2]} (é æœŸ: 0.0)")
    
    return u_grads[0] is not None

def main():
    logger.info("="*60)
    logger.info("=== æ¢¯åº¦è¨ˆç®—è¨ºæ–·é–‹å§‹ ===")
    logger.info("="*60)
    
    results = {}
    
    # æ¸¬è©¦1: åŸºæœ¬æ¢¯åº¦è¨ˆç®—
    results['basic'] = debug_simple_gradient()
    logger.info(f"âœ… åŸºæœ¬æ¢¯åº¦è¨ˆç®—: {'é€šé' if results['basic'] else 'å¤±æ•—'}")
    
    logger.info("")
    
    # æ¸¬è©¦2: 4Dæ¢¯åº¦è¨ˆç®—
    results['4d'] = debug_4d_gradient()
    logger.info(f"âœ… 4Dæ¢¯åº¦è¨ˆç®—: {'é€šé' if results['4d'] else 'å¤±æ•—'}")
    
    logger.info("")
    
    # æ¸¬è©¦3: ç¥ç¶“ç¶²è·¯æ¢¯åº¦è¨ˆç®—
    results['neural'] = debug_neural_network_gradient()
    logger.info(f"âœ… ç¥ç¶“ç¶²è·¯æ¢¯åº¦è¨ˆç®—: {'é€šé' if results['neural'] else 'å¤±æ•—'}")
    
    logger.info("")
    
    # æ¸¬è©¦4: ç‰©ç†è¨ˆç®—æ¨¡æ“¬
    results['physics'] = test_gradient_computation_issue()
    logger.info(f"âœ… ç‰©ç†è¨ˆç®—æ¨¡æ“¬: {'é€šé' if results['physics'] else 'å¤±æ•—'}")
    
    logger.info("")
    logger.info("="*60)
    logger.info("=== æ¢¯åº¦è¨ˆç®—è¨ºæ–·çµæœ ===")
    logger.info("="*60)
    
    all_passed = all(results.values())
    logger.info(f"ğŸ“Š ç¸½é«”ç‹€æ…‹: {'âœ… æ‰€æœ‰æ¸¬è©¦é€šé' if all_passed else 'âŒ å­˜åœ¨å•é¡Œ'}")
    
    for test_name, passed in results.items():
        status = "âœ… é€šé" if passed else "âŒ å¤±æ•—"
        logger.info(f"  {test_name}: {status}")
    
    if not all_passed:
        logger.error("âŒ ç™¼ç¾æ¢¯åº¦è¨ˆç®—å•é¡Œï¼Œéœ€è¦é€²ä¸€æ­¥èª¿æŸ¥")
    else:
        logger.info("âœ… æ¢¯åº¦è¨ˆç®—åŸºç¤åŠŸèƒ½æ­£å¸¸ï¼Œå•é¡Œå¯èƒ½åœ¨å…¶ä»–åœ°æ–¹")

if __name__ == "__main__":
    main()