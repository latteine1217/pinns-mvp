#!/usr/bin/env python3
"""
è¨ºæ–· autograd.grad() è¿”å› None çš„å•é¡Œ
=======================================

å°ˆé–€é‡å° compute_derivatives_3d_temporal å‡½æ•¸ä¸­ autograd.grad() 
è¿”å› None çš„å•é¡Œé€²è¡Œè©³ç´°è¨ºæ–·ã€‚
"""

import torch
import torch.autograd as autograd
import logging
import sys
import os

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.insert(0, '/Users/latteine/Documents/coding/pinns-mvp')

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_basic_autograd():
    """æ¸¬è©¦åŸºæœ¬çš„autogradåŠŸèƒ½"""
    logger.info("ğŸ”§ æ¸¬è©¦åŸºæœ¬autogradåŠŸèƒ½...")
    
    # å‰µå»ºç°¡å–®çš„è¨ˆç®—åœ–
    x = torch.tensor([[1.0, 2.0]], requires_grad=True)
    y = x**2
    
    grad_output = torch.ones_like(y)
    grads = autograd.grad(
        outputs=y,
        inputs=x,
        grad_outputs=grad_output,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    logger.info(f"   è¼¸å…¥: {x}")
    logger.info(f"   è¼¸å‡º: {y}")
    logger.info(f"   æ¢¯åº¦: {grads[0]}")
    logger.info(f"   æ¢¯åº¦æ˜¯å¦ç‚ºNone: {grads[0] is None}")
    
    return grads[0] is not None

def test_4d_autograd():
    """æ¸¬è©¦4Dè¼¸å…¥çš„autograd"""
    logger.info("ğŸ”§ æ¸¬è©¦4Dåº§æ¨™autograd...")
    
    # 4Dåº§æ¨™ [t, x, y, z]
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.2],
        [0.1, 1.5, 0.6, 0.3],
        [0.2, 2.0, 0.7, 0.4],
        [0.3, 2.5, 0.8, 0.5]
    ], requires_grad=True)
    
    # ç°¡å–®çš„å‡½æ•¸ f = x * y + t
    f = coords[:, 1:2] * coords[:, 2:3] + coords[:, 0:1]
    
    logger.info(f"   åº§æ¨™å½¢ç‹€: {coords.shape}")
    logger.info(f"   å‡½æ•¸å€¼: {f.shape} = {f.squeeze()}")
    
    grad_outputs = torch.ones_like(f)
    grads = autograd.grad(
        outputs=f,
        inputs=coords,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    logger.info(f"   æ¢¯åº¦å½¢ç‹€: {grads[0].shape if grads[0] is not None else 'None'}")
    logger.info(f"   æ¢¯åº¦å€¼: {grads[0] if grads[0] is not None else 'None'}")
    logger.info(f"   æ¢¯åº¦æ˜¯å¦ç‚ºNone: {grads[0] is None}")
    
    return grads[0] is not None

def test_original_derivative_function():
    """æ¸¬è©¦åŸå§‹çš„compute_derivatives_3d_temporalå‡½æ•¸"""
    logger.info("ğŸ”§ æ¸¬è©¦åŸå§‹çš„compute_derivatives_3d_temporalå‡½æ•¸...")
    
    from pinnx.physics.ns_3d_temporal import compute_derivatives_3d_temporal
    
    # 4Dåº§æ¨™
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.2],
        [0.1, 1.5, 0.6, 0.3]
    ], requires_grad=True)
    
    # ç°¡å–®å‡½æ•¸
    f = coords[:, 1:2] * coords[:, 2:3]  # x * y
    f.requires_grad_(True)
    
    logger.info(f"   åº§æ¨™: {coords}")
    logger.info(f"   å‡½æ•¸: {f}")
    
    # èª¿ç”¨åŸå§‹å‡½æ•¸
    try:
        derivs = compute_derivatives_3d_temporal(f, coords, order=1)
        logger.info(f"   å°æ•¸çµæœ: {derivs}")
        logger.info(f"   å°æ•¸å½¢ç‹€: {derivs.shape}")
        logger.info(f"   æ˜¯å¦å…¨é›¶: {torch.allclose(derivs, torch.zeros_like(derivs))}")
        return not torch.allclose(derivs, torch.zeros_like(derivs))
    except Exception as e:
        logger.error(f"   éŒ¯èª¤: {e}")
        return False

def test_step_by_step_debug():
    """é€æ­¥èª¿è©¦åŸå§‹å‡½æ•¸å…§éƒ¨é‚è¼¯"""
    logger.info("ğŸ”§ é€æ­¥èª¿è©¦åŸå§‹å‡½æ•¸å…§éƒ¨é‚è¼¯...")
    
    # 4Dåº§æ¨™
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.2],
        [0.1, 1.5, 0.6, 0.3]
    ], requires_grad=True)
    
    # ç°¡å–®å‡½æ•¸ f = x * y
    f = coords[:, 1:2] * coords[:, 2:3]
    logger.info(f"   å‡½æ•¸ f = x*y: {f}")
    logger.info(f"   f.requires_grad: {f.requires_grad}")
    logger.info(f"   coords.requires_grad: {coords.requires_grad}")
    
    # æ‰‹å‹•åŸ·è¡Œautograd.grad
    grad_outputs = torch.ones_like(f)
    logger.info(f"   grad_outputs: {grad_outputs}")
    
    try:
        grads = autograd.grad(
            outputs=f, 
            inputs=coords,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )
        
        logger.info(f"   grads å‹åˆ¥: {type(grads)}")
        logger.info(f"   grads é•·åº¦: {len(grads)}")
        logger.info(f"   grads[0]: {grads[0]}")
        logger.info(f"   grads[0] is None: {grads[0] is None}")
        
        if grads[0] is not None:
            logger.info(f"   âœ… æ¢¯åº¦è¨ˆç®—æˆåŠŸ!")
            logger.info(f"   æ¢¯åº¦å€¼: {grads[0]}")
            return True
        else:
            logger.error(f"   âŒ æ¢¯åº¦ç‚ºNone!")
            return False
            
    except Exception as e:
        logger.error(f"   éŒ¯èª¤: {e}")
        return False

def test_compute_graph_connectivity():
    """æ¸¬è©¦è¨ˆç®—åœ–é€£æ¥æ€§"""
    logger.info("ğŸ”§ æ¸¬è©¦è¨ˆç®—åœ–é€£æ¥æ€§...")
    
    # å‰µå»ºç¥ç¶“ç¶²è·¯
    import torch.nn as nn
    
    net = nn.Sequential(
        nn.Linear(4, 64),
        nn.Tanh(),
        nn.Linear(64, 64),
        nn.Tanh(),
        nn.Linear(64, 3)  # u, v, p (å¿½ç•¥w)
    )
    
    # 4Dåº§æ¨™
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.2],
        [0.1, 1.5, 0.6, 0.3]
    ], requires_grad=True)
    
    # ç¥ç¶“ç¶²è·¯è¼¸å‡º
    output = net(coords)
    u = output[:, 0:1]
    
    logger.info(f"   åº§æ¨™: {coords}")
    logger.info(f"   ç¶²è·¯è¼¸å‡º: {output}")
    logger.info(f"   uåˆ†é‡: {u}")
    logger.info(f"   u.requires_grad: {u.requires_grad}")
    logger.info(f"   coords.requires_grad: {coords.requires_grad}")
    
    # æ¸¬è©¦æ¢¯åº¦è¨ˆç®—
    grad_outputs = torch.ones_like(u)
    try:
        grads = autograd.grad(
            outputs=u,
            inputs=coords,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )
        
        logger.info(f"   ç¥ç¶“ç¶²è·¯æ¢¯åº¦: {grads[0]}")
        logger.info(f"   æ¢¯åº¦æ˜¯å¦ç‚ºNone: {grads[0] is None}")
        
        if grads[0] is not None:
            logger.info(f"   âœ… ç¥ç¶“ç¶²è·¯æ¢¯åº¦è¨ˆç®—æˆåŠŸ!")
            return True
        else:
            logger.error(f"   âŒ ç¥ç¶“ç¶²è·¯æ¢¯åº¦ç‚ºNone!")
            return False
            
    except Exception as e:
        logger.error(f"   éŒ¯èª¤: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ” é–‹å§‹autogradå•é¡Œè¨ºæ–·")
    logger.info("=" * 60)
    
    results = {}
    
    # 1. åŸºæœ¬autogradæ¸¬è©¦
    results['basic_autograd'] = test_basic_autograd()
    logger.info("")
    
    # 2. 4D autogradæ¸¬è©¦
    results['4d_autograd'] = test_4d_autograd()
    logger.info("")
    
    # 3. åŸå§‹å‡½æ•¸æ¸¬è©¦
    results['original_function'] = test_original_derivative_function()
    logger.info("")
    
    # 4. é€æ­¥èª¿è©¦
    results['step_by_step'] = test_step_by_step_debug()
    logger.info("")
    
    # 5. è¨ˆç®—åœ–é€£æ¥æ€§æ¸¬è©¦
    results['compute_graph'] = test_compute_graph_connectivity()
    logger.info("")
    
    # ç¸½çµ
    logger.info("=" * 60)
    logger.info("ğŸ“‹ è¨ºæ–·ç¸½çµ:")
    for test_name, result in results.items():
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        logger.info(f"   {test_name}: {status}")
    
    # è­˜åˆ¥å•é¡Œ
    if all(results.values()):
        logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ŒautogradåŠŸèƒ½æ­£å¸¸")
    else:
        failed_tests = [name for name, result in results.items() if not result]
        logger.error(f"ğŸ’¥ ä»¥ä¸‹æ¸¬è©¦å¤±æ•—: {failed_tests}")
        
        if not results['basic_autograd']:
            logger.error("   åŸºæœ¬autogradåŠŸèƒ½æœ‰å•é¡Œ!")
        elif not results['4d_autograd']:
            logger.error("   4Dåº§æ¨™autogradæœ‰å•é¡Œ!")
        elif not results['original_function']:
            logger.error("   åŸå§‹compute_derivatives_3d_temporalå‡½æ•¸æœ‰å•é¡Œ!")
        elif not results['step_by_step']:
            logger.error("   å…§éƒ¨é‚è¼¯æœ‰å•é¡Œ!")
        elif not results['compute_graph']:
            logger.error("   ç¥ç¶“ç¶²è·¯è¨ˆç®—åœ–æœ‰å•é¡Œ!")

if __name__ == "__main__":
    main()