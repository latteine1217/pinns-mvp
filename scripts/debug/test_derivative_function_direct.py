#!/usr/bin/env python3
"""
ç›´æ¥æ¸¬è©¦ compute_derivatives_3d_temporal å‡½æ•¸
=======================================
ç¹éå°å…¥å•é¡Œï¼Œç›´æ¥åœ¨æ­¤è…³æœ¬ä¸­å®šç¾©å’Œæ¸¬è©¦å‡½æ•¸
"""

import torch
import torch.autograd as autograd
import logging
from typing import Optional

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def compute_derivatives_3d_temporal(f: torch.Tensor, x: torch.Tensor, 
                                  order: int = 1, component: Optional[int] = None) -> torch.Tensor:
    """
    è¨ˆç®—3Dæ™‚é–“ä¾è³´å‡½æ•¸çš„åå¾®åˆ†
    
    Args:
        f: å¾…å¾®åˆ†çš„æ¨™é‡å ´ [batch_size, 1]
        x: åº§æ¨™è®Šæ•¸ [batch_size, 4] = [t, x, y, z] 
        order: å¾®åˆ†éšæ•¸ (1 æˆ– 2)
        component: æŒ‡å®šå¾®åˆ†è®Šæ•¸ (0=t, 1=x, 2=y, 3=z)ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
        
    Returns:
        åå¾®åˆ†çµæœ [batch_size, 4] (ä¸€éš) æˆ–æŒ‡å®šåˆ†é‡ [batch_size, 1]
    """
    # ç¢ºä¿è¨ˆç®—åœ–é€£æ¥
    if not f.requires_grad:
        f.requires_grad_(True)
    if not x.requires_grad:
        x.requires_grad_(True)
        
    # è¨ˆç®—ä¸€éšåå¾®åˆ†
    grad_outputs = torch.ones_like(f)
    
    logger.info(f"æº–å‚™è¨ˆç®—æ¢¯åº¦:")
    logger.info(f"  f.shape: {f.shape}, f.requires_grad: {f.requires_grad}")
    logger.info(f"  x.shape: {x.shape}, x.requires_grad: {x.requires_grad}")
    logger.info(f"  grad_outputs.shape: {grad_outputs.shape}")
    
    try:
        grads = autograd.grad(
            outputs=f, 
            inputs=x,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )
        
        logger.info(f"autograd.grad() è¿”å›çµæœ:")
        logger.info(f"  gradsé¡å‹: {type(grads)}")
        logger.info(f"  gradsé•·åº¦: {len(grads)}")
        logger.info(f"  grads[0]: {grads[0]}")
        logger.info(f"  grads[0] is None: {grads[0] is None}")
        
    except Exception as e:
        logger.error(f"autograd.grad() æ‹‹å‡ºç•°å¸¸: {e}")
        return torch.zeros_like(x)
    
    first_derivs = grads[0]
    if first_derivs is None:
        logger.error("âŒ first_derivs ç‚º Noneï¼Œè¿”å›é›¶å¼µé‡")
        return torch.zeros_like(x)
    
    logger.info(f"âœ… ä¸€éšå°æ•¸è¨ˆç®—æˆåŠŸ: {first_derivs}")
    
    if order == 1:
        if component is not None:
            return first_derivs[:, component:component+1]
        return first_derivs
    
    # å¦‚æœéœ€è¦äºŒéšå°æ•¸...
    elif order == 2:
        # é€™è£¡çœç•¥äºŒéšå°æ•¸çš„ä»£ç¢¼ä»¥å°ˆæ³¨æ–¼ä¸€éšå°æ•¸å•é¡Œ
        raise NotImplementedError("äºŒéšå°æ•¸æš«æ™‚è·³é")
    
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„å¾®åˆ†éšæ•¸: {order}")

def test_simple_function():
    """æ¸¬è©¦ç°¡å–®å‡½æ•¸çš„å¾®åˆ†"""
    logger.info("ğŸ”§ æ¸¬è©¦ç°¡å–®å‡½æ•¸ f = x*y çš„å¾®åˆ†...")
    
    # 4Dåº§æ¨™ [t, x, y, z]
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.2],
        [0.1, 1.5, 0.6, 0.3]
    ], requires_grad=True)
    
    # ç°¡å–®å‡½æ•¸ f = x * y
    f = coords[:, 1:2] * coords[:, 2:3]
    
    logger.info(f"è¼¸å…¥åº§æ¨™: {coords}")
    logger.info(f"å‡½æ•¸å€¼: {f}")
    
    # èª¿ç”¨å¾®åˆ†å‡½æ•¸
    derivs = compute_derivatives_3d_temporal(f, coords, order=1)
    
    logger.info(f"å¾®åˆ†çµæœ: {derivs}")
    logger.info(f"å¾®åˆ†å½¢ç‹€: {derivs.shape}")
    logger.info(f"æ˜¯å¦å…¨é›¶: {torch.allclose(derivs, torch.zeros_like(derivs))}")
    
    # æ‰‹å‹•é©—è­‰
    logger.info("ğŸ” æ‰‹å‹•é©—è­‰:")
    expected_df_dt = torch.zeros_like(f)  # fä¸ä¾è³´æ–¼t
    expected_df_dx = coords[:, 2:3]       # âˆ‚(x*y)/âˆ‚x = y
    expected_df_dy = coords[:, 1:2]       # âˆ‚(x*y)/âˆ‚y = x
    expected_df_dz = torch.zeros_like(f)  # fä¸ä¾è³´æ–¼z
    
    logger.info(f"  é æœŸ âˆ‚f/âˆ‚t = {expected_df_dt.squeeze()}")
    logger.info(f"  é æœŸ âˆ‚f/âˆ‚x = {expected_df_dx.squeeze()}")
    logger.info(f"  é æœŸ âˆ‚f/âˆ‚y = {expected_df_dy.squeeze()}")
    logger.info(f"  é æœŸ âˆ‚f/âˆ‚z = {expected_df_dz.squeeze()}")
    
    if derivs is not None and derivs.shape[1] == 4:
        logger.info(f"  å¯¦éš› âˆ‚f/âˆ‚t = {derivs[:, 0]}")
        logger.info(f"  å¯¦éš› âˆ‚f/âˆ‚x = {derivs[:, 1]}")
        logger.info(f"  å¯¦éš› âˆ‚f/âˆ‚y = {derivs[:, 2]}")
        logger.info(f"  å¯¦éš› âˆ‚f/âˆ‚z = {derivs[:, 3]}")
        
        # æª¢æŸ¥æ˜¯å¦åŒ¹é…
        matches = [
            torch.allclose(derivs[:, 0], expected_df_dt.squeeze()),
            torch.allclose(derivs[:, 1], expected_df_dx.squeeze()),
            torch.allclose(derivs[:, 2], expected_df_dy.squeeze()),
            torch.allclose(derivs[:, 3], expected_df_dz.squeeze())
        ]
        logger.info(f"  åŒ¹é…çµæœ: [dt={matches[0]}, dx={matches[1]}, dy={matches[2]}, dz={matches[3]}]")
        
        return all(matches)
    else:
        logger.error("âŒ å¾®åˆ†çµæœæ ¼å¼éŒ¯èª¤")
        return False

def test_neural_network_output():
    """æ¸¬è©¦ç¥ç¶“ç¶²è·¯è¼¸å‡ºçš„å¾®åˆ†"""
    logger.info("ğŸ”§ æ¸¬è©¦ç¥ç¶“ç¶²è·¯è¼¸å‡ºçš„å¾®åˆ†...")
    
    import torch.nn as nn
    
    # å‰µå»ºç°¡å–®ç¥ç¶“ç¶²è·¯
    net = nn.Sequential(
        nn.Linear(4, 32),
        nn.Tanh(),
        nn.Linear(32, 32),
        nn.Tanh(),
        nn.Linear(32, 3)  # u, v, p (å¿½ç•¥w)
    )
    
    # 4Dåº§æ¨™
    coords = torch.tensor([
        [0.0, 1.0, 0.5, 0.2],
        [0.1, 1.5, 0.6, 0.3]
    ], requires_grad=True)
    
    # ç¥ç¶“ç¶²è·¯è¼¸å‡º
    output = net(coords)
    u = output[:, 0:1]  # é¸æ“‡uåˆ†é‡
    
    logger.info(f"åº§æ¨™: {coords}")
    logger.info(f"ç¥ç¶“ç¶²è·¯è¼¸å‡º: {output}")
    logger.info(f"uåˆ†é‡: {u}")
    
    # èª¿ç”¨å¾®åˆ†å‡½æ•¸
    derivs = compute_derivatives_3d_temporal(u, coords, order=1)
    
    logger.info(f"ç¥ç¶“ç¶²è·¯å¾®åˆ†çµæœ: {derivs}")
    logger.info(f"å¾®åˆ†å½¢ç‹€: {derivs.shape}")
    logger.info(f"æ˜¯å¦å…¨é›¶: {torch.allclose(derivs, torch.zeros_like(derivs))}")
    
    return not torch.allclose(derivs, torch.zeros_like(derivs))

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸ” é–‹å§‹å¾®åˆ†å‡½æ•¸ç›´æ¥æ¸¬è©¦")
    logger.info("=" * 60)
    
    results = {}
    
    # 1. æ¸¬è©¦ç°¡å–®å‡½æ•¸
    results['simple_function'] = test_simple_function()
    logger.info("")
    
    # 2. æ¸¬è©¦ç¥ç¶“ç¶²è·¯è¼¸å‡º
    results['neural_network'] = test_neural_network_output()
    logger.info("")
    
    # ç¸½çµ
    logger.info("=" * 60)
    logger.info("ğŸ“‹ æ¸¬è©¦ç¸½çµ:")
    for test_name, result in results.items():
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        logger.info(f"   {test_name}: {status}")
    
    if all(results.values()):
        logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼Œå¾®åˆ†å‡½æ•¸æ­£å¸¸å·¥ä½œ")
    else:
        failed_tests = [name for name, result in results.items() if not result]
        logger.error(f"ğŸ’¥ ä»¥ä¸‹æ¸¬è©¦å¤±æ•—: {failed_tests}")

if __name__ == "__main__":
    main()