#!/usr/bin/env python3
"""
é©—è­‰å‰©é¤˜å‡è¨­Då’ŒEï¼š
- å‡è¨­Dï¼šRANS Prior æ‹–ç´¯ (prior_weight=0.7)
- å‡è¨­Eï¼šPDE æ¡æ¨£åå·® (wall_clustering=0.3)

åˆ†æé€™å…©å€‹å› ç´ å¦‚ä½•å½±éŸ¿æ¨¡å‹é æ¸¬ç¯„åœå’Œè³ªé‡å®ˆæ†
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import yaml

def verify_remaining_hypotheses(checkpoint_path, config_path):
    """é©—è­‰å‡è¨­Då’ŒE"""
    
    from scripts.train import create_model, get_device
    from pinnx.dataio.channel_flow_loader import ChannelFlowLoader
    
    device = get_device('auto')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    print("="*70)
    
    # === è¼‰å…¥æ¨¡å‹ ===
    print("\n[1/5] è¼‰å…¥æ¨¡å‹...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if 'config' in checkpoint:
        config = checkpoint['config']
        print("  âœ“ å¾ checkpoint è¼‰å…¥é…ç½®")
    elif config_path:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        print(f"  âœ“ å¾æ–‡ä»¶è¼‰å…¥é…ç½®: {config_path}")
    else:
        raise ValueError("éœ€è¦æä¾›é…ç½®")
    
    model = create_model(config, device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    epoch = checkpoint.get('epoch', 'Unknown')
    print(f"  âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ (epoch: {epoch})")
    
    # === è¼‰å…¥æ•¸æ“š ===
    print("\n[2/5] è¼‰å…¥æ•¸æ“š...")
    loader = ChannelFlowLoader(config_path=config_path)
    
    # å…¨å ´æ•¸æ“š
    field_dataset = loader.load_full_field_data()
    full_coords_np, full_fields = field_dataset.to_points(order=('x', 'y', 'z'))
    full_coords = torch.from_numpy(full_coords_np).float().to(device)
    full_u = torch.from_numpy(full_fields['u']).float().to(device)
    full_v = torch.from_numpy(full_fields['v']).float().to(device)
    
    print(f"  âœ“ å…¨å ´æ•¸æ“šé»æ•¸: {full_coords.shape[0]}")
    
    # === å‡è¨­Dï¼šåˆ†æRANS Priorå½±éŸ¿ ===
    print("\n[3/5] å‡è¨­Dé©—è­‰ï¼šRANS Prioræ‹–ç´¯åˆ†æ...")
    print("="*70)
    
    # æª¢æŸ¥é…ç½®ä¸­çš„priorè¨­ç½®
    prior_weight = config.get('losses', {}).get('prior_weight', 0.0)
    prior_type = config.get('data', {}).get('lowfi', {}).get('type', 'none')
    
    print(f"Prior æ¬Šé‡: {prior_weight}")
    print(f"Prior é¡å‹: {prior_type}")
    
    # è¼‰å…¥å¸¶æœ‰priorçš„æ•¸æ“š
    sensor_data = loader.load_sensor_data(strategy='qr_pivot', K=50)
    sensor_data_with_prior = loader.add_lowfi_prior(sensor_data)
    
    hypothesis_d_indicators = []
    
    # åˆå§‹åŒ–è®Šé‡ä»¥é¿å…æœªå®šç¾©éŒ¯èª¤
    u_suppression = 0.0
    v_suppression = 0.0
    high_speed_threshold = 12.0
    
    if sensor_data_with_prior.has_lowfi_prior():
        prior_samples = sensor_data_with_prior.lowfi_prior
        prior_fields = prior_samples.values if prior_samples is not None else {}
        if prior_fields is not None and isinstance(prior_fields, dict):
            print(f"  âœ“ è¼‰å…¥äº†å…ˆé©—å ´: {list(prior_fields.keys())}")
            
            if 'u' in prior_fields and 'v' in prior_fields:
                prior_u = prior_fields['u']
                prior_v = prior_fields['v']
                
                # åˆ†æpriorçš„çµ±è¨ˆç‰¹æ€§
                print(f"\nPriorå ´çµ±è¨ˆ:")
                print(f"  u: [{np.min(prior_u):.3f}, {np.max(prior_u):.3f}], mean={np.mean(prior_u):.3f}")
                print(f"  v: [{np.min(prior_v):.3f}, {np.max(prior_v):.3f}], mean={np.mean(prior_v):.3f}")
                
                # èˆ‡çœŸå¯¦å ´å°æ¯”
                sensor_coords = sensor_data.sensor_points
                sensor_u_true = sensor_data.sensor_data['u']
                sensor_v_true = sensor_data.sensor_data['v']
                
                print(f"\næ„Ÿæ¸¬é»çœŸå¯¦å ´çµ±è¨ˆ:")
                print(f"  u: [{np.min(sensor_u_true):.3f}, {np.max(sensor_u_true):.3f}], mean={np.mean(sensor_u_true):.3f}")
                print(f"  v: [{np.min(sensor_v_true):.3f}, {np.max(sensor_v_true):.3f}], mean={np.mean(sensor_v_true):.3f}")
                
                # è¨ˆç®—priorèˆ‡çœŸå¯¦å ´çš„å·®ç•°
                u_diff = np.abs(prior_u - sensor_u_true)
                v_diff = np.abs(prior_v - sensor_v_true)
                
                print(f"\nPriorèª¤å·®:")
                print(f"  u å¹³å‡çµ•å°èª¤å·®: {np.mean(u_diff):.3f}")
                print(f"  v å¹³å‡çµ•å°èª¤å·®: {np.mean(v_diff):.3f}")
                
                # æª¢æŸ¥prioræ˜¯å¦å£“åˆ¶äº†è®ŠåŒ–ç¯„åœ
                u_range_prior = np.max(prior_u) - np.min(prior_u)
                v_range_prior = np.max(prior_v) - np.min(prior_v)
                u_range_true = np.max(sensor_u_true) - np.min(sensor_u_true)
                v_range_true = np.max(sensor_v_true) - np.min(sensor_v_true)
                
                u_suppression = u_range_prior / u_range_true if u_range_true > 0 else 0
                v_suppression = v_range_prior / v_range_true if v_range_true > 0 else 0
                
                print(f"\nç¯„åœå£“åˆ¶æ¯”ç‡:")
                print(f"  u: {u_suppression:.3f} ({'å£“åˆ¶' if u_suppression < 0.8 else 'æ­£å¸¸'})")
                print(f"  v: {v_suppression:.3f} ({'å£“åˆ¶' if v_suppression < 0.8 else 'æ­£å¸¸'})")
                
                # æª¢æŸ¥æ˜¯å¦å­˜åœ¨æ‹–ç´¯è·¡è±¡
                if prior_weight > 0.5:
                    hypothesis_d_indicators.append(f"Prioræ¬Šé‡åé«˜ ({prior_weight})")
                
                if np.mean(u_diff) > 2.0:  # uçš„å¹³å‡èª¤å·®>2
                    hypothesis_d_indicators.append("Prior uå ´èª¤å·®è¼ƒå¤§")
                
                if v_suppression < 0.5:  # vç¯„åœè¢«åš´é‡å£“åˆ¶
                    hypothesis_d_indicators.append("Prioråš´é‡å£“åˆ¶vå ´è®ŠåŒ–ç¯„åœ")
                
                if u_suppression < 0.7:  # uç¯„åœè¢«æ˜é¡¯å£“åˆ¶
                    hypothesis_d_indicators.append("Prioræ˜é¡¯å£“åˆ¶uå ´è®ŠåŒ–ç¯„åœ")
            else:
                print("  âš ï¸ Prior å ´ä¸­ç¼ºå°‘ u æˆ– v æ•¸æ“š")
                hypothesis_d_indicators.append("Prior å ´æ•¸æ“šä¸å®Œæ•´")
        else:
            print("  âš ï¸ Prior å ´ç‚ºç©ºæˆ–æ ¼å¼éŒ¯èª¤")
            hypothesis_d_indicators.append("Prior å ´æ•¸æ“šç„¡æ•ˆ")
                
    else:
        print("  âš ï¸ æœªæ‰¾åˆ°å…ˆé©—å ´æ•¸æ“š")
        hypothesis_d_indicators.append("ç„¡å…ˆé©—å ´æ•¸æ“šï¼Œç„¡æ³•é©—è­‰")
    
    # === å‡è¨­Eï¼šPDEæ¡æ¨£åå·®åˆ†æ ===
    print("\n[4/5] å‡è¨­Eé©—è­‰ï¼šPDEæ¡æ¨£åå·®åˆ†æ...")
    print("="*70)
    
    # æª¢æŸ¥æ¡æ¨£é…ç½®
    sampling_config = config.get('training', {}).get('sampling', {})
    pde_points = sampling_config.get('pde_points', 4096)
    wall_clustering = sampling_config.get('wall_clustering', 0.0)
    
    print(f"PDE æ¡æ¨£é»æ•¸: {pde_points}")
    print(f"Wall clustering: {wall_clustering}")
    
    # æ¨¡æ“¬PDEæ¡æ¨£é»åˆ†ä½ˆï¼ˆåŸºæ–¼é…ç½®é‡ç¾æ¡æ¨£é‚è¼¯ï¼‰
    print("\næ¨¡æ“¬PDEæ¡æ¨£åˆ†ä½ˆ...")
    
    # å¾åŸŸé…ç½®ç²å–é‚Šç•Œ
    domain_config = config.get('domain', {})
    x_range = domain_config.get('x_range', [0.0, 25.13])
    y_range = domain_config.get('y_range', [-1.0, 1.0])
    
    # æ¨¡æ“¬æ¡æ¨£ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
    np.random.seed(42)  # å›ºå®šéš¨æ©Ÿç¨®å­ä»¥ä¾¿é‡ç¾
    
    if wall_clustering > 0:
        # æœ‰wall clusteringçš„æ¡æ¨£
        # åœ¨yæ–¹å‘ä½¿ç”¨biasæ¡æ¨£ï¼Œæ›´å¤šé»é è¿‘å£é¢
        n_points = pde_points
        
        # xæ–¹å‘å‡å‹»æ¡æ¨£
        x_samples = np.random.uniform(x_range[0], x_range[1], n_points)
        
        # yæ–¹å‘å¸¶wall clusteringçš„æ¡æ¨£
        # ä½¿ç”¨betaåˆ†ä½ˆåœ¨[-1, 1]å€é–“å…§åå‘é‚Šç•Œ
        alpha = 1.0 - wall_clustering + 0.1  # é¿å…alpha=0
        beta = alpha
        
        # Betaåˆ†ä½ˆç”¢ç”Ÿ[0,1]ï¼Œè½‰æ›åˆ°[-1,1]ï¼Œç„¶å¾Œæ˜ å°„åˆ°å¯¦éš›yç¯„åœ
        y_beta = np.random.beta(alpha, beta, n_points)
        y_normalized = 2 * y_beta - 1  # è½‰æ›åˆ°[-1, 1]
        
        # é€²ä¸€æ­¥å¢å¼·wall clusteringï¼ˆè®“æ›´å¤šé»é è¿‘å£é¢ï¼‰
        y_normalized = np.sign(y_normalized) * np.power(np.abs(y_normalized), 1.0 - wall_clustering)
        
        # æ˜ å°„åˆ°å¯¦éš›yç¯„åœ
        y_samples = (y_normalized + 1.0) / 2.0 * (y_range[1] - y_range[0]) + y_range[0]
    else:
        # å‡å‹»æ¡æ¨£
        x_samples = np.random.uniform(x_range[0], x_range[1], pde_points)
        y_samples = np.random.uniform(y_range[0], y_range[1], pde_points)
    
    pde_coords = np.column_stack([x_samples, y_samples])
    
    # åˆ†æPDEæ¡æ¨£é»åœ¨yæ–¹å‘çš„åˆ†ä½ˆ
    y_bins = [-1.0, -0.8, -0.5, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0]
    y_labels = ['å£é¢(-1~-0.8)', 'è¿‘å£(-0.8~-0.5)', 'ä¸­ä¸‹(-0.5~-0.2)', 
                'å…§å±¤(-0.2~0)', 'å…§å±¤(0~0.2)', 'ä¸­ä¸Š(0.2~0.5)', 
                'è¿‘å£(0.5~0.8)', 'å£é¢(0.8~1)']
    
    pde_y_hist, _ = np.histogram(y_samples, bins=y_bins)
    pde_y_frac = pde_y_hist / len(y_samples)
    
    # å…¨å ´yåˆ†ä½ˆä½œå°æ¯”
    full_y = full_coords[:, 1].cpu().numpy()
    full_y_hist, _ = np.histogram(full_y, bins=y_bins)
    full_y_frac = full_y_hist / len(full_y)
    
    print("\nyæ–¹å‘æ¡æ¨£åˆ†ä½ˆå°æ¯”:")
    print("å€åŸŸ             å…¨å ´åˆ†å¸ƒ    PDEæ¡æ¨£     åå·®æ¯”ç‡")
    hypothesis_e_indicators = []
    
    for i, label in enumerate(y_labels):
        if full_y_frac[i] > 0:
            bias_ratio = pde_y_frac[i] / full_y_frac[i]
            status = "éè¡¨ç¤º" if bias_ratio > 1.5 else "æ¬ è¡¨ç¤º" if bias_ratio < 0.5 else "æ­£å¸¸"
            print(f"{label:>12}: {full_y_frac[i]:>8.1%}    {pde_y_frac[i]:>8.1%}    {bias_ratio:>6.2f}x ({status})")
            
            # è¨˜éŒ„é¡¯è‘—åå·®
            if bias_ratio > 2.0 or bias_ratio < 0.3:
                hypothesis_e_indicators.append(f"{label} {status} ({bias_ratio:.2f}x)")
        else:
            print(f"{label:>12}: {full_y_frac[i]:>8.1%}    {pde_y_frac[i]:>8.1%}    N/A")
    
    # åˆ†æåœ¨ä¸åŒyä½ç½®çš„é€Ÿåº¦åˆ†ä½ˆè¦†è“‹
    print("\nä¸åŒyå€åŸŸçš„é€Ÿåº¦åˆ†ä½ˆè¦†è“‹:")
    
    # å°‡å…¨å ´æ•¸æ“šæŒ‰yä½ç½®åˆ†çµ„
    full_coords_np = full_coords.cpu().numpy()
    full_u_np = full_u.cpu().numpy()
    
    # ä¸­å¿ƒå€åŸŸ (|y| < 0.3) vs å£é¢å€åŸŸ (|y| > 0.7)
    center_mask = np.abs(full_coords_np[:, 1]) < 0.3
    wall_mask = np.abs(full_coords_np[:, 1]) > 0.7
    
    center_u = full_u_np[center_mask]
    wall_u = full_u_np[wall_mask]
    
    pde_center_mask = np.abs(y_samples) < 0.3
    pde_wall_mask = np.abs(y_samples) > 0.7
    
    print(f"ä¸­å¿ƒå€åŸŸé»æ•¸ - å…¨å ´: {np.sum(center_mask)}, PDEæ¡æ¨£: {np.sum(pde_center_mask)}")
    print(f"å£é¢å€åŸŸé»æ•¸ - å…¨å ´: {np.sum(wall_mask)}, PDEæ¡æ¨£: {np.sum(pde_wall_mask)}")
    
    if len(center_u) > 0 and len(wall_u) > 0:
        print(f"ä¸­å¿ƒå€åŸŸ u ç¯„åœ: [{np.min(center_u):.1f}, {np.max(center_u):.1f}]")
        print(f"å£é¢å€åŸŸ u ç¯„åœ: [{np.min(wall_u):.1f}, {np.max(wall_u):.1f}]")
        
        # æª¢æŸ¥é«˜é€Ÿå€åŸŸè¦†è“‹
        high_speed_threshold = 12.0
        center_high_speed = np.sum(center_u > high_speed_threshold) / len(center_u)
        wall_high_speed = np.sum(wall_u > high_speed_threshold) / len(wall_u)
        
        print(f"é«˜é€Ÿå€åŸŸ(u>{high_speed_threshold})æ¯”ä¾‹ - ä¸­å¿ƒ: {center_high_speed:.1%}, å£é¢: {wall_high_speed:.1%}")
        
        # æª¢æŸ¥PDEæ¡æ¨£æ˜¯å¦å……åˆ†è¦†è“‹é«˜é€Ÿå€åŸŸ
        pde_center_ratio = np.sum(pde_center_mask) / len(y_samples)
        full_center_ratio = np.sum(center_mask) / len(full_coords_np)
        
        if pde_center_ratio < 0.5 * full_center_ratio:
            hypothesis_e_indicators.append("PDEæ¡æ¨£åš´é‡æ¬ ç¼ºä¸­å¿ƒé«˜é€Ÿå€åŸŸ")
        elif pde_center_ratio < 0.8 * full_center_ratio:
            hypothesis_e_indicators.append("PDEæ¡æ¨£è¼•å¾®æ¬ ç¼ºä¸­å¿ƒé«˜é€Ÿå€åŸŸ")
    
    # æª¢æŸ¥wall_clusteringåƒæ•¸æ˜¯å¦éé«˜
    if wall_clustering > 0.5:
        hypothesis_e_indicators.append(f"wall_clusteringåƒæ•¸éé«˜ ({wall_clustering})")
    
    # === ç”Ÿæˆå¯è¦–åŒ– ===
    print("\n[5/5] ç”Ÿæˆåˆ†æåœ–...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # ç¬¬ä¸€è¡Œï¼šå‡è¨­D - Prioråˆ†æ
    if (sensor_data_with_prior.has_lowfi_prior() and 
        sensor_data_with_prior.lowfi_prior is not None and 
        isinstance(sensor_data_with_prior.lowfi_prior, dict) and
        'u' in sensor_data_with_prior.lowfi_prior):
        
        prior_u = sensor_data_with_prior.lowfi_prior['u']
        prior_v = sensor_data_with_prior.lowfi_prior['v']
        sensor_u_true = sensor_data.sensor_data['u']
        sensor_v_true = sensor_data.sensor_data['v']
        
        # Prior vs çœŸå¯¦å ´å°æ¯” (u)
        axes[0, 0].scatter(sensor_u_true, prior_u, alpha=0.7, s=30)
        axes[0, 0].plot([sensor_u_true.min(), sensor_u_true.max()], 
                        [sensor_u_true.min(), sensor_u_true.max()], 'r--', label='Perfect')
        axes[0, 0].set_xlabel('True u')
        axes[0, 0].set_ylabel('Prior u')
        axes[0, 0].set_title('Prior vs True u Field')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Prior vs çœŸå¯¦å ´å°æ¯” (v)
        axes[0, 1].scatter(sensor_v_true, prior_v, alpha=0.7, s=30)
        axes[0, 1].plot([sensor_v_true.min(), sensor_v_true.max()], 
                        [sensor_v_true.min(), sensor_v_true.max()], 'r--', label='Perfect')
        axes[0, 1].set_xlabel('True v')
        axes[0, 1].set_ylabel('Prior v')
        axes[0, 1].set_title('Prior vs True v Field')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Priorå½±éŸ¿åˆ†æ
        fields = ['u', 'v']
        suppressions = [u_suppression, v_suppression]
        colors = ['red' if s < 0.5 else 'orange' if s < 0.8 else 'green' for s in suppressions]
        
        axes[0, 2].bar(fields, suppressions, color=colors, alpha=0.7)
        axes[0, 2].axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='No suppression')
        axes[0, 2].axhline(y=0.8, color='orange', linestyle=':', alpha=0.5, label='Mild suppression')
        axes[0, 2].axhline(y=0.5, color='red', linestyle=':', alpha=0.5, label='Strong suppression')
        axes[0, 2].set_ylabel('Range Ratio (Prior/True)')
        axes[0, 2].set_title('Prior Range Suppression')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
    else:
        for i in range(3):
            axes[0, i].text(0.5, 0.5, 'No Prior Data Available', 
                           ha='center', va='center', transform=axes[0, i].transAxes)
            axes[0, i].set_title(f'Prior Analysis {i+1}')
    
    # ç¬¬äºŒè¡Œï¼šå‡è¨­E - PDEæ¡æ¨£åˆ†æ
    # yæ–¹å‘åˆ†ä½ˆå°æ¯”
    x_pos = np.arange(len(y_labels))
    width = 0.35
    axes[1, 0].bar(x_pos - width/2, full_y_frac, width, label='Full Field', alpha=0.7)
    axes[1, 0].bar(x_pos + width/2, pde_y_frac, width, label='PDE Sampling', alpha=0.7)
    axes[1, 0].set_xlabel('y Regions')
    axes[1, 0].set_ylabel('Fraction')
    axes[1, 0].set_title('y-Direction Sampling Distribution')
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels(y_labels, rotation=45, ha='right')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # PDEæ¡æ¨£é»ç©ºé–“åˆ†å¸ƒ
    axes[1, 1].scatter(full_coords_np[:, 0], full_coords_np[:, 1], 
                       c=full_u_np, cmap='viridis', s=1, alpha=0.7, label='Full Field')
    axes[1, 1].scatter(pde_coords[:500, 0], pde_coords[:500, 1], 
                       c='red', s=2, alpha=0.8, label='PDE Sample (subset)')
    axes[1, 1].set_xlabel('x')
    axes[1, 1].set_ylabel('y')
    axes[1, 1].set_title('PDE Sampling Distribution')
    axes[1, 1].legend()
    
    # ä¸åŒå€åŸŸçš„é€Ÿåº¦åˆ†ä½ˆ
    if 'center_u' in locals() and 'wall_u' in locals() and len(center_u) > 0 and len(wall_u) > 0:
        axes[1, 2].hist(center_u, bins=30, alpha=0.7, label='Center (|y|<0.3)', density=True)
        axes[1, 2].hist(wall_u, bins=30, alpha=0.7, label='Wall (|y|>0.7)', density=True)
        axes[1, 2].axvline(high_speed_threshold, color='red', linestyle='--', 
                          label=f'High-speed threshold ({high_speed_threshold})')
        axes[1, 2].set_xlabel('u velocity')
        axes[1, 2].set_ylabel('Density')
        axes[1, 2].set_title('Velocity Distribution by Region')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
    else:
        axes[1, 2].text(0.5, 0.5, 'Insufficient data for regional analysis', 
                       ha='center', va='center', transform=axes[1, 2].transAxes)
        axes[1, 2].set_title('Regional Velocity Analysis')
    
    plt.tight_layout()
    
    # ä¿å­˜çµæœ
    output_dir = Path('evaluation_results_phase4b_diagnosis')
    output_dir.mkdir(exist_ok=True)
    
    plot_path = output_dir / f'hypotheses_de_analysis_epoch{epoch}.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ å‡è¨­D&Eåˆ†æåœ–å·²ä¿å­˜: {plot_path}")
    
    # === è¨ºæ–·çµè«– ===
    print("\n" + "="*70)
    print("å‡è¨­D&Eè¨ºæ–·çµè«–:")
    print("="*70)
    
    # å‡è¨­Dçµè«–
    print("\n--- å‡è¨­Dï¼šRANS Prioræ‹–ç´¯ ---")
    if hypothesis_d_indicators:
        print("âš ï¸  æª¢æ¸¬åˆ°Prioræ‹–ç´¯æŒ‡æ¨™ï¼š")
        for indicator in hypothesis_d_indicators:
            print(f"   â€¢ {indicator}")
        hypothesis_d_confirmed = True
        print("ğŸ”´ å‡è¨­Dç¢ºèªï¼šRANS Prioræ˜¯å½±éŸ¿å› ç´ ä¹‹ä¸€")
    else:
        hypothesis_d_confirmed = False
        print("âœ… å‡è¨­Då¦å®šï¼šæœªæª¢æ¸¬åˆ°æ˜é¡¯çš„Prioræ‹–ç´¯")
    
    # å‡è¨­Eçµè«–
    print("\n--- å‡è¨­Eï¼šPDEæ¡æ¨£åå·® ---")
    if hypothesis_e_indicators:
        print("âš ï¸  æª¢æ¸¬åˆ°æ¡æ¨£åå·®æŒ‡æ¨™ï¼š")
        for indicator in hypothesis_e_indicators:
            print(f"   â€¢ {indicator}")
        hypothesis_e_confirmed = True
        print("ğŸ”´ å‡è¨­Eç¢ºèªï¼šPDEæ¡æ¨£åå·®æ˜¯å½±éŸ¿å› ç´ ä¹‹ä¸€")
    else:
        hypothesis_e_confirmed = False
        print("âœ… å‡è¨­Eå¦å®šï¼šæœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æ¡æ¨£åå·®")
    
    # ä¿å­˜æ•¸å€¼å ±å‘Š
    report_path = output_dir / f'hypotheses_de_epoch{epoch}.txt'
    with open(report_path, 'w') as f:
        f.write("="*70 + "\n")
        f.write("å‡è¨­D&Eé©—è­‰å ±å‘Š\n")
        f.write("="*70 + "\n\n")
        
        f.write(f"æ¨¡å‹: {checkpoint_path}\n")
        f.write(f"Epoch: {epoch}\n")
        f.write(f"é…ç½®: {config_path}\n\n")
        
        f.write("--- å‡è¨­Dï¼šRANS Prioræ‹–ç´¯ ---\n")
        f.write(f"Prioræ¬Šé‡: {prior_weight}\n")
        f.write(f"Prioré¡å‹: {prior_type}\n")
        if hypothesis_d_indicators:
            f.write("æª¢æ¸¬åˆ°çš„å•é¡Œ:\n")
            for indicator in hypothesis_d_indicators:
                f.write(f"  â€¢ {indicator}\n")
        f.write(f"çµè«–: {'ç¢ºèª' if hypothesis_d_confirmed else 'å¦å®š'}\n\n")
        
        f.write("--- å‡è¨­Eï¼šPDEæ¡æ¨£åå·® ---\n")
        f.write(f"PDEæ¡æ¨£é»æ•¸: {pde_points}\n")
        f.write(f"Wall clustering: {wall_clustering}\n")
        if hypothesis_e_indicators:
            f.write("æª¢æ¸¬åˆ°çš„å•é¡Œ:\n")
            for indicator in hypothesis_e_indicators:
                f.write(f"  â€¢ {indicator}\n")
        f.write(f"çµè«–: {'ç¢ºèª' if hypothesis_e_confirmed else 'å¦å®š'}\n")
    
    print(f"âœ“ æ•¸å€¼å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    return {
        'hypothesis_d_confirmed': hypothesis_d_confirmed,
        'hypothesis_e_confirmed': hypothesis_e_confirmed,
        'hypothesis_d_indicators': hypothesis_d_indicators,
        'hypothesis_e_indicators': hypothesis_e_indicators,
        'prior_weight': prior_weight,
        'wall_clustering': wall_clustering
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str, required=True, 
                        help='Checkpoint æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--config', type=str, required=True,
                        help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    result = verify_remaining_hypotheses(args.checkpoint, args.config)
    
    print(f"\næœ€çµ‚çµè«–ï¼š")
    print(f"å‡è¨­Dï¼ˆRANS Prioræ‹–ç´¯ï¼‰: {'ç¢ºèª' if result['hypothesis_d_confirmed'] else 'å¦å®š'}")
    print(f"å‡è¨­Eï¼ˆPDEæ¡æ¨£åå·®ï¼‰: {'ç¢ºèª' if result['hypothesis_e_confirmed'] else 'å¦å®š'}")
