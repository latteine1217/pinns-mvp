#!/usr/bin/env python3
"""
é©—è­‰å‡è¨­Aï¼šLossè¨ˆç®—åŸŸä¸åŒ¹é…

å°æ¯” continuity_loss åœ¨è¨“ç·´é» (4096) çš„è¨ˆç®—çµæœ
èˆ‡å…¨å ´é» (8192) çš„å¯¦éš›å®ˆæ†èª¤å·®

ç”¨æ–¼è§£é‡‹ç‚ºä½• loss ä¸‹é™ä½†å¯¦éš›å®ˆæ†èª¤å·®ä¸Šå‡çš„çŸ›ç›¾ç¾è±¡
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import yaml

def compare_divergence_domains(checkpoint_path, config_path):
    """å°æ¯”è¨“ç·´åŸŸå’Œå…¨å ´åŸŸçš„æ•£åº¦è¨ˆç®—"""
    
    from scripts.train import create_model, get_device
    from pinnx.dataio.channel_flow_loader import ChannelFlowLoader
    from pinnx.evals.metrics import conservation_error
    
    device = get_device('auto')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    print("="*70)
    
    # === è¼‰å…¥æ¨¡å‹ ===
    print("\n[1/4] è¼‰å…¥æ¨¡å‹...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if 'config' in checkpoint:
        config = checkpoint['config']
        print("  âœ“ å¾ checkpoint è¼‰å…¥é…ç½®")
    elif config_path:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        print(f"  âœ“ å¾æ–‡ä»¶è¼‰å…¥é…ç½®: {config_path}")
    else:
        raise ValueError("éœ€è¦æä¾›é…ç½®")
    
    model = create_model(config, device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    epoch = checkpoint.get('epoch', 'Unknown')
    print(f"  âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ (epoch: {epoch})")
    
    # === è¼‰å…¥æ•¸æ“š ===
    print("\n[2/4] è¼‰å…¥æ•¸æ“š...")
    loader = ChannelFlowLoader(config_path=config_path)
    
    # å…¨å ´æ•¸æ“š (8192 é»)
    channel_data = loader.load_full_field_data()
    full_coords = torch.from_numpy(channel_data.sensor_points).float().to(device)
    print(f"  âœ“ å…¨å ´æ•¸æ“šé»æ•¸: {full_coords.shape[0]}")
    
    # æ¨¡æ“¬è¨“ç·´æ™‚çš„ PDE æ¡æ¨£é» (4096 é»)
    # æŒ‰ç…§ train.py ä¸­çš„ç›¸åŒæ–¹å¼ç”Ÿæˆ
    def generate_pde_points(n_points, domain_bounds):
        """æŒ‰ç…§è¨“ç·´è…³æœ¬çš„é‚è¼¯ç”Ÿæˆ PDE é»"""
        # åƒè€ƒ train.py çš„å¯¦ç¾
        wall_clustering = config.get('wall_clustering', 0.3)
        
        # x å‡å‹»åˆ†å¸ƒ
        x = torch.rand(n_points, 1) * (domain_bounds['x'][1] - domain_bounds['x'][0]) + domain_bounds['x'][0]
        
        # y æ–¹å‘ä½¿ç”¨ wall clustering
        if wall_clustering > 0:
            # ç”Ÿæˆæ›´å¤šé è¿‘å£é¢çš„é»
            y_uniform = torch.rand(n_points, 1) * 2 - 1  # [-1, 1]
            y_clustered = torch.sign(y_uniform) * (1 - (1 - torch.abs(y_uniform))**wall_clustering)
            y = y_clustered
        else:
            y = torch.rand(n_points, 1) * 2 - 1
        
        return torch.cat([x, y], dim=1)
    
    # ç²å–åŸŸé‚Šç•Œ
    domain_bounds = {
        'x': [0.0, 25.13272],  # å¾é…ç½®æª”è®€å–
        'y': [-1.0, 1.0]
    }
    
    # ç”Ÿæˆè¨“ç·´ PDE é»
    training_pde_points = generate_pde_points(4096, domain_bounds).to(device)
    print(f"  âœ“ è¨“ç·´ PDE é»æ•¸: {training_pde_points.shape[0]}")
    
    # === åˆ†åˆ¥è¨ˆç®—æ•£åº¦ ===
    print("\n[3/4] è¨ˆç®—æ•£åº¦...")
    
    def compute_divergence(coords, model, name):
        """è¨ˆç®—çµ¦å®šé»é›†çš„æ•£åº¦"""
        coords_eval = coords.clone().detach().requires_grad_(True)
        
        # æ¨¡å‹é æ¸¬
        pred = model(coords_eval)
        pred_u = pred[:, 0]
        pred_v = pred[:, 1]
        
        # è¨ˆç®—æ¢¯åº¦
        u_x = torch.autograd.grad(pred_u, coords_eval,
                                  grad_outputs=torch.ones_like(pred_u),
                                  create_graph=True, retain_graph=True)[0][:, 0]
        
        v_y = torch.autograd.grad(pred_v, coords_eval,
                                  grad_outputs=torch.ones_like(pred_v),
                                  create_graph=True, retain_graph=True)[0][:, 1]
        
        divergence = u_x + v_y
        div_array = divergence.detach().cpu().numpy()
        
        # è¨ˆç®—çµ±è¨ˆé‡
        rms_div = np.sqrt(np.mean(div_array**2))
        mean_div = np.mean(div_array)
        std_div = np.std(div_array)
        max_abs_div = np.max(np.abs(div_array))
        
        print(f"\n  === {name} æ•£åº¦çµ±è¨ˆ ===")
        print(f"  é»æ•¸: {coords.shape[0]}")
        print(f"  RMS æ•£åº¦: {rms_div:.6e}")
        print(f"  å¹³å‡æ•£åº¦: {mean_div:.6e}")
        print(f"  æ•£åº¦æ¨™æº–å·®: {std_div:.6e}")
        print(f"  æœ€å¤§|æ•£åº¦|: {max_abs_div:.6e}")
        
        # ä½¿ç”¨å®˜æ–¹å‡½æ•¸é©—è­‰
        official_error = conservation_error(pred_u, pred_v, coords_eval)
        print(f"  å®˜æ–¹ conservation_error: {official_error:.6e}")
        
        return {
            'coords': coords_eval.detach().cpu().numpy(),
            'divergence': div_array,
            'rms': rms_div,
            'mean': mean_div,
            'std': std_div,
            'max_abs': max_abs_div,
            'official_error': official_error.item() if hasattr(official_error, 'item') else float(official_error),
            'pred_u': pred_u.detach().cpu().numpy(),
            'pred_v': pred_v.detach().cpu().numpy()
        }
    
    # è¨ˆç®—å…©å€‹åŸŸçš„æ•£åº¦
    training_results = compute_divergence(training_pde_points, model, "è¨“ç·´åŸŸ (4096é»)")
    fullfield_results = compute_divergence(full_coords, model, "å…¨å ´åŸŸ (8192é»)")
    
    # === å°æ¯”åˆ†æ ===
    print("\n[4/4] å°æ¯”åˆ†æ...")
    print("="*70)
    print("è¨“ç·´åŸŸ vs å…¨å ´åŸŸæ•£åº¦å°æ¯”:")
    print("="*70)
    
    ratio_rms = fullfield_results['rms'] / training_results['rms']
    ratio_mean = abs(fullfield_results['mean']) / abs(training_results['mean']) if training_results['mean'] != 0 else float('inf')
    ratio_max = fullfield_results['max_abs'] / training_results['max_abs']
    ratio_official = fullfield_results['official_error'] / training_results['official_error']
    
    print(f"RMS æ•£åº¦æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_rms:.3f}")
    print(f"å¹³å‡æ•£åº¦æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_mean:.3f}")
    print(f"æœ€å¤§æ•£åº¦æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_max:.3f}")
    print(f"å®˜æ–¹èª¤å·®æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_official:.3f}")
    
    # åˆ†æåŸå› 
    print("\n" + "="*70)
    print("åˆ†æçµæœ:")
    print("="*70)
    
    if ratio_rms > 1.2:
        print("âš ï¸  å…¨å ´åŸŸæ•£åº¦æ˜é¡¯å¤§æ–¼è¨“ç·´åŸŸï¼")
        print("   â†’ é€™è§£é‡‹äº†ç‚ºä½• continuity_loss ä¸‹é™ä½†å¯¦éš›å®ˆæ†èª¤å·®ä¸Šå‡")
        print("   â†’ è¨“ç·´åƒ…å„ªåŒ–äº† 4096 å€‹ç‰¹å®šé»ï¼Œæœªè¦†è“‹å…¨éƒ¨ 8192 é»")
    elif ratio_rms < 0.8:
        print("âœ… å…¨å ´åŸŸæ•£åº¦å°æ–¼è¨“ç·´åŸŸ")
        print("   â†’ Loss è¨ˆç®—åŸŸä¸åŒ¹é…ä¸æ˜¯ä¸»å› ")
    else:
        print("ğŸŸ¡ å…©åŸŸæ•£åº¦ç›¸è¿‘")
        print("   â†’ Loss è¨ˆç®—åŸŸå¯èƒ½ä¸æ˜¯ä¸»è¦å•é¡Œ")
    
    # æª¢æŸ¥æ¡æ¨£åå·®
    print("\n--- æ¡æ¨£åå·®åˆ†æ ---")
    
    # æª¢æŸ¥ y æ–¹å‘åˆ†å¸ƒ
    training_y = training_pde_points[:, 1].cpu().numpy()
    fullfield_y = full_coords[:, 1].cpu().numpy()
    
    # çµ±è¨ˆé è¿‘å£é¢çš„é»
    wall_threshold = 0.8
    training_near_wall = np.sum(np.abs(training_y) > wall_threshold) / len(training_y)
    fullfield_near_wall = np.sum(np.abs(fullfield_y) > wall_threshold) / len(fullfield_y)
    
    print(f"è¨“ç·´åŸŸé è¿‘å£é¢çš„é» (|y| > {wall_threshold}): {training_near_wall:.1%}")
    print(f"å…¨å ´åŸŸé è¿‘å£é¢çš„é» (|y| > {wall_threshold}): {fullfield_near_wall:.1%}")
    
    if abs(training_near_wall - fullfield_near_wall) > 0.1:
        print("âš ï¸  è¨“ç·´åŸŸå’Œå…¨å ´åŸŸçš„ y åˆ†å¸ƒå­˜åœ¨æ˜é¡¯å·®ç•°")
        print("   â†’ wall_clustering å¯èƒ½å°è‡´æ¡æ¨£åå·®")
    
    # === å¯è¦–åŒ– ===
    print("\nç”Ÿæˆå°æ¯”åœ–...")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # ç¬¬ä¸€è¡Œï¼šæ•£åº¦åˆ†å¸ƒå°æ¯”
    axes[0, 0].hist(training_results['divergence'], bins=50, alpha=0.7, 
                    label=f'Training (RMS={training_results["rms"]:.3e})', density=True)
    axes[0, 0].hist(fullfield_results['divergence'], bins=50, alpha=0.7, 
                    label=f'Full Field (RMS={fullfield_results["rms"]:.3e})', density=True)
    axes[0, 0].set_xlabel('Divergence')
    axes[0, 0].set_title('Divergence Distribution Comparison')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].axvline(0, color='red', linestyle='--', alpha=0.5)
    
    # æ¡æ¨£é»åˆ†å¸ƒå°æ¯” (y æ–¹å‘)
    axes[0, 1].hist(training_y, bins=50, alpha=0.7, label='Training Points', density=True)
    axes[0, 1].hist(fullfield_y, bins=50, alpha=0.7, label='Full Field Points', density=True)
    axes[0, 1].set_xlabel('y coordinate')
    axes[0, 1].set_title('Y-direction Sampling Distribution')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # æ•£åº¦ç©ºé–“åˆ†å¸ƒ
    scatter1 = axes[0, 2].scatter(training_results['coords'][:, 0], 
                                  training_results['coords'][:, 1],
                                  c=np.abs(training_results['divergence']), 
                                  cmap='hot', s=2, alpha=0.7,
                                  vmax=np.percentile(np.abs(fullfield_results['divergence']), 99))
    axes[0, 2].set_xlabel('x')
    axes[0, 2].set_ylabel('y')
    axes[0, 2].set_title('Training Points |Divergence|')
    plt.colorbar(scatter1, ax=axes[0, 2], label='|div|')
    
    # ç¬¬äºŒè¡Œï¼šè©³ç´°å°æ¯”
    scatter2 = axes[1, 0].scatter(fullfield_results['coords'][:, 0], 
                                  fullfield_results['coords'][:, 1],
                                  c=np.abs(fullfield_results['divergence']), 
                                  cmap='hot', s=2, alpha=0.7,
                                  vmax=np.percentile(np.abs(fullfield_results['divergence']), 99))
    axes[1, 0].set_xlabel('x')
    axes[1, 0].set_ylabel('y')
    axes[1, 0].set_title('Full Field |Divergence|')
    plt.colorbar(scatter2, ax=axes[1, 0], label='|div|')
    
    # çµ±è¨ˆå°æ¯”åœ–
    metrics = ['RMS', 'Mean', 'Std', 'Max Abs']
    training_vals = [training_results['rms'], abs(training_results['mean']), 
                     training_results['std'], training_results['max_abs']]
    fullfield_vals = [fullfield_results['rms'], abs(fullfield_results['mean']), 
                      fullfield_results['std'], fullfield_results['max_abs']]
    
    x_pos = np.arange(len(metrics))
    width = 0.35
    
    axes[1, 1].bar(x_pos - width/2, training_vals, width, label='Training', alpha=0.7)
    axes[1, 1].bar(x_pos + width/2, fullfield_vals, width, label='Full Field', alpha=0.7)
    axes[1, 1].set_xlabel('Metrics')
    axes[1, 1].set_ylabel('Value')
    axes[1, 1].set_title('Divergence Metrics Comparison')
    axes[1, 1].set_xticks(x_pos)
    axes[1, 1].set_xticklabels(metrics)
    axes[1, 1].legend()
    axes[1, 1].set_yscale('log')
    axes[1, 1].grid(True, alpha=0.3)
    
    # æ¯”å€¼åœ–
    ratios = [ratio_rms, ratio_mean, 1.0, ratio_max]  # ratio_std è¨­ç‚º 1.0 ä½œç‚ºåƒè€ƒ
    axes[1, 2].bar(metrics, ratios, alpha=0.7, color=['red' if r > 1.2 else 'green' if r < 0.8 else 'orange' for r in ratios])
    axes[1, 2].axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Equal')
    axes[1, 2].axhline(y=1.2, color='red', linestyle=':', alpha=0.5, label='20% threshold')
    axes[1, 2].axhline(y=0.8, color='red', linestyle=':', alpha=0.5)
    axes[1, 2].set_xlabel('Metrics')
    axes[1, 2].set_ylabel('Full Field / Training Ratio')
    axes[1, 2].set_title('Domain Ratio Analysis')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜çµæœ
    output_dir = Path('evaluation_results_phase4b_diagnosis')
    output_dir.mkdir(exist_ok=True)
    
    plot_path = output_dir / f'training_vs_fullfield_divergence_epoch{epoch}.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ å°æ¯”åœ–å·²ä¿å­˜: {plot_path}")
    
    # ä¿å­˜æ•¸å€¼å ±å‘Š
    report_path = output_dir / f'domain_comparison_epoch{epoch}.txt'
    with open(report_path, 'w') as f:
        f.write("="*70 + "\n")
        f.write("è¨“ç·´åŸŸ vs å…¨å ´åŸŸæ•£åº¦å°æ¯”å ±å‘Š\n")
        f.write("="*70 + "\n\n")
        
        f.write(f"æ¨¡å‹: {checkpoint_path}\n")
        f.write(f"Epoch: {epoch}\n")
        f.write(f"é…ç½®: {config_path}\n\n")
        
        f.write("--- æ•£åº¦çµ±è¨ˆå°æ¯” ---\n")
        f.write(f"è¨“ç·´åŸŸ (4096é») RMS æ•£åº¦: {training_results['rms']:.6e}\n")
        f.write(f"å…¨å ´åŸŸ (8192é») RMS æ•£åº¦: {fullfield_results['rms']:.6e}\n")
        f.write(f"æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_rms:.3f}\n\n")
        
        f.write(f"è¨“ç·´åŸŸå¹³å‡æ•£åº¦: {training_results['mean']:.6e}\n")
        f.write(f"å…¨å ´åŸŸå¹³å‡æ•£åº¦: {fullfield_results['mean']:.6e}\n")
        f.write(f"æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_mean:.3f}\n\n")
        
        f.write(f"è¨“ç·´åŸŸæœ€å¤§|æ•£åº¦|: {training_results['max_abs']:.6e}\n")
        f.write(f"å…¨å ´åŸŸæœ€å¤§|æ•£åº¦|: {fullfield_results['max_abs']:.6e}\n")
        f.write(f"æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_max:.3f}\n\n")
        
        f.write(f"è¨“ç·´åŸŸå®˜æ–¹èª¤å·®: {training_results['official_error']:.6e}\n")
        f.write(f"å…¨å ´åŸŸå®˜æ–¹èª¤å·®: {fullfield_results['official_error']:.6e}\n")
        f.write(f"æ¯”å€¼ (å…¨å ´/è¨“ç·´): {ratio_official:.3f}\n\n")
        
        f.write("--- æ¡æ¨£åˆ†æ ---\n")
        f.write(f"è¨“ç·´åŸŸé è¿‘å£é¢é»æ¯”ä¾‹: {training_near_wall:.1%}\n")
        f.write(f"å…¨å ´åŸŸé è¿‘å£é¢é»æ¯”ä¾‹: {fullfield_near_wall:.1%}\n")
        f.write(f"å·®ç•°: {abs(training_near_wall - fullfield_near_wall):.1%}\n\n")
        
        f.write("--- çµè«– ---\n")
        if ratio_rms > 1.2:
            f.write("âš ï¸  å‡è¨­ A è­‰å¯¦ï¼šLoss è¨ˆç®—åŸŸä¸åŒ¹é…æ˜¯ä¸»è¦å•é¡Œ\n")
            f.write("   è¨“ç·´åƒ…å„ªåŒ–äº†éƒ¨åˆ†é»ï¼Œå°è‡´ loss ä¸‹é™ä½†å…¨å ´å®ˆæ†æƒ¡åŒ–\n")
        elif ratio_rms < 0.8:
            f.write("âœ… å‡è¨­ A å¦å®šï¼šLoss è¨ˆç®—åŸŸä¸æ˜¯ä¸»è¦å•é¡Œ\n")
        else:
            f.write("ğŸŸ¡ å‡è¨­ A éƒ¨åˆ†æ”¯æŒï¼šå¯èƒ½å­˜åœ¨è¼•å¾®çš„è¨ˆç®—åŸŸå•é¡Œ\n")
    
    print(f"âœ“ æ•¸å€¼å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    print("\n" + "="*70)
    print("å‡è¨­ A é©—è­‰å®Œæˆï¼")
    print("="*70)
    
    return {
        'ratio_rms': ratio_rms,
        'ratio_official': ratio_official,
        'hypothesis_A_confirmed': ratio_rms > 1.2,
        'training_results': training_results,
        'fullfield_results': fullfield_results
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str, required=True, 
                        help='Checkpoint æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--config', type=str, required=True,
                        help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    result = compare_divergence_domains(args.checkpoint, args.config)
    
    print(f"\næœ€çµ‚çµè«–ï¼šå‡è¨­ A {'å·²è­‰å¯¦' if result['hypothesis_A_confirmed'] else 'æœªè­‰å¯¦'}")
    print(f"RMS æ•£åº¦æ¯”å€¼: {result['ratio_rms']:.3f}")
    print(f"å®˜æ–¹èª¤å·®æ¯”å€¼: {result['ratio_official']:.3f}")