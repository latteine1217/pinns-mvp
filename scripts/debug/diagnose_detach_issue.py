"""
è¨ºæ–· .detach() æ˜¯å¦å°è‡´æ¢¯åº¦æ–·é–‹

æª¢æŸ¥é»ï¼š
1. compute_derivatives_safe() æ˜¯å¦è§¸ç™¼ .detach() åˆ†æ”¯
2. æ¢¯åº¦æ˜¯å¦èƒ½æ­£ç¢ºåå‘å‚³æ’­
3. ç‰©ç†æ®˜å·®æå¤±æ˜¯å¦èƒ½å½±éŸ¿æ¨¡å‹åƒæ•¸
"""

import torch
import torch.nn as nn
import sys
sys.path.insert(0, '/Users/latteine/Documents/coding/pinns-mvp')

from pinnx.physics.ns_2d import NSEquations2D, compute_derivatives_safe

def test_gradient_flow():
    """æ¸¬è©¦æ¢¯åº¦æ˜¯å¦èƒ½é€šéç‰©ç†æ®˜å·®å‚³æ’­"""
    print("=" * 80)
    print("ğŸ” æ¸¬è©¦ 1: æ¢¯åº¦æµå‹•æ€§æª¢æŸ¥")
    print("=" * 80)
    
    # å‰µå»ºç°¡å–®æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(2, 64),
        nn.Tanh(),
        nn.Linear(64, 64),
        nn.Tanh(),
        nn.Linear(64, 3)
    )
    
    # å‰µå»ºç‰©ç†æ±‚è§£å™¨
    physics = NSEquations2D(viscosity=1e-3)
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    coords = torch.randn(16, 2, requires_grad=True)
    
    # å‰å‘å‚³æ’­
    pred = model(coords)  # [batch, 3] -> [u, v, p]
    
    print(f"\nğŸ“Š è¼¸å…¥ç‹€æ…‹:")
    print(f"  coords.requires_grad: {coords.requires_grad}")
    print(f"  pred.requires_grad: {pred.requires_grad}")
    print(f"  pred.grad_fn: {pred.grad_fn}")
    
    # æå–é€Ÿåº¦å’Œå£“åŠ›
    velocity = pred[:, :2]
    pressure = pred[:, 2:3]
    
    print(f"\nğŸ“Š åˆ†é‡ç‹€æ…‹:")
    print(f"  velocity.requires_grad: {velocity.requires_grad}")
    print(f"  pressure.requires_grad: {pressure.requires_grad}")
    
    # è¨ˆç®—ç‰©ç†æ®˜å·®
    try:
        residuals = physics.residual(coords, velocity, pressure)
        
        print(f"\nğŸ“Š æ®˜å·®è¨ˆç®—æˆåŠŸ:")
        for key, val in residuals.items():
            print(f"  {key}: shape={val.shape}, requires_grad={val.requires_grad}, grad_fn={val.grad_fn}")
        
        # è¨ˆç®—æå¤±
        loss_pde = sum(r.pow(2).mean() for r in residuals.values())
        print(f"\nğŸ“Š PDE Loss: {loss_pde.item():.6f}")
        print(f"  loss_pde.requires_grad: {loss_pde.requires_grad}")
        print(f"  loss_pde.grad_fn: {loss_pde.grad_fn}")
        
        # åå‘å‚³æ’­
        print(f"\nğŸ”™ é–‹å§‹åå‘å‚³æ’­...")
        loss_pde.backward()
        
        # æª¢æŸ¥æ¢¯åº¦
        grad_count = 0
        total_params = 0
        for name, param in model.named_parameters():
            total_params += 1
            if param.grad is not None:
                grad_count += 1
                grad_norm = param.grad.norm().item()
                if grad_norm > 1e-10:
                    print(f"  âœ… {name}: grad_norm={grad_norm:.6e}")
                else:
                    print(f"  âš ï¸  {name}: grad_norm={grad_norm:.6e} (å¤ªå°!)")
            else:
                print(f"  âŒ {name}: grad=None")
        
        print(f"\nğŸ“ˆ æ¢¯åº¦çµ±è¨ˆ:")
        print(f"  æœ‰æ¢¯åº¦çš„åƒæ•¸: {grad_count}/{total_params}")
        
        if grad_count == total_params:
            print(f"  âœ… æ‰€æœ‰åƒæ•¸éƒ½æœ‰æ¢¯åº¦ï¼")
            return True
        else:
            print(f"  âŒ éƒ¨åˆ†åƒæ•¸æ²’æœ‰æ¢¯åº¦ï¼")
            return False
            
    except Exception as e:
        print(f"\nâŒ æ®˜å·®è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_detach_branch_trigger():
    """æ¸¬è©¦ compute_derivatives_safe æ˜¯å¦è§¸ç™¼ .detach() åˆ†æ”¯"""
    print("\n" + "=" * 80)
    print("ğŸ” æ¸¬è©¦ 2: .detach() åˆ†æ”¯è§¸ç™¼æª¢æŸ¥")
    print("=" * 80)
    
    # æ¸¬è©¦å ´æ™¯ 1: f å’Œ x éƒ½æœ‰ requires_grad
    print("\nğŸ“Š å ´æ™¯ 1: f å’Œ x éƒ½æœ‰ requires_grad=True")
    x = torch.randn(16, 2, requires_grad=True)
    f = torch.randn(16, 1, requires_grad=True)
    
    print(f"  è¼¸å…¥: f.requires_grad={f.requires_grad}, x.requires_grad={x.requires_grad}")
    
    # æª¢æŸ¥æ˜¯å¦æœƒé€²å…¥ .detach() åˆ†æ”¯
    will_detach_f = not f.requires_grad
    will_detach_x = not x.requires_grad
    
    print(f"  æ˜¯å¦è§¸ç™¼ .detach()? f:{will_detach_f}, x:{will_detach_x}")
    
    grad = compute_derivatives_safe(f, x, order=1, keep_graph=True)
    print(f"  è¼¸å‡º: grad.shape={grad.shape}, requires_grad={grad.requires_grad}")
    
    # æ¸¬è©¦å ´æ™¯ 2: å¾æ¨¡å‹è¼¸å‡ºçš„ f
    print("\nğŸ“Š å ´æ™¯ 2: å¾æ¨¡å‹è¼¸å‡ºçš„ fï¼ˆçœŸå¯¦è¨“ç·´å ´æ™¯ï¼‰")
    model = nn.Sequential(
        nn.Linear(2, 32),
        nn.Tanh(),
        nn.Linear(32, 1)
    )
    
    x = torch.randn(16, 2, requires_grad=True)
    f = model(x)
    
    print(f"  è¼¸å…¥: f.requires_grad={f.requires_grad}, x.requires_grad={x.requires_grad}")
    print(f"  f.grad_fn={f.grad_fn}")
    
    will_detach_f = not f.requires_grad
    will_detach_x = not x.requires_grad
    
    print(f"  æ˜¯å¦è§¸ç™¼ .detach()? f:{will_detach_f}, x:{will_detach_x}")
    
    grad = compute_derivatives_safe(f, x, order=1, keep_graph=True)
    print(f"  è¼¸å‡º: grad.shape={grad.shape}, requires_grad={grad.requires_grad}")
    
    # æ¸¬è©¦å ´æ™¯ 3: åˆ†è§£å¾Œçš„ pred[:, 0:1]
    print("\nğŸ“Š å ´æ™¯ 3: åˆ†è§£å¾Œçš„ pred[:, 0:1]ï¼ˆns_residual_2d å…§éƒ¨ï¼‰")
    pred = model(x)  # [16, 1]
    # æ¨¡æ“¬ ns_residual_2d ä¸­çš„æ“ä½œ
    u = pred[:, 0:1].requires_grad_(True)
    
    print(f"  è¼¸å…¥: u.requires_grad={u.requires_grad}, x.requires_grad={x.requires_grad}")
    print(f"  u.grad_fn={u.grad_fn}")
    
    will_detach_u = not u.requires_grad
    will_detach_x = not x.requires_grad
    
    print(f"  æ˜¯å¦è§¸ç™¼ .detach()? u:{will_detach_u}, x:{will_detach_x}")
    
    grad = compute_derivatives_safe(u, x, order=1, keep_graph=True)
    print(f"  è¼¸å‡º: grad.shape={grad.shape}, requires_grad={grad.requires_grad}")


def test_gradient_with_vs_without_detach():
    """å°æ¯”æœ‰ç„¡ .detach() çš„æ¢¯åº¦å·®ç•°"""
    print("\n" + "=" * 80)
    print("ğŸ” æ¸¬è©¦ 3: å°æ¯”æœ‰ç„¡ .detach() çš„æ¢¯åº¦")
    print("=" * 80)
    
    def compute_grad_with_detach(f, x):
        """æ¨¡æ“¬ç•¶å‰æœ‰å•é¡Œçš„ç‰ˆæœ¬"""
        if not f.requires_grad:
            f = f.clone().detach().requires_grad_(True)
        if not x.requires_grad:
            x = x.clone().detach().requires_grad_(True)
        
        grad_outputs = torch.ones_like(f)
        grads = torch.autograd.grad(
            outputs=f,
            inputs=x,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )[0]
        return grads
    
    def compute_grad_without_detach(f, x):
        """ä¿®å¾©å¾Œçš„ç‰ˆæœ¬"""
        if not f.requires_grad:
            f.requires_grad_(True)
        if not x.requires_grad:
            x.requires_grad_(True)
        
        grad_outputs = torch.ones_like(f)
        grads = torch.autograd.grad(
            outputs=f,
            inputs=x,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )[0]
        return grads
    
    # å‰µå»ºæ¨¡å‹
    model = nn.Linear(2, 1)
    
    # æ¸¬è©¦æ•¸æ“š
    x = torch.randn(8, 2, requires_grad=True)
    
    # ç‰ˆæœ¬ 1: æœ‰ .detach()
    print("\nğŸ“Š ç‰ˆæœ¬ 1: ä½¿ç”¨ .detach() (ç•¶å‰ç‰ˆæœ¬)")
    f1 = model(x)
    grad1 = compute_grad_with_detach(f1, x)
    loss1 = grad1.pow(2).mean()
    
    model.zero_grad()
    loss1.backward()
    
    weight_grad1 = model.weight.grad.clone() if model.weight.grad is not None else None
    print(f"  Loss: {loss1.item():.6f}")
    if weight_grad1 is not None:
        print(f"  Weight grad norm: {weight_grad1.norm().item():.6e}")
    else:
        print(f"  âŒ Weight grad: None")
    
    # ç‰ˆæœ¬ 2: ç„¡ .detach()
    print("\nğŸ“Š ç‰ˆæœ¬ 2: ä¸ä½¿ç”¨ .detach() (ä¿®å¾©ç‰ˆæœ¬)")
    model.zero_grad()
    x2 = torch.randn(8, 2, requires_grad=True)
    f2 = model(x2)
    grad2 = compute_grad_without_detach(f2, x2)
    loss2 = grad2.pow(2).mean()
    
    loss2.backward()
    
    weight_grad2 = model.weight.grad.clone() if model.weight.grad is not None else None
    print(f"  Loss: {loss2.item():.6f}")
    if weight_grad2 is not None:
        print(f"  Weight grad norm: {weight_grad2.norm().item():.6e}")
    else:
        print(f"  âŒ Weight grad: None")
    
    # å°æ¯”
    print("\nğŸ“ˆ å°æ¯”çµæœ:")
    if weight_grad1 is None and weight_grad2 is not None:
        print("  âŒ ç‰ˆæœ¬ 1 (.detach()) ç„¡æ³•ç”¢ç”Ÿæ¢¯åº¦ï¼")
        print("  âœ… ç‰ˆæœ¬ 2 (ç„¡ .detach()) å¯ä»¥ç”¢ç”Ÿæ¢¯åº¦ï¼")
        return False
    elif weight_grad1 is not None and weight_grad2 is not None:
        diff = (weight_grad1 - weight_grad2).abs().max().item()
        print(f"  æ¢¯åº¦å·®ç•°: {diff:.6e}")
        if diff > 1e-6:
            print("  âš ï¸  å…©ç‰ˆæœ¬æ¢¯åº¦ä¸åŒï¼")
        else:
            print("  âœ… å…©ç‰ˆæœ¬æ¢¯åº¦ä¸€è‡´")
        return True
    else:
        print("  âŒ å…©ç‰ˆæœ¬éƒ½ç„¡æ³•ç”¢ç”Ÿæ¢¯åº¦ï¼")
        return False


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("ğŸš¨ è¨ºæ–· .detach() å°è‡´çš„æ¢¯åº¦æ–·é–‹å•é¡Œ")
    print("=" * 80)
    
    # é‹è¡Œæ¸¬è©¦
    test1_pass = test_gradient_flow()
    test_detach_branch_trigger()
    test2_pass = test_gradient_with_vs_without_detach()
    
    print("\n" + "=" * 80)
    print("ğŸ“Š è¨ºæ–·ç¸½çµ")
    print("=" * 80)
    print(f"  æ¸¬è©¦ 1 (æ¢¯åº¦æµå‹•): {'âœ… PASS' if test1_pass else 'âŒ FAIL'}")
    print(f"  æ¸¬è©¦ 3 (æœ‰ç„¡ .detach() å°æ¯”): {'âœ… PASS' if test2_pass else 'âŒ FAIL'}")
    
    if not test1_pass:
        print("\nâš ï¸  çµè«–: ç•¶å‰ç‰ˆæœ¬çš„ compute_derivatives_safe() å¯èƒ½é˜»æ–·æ¢¯åº¦ï¼")
        print("  å»ºè­°: ç§»é™¤ Line 36-38 çš„ .detach() èª¿ç”¨")
    else:
        print("\nâœ… çµè«–: æ¢¯åº¦æµå‹•æ­£å¸¸")
