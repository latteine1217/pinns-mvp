#!/usr/bin/env python3
"""
æ·±åº¦è¨ºæ–·å¾®åˆ†è¨ˆç®—å•é¡Œ
åˆ†æ compute_derivatives_3d_temporal å‡½æ•¸ä¸­çš„å¼µé‡å½¢ç‹€å’Œé›¶å€¼å•é¡Œ
"""

import sys
from pathlib import Path
import torch
import torch.autograd as autograd
import logging

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_basic_autograd():
    """æ¸¬è©¦åŸºç¤çš„ PyTorch è‡ªå‹•å¾®åˆ†åŠŸèƒ½"""
    logger.info("=== æ¸¬è©¦åŸºç¤è‡ªå‹•å¾®åˆ†åŠŸèƒ½ ===")
    
    # å‰µå»ºæ¸¬è©¦å‡½æ•¸ f = x^2 + y^2
    x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
    y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)
    
    coords = torch.stack([x, y], dim=1)  # [3, 2]
    logger.info(f"è¼¸å…¥åº§æ¨™: shape={coords.shape}")
    
    # ç°¡å–®å‡½æ•¸
    f = torch.sum(coords**2, dim=1, keepdim=True)  # [3, 1]
    logger.info(f"å‡½æ•¸å€¼: shape={f.shape}, values={f.squeeze()}")
    
    # è¨ˆç®—æ¢¯åº¦
    grad_outputs = torch.ones_like(f)
    grads = autograd.grad(
        outputs=f, 
        inputs=coords,
        grad_outputs=grad_outputs,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
        allow_unused=True
    )
    
    if grads[0] is not None:
        grad_result = grads[0]
        logger.info(f"æ¢¯åº¦çµæœ: shape={grad_result.shape}")
        logger.info(f"æ¢¯åº¦å€¼: {grad_result}")
        logger.info(f"âœ… åŸºç¤è‡ªå‹•å¾®åˆ†æ­£å¸¸")
        return True
    else:
        logger.error("âŒ åŸºç¤è‡ªå‹•å¾®åˆ†å¤±æ•—")
        return False

def test_current_derivative_function():
    """æ¸¬è©¦ç•¶å‰çš„ compute_derivatives_3d_temporal å‡½æ•¸"""
    logger.info("=== æ¸¬è©¦ç•¶å‰å¾®åˆ†å‡½æ•¸ ===")
    
    from pinnx.physics.ns_3d_temporal import compute_derivatives_3d_temporal
    
    # å‰µå»º4Dæ¸¬è©¦é»
    batch_size = 4
    t = torch.tensor([0.1, 0.2, 0.3, 0.4], requires_grad=True).unsqueeze(1)  # [4, 1]
    x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True).unsqueeze(1)  # [4, 1]
    y = torch.tensor([0.5, 1.0, 1.5, 2.0], requires_grad=True).unsqueeze(1)  # [4, 1]
    z = torch.tensor([0.2, 0.4, 0.6, 0.8], requires_grad=True).unsqueeze(1)  # [4, 1]
    
    coords = torch.cat([t, x, y, z], dim=1)  # [4, 4] = [t, x, y, z]
    logger.info(f"åº§æ¨™: shape={coords.shape}")
    logger.info(f"åº§æ¨™å…§å®¹:\n{coords}")
    
    # ç°¡å–®æ¸¬è©¦å‡½æ•¸ f = x^2 + y^2 + z^2 + t^2
    f = torch.sum(coords**2, dim=1, keepdim=True)  # [4, 1]
    logger.info(f"æ¸¬è©¦å‡½æ•¸: shape={f.shape}")
    logger.info(f"å‡½æ•¸å€¼: {f.squeeze()}")
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦æ¢¯åº¦
    logger.info(f"åº§æ¨™éœ€è¦æ¢¯åº¦: {coords.requires_grad}")
    logger.info(f"å‡½æ•¸éœ€è¦æ¢¯åº¦: {f.requires_grad}")
    
    # æ¸¬è©¦ä¸€éšåå¾®åˆ† - å…¨éƒ¨åˆ†é‡
    try:
        first_derivs_all = compute_derivatives_3d_temporal(f, coords, order=1, component=None)
        logger.info(f"ä¸€éšåå¾®åˆ† (å…¨éƒ¨): shape={first_derivs_all.shape}")
        logger.info(f"ä¸€éšåå¾®åˆ†å€¼:\n{first_derivs_all}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºé›¶
        if torch.allclose(first_derivs_all, torch.zeros_like(first_derivs_all)):
            logger.warning("âš ï¸ ä¸€éšåå¾®åˆ†ç‚ºé›¶ï¼")
        else:
            logger.info("âœ… ä¸€éšåå¾®åˆ†éé›¶")
    except Exception as e:
        logger.error(f"âŒ ä¸€éšåå¾®åˆ†è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # æ¸¬è©¦ä¸€éšåå¾®åˆ† - å–®å€‹åˆ†é‡
    try:
        for i in range(4):
            component_name = ['t', 'x', 'y', 'z'][i]
            first_deriv_i = compute_derivatives_3d_temporal(f, coords, order=1, component=i)
            logger.info(f"âˆ‚f/âˆ‚{component_name}: shape={first_deriv_i.shape}, å€¼={first_deriv_i.squeeze()}")
    except Exception as e:
        logger.error(f"âŒ å–®åˆ†é‡åå¾®åˆ†è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def test_simple_derivative_implementation():
    """æ¸¬è©¦ç°¡åŒ–çš„å¾®åˆ†å¯¦ç¾"""
    logger.info("=== æ¸¬è©¦ç°¡åŒ–å¾®åˆ†å¯¦ç¾ ===")
    
    def simple_grad(f, x, component=None):
        """ç°¡åŒ–çš„æ¢¯åº¦è¨ˆç®—"""
        grad_outputs = torch.ones_like(f)
        grads = autograd.grad(
            outputs=f.sum() if f.shape[1] > 1 else f,  # ç¢ºä¿æ¨™é‡è¼¸å‡º
            inputs=x,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )
        
        if grads[0] is None:
            return torch.zeros_like(x)
        
        grad_result = grads[0]
        
        if component is not None:
            return grad_result[:, component:component+1]
        return grad_result
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    batch_size = 4
    coords = torch.tensor([
        [0.1, 1.0, 0.5, 0.2],
        [0.2, 2.0, 1.0, 0.4],
        [0.3, 3.0, 1.5, 0.6],
        [0.4, 4.0, 2.0, 0.8]
    ], requires_grad=True)  # [4, 4]
    
    # æ¸¬è©¦å‡½æ•¸ f = x^2 + y^2 (åªä½¿ç”¨x,yåˆ†é‡)
    f = coords[:, 1:2]**2 + coords[:, 2:3]**2  # [4, 1]
    
    logger.info(f"åº§æ¨™: shape={coords.shape}")
    logger.info(f"å‡½æ•¸: shape={f.shape}, å€¼={f.squeeze()}")
    
    # æ¸¬è©¦ç°¡åŒ–æ¢¯åº¦è¨ˆç®—
    try:
        grad_all = simple_grad(f, coords)
        logger.info(f"ç°¡åŒ–æ¢¯åº¦ (å…¨éƒ¨): shape={grad_all.shape}")
        logger.info(f"ç°¡åŒ–æ¢¯åº¦å€¼:\n{grad_all}")
        
        # åˆ†é‡æ¸¬è©¦
        for i in range(4):
            grad_i = simple_grad(f, coords, component=i)
            component_name = ['t', 'x', 'y', 'z'][i]
            logger.info(f"âˆ‚f/âˆ‚{component_name}: shape={grad_i.shape}, å€¼={grad_i.squeeze()}")
            
        logger.info("âœ… ç°¡åŒ–å¾®åˆ†å¯¦ç¾æ­£å¸¸")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ç°¡åŒ–å¾®åˆ†å¯¦ç¾å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_neural_network_gradients():
    """æ¸¬è©¦ç¥ç¶“ç¶²è·¯è¼¸å‡ºçš„å¾®åˆ†"""
    logger.info("=== æ¸¬è©¦ç¥ç¶“ç¶²è·¯æ¢¯åº¦ ===")
    
    import torch.nn as nn
    
    # å‰µå»ºç°¡å–®ç¥ç¶“ç¶²è·¯
    model = nn.Sequential(
        nn.Linear(4, 32),
        nn.Tanh(),
        nn.Linear(32, 1)
    )
    
    # æ¸¬è©¦æ•¸æ“š
    coords = torch.tensor([
        [0.1, 1.0, 0.5, 0.2],
        [0.2, 2.0, 1.0, 0.4],
        [0.3, 3.0, 1.5, 0.6],
        [0.4, 4.0, 2.0, 0.8]
    ], requires_grad=True)
    
    logger.info(f"è¼¸å…¥åº§æ¨™: shape={coords.shape}, requires_grad={coords.requires_grad}")
    
    # æ¨¡å‹é æ¸¬
    output = model(coords)
    logger.info(f"æ¨¡å‹è¼¸å‡º: shape={output.shape}, å€¼={output.squeeze()}")
    
    # è¨ˆç®—æ¢¯åº¦
    try:
        grad_outputs = torch.ones_like(output)
        grads = autograd.grad(
            outputs=output,
            inputs=coords,
            grad_outputs=grad_outputs,
            create_graph=True,
            retain_graph=True,
            only_inputs=True,
            allow_unused=True
        )
        
        if grads[0] is not None:
            grad_result = grads[0]
            logger.info(f"ç¥ç¶“ç¶²è·¯æ¢¯åº¦: shape={grad_result.shape}")
            logger.info(f"æ¢¯åº¦å€¼:\n{grad_result}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºé›¶
            grad_norm = torch.norm(grad_result)
            logger.info(f"æ¢¯åº¦ç¯„æ•¸: {grad_norm.item()}")
            
            if grad_norm < 1e-10:
                logger.warning("âš ï¸ ç¥ç¶“ç¶²è·¯æ¢¯åº¦æ¥è¿‘é›¶")
            else:
                logger.info("âœ… ç¥ç¶“ç¶²è·¯æ¢¯åº¦æ­£å¸¸")
                
            return True
        else:
            logger.error("âŒ ç¥ç¶“ç¶²è·¯æ¢¯åº¦ç‚º None")
            return False
            
    except Exception as e:
        logger.error(f"âŒ ç¥ç¶“ç¶²è·¯æ¢¯åº¦è¨ˆç®—å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»è¨ºæ–·å‡½æ•¸"""
    logger.info("ğŸ” é–‹å§‹å¾®åˆ†è¨ˆç®—æ·±åº¦è¨ºæ–·")
    
    results = {}
    
    try:
        # 1. åŸºç¤è‡ªå‹•å¾®åˆ†æ¸¬è©¦
        logger.info("\n" + "="*60)
        results['basic_autograd'] = test_basic_autograd()
        
        # 2. ç•¶å‰å¾®åˆ†å‡½æ•¸æ¸¬è©¦
        logger.info("\n" + "="*60)
        results['current_function'] = test_current_derivative_function()
        
        # 3. ç°¡åŒ–å¾®åˆ†å¯¦ç¾æ¸¬è©¦
        logger.info("\n" + "="*60)
        results['simplified_implementation'] = test_simple_derivative_implementation()
        
        # 4. ç¥ç¶“ç¶²è·¯æ¢¯åº¦æ¸¬è©¦
        logger.info("\n" + "="*60)
        results['neural_network_gradients'] = test_neural_network_gradients()
        
        # ç¸½çµ
        logger.info("\n" + "="*60)
        logger.info("ğŸ“‹ å¾®åˆ†è¨ˆç®—è¨ºæ–·ç¸½çµ:")
        
        for test_name, success in results.items():
            status = "âœ… é€šé" if success else "âŒ å¤±æ•—"
            logger.info(f"   {test_name}: {status}")
            
        return results
        
    except Exception as e:
        logger.error(f"âŒ è¨ºæ–·å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return results

if __name__ == "__main__":
    results = main()