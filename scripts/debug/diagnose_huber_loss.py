#!/usr/bin/env python3
"""
è¨ºæ–· Huber æå¤±åœ¨ Phase 6C ä¸­çš„æ•¸å€¼è¡Œç‚º

Purpose:
    åˆ†æ PyTorch smooth_l1_loss åœ¨å¯¦éš› Î½_t/Î½ åˆ†å¸ƒä¸‹çš„æ•¸å€¼è¡¨ç¾
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

def compute_huber_loss(nu_t_ratio, target, beta):
    """æ‰‹å‹•è¨ˆç®— Huber æå¤±"""
    diff = torch.abs(nu_t_ratio - target)
    mask = diff < beta
    
    # å¹³æ–¹å€
    loss_quad = 0.5 * (nu_t_ratio - target)**2 / beta
    # ç·šæ€§å€
    loss_linear = diff - 0.5 * beta
    
    loss = torch.where(mask, loss_quad, loss_linear)
    return loss, mask

def main():
    print("=" * 80)
    print("ğŸ” Huber æå¤±è¨ºæ–·ï¼šPhase 6C-v1 vs Phase 6C-v2")
    print("=" * 80)
    
    # æ¨¡æ“¬å¯¦éš›çš„ Î½_t/Î½ åˆ†å¸ƒï¼ˆæ ¹æ“šè¨“ç·´æ—¥èªŒï¼‰
    # Phase 6C-v2 @ Epoch 0: Î½_t penalty mean=656.5614 (å–®é»)
    # å°æ‡‰çš„ Î½_t/Î½ ç¯„åœï¼šéœ€åæ¨
    
    # æ¸¬è©¦æ¡ˆä¾‹ 1ï¼šå…¸å‹å€¼
    target = 100.0
    beta_v1 = 100.0
    beta_v2 = 1000.0
    
    # æ¨¡æ“¬ Î½_t/Î½ ç¯„åœ [0, 2000]
    nu_t_ratios = torch.linspace(0, 2000, 1000)
    
    print(f"\nğŸ“Š é…ç½®ï¼š")
    print(f"   Target: {target:.1f}")
    print(f"   Beta (v1): {beta_v1:.1f}")
    print(f"   Beta (v2): {beta_v2:.1f}")
    
    # è¨ˆç®—æå¤±
    loss_v1_pytorch = F.smooth_l1_loss(
        nu_t_ratios, 
        torch.full_like(nu_t_ratios, target), 
        beta=beta_v1, 
        reduction='none'
    )
    
    loss_v2_pytorch = F.smooth_l1_loss(
        nu_t_ratios, 
        torch.full_like(nu_t_ratios, target), 
        beta=beta_v2, 
        reduction='none'
    )
    
    loss_log1p = torch.log1p(torch.relu(nu_t_ratios - target))
    
    # æ‰‹å‹•è¨ˆç®—é©—è­‰
    loss_v2_manual, mask_v2 = compute_huber_loss(nu_t_ratios, target, beta_v2)
    
    print(f"\nâœ… é©—è­‰ï¼šPyTorch vs æ‰‹å‹•è¨ˆç®—")
    print(f"   Max diff: {(loss_v2_pytorch - loss_v2_manual).abs().max():.2e}")
    
    # é—œéµé»åˆ†æ
    test_points = [500, 700, 900, 1100, 1200, 1500]
    print(f"\nğŸ“Š é—œéµé»åˆ†æï¼ˆtarget={target}ï¼‰ï¼š")
    print(f"{'Î½_t/Î½':>8} | {'Î²=100 (v1)':>12} | {'Î²=1000 (v2)':>13} | {'log1p':>10} | {'Ratio v2/v1':>12}")
    print("-" * 80)
    
    for nu_t_val in test_points:
        idx = (nu_t_ratios - nu_t_val).abs().argmin()
        loss_v1 = loss_v1_pytorch[idx].item()
        loss_v2 = loss_v2_pytorch[idx].item()
        loss_log = loss_log1p[idx].item()
        ratio = loss_v2 / loss_v1 if loss_v1 > 0 else float('inf')
        
        print(f"{nu_t_val:8.1f} | {loss_v1:12.2f} | {loss_v2:13.2f} | {loss_log:10.2f} | {ratio:12.2f}")
    
    # åæ¨ï¼šå¦‚æœå–®é»å¹³å‡æå¤± = 656.56ï¼Œå°æ‡‰çš„ Î½_t/Î½ æ˜¯å¤šå°‘ï¼Ÿ
    print(f"\nğŸ” åæ¨åˆ†æï¼š")
    print(f"   ç›®æ¨™ï¼šæ‰¾åˆ°ä½¿ Huber loss â‰ˆ 656.56 çš„ Î½_t/Î½ å€¼")
    
    target_loss = 656.56
    # ä½¿ç”¨ Î²=1000
    idx_v2 = (loss_v2_pytorch - target_loss).abs().argmin()
    nu_t_inferred = nu_t_ratios[idx_v2].item()
    
    print(f"   çµæœï¼šÎ½_t/Î½ â‰ˆ {nu_t_inferred:.1f}")
    print(f"   é©—è­‰ï¼šHuber(Î²=1000) = {loss_v2_pytorch[idx_v2].item():.2f}")
    print(f"   å°æ‡‰ log1p = {loss_log1p[idx_v2].item():.2f}")
    
    # ç¹ªåœ–
    plt.figure(figsize=(14, 5))
    
    # å­åœ– 1ï¼šæå¤±æ›²ç·š
    plt.subplot(1, 2, 1)
    plt.plot(nu_t_ratios.numpy(), loss_v1_pytorch.numpy(), 
             label=f'Huber (Î²={beta_v1})', linewidth=2, alpha=0.7)
    plt.plot(nu_t_ratios.numpy(), loss_v2_pytorch.numpy(), 
             label=f'Huber (Î²={beta_v2})', linewidth=2, alpha=0.7)
    plt.plot(nu_t_ratios.numpy(), loss_log1p.numpy(), 
             label='log1p (Phase 6B)', linewidth=2, linestyle='--', alpha=0.7)
    
    plt.axhline(y=656.56, color='red', linestyle=':', label='å¯¦éš›è§€æ¸¬å€¼ (656.56)')
    plt.axvline(x=target, color='green', linestyle=':', alpha=0.5, label='Target (100)')
    
    plt.xlabel('Î½_t/Î½')
    plt.ylabel('Loss Value')
    plt.title('Penalty Function Comparison')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 2000)
    plt.ylim(0, 1000)
    
    # å­åœ– 2ï¼šæ¢¯åº¦æ›²ç·š
    plt.subplot(1, 2, 2)
    
    # æ•¸å€¼è¨ˆç®—æ¢¯åº¦
    spacing = float(nu_t_ratios[1] - nu_t_ratios[0])
    grad_v1 = torch.gradient(loss_v1_pytorch, spacing=(spacing,))[0]
    grad_v2 = torch.gradient(loss_v2_pytorch, spacing=(spacing,))[0]
    grad_log1p = 1.0 / (1.0 + torch.relu(nu_t_ratios - target))
    
    plt.plot(nu_t_ratios.numpy(), grad_v1.numpy(), 
             label=f'âˆ‚Huber/âˆ‚Î½_t (Î²={beta_v1})', linewidth=2, alpha=0.7)
    plt.plot(nu_t_ratios.numpy(), grad_v2.numpy(), 
             label=f'âˆ‚Huber/âˆ‚Î½_t (Î²={beta_v2})', linewidth=2, alpha=0.7)
    plt.plot(nu_t_ratios.numpy(), grad_log1p.numpy(), 
             label='âˆ‚log1p/âˆ‚Î½_t', linewidth=2, linestyle='--', alpha=0.7)
    
    plt.axvline(x=target, color='green', linestyle=':', alpha=0.5, label='Target (100)')
    plt.xlabel('Î½_t/Î½')
    plt.ylabel('Gradient')
    plt.title('Gradient Comparison (Non-saturation Check)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xlim(0, 2000)
    plt.ylim(0, 2)
    
    plt.tight_layout()
    output_path = '/Users/latteine/Documents/coding/pinns-mvp/results/debug_huber_loss_analysis.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… åœ–è¡¨å·²ä¿å­˜ï¼š{output_path}")
    
    # ç¸½çµåˆ†æ
    print(f"\n" + "=" * 80)
    print("ğŸ“ è¨ºæ–·çµè«–ï¼š")
    print("=" * 80)
    
    # è¨ˆç®—å¹³å‡æå¤±ï¼ˆæ¨¡æ“¬ 2048 å€‹æ¡æ¨£é»ï¼‰
    # å‡è¨­ Î½_t/Î½ ~ Uniform[500, 1200]ï¼ˆæ ¹æ“šè¨“ç·´æ—¥èªŒæ¨æ¸¬ï¼‰
    nu_t_sample = torch.linspace(500, 1200, 2048)
    loss_v2_sample = F.smooth_l1_loss(
        nu_t_sample, 
        torch.full_like(nu_t_sample, target), 
        beta=beta_v2, 
        reduction='mean'
    )
    
    print(f"1. æ¨¡æ“¬æ¡æ¨£ï¼ˆÎ½_t/Î½ âˆˆ [500, 1200]ï¼ŒN=2048ï¼‰ï¼š")
    print(f"   - å¹³å‡ Huber æå¤±ï¼ˆÎ²=1000ï¼‰ï¼š{loss_v2_sample.item():.2f}")
    print(f"   - å°æ‡‰ turbulent_viscosity_lossï¼š{loss_v2_sample.item():.2f}ï¼ˆæ‡‰èˆ‡æ—¥èªŒä¸­çš„ 431K æ¯”è¼ƒï¼‰")
    
    print(f"\n2. å•é¡Œè¨ºæ–·ï¼š")
    if loss_v2_sample.item() > 100:
        print(f"   âš ï¸ å³ä½¿ Î²=1000ï¼Œæå¤±ä»éå¤§ï¼ˆ>{loss_v2_sample.item():.0f}ï¼‰")
        print(f"   âŒ åŸå› ï¼šå¯¦éš› Î½_t/Î½ é è¶… target=100ï¼ˆå¯èƒ½åœ¨ 700-1100 ç¯„åœï¼‰")
        print(f"   ğŸ’¡ å»ºè­°ï¼š")
        print(f"      (1) æå‡ target: 100 â†’ 500-800ï¼ˆæ¥è¿‘å¯¦éš›ä¸­å¿ƒå€¼ï¼‰")
        print(f"      (2) æˆ–å¤§å¹…æå‡ Î²: 1000 â†’ 5000ï¼ˆä½¿æ›´å¤šå€åŸŸè™•æ–¼å¹³æ–¹å€ï¼‰")
        print(f"      (3) æˆ–é™ä½æ¬Šé‡ï¼š0.001 â†’ 0.0001ï¼ˆ10 å€ï¼‰")
    else:
        print(f"   âœ… æå¤±åœ¨åˆç†ç¯„åœï¼ˆ<100ï¼‰")
    
    print(f"\n3. æ¢¯åº¦åˆ†æï¼š")
    grad_at_1000 = grad_v2[(nu_t_ratios - 1000).abs().argmin()].item()
    grad_log1p_at_1000 = grad_log1p[(nu_t_ratios - 1000).abs().argmin()].item()
    print(f"   - Huber æ¢¯åº¦ @ Î½_t/Î½=1000ï¼ˆÎ²=1000ï¼‰ï¼š{grad_at_1000:.4f}")
    print(f"   - log1p æ¢¯åº¦ @ Î½_t/Î½=1000ï¼š{grad_log1p_at_1000:.6f}")
    print(f"   - æ¢¯åº¦æå‡å€æ•¸ï¼š{grad_at_1000/grad_log1p_at_1000:.1f}Ã—")
    
    if grad_at_1000 > 0.5:
        print(f"   âœ… Huber æ¢¯åº¦æœªé£½å’Œï¼ˆ>{grad_at_1000:.2f}ï¼‰")
    else:
        print(f"   âš ï¸ Huber æ¢¯åº¦å¯èƒ½ä»åå°ï¼ˆ<{grad_at_1000:.2f}ï¼‰")

if __name__ == "__main__":
    main()
