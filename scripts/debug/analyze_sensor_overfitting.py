#!/usr/bin/env python3
"""
é©—è­‰å‡è¨­Cï¼šK=50æ„Ÿæ¸¬é»éæ“¬åˆåˆ†æ

åˆ†æK=50æ„Ÿæ¸¬é»çš„çµ±è¨ˆåˆ†å¸ƒï¼Œæª¢æŸ¥æ˜¯å¦éåº¦é›†ä¸­åœ¨ä½é€Ÿå€åŸŸ
å°è‡´æ¨¡å‹é æ¸¬ç¯„åœå¡Œç¸®
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import yaml

def analyze_sensor_overfitting(checkpoint_path, config_path):
    """åˆ†æK=50æ„Ÿæ¸¬é»éæ“¬åˆå•é¡Œ"""
    
    from scripts.train import create_model, get_device
    from pinnx.dataio.channel_flow_loader import ChannelFlowLoader
    
    device = get_device('auto')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    print("="*70)
    
    # === è¼‰å…¥æ¨¡å‹ ===
    print("\n[1/4] è¼‰å…¥æ¨¡å‹...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if 'config' in checkpoint:
        config = checkpoint['config']
        print("  âœ“ å¾ checkpoint è¼‰å…¥é…ç½®")
    elif config_path:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        print(f"  âœ“ å¾æ–‡ä»¶è¼‰å…¥é…ç½®: {config_path}")
    else:
        raise ValueError("éœ€è¦æä¾›é…ç½®")
    
    model = create_model(config, device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    epoch = checkpoint.get('epoch', 'Unknown')
    print(f"  âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ (epoch: {epoch})")
    
    # === è¼‰å…¥æ•¸æ“š ===
    print("\n[2/4] è¼‰å…¥æ•¸æ“š...")
    loader = ChannelFlowLoader(config_path=config_path)
    
    # å…¨å ´æ•¸æ“š
    channel_data = loader.load_full_field_data()
    full_coords = torch.from_numpy(channel_data.sensor_points).float().to(device)
    full_u = torch.from_numpy(channel_data.sensor_data['u']).float().to(device)
    full_v = torch.from_numpy(channel_data.sensor_data['v']).float().to(device)
    full_p = torch.from_numpy(channel_data.sensor_data['p']).float().to(device)
    
    # K=50 æ„Ÿæ¸¬é»æ•¸æ“š
    sensor_data = loader.load_sensor_data(strategy='qr_pivot', K=50)
    sensor_coords = torch.from_numpy(sensor_data.sensor_points).float().to(device)
    sensor_u = torch.from_numpy(sensor_data.sensor_data['u']).float().to(device)
    sensor_v = torch.from_numpy(sensor_data.sensor_data['v']).float().to(device)
    sensor_p = torch.from_numpy(sensor_data.sensor_data['p']).float().to(device)
    
    print(f"  âœ“ å…¨å ´æ•¸æ“šé»æ•¸: {full_coords.shape[0]}")
    print(f"  âœ“ K=50 æ„Ÿæ¸¬é»æ•¸: {sensor_coords.shape[0]}")
    
    # === çµ±è¨ˆåˆ†æ ===
    print("\n[3/4] æ„Ÿæ¸¬é»çµ±è¨ˆåˆ†æ...")
    
    # è½‰ç‚ºnumpyä»¥ä¾¿åˆ†æ
    full_coords_np = full_coords.cpu().numpy()
    full_u_np = full_u.cpu().numpy()
    full_v_np = full_v.cpu().numpy()
    full_p_np = full_p.cpu().numpy()
    
    sensor_coords_np = sensor_coords.cpu().numpy()
    sensor_u_np = sensor_u.cpu().numpy()
    sensor_v_np = sensor_v.cpu().numpy()
    sensor_p_np = sensor_p.cpu().numpy()
    
    # è¨ˆç®—çµ±è¨ˆé‡
    def compute_stats(data, name):
        return {
            'min': np.min(data),
            'max': np.max(data),
            'mean': np.mean(data),
            'std': np.std(data),
            'median': np.median(data),
            'p25': np.percentile(data, 25),
            'p75': np.percentile(data, 75),
            'range': np.max(data) - np.min(data)
        }
    
    # å„å ´çš„çµ±è¨ˆ
    full_u_stats = compute_stats(full_u_np, "å…¨å ´ u")
    sensor_u_stats = compute_stats(sensor_u_np, "æ„Ÿæ¸¬ u")
    
    full_v_stats = compute_stats(full_v_np, "å…¨å ´ v")
    sensor_v_stats = compute_stats(sensor_v_np, "æ„Ÿæ¸¬ v")
    
    full_p_stats = compute_stats(full_p_np, "å…¨å ´ p")
    sensor_p_stats = compute_stats(sensor_p_np, "æ„Ÿæ¸¬ p")
    
    print("="*70)
    print("çµ±è¨ˆå°æ¯”åˆ†æ:")
    print("="*70)
    
    def print_comparison(field, full_stats, sensor_stats):
        print(f"\n--- {field} velocity ---")
        print(f"                  å…¨å ´              æ„Ÿæ¸¬é»           è¦†è“‹ç‡")
        print(f"ç¯„åœ:    [{full_stats['min']:.3f}, {full_stats['max']:.3f}]  [{sensor_stats['min']:.3f}, {sensor_stats['max']:.3f}]  {100*sensor_stats['range']/full_stats['range']:.1f}%")
        print(f"å‡å€¼:    {full_stats['mean']:.3f}              {sensor_stats['mean']:.3f}             {100*abs(sensor_stats['mean']-full_stats['mean'])/abs(full_stats['mean']):.1f}% diff")
        print(f"æ¨™æº–å·®:  {full_stats['std']:.3f}              {sensor_stats['std']:.3f}             {100*sensor_stats['std']/full_stats['std']:.1f}%")
        print(f"ä¸­ä½æ•¸:  {full_stats['median']:.3f}              {sensor_stats['median']:.3f}")
    
    print_comparison("u", full_u_stats, sensor_u_stats)
    print_comparison("v", full_v_stats, sensor_v_stats)
    print_comparison("p", full_p_stats, sensor_p_stats)
    
    # åˆ†æé€Ÿåº¦å€é–“è¦†è“‹
    print("\n--- é€Ÿåº¦å€é–“è¦†è“‹åˆ†æ ---")
    
    # å®šç¾©é€Ÿåº¦å€é–“
    u_bins = [0, 2, 5, 8, 12, 16.5]
    bin_labels = ['0-2', '2-5', '5-8', '8-12', '12-16.5']
    
    full_u_hist, _ = np.histogram(full_u_np, bins=u_bins)
    sensor_u_hist, _ = np.histogram(sensor_u_np, bins=u_bins)
    
    full_u_frac = full_u_hist / len(full_u_np)
    sensor_u_frac = sensor_u_hist / len(sensor_u_np)
    
    print("é€Ÿåº¦å€é–“     å…¨å ´åˆ†å¸ƒ    æ„Ÿæ¸¬é»åˆ†å¸ƒ   é/æ¬ è¡¨ç¤º")
    for i, label in enumerate(bin_labels):
        if full_u_frac[i] > 0:
            ratio = sensor_u_frac[i] / full_u_frac[i]
            status = "éè¡¨ç¤º" if ratio > 1.5 else "æ¬ è¡¨ç¤º" if ratio < 0.5 else "æ­£å¸¸"
            print(f"u âˆˆ [{label:>8}]: {full_u_frac[i]:>8.1%}    {sensor_u_frac[i]:>10.1%}    {ratio:>5.2f}x ({status})")
        else:
            print(f"u âˆˆ [{label:>8}]: {full_u_frac[i]:>8.1%}    {sensor_u_frac[i]:>10.1%}    N/A")
    
    # ç©ºé–“åˆ†å¸ƒåˆ†æ
    print("\n--- ç©ºé–“åˆ†å¸ƒåˆ†æ ---")
    
    # yæ–¹å‘åˆ†å¸ƒ
    y_bins = [-1.0, -0.5, 0.0, 0.5, 1.0]
    y_labels = ['ä¸‹å±¤(-1~-0.5)', 'ä¸­ä¸‹(-0.5~0)', 'ä¸­ä¸Š(0~0.5)', 'ä¸Šå±¤(0.5~1)']
    
    full_y_hist, _ = np.histogram(full_coords_np[:, 1], bins=y_bins)
    sensor_y_hist, _ = np.histogram(sensor_coords_np[:, 1], bins=y_bins)
    
    full_y_frac = full_y_hist / len(full_coords_np)
    sensor_y_frac = sensor_y_hist / len(sensor_coords_np)
    
    print("y ä½ç½®å€é–“      å…¨å ´åˆ†å¸ƒ    æ„Ÿæ¸¬é»åˆ†å¸ƒ   é/æ¬ è¡¨ç¤º")
    for i, label in enumerate(y_labels):
        if full_y_frac[i] > 0:
            ratio = sensor_y_frac[i] / full_y_frac[i]
            status = "éè¡¨ç¤º" if ratio > 1.5 else "æ¬ è¡¨ç¤º" if ratio < 0.5 else "æ­£å¸¸"
            print(f"{label:>12}: {full_y_frac[i]:>8.1%}    {sensor_y_frac[i]:>10.1%}    {ratio:>5.2f}x ({status})")
        else:
            print(f"{label:>12}: {full_y_frac[i]:>8.1%}    {sensor_y_frac[i]:>10.1%}    N/A")
    
    # === æ¨¡å‹æ€§èƒ½åˆ†æ ===
    print("\n[4/4] æ¨¡å‹åœ¨æ„Ÿæ¸¬é»çš„æ€§èƒ½åˆ†æ...")
    
    # åœ¨æ„Ÿæ¸¬é»è™•é æ¸¬
    with torch.no_grad():
        sensor_pred = model(sensor_coords)
    
    sensor_pred_u = sensor_pred[:, 0].cpu().numpy()
    sensor_pred_v = sensor_pred[:, 1].cpu().numpy()
    sensor_pred_p = sensor_pred[:, 2].cpu().numpy()
    
    # è¨ˆç®—æ„Ÿæ¸¬é»èª¤å·®
    sensor_u_error = np.abs(sensor_pred_u - sensor_u_np)
    sensor_v_error = np.abs(sensor_pred_v - sensor_v_np)
    sensor_p_error = np.abs(sensor_pred_p - sensor_p_np)
    
    sensor_u_rel_error = sensor_u_error / (np.abs(sensor_u_np) + 1e-8)
    sensor_v_rel_error = sensor_v_error / (np.abs(sensor_v_np) + 1e-8)
    
    print(f"\næ„Ÿæ¸¬é»é æ¸¬èª¤å·®:")
    print(f"u çµ•å°èª¤å·®: mean={np.mean(sensor_u_error):.6f}, max={np.max(sensor_u_error):.6f}")
    print(f"v çµ•å°èª¤å·®: mean={np.mean(sensor_v_error):.6f}, max={np.max(sensor_v_error):.6f}")
    print(f"p çµ•å°èª¤å·®: mean={np.mean(sensor_p_error):.6f}, max={np.max(sensor_p_error):.6f}")
    
    print(f"u ç›¸å°èª¤å·®: mean={np.mean(sensor_u_rel_error):.3%}, median={np.median(sensor_u_rel_error):.3%}")
    print(f"v ç›¸å°èª¤å·®: mean={np.mean(sensor_v_rel_error):.3%}, median={np.median(sensor_v_rel_error):.3%}")
    
    # å…¨å ´é æ¸¬ï¼ˆç”¨æ–¼å°æ¯”ï¼‰
    with torch.no_grad():
        full_pred = model(full_coords)
    
    full_pred_u = full_pred[:, 0].cpu().numpy()
    full_pred_v = full_pred[:, 1].cpu().numpy()
    
    full_pred_u_stats = compute_stats(full_pred_u, "é æ¸¬ u")
    full_pred_v_stats = compute_stats(full_pred_v, "é æ¸¬ v")
    
    print(f"\næ¨¡å‹é æ¸¬ç¯„åœ:")
    print(f"u: [{full_pred_u_stats['min']:.3f}, {full_pred_u_stats['max']:.3f}] (åƒè€ƒ: [{full_u_stats['min']:.3f}, {full_u_stats['max']:.3f}])")
    print(f"v: [{full_pred_v_stats['min']:.3f}, {full_pred_v_stats['max']:.3f}] (åƒè€ƒ: [{full_v_stats['min']:.3f}, {full_v_stats['max']:.3f}])")
    
    range_coverage_u = full_pred_u_stats['range'] / full_u_stats['range']
    range_coverage_v = full_pred_v_stats['range'] / full_v_stats['range']
    
    print(f"é æ¸¬ç¯„åœè¦†è“‹ç‡: u={range_coverage_u:.1%}, v={range_coverage_v:.1%}")
    
    # === éæ“¬åˆåˆ†æçµè«– ===
    print("\n" + "="*70)
    print("éæ“¬åˆè¨ºæ–·çµè«–:")
    print("="*70)
    
    # æª¢æŸ¥æ˜¯å¦å­˜åœ¨éæ“¬åˆæŒ‡æ¨™
    overfitting_indicators = []
    
    # 1. æ„Ÿæ¸¬é»ç¯„åœè¦†è“‹ä¸è¶³
    u_coverage = sensor_u_stats['range'] / full_u_stats['range']
    v_coverage = sensor_v_stats['range'] / full_v_stats['range']
    
    if u_coverage < 0.7:
        overfitting_indicators.append(f"u æ„Ÿæ¸¬é»ç¯„åœè¦†è“‹ä¸è¶³ ({u_coverage:.1%})")
    if v_coverage < 0.7:
        overfitting_indicators.append(f"v æ„Ÿæ¸¬é»ç¯„åœè¦†è“‹ä¸è¶³ ({v_coverage:.1%})")
    
    # 2. æ„Ÿæ¸¬é»åå‘ä½é€Ÿå€
    high_speed_threshold = 10.0
    full_high_speed_frac = np.sum(full_u_np > high_speed_threshold) / len(full_u_np)
    sensor_high_speed_frac = np.sum(sensor_u_np > high_speed_threshold) / len(sensor_u_np)
    
    if full_high_speed_frac > 0 and sensor_high_speed_frac / full_high_speed_frac < 0.5:
        overfitting_indicators.append(f"é«˜é€Ÿå€åŸŸ (u>{high_speed_threshold}) æ¬ è¡¨ç¤º")
    
    # 3. æ„Ÿæ¸¬é»èª¤å·®éå°
    if np.mean(sensor_u_rel_error) < 0.05:  # 5%
        overfitting_indicators.append("æ„Ÿæ¸¬é»èª¤å·®ç•°å¸¸å°ï¼Œå¯èƒ½éæ“¬åˆ")
    
    # 4. é æ¸¬ç¯„åœåš´é‡å¡Œç¸®
    if range_coverage_u < 0.6 or range_coverage_v < 0.3:
        overfitting_indicators.append("é æ¸¬ç¯„åœåš´é‡å¡Œç¸®")
    
    if overfitting_indicators:
        print("âš ï¸  æª¢æ¸¬åˆ°éæ“¬åˆæŒ‡æ¨™ï¼š")
        for indicator in overfitting_indicators:
            print(f"   â€¢ {indicator}")
        
        hypothesis_confirmed = True
        print("\nğŸ”´ å‡è¨­ C ç¢ºèªï¼šK=50 æ„Ÿæ¸¬é»éæ“¬åˆæ˜¯ä¸»è¦å•é¡Œ")
    else:
        hypothesis_confirmed = False
        print("âœ… å‡è¨­ C å¦å®šï¼šæœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æ„Ÿæ¸¬é»éæ“¬åˆ")
    
    # === å¯è¦–åŒ– ===
    print("\nç”Ÿæˆéæ“¬åˆåˆ†æåœ–...")
    
    fig, axes = plt.subplots(3, 3, figsize=(18, 15))
    
    # ç¬¬ä¸€è¡Œï¼šåˆ†å¸ƒå°æ¯”
    axes[0, 0].hist(full_u_np, bins=50, alpha=0.7, label='Full Field', density=True)
    axes[0, 0].hist(sensor_u_np, bins=20, alpha=0.7, label='K=50 Sensors', density=True)
    axes[0, 0].set_xlabel('u velocity')
    axes[0, 0].set_title('u Velocity Distribution')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    axes[0, 1].hist(full_v_np, bins=50, alpha=0.7, label='Full Field', density=True)
    axes[0, 1].hist(sensor_v_np, bins=20, alpha=0.7, label='K=50 Sensors', density=True)
    axes[0, 1].set_xlabel('v velocity')
    axes[0, 1].set_title('v Velocity Distribution')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    axes[0, 2].hist(full_coords_np[:, 1], bins=50, alpha=0.7, label='Full Field', density=True)
    axes[0, 2].hist(sensor_coords_np[:, 1], bins=20, alpha=0.7, label='K=50 Sensors', density=True)
    axes[0, 2].set_xlabel('y coordinate')
    axes[0, 2].set_title('Spatial Distribution (y)')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # ç¬¬äºŒè¡Œï¼šæ„Ÿæ¸¬é»ä½ç½®å’Œå€¼
    scatter1 = axes[1, 0].scatter(full_coords_np[:, 0], full_coords_np[:, 1], 
                                  c=full_u_np, cmap='viridis', s=2, alpha=0.7)
    axes[1, 0].scatter(sensor_coords_np[:, 0], sensor_coords_np[:, 1], 
                       c='red', s=20, marker='x', alpha=1.0, label='K=50 Sensors')
    axes[1, 0].set_xlabel('x')
    axes[1, 0].set_ylabel('y')
    axes[1, 0].set_title('Sensor Locations on u Field')
    axes[1, 0].legend()
    plt.colorbar(scatter1, ax=axes[1, 0], label='u')
    
    # æ„Ÿæ¸¬é»èª¤å·®åˆ†å¸ƒ
    axes[1, 1].scatter(sensor_u_np, sensor_pred_u, alpha=0.7)
    axes[1, 1].plot([sensor_u_np.min(), sensor_u_np.max()], 
                    [sensor_u_np.min(), sensor_u_np.max()], 'r--', label='Perfect')
    axes[1, 1].set_xlabel('Reference u')
    axes[1, 1].set_ylabel('Predicted u')
    axes[1, 1].set_title('Sensor Point Accuracy (u)')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # é€Ÿåº¦å€é–“çµ±è¨ˆ
    x_pos = np.arange(len(bin_labels))
    width = 0.35
    axes[1, 2].bar(x_pos - width/2, full_u_frac, width, label='Full Field', alpha=0.7)
    axes[1, 2].bar(x_pos + width/2, sensor_u_frac, width, label='K=50 Sensors', alpha=0.7)
    axes[1, 2].set_xlabel('Velocity Bins')
    axes[1, 2].set_ylabel('Fraction')
    axes[1, 2].set_title('Velocity Range Coverage')
    axes[1, 2].set_xticks(x_pos)
    axes[1, 2].set_xticklabels(bin_labels)
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    # ç¬¬ä¸‰è¡Œï¼šé æ¸¬ç¯„åœå°æ¯”
    axes[2, 0].hist(full_u_np, bins=50, alpha=0.7, label='Reference', density=True)
    axes[2, 0].hist(full_pred_u, bins=50, alpha=0.7, label='Predicted', density=True)
    axes[2, 0].set_xlabel('u velocity')
    axes[2, 0].set_title('Prediction Range vs Reference')
    axes[2, 0].legend()
    axes[2, 0].grid(True, alpha=0.3)
    
    axes[2, 1].hist(full_v_np, bins=50, alpha=0.7, label='Reference', density=True)
    axes[2, 1].hist(full_pred_v, bins=50, alpha=0.7, label='Predicted', density=True)
    axes[2, 1].set_xlabel('v velocity')
    axes[2, 1].set_title('Prediction Range vs Reference')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)
    
    # ç¯„åœè¦†è“‹ç‡
    fields = ['u', 'v']
    coverages = [range_coverage_u, range_coverage_v]
    colors = ['red' if c < 0.6 else 'orange' if c < 0.8 else 'green' for c in coverages]
    
    axes[2, 2].bar(fields, coverages, color=colors, alpha=0.7)
    axes[2, 2].axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Full coverage')
    axes[2, 2].axhline(y=0.8, color='orange', linestyle=':', alpha=0.5, label='80% threshold')
    axes[2, 2].axhline(y=0.6, color='red', linestyle=':', alpha=0.5, label='60% threshold')
    axes[2, 2].set_ylabel('Range Coverage Ratio')
    axes[2, 2].set_title('Prediction Range Coverage')
    axes[2, 2].legend()
    axes[2, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜çµæœ
    output_dir = Path('evaluation_results_phase4b_diagnosis')
    output_dir.mkdir(exist_ok=True)
    
    plot_path = output_dir / f'sensor_overfitting_analysis_epoch{epoch}.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ“ éæ“¬åˆåˆ†æåœ–å·²ä¿å­˜: {plot_path}")
    
    # ä¿å­˜æ•¸å€¼å ±å‘Š
    report_path = output_dir / f'sensor_overfitting_epoch{epoch}.txt'
    with open(report_path, 'w') as f:
        f.write("="*70 + "\n")
        f.write("K=50 æ„Ÿæ¸¬é»éæ“¬åˆåˆ†æå ±å‘Š\n")
        f.write("="*70 + "\n\n")
        
        f.write(f"æ¨¡å‹: {checkpoint_path}\n")
        f.write(f"Epoch: {epoch}\n")
        f.write(f"é…ç½®: {config_path}\n\n")
        
        f.write("--- çµ±è¨ˆå°æ¯” ---\n")
        f.write(f"u ç¯„åœè¦†è“‹: {u_coverage:.1%}\n")
        f.write(f"v ç¯„åœè¦†è“‹: {v_coverage:.1%}\n")
        f.write(f"é æ¸¬ u ç¯„åœè¦†è“‹: {range_coverage_u:.1%}\n")
        f.write(f"é æ¸¬ v ç¯„åœè¦†è“‹: {range_coverage_v:.1%}\n\n")
        
        f.write("--- æ„Ÿæ¸¬é»èª¤å·® ---\n")
        f.write(f"u å¹³å‡ç›¸å°èª¤å·®: {np.mean(sensor_u_rel_error):.3%}\n")
        f.write(f"v å¹³å‡ç›¸å°èª¤å·®: {np.mean(sensor_v_rel_error):.3%}\n\n")
        
        f.write("--- éæ“¬åˆæŒ‡æ¨™ ---\n")
        if overfitting_indicators:
            for indicator in overfitting_indicators:
                f.write(f"âš ï¸  {indicator}\n")
        else:
            f.write("âœ… æœªæª¢æ¸¬åˆ°æ˜é¡¯éæ“¬åˆ\n")
        
        f.write(f"\n--- çµè«– ---\n")
        if hypothesis_confirmed:
            f.write("ğŸ”´ å‡è¨­ C ç¢ºèªï¼šK=50 æ„Ÿæ¸¬é»éæ“¬åˆæ˜¯ä¸»è¦å•é¡Œ\n")
        else:
            f.write("âœ… å‡è¨­ C å¦å®šï¼šæœªæª¢æ¸¬åˆ°æ˜é¡¯çš„æ„Ÿæ¸¬é»éæ“¬åˆ\n")
    
    print(f"âœ“ æ•¸å€¼å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    print("\n" + "="*70)
    print("å‡è¨­ C é©—è­‰å®Œæˆï¼")
    print("="*70)
    
    return {
        'hypothesis_confirmed': hypothesis_confirmed,
        'overfitting_indicators': overfitting_indicators,
        'u_coverage': u_coverage,
        'v_coverage': v_coverage,
        'range_coverage_u': range_coverage_u,
        'range_coverage_v': range_coverage_v,
        'sensor_u_error': np.mean(sensor_u_rel_error),
        'sensor_v_error': np.mean(sensor_v_rel_error)
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--checkpoint', type=str, required=True, 
                        help='Checkpoint æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--config', type=str, required=True,
                        help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    result = analyze_sensor_overfitting(args.checkpoint, args.config)
    
    print(f"\næœ€çµ‚çµè«–ï¼šå‡è¨­ C {'å·²ç¢ºèª' if result['hypothesis_confirmed'] else 'æœªç¢ºèª'}")
    print(f"éæ“¬åˆæŒ‡æ¨™æ•¸é‡: {len(result['overfitting_indicators'])}")