"""
ç¶œåˆè©•ä¼°è…³æœ¬ï¼š3D VS-PINN èˆ‡ JHTDB åƒè€ƒæ•¸æ“šæ¯”è¼ƒ

åŠŸèƒ½ï¼š
1. è¼‰å…¥è¨“ç·´å®Œæˆçš„æ¨¡å‹èˆ‡ JHTDB åƒè€ƒæ•¸æ“š
2. è¨ˆç®—å…¨é¢çš„ç‰©ç†é©—è­‰æŒ‡æ¨™ï¼ˆèª¤å·®ã€å®ˆæ†ã€èƒ½è­œã€å£å‰ªæ‡‰åŠ›ï¼‰
3. ç”Ÿæˆé«˜è³ªé‡å¯è¦–åŒ–ï¼ˆå ´æ¯”è¼ƒã€èª¤å·®åˆ†å¸ƒã€çµ±è¨ˆåœ–ï¼‰
4. è¼¸å‡ºçµæ§‹åŒ–è©•ä¼°å ±å‘Šï¼ˆMarkdown + JSONï¼‰
5. é©—è­‰æˆæ•—é–€æª»ï¼ˆç›¸å° L2 â‰¤ 10-15%ï¼Œçµ±è¨ˆæ”¹å–„ â‰¥ 30%ï¼‰

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python scripts/comprehensive_evaluation.py \
        --checkpoint checkpoints/vs_pinn_3d_full_training_latest.pth \
        --config configs/vs_pinn_3d_full_training.yml \
        --reference data/jhtdb/channel_flow_re1000/cutout3d_128x128x32.npz \
        --output_dir results/comprehensive_eval_<timestamp>
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import yaml
import logging
import json
from typing import Dict, Tuple, Optional, List
import argparse
from datetime import datetime

# è¨­ç½®æ¨£å¼
sns.set_context("paper", font_scale=1.5)
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 150
plt.rcParams['font.family'] = 'sans-serif'

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å°å…¥å°ˆæ¡ˆæ¨¡çµ„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

import pinnx
from pinnx.models.fourier_mlp import PINNNet
# from pinnx.physics.vs_pinn_channel_flow import create_vs_pinn_channel_flow  # Not needed for evaluation
from pinnx.evals.metrics import (
    relative_L2, rmse_metrics, field_statistics,
    energy_spectrum_1d, conservation_error
)
from pinnx.utils.denormalization import denormalize_output  # TASK-008: åæ¨™æº–åŒ–å·¥å…·


# ============================================================
# æ¨¡å‹è¼‰å…¥èˆ‡æ¨ç†
# ============================================================

def load_trained_model(checkpoint_path: Path, config: Dict, device: torch.device):
    """è¼‰å…¥è¨“ç·´å®Œæˆçš„æ¨¡å‹ï¼ˆå« physics ç‹€æ…‹æ¢å¾©ï¼‰"""
    logger.info(f"ğŸ“¥ Loading model from {checkpoint_path}")
    
    # ğŸ” STEP 1: é å…ˆæª¢æŸ¥æª¢æŸ¥é»æ¶æ§‹ï¼Œå‹•æ…‹èª¿æ•´é…ç½®
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    model_state = checkpoint.get('model_state_dict', checkpoint)
    
    # ğŸ†• å„ªå…ˆä½¿ç”¨ checkpoint ä¸­ä¿å­˜çš„é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'config' in checkpoint:
        ckpt_config = checkpoint['config']
        logger.info("âœ… Using config from checkpoint (overriding file config)")
        
        # åˆä½µé…ç½®ï¼šcheckpoint å„ªå…ˆï¼Œä½†ä¿ç•™è©•ä¼°ç›¸é—œçš„è¨­ç½®
        eval_settings = config.get('evaluation', {})
        config = ckpt_config
        config['evaluation'] = eval_settings  # ä¿ç•™è©•ä¼°è¨­ç½®
    else:
        logger.warning("âš ï¸ No config in checkpoint, using file config (may cause architecture mismatch!)")
    
    # æª¢æ¸¬ Fourier ç‰¹å¾µæ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æŒ ManualScalingWrapperï¼‰
    has_fourier = 'fourier.B' in model_state or 'base_model.fourier.B' in model_state
    
    # æª¢æ¸¬æ˜¯å¦ä½¿ç”¨ wrapperï¼ˆé€šé base_model. å‰ç¶´æˆ– input_min/maxï¼‰
    is_wrapped = ('base_model.hidden_layers.0.linear.weight' in model_state or
                  'input_min' in model_state)
    
    # æª¢æ¸¬è¼¸å…¥ç¶­åº¦ï¼ˆå¾ Fourier B çŸ©é™£æˆ–ç¬¬ä¸€å±¤æ¬Šé‡æ¨æ–·ï¼‰
    input_proj_shape = None
    if 'base_model.fourier.B' in model_state:
        input_proj_shape = model_state['base_model.fourier.B']
    elif 'fourier.B' in model_state:
        input_proj_shape = model_state['fourier.B']
    elif 'base_model.hidden_layers.0.linear.weight' in model_state:
        input_proj_shape = model_state['base_model.hidden_layers.0.linear.weight']
    elif 'hidden_layers.0.linear.weight' in model_state:
        input_proj_shape = model_state['hidden_layers.0.linear.weight']
    
    if input_proj_shape is not None:
        if has_fourier:
            # Fourier B matrix å½¢ç‹€: (input_dim, m)
            # å¯¦éš›è¼¸å…¥ç¶­åº¦æ˜¯ B.shape[0]ï¼Œè¼¸å‡ºæ˜¯ 2*m
            input_dim = input_proj_shape.shape[0]
            fourier_dim = input_proj_shape.shape[1] * 2  # sin + cos
        else:
            # Hidden layer å½¢ç‹€: (hidden_size, input_dim)
            input_dim = input_proj_shape.shape[1]
            fourier_dim = None
        
        logger.info(f"ğŸ” Checkpoint architecture detected:")
        logger.info(f"   Input dim: {input_dim}, Has Fourier: {has_fourier}, Wrapped: {is_wrapped}")
        if fourier_dim:
            logger.info(f"   Fourier output dim: {fourier_dim}")
        
        # å‹•æ…‹èª¿æ•´é…ç½®ä»¥åŒ¹é…æª¢æŸ¥é»
        if 'model' not in config:
            config['model'] = {}
        if 'fourier_features' not in config['model']:
            config['model']['fourier_features'] = {}
        
        if has_fourier and fourier_dim:  # Fourier enabled
            config['model']['use_fourier'] = True
            config['model']['fourier_features']['type'] = 'standard'
            # å¾ B çŸ©é™£æ¨æ–· m
            fourier_m = input_proj_shape.shape[1]
            config['model']['fourier_features']['fourier_m'] = int(fourier_m)
            if config['model']['fourier_features'].get('fourier_sigma', 0) == 0:
                config['model']['fourier_features']['fourier_sigma'] = 5.0
            logger.info(f"âœ… Config adjusted to Fourier ENABLED (m={fourier_m})")
        else:  # Fourier disabled
            config['model']['use_fourier'] = False
            config['model']['fourier_features']['type'] = 'disabled'
            config['model']['fourier_features']['fourier_m'] = 0
            config['model']['fourier_features']['fourier_sigma'] = 0.0
            logger.info("âœ… Config adjusted to Fourier DISABLED")
    
    # ğŸ”§ å¾é…ç½®æ–‡ä»¶æ§‹å»º statistics ä»¥æ”¯æŒ 3D æ¨¡å‹
    # é€™ç¢ºä¿ ManualScalingWrapper èƒ½æ­£ç¢ºè¨­ç½® input_min/max çš„å½¢ç‹€
    statistics = None
    if 'physics' in config and 'domain' in config['physics']:
        domain = config['physics']['domain']
        statistics = {
            'x': {'range': domain.get('x_range', [0.0, 25.13])},
            'y': {'range': domain.get('y_range', [-1.0, 1.0])}
        }
        # å¦‚æœæ˜¯ 3Dï¼Œæ·»åŠ  z ç¯„åœ
        if 'z_range' in domain:
            statistics['z'] = {'range': domain['z_range']}
        logger.info(f"ğŸ“ Constructed statistics from config: {list(statistics.keys())}")
    
    # ğŸ”§ CRITICAL FIX: è‹¥ checkpoint ä½¿ç”¨ ManualScalingWrapperï¼Œ
    # å‰‡å¿…é ˆå‰µå»º plain modelï¼ˆé VS-PINNï¼‰ï¼Œå› ç‚º checkpoint çš„ base_model ä¸å« input_scale_factors
    has_wrapper = (is_wrapped and 
                   'input_min' in model_state and 
                   'input_max' in model_state)
    
    original_physics_type = config.get('physics', {}).get('type', '')
    if has_wrapper and original_physics_type == 'vs_pinn_channel_flow':
        # è‡¨æ™‚ç¦ç”¨ VS-PINNï¼Œé¿å… create_model() å‰µå»ºå¸¶ input_scale_factors çš„æ¨¡å‹
        logger.info("âš ï¸  Checkpoint uses ManualScalingWrapper â†’ Disabling VS-PINN mode for model creation")
        config['physics']['type'] = 'channel_flow_3d'  # ä½¿ç”¨æ™®é€šç‰©ç†é¡å‹
    
    # å‰µå»ºæ¨¡å‹æ¶æ§‹
    from pinnx.train.factory import create_model, create_physics
    base_model = create_model(config, device, statistics=statistics)
    
    # æ¢å¾©åŸå§‹ physics typeï¼ˆç”¨æ–¼å¾ŒçºŒ physics å‰µå»ºï¼‰
    if has_wrapper and original_physics_type == 'vs_pinn_channel_flow':
        config['physics']['type'] = original_physics_type
        logger.info("âœ… Restored physics type to vs_pinn_channel_flow for physics module creation")
    
    # ğŸ”§ æª¢æŸ¥ create_model() æ˜¯å¦å·²ç¶“å‰µå»ºäº† wrapperï¼ˆé¿å…é›™é‡åŒ…è£ï¼‰
    model_already_wrapped = hasattr(base_model, 'input_min') and hasattr(base_model, 'input_max')
    
    if has_wrapper and not model_already_wrapped:
        # Checkpoint ä½¿ç”¨ wrapperï¼Œä½† create_model() æ²’æœ‰å‰µå»º â†’ éœ€è¦æ‰‹å‹•åŒ…è£
        logger.info("ğŸ”§ Checkpoint uses ManualScalingWrapper, manually applying wrapper")
        from pinnx.models.wrappers import ManualScalingWrapper
        
        # å¾ checkpoint æå–ç¸®æ”¾ç¯„åœ
        input_min = model_state['input_min'].cpu().numpy()
        input_max = model_state['input_max'].cpu().numpy()
        output_min = model_state.get('output_min', torch.zeros(4)).cpu().numpy()
        output_max = model_state.get('output_max', torch.ones(4)).cpu().numpy()
        
        # å¾é…ç½®æ¨æ–·è¼¸å…¥è®Šæ•¸åç¨±ï¼ˆx, y, zï¼‰
        domain = config.get('physics', {}).get('domain', {})
        input_keys = ['x', 'y']
        if 'z_range' in domain or len(input_min) >= 3:
            input_keys.append('z')
        
        # æ§‹å»º input/output ranges å­—å…¸
        input_ranges = {key: (float(input_min[i]), float(input_max[i])) 
                       for i, key in enumerate(input_keys[:len(input_min)])}
        
        output_keys = ['u', 'v', 'w', 'p'] if len(output_min) >= 4 else ['u', 'v', 'p']
        output_ranges = {key: (float(output_min[i]), float(output_max[i])) 
                        for i, key in enumerate(output_keys[:len(output_min)])}
        
        model = ManualScalingWrapper(
            base_model,
            input_ranges=input_ranges,
            output_ranges=output_ranges
        ).to(device)
        logger.info(f"   Input ranges: {input_ranges}")
        logger.info(f"   Output ranges: {list(output_ranges.keys())}")
    elif model_already_wrapped:
        # create_model() å·²ç¶“å‰µå»ºäº† wrapper â†’ ç›´æ¥ä½¿ç”¨
        model = base_model
        logger.info("âœ… Model already wrapped by create_model(), using directly")
    else:
        # Checkpoint ä¸ä½¿ç”¨ wrapper â†’ ç›´æ¥ä½¿ç”¨ base model
        model = base_model
        logger.info("â„¹ï¸  Checkpoint uses bare model (no wrapper)")
    
    # ğŸ†• å‰µå»º physics å°è±¡ï¼ˆç”¨æ–¼æ¢å¾© VS-PINN ç¸®æ”¾åƒæ•¸ï¼‰
    physics = None
    physics_type = config.get('physics', {}).get('type', '')
    if physics_type == 'vs_pinn_channel_flow':
        physics = create_physics(config, device)
        logger.info("âœ… Created VS-PINN physics module")
     
    # è¼‰å…¥æ¬Šé‡ï¼ˆä½¿ç”¨å·²è¼‰å…¥çš„ checkpointï¼‰
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        epoch = checkpoint.get('epoch', 'unknown')
        logger.info(f"âœ… Loaded model checkpoint from epoch {epoch}")
    else:
        model.load_state_dict(checkpoint)
        logger.info(f"âœ… Loaded model checkpoint (legacy format)")
    
    # è½‰ç§»åˆ°ç›®æ¨™è¨­å‚™
    model = model.to(device)
    
    # ğŸ†• æ¢å¾© physics çš„ state_dictï¼ˆVS-PINN ç¸®æ”¾åƒæ•¸ç­‰ï¼‰
    if 'physics_state_dict' in checkpoint and physics is not None:
        physics.load_state_dict(checkpoint['physics_state_dict'])
        logger.info(f"âœ… Restored physics state: {list(checkpoint['physics_state_dict'].keys())}")
        
        # ğŸ” æ‰“å°æ¢å¾©çš„ç¸®æ”¾åƒæ•¸ï¼ˆç”¨æ–¼é©—è­‰ï¼‰
        if hasattr(physics, 'N_x'):
            logger.info(f"   VS-PINN ç¸®æ”¾åƒæ•¸: N_x={physics.N_x.item():.2f}, "
                       f"N_y={physics.N_y.item():.2f}, N_z={physics.N_z.item():.2f}")
    elif 'physics_state_dict' not in checkpoint:
        logger.warning("âš ï¸ No physics_state_dict in checkpoint (legacy checkpoint?)")
        if physics_type == 'vs_pinn_channel_flow':
            logger.warning("âš ï¸ Using default VS-PINN scaling parameters - predictions may be incorrect!")
    
    model.eval()
    return model, physics


def load_jhtdb_reference(data_path: Path) -> Dict[str, np.ndarray]:
    """è¼‰å…¥ JHTDB åƒè€ƒæ•¸æ“šï¼ˆæ”¯æ´ 2D/3Dï¼‰"""
    logger.info(f"ğŸ“¥ Loading JHTDB reference from {data_path}")
    
    data = np.load(data_path, allow_pickle=True)
    
    # æª¢æŸ¥æ•¸æ“šç¶­åº¦ï¼ˆ2D æˆ– 3Dï¼‰
    is_3d = 'z' in data and 'w' in data
    
    if is_3d:
        required_fields = ['u', 'v', 'w', 'p', 'x', 'y', 'z']
        domain_info = (f"domain: x[{data['x'].min():.2f}, {data['x'].max():.2f}], "
                      f"y[{data['y'].min():.2f}, {data['y'].max():.2f}], "
                      f"z[{data['z'].min():.2f}, {data['z'].max():.2f}]")
    else:
        required_fields = ['u', 'v', 'p', 'x', 'y']
        domain_info = (f"domain: x[{data['x'].min():.2f}, {data['x'].max():.2f}], "
                      f"y[{data['y'].min():.2f}, {data['y'].max():.2f}] (2D slice)")
    
    # æª¢æŸ¥æ•¸æ“šæ ¼å¼
    missing_fields = [f for f in required_fields if f not in data]
    if missing_fields:
        logger.warning(f"âš ï¸  Missing fields: {missing_fields}")
    
    logger.info(f"âœ… Loaded reference data ({'3D' if is_3d else '2D'}): u{data['u'].shape}, {domain_info}")
    
    return {key: data[key] for key in data.files}


def predict_on_grid(model, x: np.ndarray, y: np.ndarray, z: np.ndarray, 
                    device: torch.device, batch_size: int = 10000, 
                    physics=None, config: Dict = None) -> Dict[str, np.ndarray]:
    """åœ¨ç¶²æ ¼ä¸Šé€²è¡Œé æ¸¬
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        physics: VS-PINN physics æ¨¡çµ„ï¼ˆç”¨æ–¼åº§æ¨™ç¸®æ”¾ï¼‰
        config: é…ç½®å­—å…¸ï¼ˆç”¨æ–¼åæ¨™æº–åŒ–ï¼ŒTASK-008ï¼‰
        ...
    """
    logger.info(f"ğŸ”® Predicting on grid: {len(x)}Ã—{len(y)}Ã—{len(z)} = {len(x)*len(y)*len(z)} points")
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ VS-PINN ç¸®æ”¾
    use_vs_pinn = physics is not None and hasattr(physics, 'scale_coordinates')
    if use_vs_pinn:
        logger.info(f"ğŸ”§ Using VS-PINN coordinate scaling: N_x={physics.N_x.item():.2f}, N_y={physics.N_y.item():.2f}, N_z={physics.N_z.item():.2f}")
    else:
        logger.info(f"ğŸ”§ Using direct model inference (no scaling)")
    
    # ç”Ÿæˆç¶²æ ¼é»
    X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
    points = np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1)
    n_points = points.shape[0]
    
    # åˆ†æ‰¹é æ¸¬
    u_list, v_list, w_list, p_list = [], [], [], []
    
    with torch.no_grad():
        for i in range(0, n_points, batch_size):
            batch = torch.tensor(points[i:i+batch_size], dtype=torch.float32, device=device)
            
            # ğŸ†• æ‡‰ç”¨ VS-PINN åº§æ¨™ç¸®æ”¾ï¼ˆå¦‚æœæœ‰ï¼‰
            if use_vs_pinn:
                batch = physics.scale_coordinates(batch)
            
            # æ¨¡å‹æ¨ç†ï¼ˆè¼¸å‡ºç‚ºæ¨™æº–åŒ–ç©ºé–“ï¼‰
            pred = model(batch)
            
            # âœ… TASK-008: åæ¨™æº–åŒ–å›ç‰©ç†ç©ºé–“
            if config is not None:
                pred_physical = denormalize_output(
                    pred.cpu().numpy(), 
                    config, 
                    output_norm_type='training_data_norm',
                    verbose=False
                )
            else:
                # å‘å¾Œç›¸å®¹ï¼šç„¡é…ç½®æ™‚ä¸åæ¨™æº–åŒ–
                logger.warning("âš ï¸ config ç‚º Noneï¼Œè·³éåæ¨™æº–åŒ–ï¼ˆå¯èƒ½å°è‡´é‡ç´šéŒ¯èª¤ï¼‰")
                pred_physical = pred.cpu().numpy()
            
            u_list.append(pred_physical[:, 0])
            v_list.append(pred_physical[:, 1])
            w_list.append(pred_physical[:, 2])
            p_list.append(pred_physical[:, 3])
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"  Progress: {i+len(batch)}/{n_points} ({100*(i+len(batch))/n_points:.1f}%)")
    
    # é‡å¡‘ç‚º 3D ç¶²æ ¼
    shape = (len(x), len(y), len(z))
    
    results = {
        'u': np.concatenate(u_list).reshape(shape),
        'v': np.concatenate(v_list).reshape(shape),
        'w': np.concatenate(w_list).reshape(shape),
        'p': np.concatenate(p_list).reshape(shape),
        'x': x,
        'y': y,
        'z': z
    }
    
    logger.info(f"âœ… Prediction complete")
    return results


# ============================================================
# ç‰©ç†æŒ‡æ¨™è¨ˆç®—
# ============================================================

def compute_error_metrics(pred: Dict[str, np.ndarray], 
                          ref: Dict[str, np.ndarray]) -> Dict[str, float]:
    """è¨ˆç®—èª¤å·®æŒ‡æ¨™"""
    logger.info("ğŸ“Š Computing error metrics...")
    
    metrics = {}
    
    for field in ['u', 'v', 'w', 'p']:
        if field not in pred or field not in ref:
            continue
        
        pred_field = pred[field].flatten()
        ref_field = ref[field].flatten()
        
        # ç›¸å° L2 èª¤å·®
        l2_error = np.linalg.norm(pred_field - ref_field) / (np.linalg.norm(ref_field) + 1e-12)
        
        # RMSE
        rmse = np.sqrt(np.mean((pred_field - ref_field)**2))
        
        # ç›¸å° RMSE
        rel_rmse = rmse / (np.std(ref_field) + 1e-12)
        
        # æœ€å¤§çµ•å°èª¤å·®
        max_error = np.max(np.abs(pred_field - ref_field))
        
        metrics[f'{field}_l2_error'] = l2_error
        metrics[f'{field}_rmse'] = rmse
        metrics[f'{field}_rel_rmse'] = rel_rmse
        metrics[f'{field}_max_error'] = max_error
    
    # ç¶œåˆæŒ‡æ¨™
    metrics['overall_l2_error'] = np.mean([
        metrics.get(f'{f}_l2_error', 0) for f in ['u', 'v', 'w']
    ])
    
    logger.info(f"âœ… Overall L2 error: {metrics['overall_l2_error']:.4f}")
    
    return metrics


def compute_field_statistics(pred: Dict[str, np.ndarray], 
                             ref: Dict[str, np.ndarray]) -> Dict[str, Dict[str, float]]:
    """è¨ˆç®—å ´çµ±è¨ˆé‡"""
    logger.info("ğŸ“Š Computing field statistics...")
    
    stats = {'pred': {}, 'ref': {}, 'improvement': {}}
    
    for field in ['u', 'v', 'w', 'p']:
        if field not in pred or field not in ref:
            continue
        
        pred_field = pred[field].flatten()
        ref_field = ref[field].flatten()
        
        # é æ¸¬å ´çµ±è¨ˆ
        stats['pred'][f'{field}_mean'] = np.mean(pred_field)
        stats['pred'][f'{field}_std'] = np.std(pred_field)
        stats['pred'][f'{field}_min'] = np.min(pred_field)
        stats['pred'][f'{field}_max'] = np.max(pred_field)
        
        # åƒè€ƒå ´çµ±è¨ˆ
        stats['ref'][f'{field}_mean'] = np.mean(ref_field)
        stats['ref'][f'{field}_std'] = np.std(ref_field)
        stats['ref'][f'{field}_min'] = np.min(ref_field)
        stats['ref'][f'{field}_max'] = np.max(ref_field)
        
        # çµ±è¨ˆé‡èª¤å·®
        mean_error = np.abs(stats['pred'][f'{field}_mean'] - stats['ref'][f'{field}_mean'])
        std_error = np.abs(stats['pred'][f'{field}_std'] - stats['ref'][f'{field}_std'])
        
        stats['improvement'][f'{field}_mean_error'] = mean_error
        stats['improvement'][f'{field}_std_error'] = std_error
    
    return stats


def compute_wall_shear_stress_comparison(pred: Dict[str, np.ndarray], 
                                         ref: Dict[str, np.ndarray]) -> Dict[str, float]:
    """æ¯”è¼ƒå£é¢å‰ªæ‡‰åŠ›ï¼ˆæ”¯æ´ 2D/3Dï¼‰"""
    logger.info("ğŸ“Š Computing wall shear stress comparison...")
    
    # æª¢æ¸¬ 2D vs 3Dï¼ˆä½¿ç”¨ reference åˆ¤æ–·ï¼Œå› ç‚º pred å¯èƒ½è¢«æ“´ç¶­ï¼‰
    is_2d = len(ref['u'].shape) == 2
    
    # è¨ˆç®—é€Ÿåº¦æ¢¯åº¦ï¼ˆä½¿ç”¨æœ‰é™å·®åˆ†ï¼‰
    dy = pred['y'][1] - pred['y'][0]
    
    # ä¸‹å£é¢å‰ªæ‡‰åŠ›ï¼šÏ„_w = Î¼ * âˆ‚u/âˆ‚y
    if is_2d:
        # 2D: shape (nx, ny) - squeeze pred if needed
        pred_u = pred['u'].squeeze() if pred['u'].ndim == 3 else pred['u']
        pred_tau_lower = (pred_u[:, 1] - pred_u[:, 0]) / dy
        ref_tau_lower = (ref['u'][:, 1] - ref['u'][:, 0]) / dy
    else:
        # 3D: shape (nx, ny, nz)
        pred_tau_lower = (pred['u'][:, 1, :] - pred['u'][:, 0, :]) / dy
        ref_tau_lower = (ref['u'][:, 1, :] - ref['u'][:, 0, :]) / dy
    
    # çµ±è¨ˆé‡
    metrics = {
        'pred_tau_mean': float(np.mean(pred_tau_lower)),
        'pred_tau_std': float(np.std(pred_tau_lower)),
        'ref_tau_mean': float(np.mean(ref_tau_lower)),
        'ref_tau_std': float(np.std(ref_tau_lower)),
        'tau_rmse': float(np.sqrt(np.mean((pred_tau_lower - ref_tau_lower)**2))),
        'tau_rel_error': float(np.abs(np.mean(pred_tau_lower) - np.mean(ref_tau_lower)) / (np.abs(np.mean(ref_tau_lower)) + 1e-12))
    }
    
    logger.info(f"âœ… Wall shear stress ({'2D' if is_2d else '3D'}): pred={metrics['pred_tau_mean']:.6f}, "
                f"ref={metrics['ref_tau_mean']:.6f}, error={metrics['tau_rel_error']:.2%}")
    
    return metrics


def compute_energy_spectrum_comparison(pred: Dict[str, np.ndarray], 
                                       ref: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
    """æ¯”è¼ƒèƒ½é‡è­œï¼ˆæ”¯æ´ 2D/3Dï¼‰"""
    logger.info("ğŸ“Š Computing energy spectrum comparison...")
    
    # æª¢æ¸¬ 2D vs 3Dï¼ˆä½¿ç”¨ reference åˆ¤æ–·ï¼‰
    is_2d = len(ref['u'].shape) == 2
    
    if is_2d:
        # 2D: ç›´æ¥ä½¿ç”¨æ•´å€‹å ´ï¼ˆsqueeze pred ç§»é™¤ z ç¶­åº¦ï¼‰
        pred_u = pred['u'].squeeze() if pred['u'].ndim == 3 else pred['u']
        pred_v = pred['v'].squeeze() if pred['v'].ndim == 3 else pred['v']
        pred_ke = 0.5 * (pred_u**2 + pred_v**2)
        ref_ke = 0.5 * (ref['u']**2 + ref['v']**2)
    else:
        # 3D: é¸æ“‡ä¸­é–“ y å¹³é¢
        y_mid = len(pred['y']) // 2
        pred_ke = 0.5 * (pred['u'][:, y_mid, :]**2 + pred['v'][:, y_mid, :]**2 + pred['w'][:, y_mid, :]**2)
        ref_ke = 0.5 * (ref['u'][:, y_mid, :]**2 + ref['v'][:, y_mid, :]**2 + ref['w'][:, y_mid, :]**2)
    
    # 2D FFT
    pred_fft = np.fft.fft2(pred_ke)
    ref_fft = np.fft.fft2(ref_ke)
    
    # èƒ½é‡è­œ
    pred_spectrum = np.abs(np.fft.fftshift(pred_fft))**2
    ref_spectrum = np.abs(np.fft.fftshift(ref_fft))**2
    
    # å¾‘å‘å¹³å‡
    h, w = pred_ke.shape
    center_h, center_w = h // 2, w // 2
    
    y_idx, x_idx = np.ogrid[:h, :w]
    r = np.sqrt((x_idx - center_w)**2 + (y_idx - center_h)**2).astype(int)
    
    k_max = min(center_h, center_w)
    k_bins = np.arange(1, k_max)
    
    pred_radial = np.zeros(len(k_bins))
    ref_radial = np.zeros(len(k_bins))
    
    for i, k in enumerate(k_bins):
        mask = (r == k)
        if mask.sum() > 0:
            pred_radial[i] = pred_spectrum[mask].mean()
            ref_radial[i] = ref_spectrum[mask].mean()
    
    # è¨ˆç®—èƒ½è­œ RMSE
    spectrum_rmse = np.sqrt(np.mean((pred_radial - ref_radial)**2))
    spectrum_rel_error = spectrum_rmse / (np.mean(ref_radial) + 1e-12)
    
    logger.info(f"âœ… Energy spectrum RMSE: {spectrum_rmse:.2e}, rel_error: {spectrum_rel_error:.2%}")
    
    return {
        'k': k_bins,
        'pred_spectrum': pred_radial,
        'ref_spectrum': ref_radial,
        'spectrum_rmse': spectrum_rmse,
        'spectrum_rel_error': spectrum_rel_error
    }


# ============================================================
# å¯è¦–åŒ–å‡½æ•¸
# ============================================================

def plot_error_distribution(pred: Dict[str, np.ndarray], 
                            ref: Dict[str, np.ndarray], 
                            save_dir: Path):
    """ç¹ªè£½èª¤å·®åˆ†å¸ƒåœ–ï¼ˆæ”¯æ´ 2D/3Dï¼‰"""
    logger.info("ğŸ¨ Plotting error distribution...")
    
    # æª¢æ¸¬ 2D vs 3D
    is_2d = len(ref['u'].shape) == 2
    
    # æ±ºå®šè¦ç¹ªè£½çš„å ´ï¼ˆåƒ…ç¹ªè£½åƒè€ƒè³‡æ–™ä¸­å­˜åœ¨çš„å ´ï¼‰
    available_fields = [f for f in ['u', 'v', 'w', 'p'] if f in ref]
    n_fields = len(available_fields)
    n_rows = (n_fields + 1) // 2
    n_cols = 2
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 7*n_rows))
    if n_fields == 1:
        axes = np.array([axes])
    axes = axes.flatten()
    
    # æå–åˆ‡ç‰‡
    if is_2d:
        # 2D: squeeze pred
        def get_slice(data_dict, field):
            if field not in data_dict:
                return None
            d = data_dict[field]
            return d.squeeze() if d.ndim == 3 else d
        z_label = "2D slice"
    else:
        # 3D: é¸æ“‡ä¸­é–“ z å¹³é¢
        z_mid = len(pred['z']) // 2
        def get_slice(data_dict, field):
            if field not in data_dict:
                return None
            return data_dict[field][:, :, z_mid]
        z_label = f"z={pred['z'][z_mid]:.2f}"
    
    for idx, field in enumerate(available_fields):
        ax = axes[idx]
        
        pred_slice = get_slice(pred, field)
        ref_slice = get_slice(ref, field)
        
        if pred_slice is None or ref_slice is None:
            ax.text(0.5, 0.5, f'{field.upper()} not available', 
                   ha='center', va='center', fontsize=16)
            ax.axis('off')
            continue
        
        error = np.abs(pred_slice - ref_slice)
        
        im = ax.contourf(pred['x'], pred['y'], error.T, levels=20, cmap='hot')
        ax.set_xlabel('x', fontsize=14)
        ax.set_ylabel('y', fontsize=14)
        ax.set_title(f'{field.upper()} Absolute Error ({z_label})', fontsize=16)
        plt.colorbar(im, ax=ax, label='|pred - ref|')
    
    # éš±è—å¤šé¤˜çš„å­åœ–
    for idx in range(n_fields, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'error_distribution.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved error distribution to {save_dir / 'error_distribution.png'}")
    plt.close()


def plot_field_comparison(pred: Dict[str, np.ndarray], 
                          ref: Dict[str, np.ndarray], 
                          save_dir: Path):
    """ç¹ªè£½å ´æ¯”è¼ƒåœ–ï¼ˆé æ¸¬ vs åƒè€ƒï¼Œæ”¯æ´ 2D/3Dï¼‰"""
    logger.info("ğŸ¨ Plotting field comparison...")
    
    # æª¢æ¸¬ 2D vs 3D
    is_2d = len(ref['u'].shape) == 2
    
    # æ±ºå®šè¦ç¹ªè£½çš„å ´
    available_fields = [f for f in ['u', 'v', 'w', 'p'] if f in ref]
    
    # æå–åˆ‡ç‰‡
    if is_2d:
        def get_slice(data_dict, field):
            if field not in data_dict:
                return None
            d = data_dict[field]
            return d.squeeze() if d.ndim == 3 else d
    else:
        z_mid = len(pred['z']) // 2
        def get_slice(data_dict, field):
            if field not in data_dict:
                return None
            return data_dict[field][:, :, z_mid]
    
    for field in available_fields:
        fig, axes = plt.subplots(1, 3, figsize=(20, 5))
        
        pred_slice = get_slice(pred, field)
        ref_slice = get_slice(ref, field)
        
        if pred_slice is None or ref_slice is None:
            logger.warning(f"âš ï¸ Skipping {field} - not available in data")
            plt.close()
            continue
        
        # çµ±ä¸€è‰²éš
        vmin = min(pred_slice.min(), ref_slice.min())
        vmax = max(pred_slice.max(), ref_slice.max())
        
        # é æ¸¬å ´
        im0 = axes[0].contourf(pred['x'], pred['y'], pred_slice.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)
        axes[0].set_title(f'{field.upper()} - Predicted', fontsize=16)
        axes[0].set_xlabel('x', fontsize=14)
        axes[0].set_ylabel('y', fontsize=14)
        plt.colorbar(im0, ax=axes[0])
        
        # åƒè€ƒå ´
        im1 = axes[1].contourf(ref['x'], ref['y'], ref_slice.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)
        axes[1].set_title(f'{field.upper()} - Reference (JHTDB)', fontsize=16)
        axes[1].set_xlabel('x', fontsize=14)
        axes[1].set_ylabel('y', fontsize=14)
        plt.colorbar(im1, ax=axes[1])
        
        # èª¤å·®å ´
        error = pred_slice - ref_slice
        im2 = axes[2].contourf(pred['x'], pred['y'], error.T, levels=20, cmap='seismic')
        axes[2].set_title(f'{field.upper()} - Error', fontsize=16)
        axes[2].set_xlabel('x', fontsize=14)
        axes[2].set_ylabel('y', fontsize=14)
        plt.colorbar(im2, ax=axes[2], label='pred - ref')
        
        plt.tight_layout()
        plt.savefig(save_dir / f'field_comparison_{field}.png', dpi=150, bbox_inches='tight')
        logger.info(f"âœ… Saved {field} comparison")
        plt.close()


def plot_velocity_profiles(pred: Dict[str, np.ndarray], 
                           ref: Dict[str, np.ndarray], 
                           save_dir: Path):
    """ç¹ªè£½é€Ÿåº¦å‰–é¢æ¯”è¼ƒï¼ˆæ”¯æ´ 2D/3Dï¼‰"""
    logger.info("ğŸ¨ Plotting velocity profiles...")
    
    # æª¢æ¸¬ 2D vs 3D
    is_2d = len(ref['u'].shape) == 2
    
    # æ±ºå®šè¦ç¹ªè£½çš„é€Ÿåº¦åˆ†é‡
    available_fields = [f for f in ['u', 'v', 'w'] if f in ref]
    n_fields = len(available_fields)
    
    # é¸æ“‡åŸŸä¸­å¿ƒ
    x_mid = len(pred['x']) // 2
    
    fig, axes = plt.subplots(1, n_fields, figsize=(6*n_fields, 5))
    if n_fields == 1:
        axes = [axes]
    
    for idx, field in enumerate(available_fields):
        ax = axes[idx]
        
        if is_2d:
            # 2D: squeeze pred
            pred_field = pred[field].squeeze() if pred[field].ndim == 3 else pred[field]
            pred_profile = pred_field[x_mid, :]
            ref_profile = ref[field][x_mid, :]
            title_suffix = f"x={pred['x'][x_mid]:.2f}"
        else:
            # 3D: ä½¿ç”¨ä¸­é–“ z
            z_mid = len(pred['z']) // 2
            pred_profile = pred[field][x_mid, :, z_mid]
            ref_profile = ref[field][x_mid, :, z_mid]
            title_suffix = f"x={pred['x'][x_mid]:.2f}, z={pred['z'][z_mid]:.2f}"
        
        ax.plot(pred_profile, pred['y'], 'b-', linewidth=2, label='Predicted')
        ax.plot(ref_profile, ref['y'], 'r--', linewidth=2, label='JHTDB Reference')
        
        ax.set_xlabel(f'{field}', fontsize=14)
        ax.set_ylabel('y', fontsize=14)
        ax.set_title(f'{field.upper()} Velocity Profile ({title_suffix})', fontsize=16)
        ax.legend(fontsize=12)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'velocity_profiles_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved velocity profiles")
    plt.close()


def plot_energy_spectrum(spectrum_data: Dict, save_dir: Path):
    """ç¹ªè£½èƒ½é‡è­œæ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting energy spectrum...")
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    k = spectrum_data['k']
    pred_spec = spectrum_data['pred_spectrum']
    ref_spec = spectrum_data['ref_spectrum']
    
    # ç·šæ€§å°ºåº¦
    axes[0].plot(k, pred_spec, 'b-', linewidth=2, label='Predicted')
    axes[0].plot(k, ref_spec, 'r--', linewidth=2, label='JHTDB Reference')
    axes[0].set_xlabel('Wavenumber k', fontsize=14)
    axes[0].set_ylabel('E(k)', fontsize=14)
    axes[0].set_title('Energy Spectrum (Linear Scale)', fontsize=16)
    axes[0].legend(fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # å°æ•¸å°ºåº¦
    axes[1].loglog(k, pred_spec, 'b-', linewidth=2, label='Predicted')
    axes[1].loglog(k, ref_spec, 'r--', linewidth=2, label='JHTDB Reference')
    
    # ç¹ªè£½ -5/3 å¾‹åƒè€ƒç·š
    k_ref = k[len(k)//4:len(k)//2]
    spec_ref = ref_spec[len(k)//4] * (k_ref / k[len(k)//4])**(-5/3)
    axes[1].loglog(k_ref, spec_ref, 'k:', linewidth=1.5, label=r'$k^{-5/3}$ law')
    
    axes[1].set_xlabel('Wavenumber k', fontsize=14)
    axes[1].set_ylabel('E(k)', fontsize=14)
    axes[1].set_title('Energy Spectrum (Log-Log Scale)', fontsize=16)
    axes[1].legend(fontsize=12)
    axes[1].grid(True, alpha=0.3, which='both')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'energy_spectrum_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved energy spectrum")
    plt.close()


def plot_wall_shear_stress(pred: Dict[str, np.ndarray], 
                           ref: Dict[str, np.ndarray], 
                           save_dir: Path):
    """ç¹ªè£½å£é¢å‰ªæ‡‰åŠ›æ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting wall shear stress...")
    
    # æª¢æ¸¬ç¶­åº¦ï¼š2D (nx, ny) æˆ– 3D (nx, ny, nz)
    is_2d = len(ref['u'].shape) == 2
    
    # Squeeze pred data if needed (é æ¸¬ç¸½æ˜¯3Dï¼Œä½†åƒè€ƒå¯èƒ½æ˜¯2D)
    pred_u = pred['u'].squeeze()
    
    # è¨ˆç®—å£é¢å‰ªæ‡‰åŠ›
    dy = pred['y'][1] - pred['y'][0]
    
    if is_2d:
        # 2D: shape (nx, ny) -> tau shape (nx,)
        pred_tau = (pred_u[:, 1] - pred_u[:, 0]) / dy
        ref_tau = (ref['u'][:, 1] - ref['u'][:, 0]) / dy
        
        # 2D ç¹ªè£½ï¼šç·šåœ–
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # é æ¸¬ vs åƒè€ƒ
        axes[0].plot(pred['x'], pred_tau, 'b-', linewidth=2, label='Predicted')
        axes[0].plot(ref['x'], ref_tau, 'r--', linewidth=2, label='Reference')
        axes[0].set_title('Wall Shear Stress Comparison', fontsize=16)
        axes[0].set_xlabel('x', fontsize=14)
        axes[0].set_ylabel(r'$\tau_w$', fontsize=14)
        axes[0].legend(fontsize=12)
        axes[0].grid(True, alpha=0.3)
        
        # èª¤å·®
        error = pred_tau - ref_tau
        axes[1].plot(pred['x'], error, 'k-', linewidth=2)
        axes[1].axhline(y=0, color='gray', linestyle='--', alpha=0.5)
        axes[1].set_title('Wall Shear Stress Error', fontsize=16)
        axes[1].set_xlabel('x', fontsize=14)
        axes[1].set_ylabel(r'$\Delta\tau_w$', fontsize=14)
        axes[1].grid(True, alpha=0.3)
        
    else:
        # 3D: shape (nx, ny, nz) -> tau shape (nx, nz)
        pred_tau = (pred_u[:, 1, :] - pred_u[:, 0, :]) / dy
        ref_tau = (ref['u'][:, 1, :] - ref['u'][:, 0, :]) / dy
        
        # 3D ç¹ªè£½ï¼šç­‰é«˜ç·šåœ–
        fig, axes = plt.subplots(1, 3, figsize=(20, 5))
        
        # çµ±ä¸€è‰²éš
        vmin = min(pred_tau.min(), ref_tau.min())
        vmax = max(pred_tau.max(), ref_tau.max())
        
        # é æ¸¬
        im0 = axes[0].contourf(pred['x'], pred['z'], pred_tau.T, levels=20, cmap='viridis', vmin=vmin, vmax=vmax)
        axes[0].set_title('Wall Shear Stress - Predicted', fontsize=16)
        axes[0].set_xlabel('x', fontsize=14)
        axes[0].set_ylabel('z', fontsize=14)
        plt.colorbar(im0, ax=axes[0], label=r'$\tau_w$')
        
        # åƒè€ƒ
        im1 = axes[1].contourf(ref['x'], ref['z'], ref_tau.T, levels=20, cmap='viridis', vmin=vmin, vmax=vmax)
        axes[1].set_title('Wall Shear Stress - Reference', fontsize=16)
        axes[1].set_xlabel('x', fontsize=14)
        axes[1].set_ylabel('z', fontsize=14)
        plt.colorbar(im1, ax=axes[1], label=r'$\tau_w$')
        
        # èª¤å·®
        error = pred_tau - ref_tau
        im2 = axes[2].contourf(pred['x'], pred['z'], error.T, levels=20, cmap='seismic')
        axes[2].set_title('Wall Shear Stress - Error', fontsize=16)
        axes[2].set_xlabel('x', fontsize=14)
        axes[2].set_ylabel('z', fontsize=14)
        plt.colorbar(im2, ax=axes[2], label=r'$\Delta\tau_w$')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'wall_shear_stress_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved wall shear stress comparison")
    plt.close()


def plot_statistics_comparison(stats: Dict, save_dir: Path):
    """ç¹ªè£½çµ±è¨ˆé‡æ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting statistics comparison...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    fields = ['u', 'v', 'w', 'p']
    metrics = ['mean', 'std', 'min', 'max']
    
    for idx, field in enumerate(fields):
        ax = axes[idx // 2, idx % 2]
        
        pred_values = [stats['pred'].get(f'{field}_{m}', 0) for m in metrics]
        ref_values = [stats['ref'].get(f'{field}_{m}', 0) for m in metrics]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, pred_values, width, label='Predicted', alpha=0.8)
        ax.bar(x + width/2, ref_values, width, label='JHTDB Reference', alpha=0.8)
        
        ax.set_xlabel('Statistics', fontsize=14)
        ax.set_ylabel('Value', fontsize=14)
        ax.set_title(f'{field.upper()} Statistics Comparison', fontsize=16)
        ax.set_xticks(x)
        ax.set_xticklabels(metrics)
        ax.legend(fontsize=12)
        ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'statistics_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved statistics comparison")
    plt.close()


# ============================================================
# å ±å‘Šç”Ÿæˆ
# ============================================================

def generate_markdown_report(metrics: Dict, stats: Dict, spectrum_data: Dict, 
                            wall_metrics: Dict, config: Dict, 
                            checkpoint_path: str, save_path: Path):
    """ç”Ÿæˆ Markdown è©•ä¼°å ±å‘Š"""
    logger.info("ğŸ“ Generating Markdown report...")
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # æª¢æŸ¥æˆæ•—é–€æª»
    overall_l2 = metrics.get('overall_l2_error', float('inf'))
    threshold_pass = "âœ… PASS" if overall_l2 <= 0.15 else "âŒ FAIL"
    
    report = f"""# 3D VS-PINN ç¶œåˆè©•ä¼°å ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {timestamp}  
**æª¢æŸ¥é»**: `{checkpoint_path}`  
**é…ç½®æ–‡ä»¶**: `{config.get('_config_path', 'N/A')}`

---

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

### æˆæ•—é–€æª»é©—è­‰

| æŒ‡æ¨™ | ç›®æ¨™ | å¯¦éš›å€¼ | ç‹€æ…‹ |
|------|------|--------|------|
| æ•´é«”ç›¸å° L2 èª¤å·® | â‰¤ 10-15% | **{overall_l2:.2%}** | {threshold_pass} |
| u ç›¸å° L2 èª¤å·® | â‰¤ 15% | {metrics.get('u_l2_error', 0):.2%} | {"âœ…" if metrics.get('u_l2_error', 1) <= 0.15 else "âŒ"} |
| v ç›¸å° L2 èª¤å·® | â‰¤ 15% | {metrics.get('v_l2_error', 0):.2%} | {"âœ…" if metrics.get('v_l2_error', 1) <= 0.15 else "âŒ"} |
| w ç›¸å° L2 èª¤å·® | â‰¤ 15% | {metrics.get('w_l2_error', 0):.2%} | {"âœ…" if metrics.get('w_l2_error', 1) <= 0.15 else "âŒ"} |
| p ç›¸å° L2 èª¤å·® | â‰¤ 20% | {metrics.get('p_l2_error', 0):.2%} | {"âœ…" if metrics.get('p_l2_error', 1) <= 0.20 else "âŒ"} |

### é—œéµç™¼ç¾

- **æ•´é«”ç²¾åº¦**: ç›¸å° L2 èª¤å·® = **{overall_l2:.2%}**
- **å£é¢å‰ªæ‡‰åŠ›**: ç›¸å°èª¤å·® = **{wall_metrics.get('tau_rel_error', 0):.2%}**
- **èƒ½é‡è­œ**: ç›¸å°èª¤å·® = **{spectrum_data.get('spectrum_rel_error', 0):.2%}**

---

## ğŸ¯ è©³ç´°èª¤å·®åˆ†æ

### å ´èª¤å·®æŒ‡æ¨™

| å ´ | ç›¸å° L2 | RMSE | ç›¸å° RMSE | æœ€å¤§èª¤å·® |
|---|---------|------|-----------|----------|
| **u** | {metrics.get('u_l2_error', 0):.4f} | {metrics.get('u_rmse', 0):.6f} | {metrics.get('u_rel_rmse', 0):.4f} | {metrics.get('u_max_error', 0):.6f} |
| **v** | {metrics.get('v_l2_error', 0):.4f} | {metrics.get('v_rmse', 0):.6f} | {metrics.get('v_rel_rmse', 0):.4f} | {metrics.get('v_max_error', 0):.6f} |
| **w** | {metrics.get('w_l2_error', 0):.4f} | {metrics.get('w_rmse', 0):.6f} | {metrics.get('w_rel_rmse', 0):.4f} | {metrics.get('w_max_error', 0):.6f} |
| **p** | {metrics.get('p_l2_error', 0):.4f} | {metrics.get('p_rmse', 0):.6f} | {metrics.get('p_rel_rmse', 0):.4f} | {metrics.get('p_max_error', 0):.6f} |

---

## ğŸ“ˆ å ´çµ±è¨ˆæ¯”è¼ƒ

### æµå‘é€Ÿåº¦ (u)

| çµ±è¨ˆé‡ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | çµ•å°èª¤å·® | ç›¸å°èª¤å·® |
|--------|--------|-----------|----------|----------|
| Mean | {stats['pred'].get('u_mean', 0):.6f} | {stats['ref'].get('u_mean', 0):.6f} | {stats['improvement'].get('u_mean_error', 0):.6f} | {stats['improvement'].get('u_mean_error', 0) / (abs(stats['ref'].get('u_mean', 1)) + 1e-12):.2%} |
| Std | {stats['pred'].get('u_std', 0):.6f} | {stats['ref'].get('u_std', 0):.6f} | {stats['improvement'].get('u_std_error', 0):.6f} | {stats['improvement'].get('u_std_error', 0) / (stats['ref'].get('u_std', 1) + 1e-12):.2%} |
| Min | {stats['pred'].get('u_min', 0):.6f} | {stats['ref'].get('u_min', 0):.6f} | - | - |
| Max | {stats['pred'].get('u_max', 0):.6f} | {stats['ref'].get('u_max', 0):.6f} | - | - |

### æ³•å‘é€Ÿåº¦ (v)

| çµ±è¨ˆé‡ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | çµ•å°èª¤å·® | ç›¸å°èª¤å·® |
|--------|--------|-----------|----------|----------|
| Mean | {stats['pred'].get('v_mean', 0):.6f} | {stats['ref'].get('v_mean', 0):.6f} | {stats['improvement'].get('v_mean_error', 0):.6f} | {stats['improvement'].get('v_mean_error', 0) / (abs(stats['ref'].get('v_mean', 1e-12)) + 1e-12):.2%} |
| Std | {stats['pred'].get('v_std', 0):.6f} | {stats['ref'].get('v_std', 0):.6f} | {stats['improvement'].get('v_std_error', 0):.6f} | {stats['improvement'].get('v_std_error', 0) / (stats['ref'].get('v_std', 1) + 1e-12):.2%} |

### å±•å‘é€Ÿåº¦ (w)

| çµ±è¨ˆé‡ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | çµ•å°èª¤å·® | ç›¸å°èª¤å·® |
|--------|--------|-----------|----------|----------|
| Mean | {stats['pred'].get('w_mean', 0):.6f} | {stats['ref'].get('w_mean', 0):.6f} | {stats['improvement'].get('w_mean_error', 0):.6f} | {stats['improvement'].get('w_mean_error', 0) / (abs(stats['ref'].get('w_mean', 1e-12)) + 1e-12):.2%} |
| Std | {stats['pred'].get('w_std', 0):.6f} | {stats['ref'].get('w_std', 0):.6f} | {stats['improvement'].get('w_std_error', 0):.6f} | {stats['improvement'].get('w_std_error', 0) / (stats['ref'].get('w_std', 1) + 1e-12):.2%} |

---

## ğŸŒŠ ç‰©ç†é©—è­‰

### å£é¢å‰ªæ‡‰åŠ›

| æŒ‡æ¨™ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | èª¤å·® |
|------|--------|-----------|------|
| Mean Ï„_w | {wall_metrics.get('pred_tau_mean', 0):.6f} | {wall_metrics.get('ref_tau_mean', 0):.6f} | {wall_metrics.get('tau_rel_error', 0):.2%} |
| Std Ï„_w | {wall_metrics.get('pred_tau_std', 0):.6f} | {wall_metrics.get('ref_tau_std', 0):.6f} | - |
| RMSE | {wall_metrics.get('tau_rmse', 0):.6f} | - | - |

### èƒ½é‡è­œ

- **è­œ RMSE**: {spectrum_data.get('spectrum_rmse', 0):.6e}
- **è­œç›¸å°èª¤å·®**: {spectrum_data.get('spectrum_rel_error', 0):.2%}
- **æ³¢æ•¸ç¯„åœ**: k âˆˆ [{spectrum_data['k'][0]:.2f}, {spectrum_data['k'][-1]:.2f}]

---

## ğŸ“ è¼¸å‡ºæ–‡ä»¶

### å¯è¦–åŒ–åœ–è¡¨

- `error_distribution.png` - èª¤å·®åˆ†å¸ƒï¼ˆ4 å ´ï¼‰
- `field_comparison_u.png` - u å ´æ¯”è¼ƒï¼ˆé æ¸¬ vs åƒè€ƒ vs èª¤å·®ï¼‰
- `field_comparison_v.png` - v å ´æ¯”è¼ƒ
- `field_comparison_w.png` - w å ´æ¯”è¼ƒ
- `field_comparison_p.png` - p å ´æ¯”è¼ƒ
- `velocity_profiles_comparison.png` - é€Ÿåº¦å‰–é¢æ¯”è¼ƒ
- `energy_spectrum_comparison.png` - èƒ½é‡è­œæ¯”è¼ƒï¼ˆç·šæ€§ & å°æ•¸ï¼‰
- `wall_shear_stress_comparison.png` - å£é¢å‰ªæ‡‰åŠ›æ¯”è¼ƒ
- `statistics_comparison.png` - çµ±è¨ˆé‡æ¯”è¼ƒ

### æ•¸æ“šæ–‡ä»¶

- `evaluation_metrics.json` - å®Œæ•´æŒ‡æ¨™ï¼ˆJSON æ ¼å¼ï¼‰
- `predicted_field.npz` - é æ¸¬æµå ´æ•¸æ“š

---

## ğŸ”§ è¨“ç·´é…ç½®

```yaml
Model: {config.get('model', {})}
Physics: {config.get('physics', {})}
Training: {config.get('training', {})}
```

---

**å ±å‘ŠçµæŸ** | ç”Ÿæˆæ–¼ {timestamp}
"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"âœ… Saved Markdown report to {save_path}")


def save_metrics_json(metrics: Dict, stats: Dict, spectrum_data: Dict, 
                      wall_metrics: Dict, save_path: Path):
    """ä¿å­˜æŒ‡æ¨™ç‚º JSON æ ¼å¼"""
    logger.info("ğŸ’¾ Saving metrics to JSON...")
    
    # éè¿´è½‰æ› numpy é¡å‹ç‚º Python åŸç”Ÿé¡å‹
    def convert_to_json_serializable(obj):
        if isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        else:
            return obj
    
    data = {
        'timestamp': datetime.now().isoformat(),
        'error_metrics': convert_to_json_serializable(metrics),
        'field_statistics': convert_to_json_serializable(stats),
        'wall_shear_stress': convert_to_json_serializable(wall_metrics),
        'energy_spectrum': {
            'spectrum_rmse': float(spectrum_data.get('spectrum_rmse', 0)),
            'spectrum_rel_error': float(spectrum_data.get('spectrum_rel_error', 0))
        },
        'success_criteria': {
            'overall_l2_threshold': 0.15,
            'overall_l2_actual': float(metrics.get('overall_l2_error', 0)),
            'passed': bool(metrics.get('overall_l2_error', 1) <= 0.15)
        }
    }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… Saved JSON metrics to {save_path}")


# ============================================================
# ä¸»å‡½æ•¸
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='Comprehensive 3D VS-PINN Evaluation')
    parser.add_argument('--checkpoint', type=str, required=True, help='Path to checkpoint')
    parser.add_argument('--config', type=str, required=True, help='Path to config YAML')
    parser.add_argument('--reference', type=str, required=True, help='Path to JHTDB reference data')
    parser.add_argument('--output_dir', type=str, default=None, help='Output directory')
    parser.add_argument('--device', type=str, default='auto', help='Device (cuda/cpu/mps/auto)')
    parser.add_argument('--batch_size', type=int, default=10000, help='Batch size for prediction')
    
    args = parser.parse_args()
    
    # è¨­ç½®è¼¸å‡ºç›®éŒ„
    if args.output_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path(f'results/comprehensive_eval_{timestamp}')
    else:
        output_dir = Path(args.output_dir)
    
    output_dir.mkdir(exist_ok=True, parents=True)
    logger.info(f"ğŸ“ Output directory: {output_dir}")
    
    # è¼‰å…¥é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    config['_config_path'] = args.config
    
    # è¨­ç½®è¨­å‚™
    if args.device == 'auto':
        if torch.cuda.is_available():
            device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            device = torch.device('mps')
        else:
            device = torch.device('cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ğŸ–¥ï¸  Using device: {device}")
    
    # ========== è¼‰å…¥æ¨¡å‹èˆ‡æ•¸æ“š ==========
    model, physics = load_trained_model(Path(args.checkpoint), config, device)
    ref_data = load_jhtdb_reference(Path(args.reference))
    
    # æª¢æ¸¬ 2D æˆ– 3D
    is_3d = 'z' in ref_data and 'w' in ref_data
    
    # ========== é æ¸¬ ==========
    # ğŸ†• å‚³é physics æ¨¡çµ„ä»¥ä½¿ç”¨ VS-PINN ç¸®æ”¾
    if is_3d:
        pred_data = predict_on_grid(
            model, 
            ref_data['x'], 
            ref_data['y'], 
            ref_data['z'], 
            device, 
            batch_size=args.batch_size,
            physics=physics,  # ğŸ†• å‚³é physics
            config=config     # âœ… TASK-008: å‚³é config ç”¨æ–¼åæ¨™æº–åŒ–
        )
    else:
        # 2D slice: ä½¿ç”¨å›ºå®š z å€¼ï¼ˆå¾é…ç½®æˆ–åƒè€ƒè³‡æ–™æ¨æ–·ï¼‰
        if isinstance(ref_data.get('slice_position'), np.ndarray):
            z_fixed_val = float(ref_data['slice_position'])
        elif 'slice_position' in ref_data:
            z_fixed_val = float(ref_data['slice_position'])
        else:
            z_fixed_val = 4.71  # é»˜èª z=Ï€/2
        
        z_fixed = np.array([z_fixed_val])
        logger.info(f"ğŸ“ 2D slice detected, using fixed z={z_fixed_val:.3f}")
        pred_data = predict_on_grid(
            model, 
            ref_data['x'], 
            ref_data['y'], 
            z_fixed,  # å–®ä¸€ z å€¼
            device, 
            batch_size=args.batch_size,
            physics=physics,
            config=config
        )
    
    # ä¿å­˜é æ¸¬å ´
    np.savez(
        output_dir / 'predicted_field.npz',
        **pred_data
    )
    logger.info(f"ğŸ’¾ Saved predicted field to {output_dir / 'predicted_field.npz'}")
    
    # ========== è¨ˆç®—æŒ‡æ¨™ ==========
    error_metrics = compute_error_metrics(pred_data, ref_data)
    field_stats = compute_field_statistics(pred_data, ref_data)
    wall_metrics = compute_wall_shear_stress_comparison(pred_data, ref_data)
    spectrum_data = compute_energy_spectrum_comparison(pred_data, ref_data)
    
    # ========== å¯è¦–åŒ– ==========
    plot_error_distribution(pred_data, ref_data, output_dir)
    plot_field_comparison(pred_data, ref_data, output_dir)
    plot_velocity_profiles(pred_data, ref_data, output_dir)
    plot_energy_spectrum(spectrum_data, output_dir)
    plot_wall_shear_stress(pred_data, ref_data, output_dir)
    plot_statistics_comparison(field_stats, output_dir)
    
    # ========== ç”Ÿæˆå ±å‘Š ==========
    generate_markdown_report(
        error_metrics, field_stats, spectrum_data, wall_metrics,
        config, args.checkpoint, output_dir / 'evaluation_report.md'
    )
    
    save_metrics_json(
        error_metrics, field_stats, spectrum_data, wall_metrics,
        output_dir / 'evaluation_metrics.json'
    )
    
    # ========== çµ‚ç«¯è¼¸å‡ºæ‘˜è¦ ==========
    logger.info("\n" + "="*60)
    logger.info("ğŸ‰ è©•ä¼°å®Œæˆï¼")
    logger.info("="*60)
    logger.info(f"ğŸ“Š æ•´é«”ç›¸å° L2 èª¤å·®: {error_metrics.get('overall_l2_error', 0):.2%}")
    logger.info(f"ğŸŒŠ å£é¢å‰ªæ‡‰åŠ›èª¤å·®: {wall_metrics.get('tau_rel_error', 0):.2%}")
    logger.info(f"ğŸ“ˆ èƒ½é‡è­œèª¤å·®: {spectrum_data.get('spectrum_rel_error', 0):.2%}")
    logger.info(f"ğŸ“ çµæœä¿å­˜æ–¼: {output_dir}")
    logger.info("="*60)
    
    # æª¢æŸ¥æˆæ•—
    if error_metrics.get('overall_l2_error', 1) <= 0.15:
        logger.info("âœ… æˆåŠŸï¼æ•´é«”èª¤å·®ä½æ–¼ 15% é–€æª»")
    else:
        logger.warning("âš ï¸  æ•´é«”èª¤å·®è¶…é 15% é–€æª»ï¼Œå»ºè­°é€²ä¸€æ­¥èª¿å„ª")


if __name__ == '__main__':
    main()
