"""
ç¶œåˆè©•ä¼°è…³æœ¬ï¼š3D VS-PINN èˆ‡ JHTDB åƒè€ƒæ•¸æ“šæ¯”è¼ƒ

åŠŸèƒ½ï¼š
1. è¼‰å…¥è¨“ç·´å®Œæˆçš„æ¨¡å‹èˆ‡ JHTDB åƒè€ƒæ•¸æ“š
2. è¨ˆç®—å…¨é¢çš„ç‰©ç†é©—è­‰æŒ‡æ¨™ï¼ˆèª¤å·®ã€å®ˆæ†ã€èƒ½è­œã€å£å‰ªæ‡‰åŠ›ï¼‰
3. ç”Ÿæˆé«˜è³ªé‡å¯è¦–åŒ–ï¼ˆå ´æ¯”è¼ƒã€èª¤å·®åˆ†å¸ƒã€çµ±è¨ˆåœ–ï¼‰
4. è¼¸å‡ºçµæ§‹åŒ–è©•ä¼°å ±å‘Šï¼ˆMarkdown + JSONï¼‰
5. é©—è­‰æˆæ•—é–€æª»ï¼ˆç›¸å° L2 â‰¤ 10-15%ï¼Œçµ±è¨ˆæ”¹å–„ â‰¥ 30%ï¼‰

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python scripts/comprehensive_evaluation.py \
        --checkpoint checkpoints/vs_pinn_3d_full_training_latest.pth \
        --config configs/vs_pinn_3d_full_training.yml \
        --reference data/jhtdb/channel_flow_re1000/cutout3d_128x128x32.npz \
        --output_dir results/comprehensive_eval_<timestamp>
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import yaml
import logging
import json
from typing import Dict, Tuple, Optional, List
import argparse
from datetime import datetime

# è¨­ç½®æ¨£å¼
sns.set_context("paper", font_scale=1.5)
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 150
plt.rcParams['font.family'] = 'sans-serif'

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å°å…¥å°ˆæ¡ˆæ¨¡çµ„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

import pinnx
from pinnx.models.fourier_mlp import PINNNet
# from pinnx.physics.vs_pinn_channel_flow import create_vs_pinn_channel_flow  # Not needed for evaluation
from pinnx.evals.metrics import (
    relative_L2, rmse_metrics, field_statistics,
    energy_spectrum_1d, conservation_error
)


# ============================================================
# æ¨¡å‹è¼‰å…¥èˆ‡æ¨ç†
# ============================================================

def load_trained_model(checkpoint_path: Path, config: Dict, device: torch.device):
    """è¼‰å…¥è¨“ç·´å®Œæˆçš„æ¨¡å‹"""
    logger.info(f"ğŸ“¥ Loading model from {checkpoint_path}")
    
    # ğŸ”§ å¾é…ç½®æ–‡ä»¶æ§‹å»º statistics ä»¥æ”¯æŒ 3D æ¨¡å‹
    # é€™ç¢ºä¿ ManualScalingWrapper èƒ½æ­£ç¢ºè¨­ç½® input_min/max çš„å½¢ç‹€
    statistics = None
    if 'physics' in config and 'domain' in config['physics']:
        domain = config['physics']['domain']
        statistics = {
            'x': {'range': domain.get('x_range', [0.0, 25.13])},
            'y': {'range': domain.get('y_range', [-1.0, 1.0])}
        }
        # å¦‚æœæ˜¯ 3Dï¼Œæ·»åŠ  z ç¯„åœ
        if 'z_range' in domain:
            statistics['z'] = {'range': domain['z_range']}
        logger.info(f"ğŸ“ Constructed statistics from config: {list(statistics.keys())}")
    
    # å‰µå»ºæ¨¡å‹æ¶æ§‹
    from scripts.train import create_model
    model = create_model(config, device, statistics=statistics)
    
    # è¼‰å…¥æ¬Šé‡
    checkpoint = torch.load(checkpoint_path, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        epoch = checkpoint.get('epoch', 'unknown')
        logger.info(f"âœ… Loaded checkpoint from epoch {epoch}")
    else:
        model.load_state_dict(checkpoint)
        logger.info(f"âœ… Loaded checkpoint (legacy format)")
    
    model.eval()
    return model


def load_jhtdb_reference(data_path: Path) -> Dict[str, np.ndarray]:
    """è¼‰å…¥ JHTDB åƒè€ƒæ•¸æ“š"""
    logger.info(f"ğŸ“¥ Loading JHTDB reference from {data_path}")
    
    data = np.load(data_path)
    
    # æª¢æŸ¥æ•¸æ“šæ ¼å¼
    required_fields = ['u', 'v', 'w', 'p', 'x', 'y', 'z']
    for field in required_fields:
        if field not in data:
            logger.warning(f"âš ï¸  Field '{field}' not found in reference data")
    
    logger.info(f"âœ… Loaded reference data: u{data['u'].shape}, "
                f"domain: x[{data['x'].min():.2f}, {data['x'].max():.2f}], "
                f"y[{data['y'].min():.2f}, {data['y'].max():.2f}], "
                f"z[{data['z'].min():.2f}, {data['z'].max():.2f}]")
    
    return {key: data[key] for key in data.files}


def predict_on_grid(model, x: np.ndarray, y: np.ndarray, z: np.ndarray, 
                    device: torch.device, batch_size: int = 10000) -> Dict[str, np.ndarray]:
    """åœ¨ç¶²æ ¼ä¸Šé€²è¡Œé æ¸¬"""
    logger.info(f"ğŸ”® Predicting on grid: {len(x)}Ã—{len(y)}Ã—{len(z)} = {len(x)*len(y)*len(z)} points")
    
    # ç”Ÿæˆç¶²æ ¼é»
    X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
    points = np.stack([X.flatten(), Y.flatten(), Z.flatten()], axis=1)
    n_points = points.shape[0]
    
    # åˆ†æ‰¹é æ¸¬
    u_list, v_list, w_list, p_list = [], [], [], []
    
    with torch.no_grad():
        for i in range(0, n_points, batch_size):
            batch = torch.tensor(points[i:i+batch_size], dtype=torch.float32, device=device)
            pred = model(batch)
            
            u_list.append(pred[:, 0].cpu().numpy())
            v_list.append(pred[:, 1].cpu().numpy())
            w_list.append(pred[:, 2].cpu().numpy())
            p_list.append(pred[:, 3].cpu().numpy())
            
            if (i // batch_size + 1) % 10 == 0:
                logger.info(f"  Progress: {i+len(batch)}/{n_points} ({100*(i+len(batch))/n_points:.1f}%)")
    
    # é‡å¡‘ç‚º 3D ç¶²æ ¼
    shape = (len(x), len(y), len(z))
    
    results = {
        'u': np.concatenate(u_list).reshape(shape),
        'v': np.concatenate(v_list).reshape(shape),
        'w': np.concatenate(w_list).reshape(shape),
        'p': np.concatenate(p_list).reshape(shape),
        'x': x,
        'y': y,
        'z': z
    }
    
    logger.info(f"âœ… Prediction complete")
    return results


# ============================================================
# ç‰©ç†æŒ‡æ¨™è¨ˆç®—
# ============================================================

def compute_error_metrics(pred: Dict[str, np.ndarray], 
                          ref: Dict[str, np.ndarray]) -> Dict[str, float]:
    """è¨ˆç®—èª¤å·®æŒ‡æ¨™"""
    logger.info("ğŸ“Š Computing error metrics...")
    
    metrics = {}
    
    for field in ['u', 'v', 'w', 'p']:
        if field not in pred or field not in ref:
            continue
        
        pred_field = pred[field].flatten()
        ref_field = ref[field].flatten()
        
        # ç›¸å° L2 èª¤å·®
        l2_error = np.linalg.norm(pred_field - ref_field) / (np.linalg.norm(ref_field) + 1e-12)
        
        # RMSE
        rmse = np.sqrt(np.mean((pred_field - ref_field)**2))
        
        # ç›¸å° RMSE
        rel_rmse = rmse / (np.std(ref_field) + 1e-12)
        
        # æœ€å¤§çµ•å°èª¤å·®
        max_error = np.max(np.abs(pred_field - ref_field))
        
        metrics[f'{field}_l2_error'] = l2_error
        metrics[f'{field}_rmse'] = rmse
        metrics[f'{field}_rel_rmse'] = rel_rmse
        metrics[f'{field}_max_error'] = max_error
    
    # ç¶œåˆæŒ‡æ¨™
    metrics['overall_l2_error'] = np.mean([
        metrics.get(f'{f}_l2_error', 0) for f in ['u', 'v', 'w']
    ])
    
    logger.info(f"âœ… Overall L2 error: {metrics['overall_l2_error']:.4f}")
    
    return metrics


def compute_field_statistics(pred: Dict[str, np.ndarray], 
                             ref: Dict[str, np.ndarray]) -> Dict[str, Dict[str, float]]:
    """è¨ˆç®—å ´çµ±è¨ˆé‡"""
    logger.info("ğŸ“Š Computing field statistics...")
    
    stats = {'pred': {}, 'ref': {}, 'improvement': {}}
    
    for field in ['u', 'v', 'w', 'p']:
        if field not in pred or field not in ref:
            continue
        
        pred_field = pred[field].flatten()
        ref_field = ref[field].flatten()
        
        # é æ¸¬å ´çµ±è¨ˆ
        stats['pred'][f'{field}_mean'] = np.mean(pred_field)
        stats['pred'][f'{field}_std'] = np.std(pred_field)
        stats['pred'][f'{field}_min'] = np.min(pred_field)
        stats['pred'][f'{field}_max'] = np.max(pred_field)
        
        # åƒè€ƒå ´çµ±è¨ˆ
        stats['ref'][f'{field}_mean'] = np.mean(ref_field)
        stats['ref'][f'{field}_std'] = np.std(ref_field)
        stats['ref'][f'{field}_min'] = np.min(ref_field)
        stats['ref'][f'{field}_max'] = np.max(ref_field)
        
        # çµ±è¨ˆé‡èª¤å·®
        mean_error = np.abs(stats['pred'][f'{field}_mean'] - stats['ref'][f'{field}_mean'])
        std_error = np.abs(stats['pred'][f'{field}_std'] - stats['ref'][f'{field}_std'])
        
        stats['improvement'][f'{field}_mean_error'] = mean_error
        stats['improvement'][f'{field}_std_error'] = std_error
    
    return stats


def compute_wall_shear_stress_comparison(pred: Dict[str, np.ndarray], 
                                         ref: Dict[str, np.ndarray]) -> Dict[str, float]:
    """æ¯”è¼ƒå£é¢å‰ªæ‡‰åŠ›"""
    logger.info("ğŸ“Š Computing wall shear stress comparison...")
    
    # å‡è¨­å£é¢åœ¨ y æ–¹å‘çš„é‚Šç•Œ
    y_idx_lower = 0  # ä¸‹å£é¢
    y_idx_upper = -1  # ä¸Šå£é¢
    
    # è¨ˆç®—é€Ÿåº¦æ¢¯åº¦ï¼ˆä½¿ç”¨æœ‰é™å·®åˆ†ï¼‰
    dy = pred['y'][1] - pred['y'][0]
    
    # ä¸‹å£é¢å‰ªæ‡‰åŠ›ï¼šÏ„_w = Î¼ * âˆ‚u/âˆ‚y
    pred_tau_lower = (pred['u'][:, 1, :] - pred['u'][:, 0, :]) / dy
    ref_tau_lower = (ref['u'][:, 1, :] - ref['u'][:, 0, :]) / dy
    
    # çµ±è¨ˆé‡
    metrics = {
        'pred_tau_mean': np.mean(pred_tau_lower),
        'pred_tau_std': np.std(pred_tau_lower),
        'ref_tau_mean': np.mean(ref_tau_lower),
        'ref_tau_std': np.std(ref_tau_lower),
        'tau_rmse': np.sqrt(np.mean((pred_tau_lower - ref_tau_lower)**2)),
        'tau_rel_error': np.abs(np.mean(pred_tau_lower) - np.mean(ref_tau_lower)) / (np.abs(np.mean(ref_tau_lower)) + 1e-12)
    }
    
    logger.info(f"âœ… Wall shear stress: pred={metrics['pred_tau_mean']:.6f}, "
                f"ref={metrics['ref_tau_mean']:.6f}, error={metrics['tau_rel_error']:.2%}")
    
    return metrics


def compute_energy_spectrum_comparison(pred: Dict[str, np.ndarray], 
                                       ref: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
    """æ¯”è¼ƒèƒ½é‡è­œ"""
    logger.info("ğŸ“Š Computing energy spectrum comparison...")
    
    # é¸æ“‡ä¸­é–“ y å¹³é¢
    y_mid = len(pred['y']) // 2
    
    # è¨ˆç®—å‹•èƒ½ï¼š0.5 * (uÂ² + vÂ² + wÂ²)
    pred_ke = 0.5 * (pred['u'][:, y_mid, :]**2 + pred['v'][:, y_mid, :]**2 + pred['w'][:, y_mid, :]**2)
    ref_ke = 0.5 * (ref['u'][:, y_mid, :]**2 + ref['v'][:, y_mid, :]**2 + ref['w'][:, y_mid, :]**2)
    
    # 2D FFT
    pred_fft = np.fft.fft2(pred_ke)
    ref_fft = np.fft.fft2(ref_ke)
    
    # èƒ½é‡è­œ
    pred_spectrum = np.abs(np.fft.fftshift(pred_fft))**2
    ref_spectrum = np.abs(np.fft.fftshift(ref_fft))**2
    
    # å¾‘å‘å¹³å‡
    h, w = pred_ke.shape
    center_h, center_w = h // 2, w // 2
    
    y_idx, x_idx = np.ogrid[:h, :w]
    r = np.sqrt((x_idx - center_w)**2 + (y_idx - center_h)**2).astype(int)
    
    k_max = min(center_h, center_w)
    k_bins = np.arange(1, k_max)
    
    pred_radial = np.zeros(len(k_bins))
    ref_radial = np.zeros(len(k_bins))
    
    for i, k in enumerate(k_bins):
        mask = (r == k)
        if mask.sum() > 0:
            pred_radial[i] = pred_spectrum[mask].mean()
            ref_radial[i] = ref_spectrum[mask].mean()
    
    # è¨ˆç®—èƒ½è­œ RMSE
    spectrum_rmse = np.sqrt(np.mean((pred_radial - ref_radial)**2))
    spectrum_rel_error = spectrum_rmse / (np.mean(ref_radial) + 1e-12)
    
    logger.info(f"âœ… Energy spectrum RMSE: {spectrum_rmse:.2e}, rel_error: {spectrum_rel_error:.2%}")
    
    return {
        'k': k_bins,
        'pred_spectrum': pred_radial,
        'ref_spectrum': ref_radial,
        'spectrum_rmse': spectrum_rmse,
        'spectrum_rel_error': spectrum_rel_error
    }


# ============================================================
# å¯è¦–åŒ–å‡½æ•¸
# ============================================================

def plot_error_distribution(pred: Dict[str, np.ndarray], 
                            ref: Dict[str, np.ndarray], 
                            save_dir: Path):
    """ç¹ªè£½èª¤å·®åˆ†å¸ƒåœ–"""
    logger.info("ğŸ¨ Plotting error distribution...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 14))
    
    # é¸æ“‡ä¸­é–“ z å¹³é¢
    z_mid = len(pred['z']) // 2
    
    for idx, field in enumerate(['u', 'v', 'w', 'p']):
        ax = axes[idx // 2, idx % 2]
        
        pred_slice = pred[field][:, :, z_mid]
        ref_slice = ref[field][:, :, z_mid]
        error = np.abs(pred_slice - ref_slice)
        
        im = ax.contourf(pred['x'], pred['y'], error.T, levels=20, cmap='hot')
        ax.set_xlabel('x', fontsize=14)
        ax.set_ylabel('y', fontsize=14)
        ax.set_title(f'{field.upper()} Absolute Error (z={pred["z"][z_mid]:.2f})', fontsize=16)
        plt.colorbar(im, ax=ax, label='|pred - ref|')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'error_distribution.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved error distribution to {save_dir / 'error_distribution.png'}")
    plt.close()


def plot_field_comparison(pred: Dict[str, np.ndarray], 
                          ref: Dict[str, np.ndarray], 
                          save_dir: Path):
    """ç¹ªè£½å ´æ¯”è¼ƒåœ–ï¼ˆé æ¸¬ vs åƒè€ƒï¼‰"""
    logger.info("ğŸ¨ Plotting field comparison...")
    
    z_mid = len(pred['z']) // 2
    
    for field in ['u', 'v', 'w', 'p']:
        fig, axes = plt.subplots(1, 3, figsize=(20, 5))
        
        pred_slice = pred[field][:, :, z_mid]
        ref_slice = ref[field][:, :, z_mid]
        
        # çµ±ä¸€è‰²éš
        vmin = min(pred_slice.min(), ref_slice.min())
        vmax = max(pred_slice.max(), ref_slice.max())
        
        # é æ¸¬å ´
        im0 = axes[0].contourf(pred['x'], pred['y'], pred_slice.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)
        axes[0].set_title(f'{field.upper()} - Predicted', fontsize=16)
        axes[0].set_xlabel('x', fontsize=14)
        axes[0].set_ylabel('y', fontsize=14)
        plt.colorbar(im0, ax=axes[0])
        
        # åƒè€ƒå ´
        im1 = axes[1].contourf(ref['x'], ref['y'], ref_slice.T, levels=20, cmap='RdBu_r', vmin=vmin, vmax=vmax)
        axes[1].set_title(f'{field.upper()} - Reference (JHTDB)', fontsize=16)
        axes[1].set_xlabel('x', fontsize=14)
        axes[1].set_ylabel('y', fontsize=14)
        plt.colorbar(im1, ax=axes[1])
        
        # èª¤å·®å ´
        error = pred_slice - ref_slice
        im2 = axes[2].contourf(pred['x'], pred['y'], error.T, levels=20, cmap='seismic')
        axes[2].set_title(f'{field.upper()} - Error', fontsize=16)
        axes[2].set_xlabel('x', fontsize=14)
        axes[2].set_ylabel('y', fontsize=14)
        plt.colorbar(im2, ax=axes[2], label='pred - ref')
        
        plt.tight_layout()
        plt.savefig(save_dir / f'field_comparison_{field}.png', dpi=150, bbox_inches='tight')
        logger.info(f"âœ… Saved {field} comparison")
        plt.close()


def plot_velocity_profiles(pred: Dict[str, np.ndarray], 
                           ref: Dict[str, np.ndarray], 
                           save_dir: Path):
    """ç¹ªè£½é€Ÿåº¦å‰–é¢æ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting velocity profiles...")
    
    # é¸æ“‡åŸŸä¸­å¿ƒ
    x_mid = len(pred['x']) // 2
    z_mid = len(pred['z']) // 2
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    for idx, field in enumerate(['u', 'v', 'w']):
        ax = axes[idx]
        
        pred_profile = pred[field][x_mid, :, z_mid]
        ref_profile = ref[field][x_mid, :, z_mid]
        
        ax.plot(pred_profile, pred['y'], 'b-', linewidth=2, label='Predicted')
        ax.plot(ref_profile, ref['y'], 'r--', linewidth=2, label='JHTDB Reference')
        
        ax.set_xlabel(f'{field}', fontsize=14)
        ax.set_ylabel('y', fontsize=14)
        ax.set_title(f'{field.upper()} Velocity Profile (x={pred["x"][x_mid]:.2f}, z={pred["z"][z_mid]:.2f})', fontsize=16)
        ax.legend(fontsize=12)
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'velocity_profiles_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved velocity profiles")
    plt.close()


def plot_energy_spectrum(spectrum_data: Dict, save_dir: Path):
    """ç¹ªè£½èƒ½é‡è­œæ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting energy spectrum...")
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    k = spectrum_data['k']
    pred_spec = spectrum_data['pred_spectrum']
    ref_spec = spectrum_data['ref_spectrum']
    
    # ç·šæ€§å°ºåº¦
    axes[0].plot(k, pred_spec, 'b-', linewidth=2, label='Predicted')
    axes[0].plot(k, ref_spec, 'r--', linewidth=2, label='JHTDB Reference')
    axes[0].set_xlabel('Wavenumber k', fontsize=14)
    axes[0].set_ylabel('E(k)', fontsize=14)
    axes[0].set_title('Energy Spectrum (Linear Scale)', fontsize=16)
    axes[0].legend(fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # å°æ•¸å°ºåº¦
    axes[1].loglog(k, pred_spec, 'b-', linewidth=2, label='Predicted')
    axes[1].loglog(k, ref_spec, 'r--', linewidth=2, label='JHTDB Reference')
    
    # ç¹ªè£½ -5/3 å¾‹åƒè€ƒç·š
    k_ref = k[len(k)//4:len(k)//2]
    spec_ref = ref_spec[len(k)//4] * (k_ref / k[len(k)//4])**(-5/3)
    axes[1].loglog(k_ref, spec_ref, 'k:', linewidth=1.5, label=r'$k^{-5/3}$ law')
    
    axes[1].set_xlabel('Wavenumber k', fontsize=14)
    axes[1].set_ylabel('E(k)', fontsize=14)
    axes[1].set_title('Energy Spectrum (Log-Log Scale)', fontsize=16)
    axes[1].legend(fontsize=12)
    axes[1].grid(True, alpha=0.3, which='both')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'energy_spectrum_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved energy spectrum")
    plt.close()


def plot_wall_shear_stress(pred: Dict[str, np.ndarray], 
                           ref: Dict[str, np.ndarray], 
                           save_dir: Path):
    """ç¹ªè£½å£é¢å‰ªæ‡‰åŠ›æ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting wall shear stress...")
    
    # è¨ˆç®—å£é¢å‰ªæ‡‰åŠ›
    dy = pred['y'][1] - pred['y'][0]
    pred_tau = (pred['u'][:, 1, :] - pred['u'][:, 0, :]) / dy
    ref_tau = (ref['u'][:, 1, :] - ref['u'][:, 0, :]) / dy
    
    fig, axes = plt.subplots(1, 3, figsize=(20, 5))
    
    # çµ±ä¸€è‰²éš
    vmin = min(pred_tau.min(), ref_tau.min())
    vmax = max(pred_tau.max(), ref_tau.max())
    
    # é æ¸¬
    im0 = axes[0].contourf(pred['x'], pred['z'], pred_tau.T, levels=20, cmap='viridis', vmin=vmin, vmax=vmax)
    axes[0].set_title('Wall Shear Stress - Predicted', fontsize=16)
    axes[0].set_xlabel('x', fontsize=14)
    axes[0].set_ylabel('z', fontsize=14)
    plt.colorbar(im0, ax=axes[0], label=r'$\tau_w$')
    
    # åƒè€ƒ
    im1 = axes[1].contourf(ref['x'], ref['z'], ref_tau.T, levels=20, cmap='viridis', vmin=vmin, vmax=vmax)
    axes[1].set_title('Wall Shear Stress - Reference', fontsize=16)
    axes[1].set_xlabel('x', fontsize=14)
    axes[1].set_ylabel('z', fontsize=14)
    plt.colorbar(im1, ax=axes[1], label=r'$\tau_w$')
    
    # èª¤å·®
    error = pred_tau - ref_tau
    im2 = axes[2].contourf(pred['x'], pred['z'], error.T, levels=20, cmap='seismic')
    axes[2].set_title('Wall Shear Stress - Error', fontsize=16)
    axes[2].set_xlabel('x', fontsize=14)
    axes[2].set_ylabel('z', fontsize=14)
    plt.colorbar(im2, ax=axes[2], label=r'$\Delta\tau_w$')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'wall_shear_stress_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved wall shear stress comparison")
    plt.close()


def plot_statistics_comparison(stats: Dict, save_dir: Path):
    """ç¹ªè£½çµ±è¨ˆé‡æ¯”è¼ƒ"""
    logger.info("ğŸ¨ Plotting statistics comparison...")
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    fields = ['u', 'v', 'w', 'p']
    metrics = ['mean', 'std', 'min', 'max']
    
    for idx, field in enumerate(fields):
        ax = axes[idx // 2, idx % 2]
        
        pred_values = [stats['pred'].get(f'{field}_{m}', 0) for m in metrics]
        ref_values = [stats['ref'].get(f'{field}_{m}', 0) for m in metrics]
        
        x = np.arange(len(metrics))
        width = 0.35
        
        ax.bar(x - width/2, pred_values, width, label='Predicted', alpha=0.8)
        ax.bar(x + width/2, ref_values, width, label='JHTDB Reference', alpha=0.8)
        
        ax.set_xlabel('Statistics', fontsize=14)
        ax.set_ylabel('Value', fontsize=14)
        ax.set_title(f'{field.upper()} Statistics Comparison', fontsize=16)
        ax.set_xticks(x)
        ax.set_xticklabels(metrics)
        ax.legend(fontsize=12)
        ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.savefig(save_dir / 'statistics_comparison.png', dpi=150, bbox_inches='tight')
    logger.info(f"âœ… Saved statistics comparison")
    plt.close()


# ============================================================
# å ±å‘Šç”Ÿæˆ
# ============================================================

def generate_markdown_report(metrics: Dict, stats: Dict, spectrum_data: Dict, 
                            wall_metrics: Dict, config: Dict, 
                            checkpoint_path: str, save_path: Path):
    """ç”Ÿæˆ Markdown è©•ä¼°å ±å‘Š"""
    logger.info("ğŸ“ Generating Markdown report...")
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    # æª¢æŸ¥æˆæ•—é–€æª»
    overall_l2 = metrics.get('overall_l2_error', float('inf'))
    threshold_pass = "âœ… PASS" if overall_l2 <= 0.15 else "âŒ FAIL"
    
    report = f"""# 3D VS-PINN ç¶œåˆè©•ä¼°å ±å‘Š

**ç”Ÿæˆæ™‚é–“**: {timestamp}  
**æª¢æŸ¥é»**: `{checkpoint_path}`  
**é…ç½®æ–‡ä»¶**: `{config.get('_config_path', 'N/A')}`

---

## ğŸ“Š åŸ·è¡Œæ‘˜è¦

### æˆæ•—é–€æª»é©—è­‰

| æŒ‡æ¨™ | ç›®æ¨™ | å¯¦éš›å€¼ | ç‹€æ…‹ |
|------|------|--------|------|
| æ•´é«”ç›¸å° L2 èª¤å·® | â‰¤ 10-15% | **{overall_l2:.2%}** | {threshold_pass} |
| u ç›¸å° L2 èª¤å·® | â‰¤ 15% | {metrics.get('u_l2_error', 0):.2%} | {"âœ…" if metrics.get('u_l2_error', 1) <= 0.15 else "âŒ"} |
| v ç›¸å° L2 èª¤å·® | â‰¤ 15% | {metrics.get('v_l2_error', 0):.2%} | {"âœ…" if metrics.get('v_l2_error', 1) <= 0.15 else "âŒ"} |
| w ç›¸å° L2 èª¤å·® | â‰¤ 15% | {metrics.get('w_l2_error', 0):.2%} | {"âœ…" if metrics.get('w_l2_error', 1) <= 0.15 else "âŒ"} |
| p ç›¸å° L2 èª¤å·® | â‰¤ 20% | {metrics.get('p_l2_error', 0):.2%} | {"âœ…" if metrics.get('p_l2_error', 1) <= 0.20 else "âŒ"} |

### é—œéµç™¼ç¾

- **æ•´é«”ç²¾åº¦**: ç›¸å° L2 èª¤å·® = **{overall_l2:.2%}**
- **å£é¢å‰ªæ‡‰åŠ›**: ç›¸å°èª¤å·® = **{wall_metrics.get('tau_rel_error', 0):.2%}**
- **èƒ½é‡è­œ**: ç›¸å°èª¤å·® = **{spectrum_data.get('spectrum_rel_error', 0):.2%}**

---

## ğŸ¯ è©³ç´°èª¤å·®åˆ†æ

### å ´èª¤å·®æŒ‡æ¨™

| å ´ | ç›¸å° L2 | RMSE | ç›¸å° RMSE | æœ€å¤§èª¤å·® |
|---|---------|------|-----------|----------|
| **u** | {metrics.get('u_l2_error', 0):.4f} | {metrics.get('u_rmse', 0):.6f} | {metrics.get('u_rel_rmse', 0):.4f} | {metrics.get('u_max_error', 0):.6f} |
| **v** | {metrics.get('v_l2_error', 0):.4f} | {metrics.get('v_rmse', 0):.6f} | {metrics.get('v_rel_rmse', 0):.4f} | {metrics.get('v_max_error', 0):.6f} |
| **w** | {metrics.get('w_l2_error', 0):.4f} | {metrics.get('w_rmse', 0):.6f} | {metrics.get('w_rel_rmse', 0):.4f} | {metrics.get('w_max_error', 0):.6f} |
| **p** | {metrics.get('p_l2_error', 0):.4f} | {metrics.get('p_rmse', 0):.6f} | {metrics.get('p_rel_rmse', 0):.4f} | {metrics.get('p_max_error', 0):.6f} |

---

## ğŸ“ˆ å ´çµ±è¨ˆæ¯”è¼ƒ

### æµå‘é€Ÿåº¦ (u)

| çµ±è¨ˆé‡ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | çµ•å°èª¤å·® | ç›¸å°èª¤å·® |
|--------|--------|-----------|----------|----------|
| Mean | {stats['pred'].get('u_mean', 0):.6f} | {stats['ref'].get('u_mean', 0):.6f} | {stats['improvement'].get('u_mean_error', 0):.6f} | {stats['improvement'].get('u_mean_error', 0) / (abs(stats['ref'].get('u_mean', 1)) + 1e-12):.2%} |
| Std | {stats['pred'].get('u_std', 0):.6f} | {stats['ref'].get('u_std', 0):.6f} | {stats['improvement'].get('u_std_error', 0):.6f} | {stats['improvement'].get('u_std_error', 0) / (stats['ref'].get('u_std', 1) + 1e-12):.2%} |
| Min | {stats['pred'].get('u_min', 0):.6f} | {stats['ref'].get('u_min', 0):.6f} | - | - |
| Max | {stats['pred'].get('u_max', 0):.6f} | {stats['ref'].get('u_max', 0):.6f} | - | - |

### æ³•å‘é€Ÿåº¦ (v)

| çµ±è¨ˆé‡ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | çµ•å°èª¤å·® | ç›¸å°èª¤å·® |
|--------|--------|-----------|----------|----------|
| Mean | {stats['pred'].get('v_mean', 0):.6f} | {stats['ref'].get('v_mean', 0):.6f} | {stats['improvement'].get('v_mean_error', 0):.6f} | {stats['improvement'].get('v_mean_error', 0) / (abs(stats['ref'].get('v_mean', 1e-12)) + 1e-12):.2%} |
| Std | {stats['pred'].get('v_std', 0):.6f} | {stats['ref'].get('v_std', 0):.6f} | {stats['improvement'].get('v_std_error', 0):.6f} | {stats['improvement'].get('v_std_error', 0) / (stats['ref'].get('v_std', 1) + 1e-12):.2%} |

### å±•å‘é€Ÿåº¦ (w)

| çµ±è¨ˆé‡ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | çµ•å°èª¤å·® | ç›¸å°èª¤å·® |
|--------|--------|-----------|----------|----------|
| Mean | {stats['pred'].get('w_mean', 0):.6f} | {stats['ref'].get('w_mean', 0):.6f} | {stats['improvement'].get('w_mean_error', 0):.6f} | {stats['improvement'].get('w_mean_error', 0) / (abs(stats['ref'].get('w_mean', 1e-12)) + 1e-12):.2%} |
| Std | {stats['pred'].get('w_std', 0):.6f} | {stats['ref'].get('w_std', 0):.6f} | {stats['improvement'].get('w_std_error', 0):.6f} | {stats['improvement'].get('w_std_error', 0) / (stats['ref'].get('w_std', 1) + 1e-12):.2%} |

---

## ğŸŒŠ ç‰©ç†é©—è­‰

### å£é¢å‰ªæ‡‰åŠ›

| æŒ‡æ¨™ | é æ¸¬å€¼ | JHTDB åƒè€ƒ | èª¤å·® |
|------|--------|-----------|------|
| Mean Ï„_w | {wall_metrics.get('pred_tau_mean', 0):.6f} | {wall_metrics.get('ref_tau_mean', 0):.6f} | {wall_metrics.get('tau_rel_error', 0):.2%} |
| Std Ï„_w | {wall_metrics.get('pred_tau_std', 0):.6f} | {wall_metrics.get('ref_tau_std', 0):.6f} | - |
| RMSE | {wall_metrics.get('tau_rmse', 0):.6f} | - | - |

### èƒ½é‡è­œ

- **è­œ RMSE**: {spectrum_data.get('spectrum_rmse', 0):.6e}
- **è­œç›¸å°èª¤å·®**: {spectrum_data.get('spectrum_rel_error', 0):.2%}
- **æ³¢æ•¸ç¯„åœ**: k âˆˆ [{spectrum_data['k'][0]:.2f}, {spectrum_data['k'][-1]:.2f}]

---

## ğŸ“ è¼¸å‡ºæ–‡ä»¶

### å¯è¦–åŒ–åœ–è¡¨

- `error_distribution.png` - èª¤å·®åˆ†å¸ƒï¼ˆ4 å ´ï¼‰
- `field_comparison_u.png` - u å ´æ¯”è¼ƒï¼ˆé æ¸¬ vs åƒè€ƒ vs èª¤å·®ï¼‰
- `field_comparison_v.png` - v å ´æ¯”è¼ƒ
- `field_comparison_w.png` - w å ´æ¯”è¼ƒ
- `field_comparison_p.png` - p å ´æ¯”è¼ƒ
- `velocity_profiles_comparison.png` - é€Ÿåº¦å‰–é¢æ¯”è¼ƒ
- `energy_spectrum_comparison.png` - èƒ½é‡è­œæ¯”è¼ƒï¼ˆç·šæ€§ & å°æ•¸ï¼‰
- `wall_shear_stress_comparison.png` - å£é¢å‰ªæ‡‰åŠ›æ¯”è¼ƒ
- `statistics_comparison.png` - çµ±è¨ˆé‡æ¯”è¼ƒ

### æ•¸æ“šæ–‡ä»¶

- `evaluation_metrics.json` - å®Œæ•´æŒ‡æ¨™ï¼ˆJSON æ ¼å¼ï¼‰
- `predicted_field.npz` - é æ¸¬æµå ´æ•¸æ“š

---

## ğŸ”§ è¨“ç·´é…ç½®

```yaml
Model: {config.get('model', {})}
Physics: {config.get('physics', {})}
Training: {config.get('training', {})}
```

---

**å ±å‘ŠçµæŸ** | ç”Ÿæˆæ–¼ {timestamp}
"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"âœ… Saved Markdown report to {save_path}")


def save_metrics_json(metrics: Dict, stats: Dict, spectrum_data: Dict, 
                      wall_metrics: Dict, save_path: Path):
    """ä¿å­˜æŒ‡æ¨™ç‚º JSON æ ¼å¼"""
    logger.info("ğŸ’¾ Saving metrics to JSON...")
    
    # éè¿´è½‰æ› numpy é¡å‹ç‚º Python åŸç”Ÿé¡å‹
    def convert_to_json_serializable(obj):
        if isinstance(obj, dict):
            return {k: convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_json_serializable(item) for item in obj]
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        else:
            return obj
    
    data = {
        'timestamp': datetime.now().isoformat(),
        'error_metrics': convert_to_json_serializable(metrics),
        'field_statistics': convert_to_json_serializable(stats),
        'wall_shear_stress': convert_to_json_serializable(wall_metrics),
        'energy_spectrum': {
            'spectrum_rmse': float(spectrum_data.get('spectrum_rmse', 0)),
            'spectrum_rel_error': float(spectrum_data.get('spectrum_rel_error', 0))
        },
        'success_criteria': {
            'overall_l2_threshold': 0.15,
            'overall_l2_actual': float(metrics.get('overall_l2_error', 0)),
            'passed': bool(metrics.get('overall_l2_error', 1) <= 0.15)
        }
    }
    
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… Saved JSON metrics to {save_path}")


# ============================================================
# ä¸»å‡½æ•¸
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='Comprehensive 3D VS-PINN Evaluation')
    parser.add_argument('--checkpoint', type=str, required=True, help='Path to checkpoint')
    parser.add_argument('--config', type=str, required=True, help='Path to config YAML')
    parser.add_argument('--reference', type=str, required=True, help='Path to JHTDB reference data')
    parser.add_argument('--output_dir', type=str, default=None, help='Output directory')
    parser.add_argument('--device', type=str, default='auto', help='Device (cuda/cpu/mps/auto)')
    parser.add_argument('--batch_size', type=int, default=10000, help='Batch size for prediction')
    
    args = parser.parse_args()
    
    # è¨­ç½®è¼¸å‡ºç›®éŒ„
    if args.output_dir is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = Path(f'results/comprehensive_eval_{timestamp}')
    else:
        output_dir = Path(args.output_dir)
    
    output_dir.mkdir(exist_ok=True, parents=True)
    logger.info(f"ğŸ“ Output directory: {output_dir}")
    
    # è¼‰å…¥é…ç½®
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    config['_config_path'] = args.config
    
    # è¨­ç½®è¨­å‚™
    if args.device == 'auto':
        if torch.cuda.is_available():
            device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            device = torch.device('mps')
        else:
            device = torch.device('cpu')
    else:
        device = torch.device(args.device)
    
    logger.info(f"ğŸ–¥ï¸  Using device: {device}")
    
    # ========== è¼‰å…¥æ¨¡å‹èˆ‡æ•¸æ“š ==========
    model = load_trained_model(Path(args.checkpoint), config, device)
    ref_data = load_jhtdb_reference(Path(args.reference))
    
    # ========== é æ¸¬ ==========
    pred_data = predict_on_grid(
        model, 
        ref_data['x'], 
        ref_data['y'], 
        ref_data['z'], 
        device, 
        batch_size=args.batch_size
    )
    
    # ä¿å­˜é æ¸¬å ´
    np.savez(
        output_dir / 'predicted_field.npz',
        **pred_data
    )
    logger.info(f"ğŸ’¾ Saved predicted field to {output_dir / 'predicted_field.npz'}")
    
    # ========== è¨ˆç®—æŒ‡æ¨™ ==========
    error_metrics = compute_error_metrics(pred_data, ref_data)
    field_stats = compute_field_statistics(pred_data, ref_data)
    wall_metrics = compute_wall_shear_stress_comparison(pred_data, ref_data)
    spectrum_data = compute_energy_spectrum_comparison(pred_data, ref_data)
    
    # ========== å¯è¦–åŒ– ==========
    plot_error_distribution(pred_data, ref_data, output_dir)
    plot_field_comparison(pred_data, ref_data, output_dir)
    plot_velocity_profiles(pred_data, ref_data, output_dir)
    plot_energy_spectrum(spectrum_data, output_dir)
    plot_wall_shear_stress(pred_data, ref_data, output_dir)
    plot_statistics_comparison(field_stats, output_dir)
    
    # ========== ç”Ÿæˆå ±å‘Š ==========
    generate_markdown_report(
        error_metrics, field_stats, spectrum_data, wall_metrics,
        config, args.checkpoint, output_dir / 'evaluation_report.md'
    )
    
    save_metrics_json(
        error_metrics, field_stats, spectrum_data, wall_metrics,
        output_dir / 'evaluation_metrics.json'
    )
    
    # ========== çµ‚ç«¯è¼¸å‡ºæ‘˜è¦ ==========
    logger.info("\n" + "="*60)
    logger.info("ğŸ‰ è©•ä¼°å®Œæˆï¼")
    logger.info("="*60)
    logger.info(f"ğŸ“Š æ•´é«”ç›¸å° L2 èª¤å·®: {error_metrics.get('overall_l2_error', 0):.2%}")
    logger.info(f"ğŸŒŠ å£é¢å‰ªæ‡‰åŠ›èª¤å·®: {wall_metrics.get('tau_rel_error', 0):.2%}")
    logger.info(f"ğŸ“ˆ èƒ½é‡è­œèª¤å·®: {spectrum_data.get('spectrum_rel_error', 0):.2%}")
    logger.info(f"ğŸ“ çµæœä¿å­˜æ–¼: {output_dir}")
    logger.info("="*60)
    
    # æª¢æŸ¥æˆæ•—
    if error_metrics.get('overall_l2_error', 1) <= 0.15:
        logger.info("âœ… æˆåŠŸï¼æ•´é«”èª¤å·®ä½æ–¼ 15% é–€æª»")
    else:
        logger.warning("âš ï¸  æ•´é«”èª¤å·®è¶…é 15% é–€æª»ï¼Œå»ºè­°é€²ä¸€æ­¥èª¿å„ª")


if __name__ == '__main__':
    main()
