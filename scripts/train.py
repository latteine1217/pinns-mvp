#!/usr/bin/env python3
"""
PINNs é€†é‡å»ºä¸»è¨“ç·´è…³æœ¬
è² è²¬å”èª¿è³‡æ–™è¼‰å…¥ã€æ¨¡å‹å»ºç«‹ã€è¨“ç·´è¿´åœˆèˆ‡è©•ä¼°è¼¸å‡º
"""

import argparse
import logging
import os
import random
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List

import numpy as np
import torch
import torch.nn as nn
import yaml

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from pinnx.models.fourier_mlp import PINNNet, create_enhanced_pinn, init_siren_weights
from pinnx.models.wrappers import ScaledPINNWrapper
from pinnx.physics.scaling import VSScaler
from pinnx.physics.ns_2d import NSEquations2D
from pinnx.physics import create_vs_pinn_channel_flow, VSPINNChannelFlow  # VS-PINN
from pinnx.losses.residuals import NSResidualLoss, BoundaryConditionLoss
from pinnx.losses.priors import PriorLossManager
from pinnx.losses.weighting import GradNormWeighter, CausalWeighter, AdaptiveWeightScheduler
from pinnx.losses import MeanConstraintLoss  # â­ Phase 6C: å‡å€¼ç´„æŸæå¤±
from pinnx.train.loop import TrainingLoopManager, apply_point_weights_to_loss  # è‡ªé©æ‡‰æ¡æ¨£ç®¡ç†å™¨
from pinnx.train.trainer import Trainer  # æ–°çš„è¨“ç·´å™¨é¡
from pinnx.utils.normalization import InputNormalizer, NormalizationConfig
from pinnx.evals.metrics import relative_L2

# å¾é‡æ§‹æ¨¡çµ„å°å…¥é…ç½®ã€æª¢æŸ¥é»èˆ‡å·¥å» å‡½æ•¸
from pinnx.train.config_loader import (
    load_config,
    normalize_config_structure,
    derive_loss_weights,
)
from pinnx.train.checkpointing import (
    save_checkpoint,
    load_checkpoint,
)
from pinnx.train.factory import (
    get_device,
    create_model,
    create_physics,
    create_optimizer,
)
from pinnx.utils.setup import (
    setup_logging,
    set_random_seed,
)

# ============================================================================
# å…¨å±€è®Šæ•¸ï¼ˆä¿ç•™è¨“ç·´å°ˆç”¨å¿«å–ï¼‰
# ============================================================================


def _collect_coordinate_tensors(training_data: Dict[str, torch.Tensor]) -> List[torch.Tensor]:
    prefixes = ['sensors', 'pde', 'bc', 'ic']
    coords: List[torch.Tensor] = []
    axes = ['x', 'y', 'z']
    for prefix in prefixes:
        components = []
        for axis in axes:
            key = f'{axis}_{prefix}'
            if key in training_data and training_data[key].numel() > 0:
                components.append(training_data[key])
        if components:
            coords.append(torch.cat(components, dim=1))
    return coords


def create_input_normalizer(
    config: Dict[str, Any],
    training_data: Dict[str, torch.Tensor],
    is_vs_pinn: bool,
    device: torch.device
) -> Optional[InputNormalizer]:
    scaling_cfg = config.get('model', {}).get('scaling', {})
    norm_type = scaling_cfg.get('input_norm', 'none')
    if norm_type is None:
        norm_type = 'none'

    norm_type = norm_type.lower()

    if norm_type in ('none', 'identity'):
        return None

    if is_vs_pinn and norm_type in ('vs_pinn', 'channel_flow'):
        # VS-PINN already applies dedicated scaling; avoid double normalization.
        return None

    bounds_tensor: Optional[torch.Tensor] = None
    if norm_type in ('channel_flow',):
        domain = config.get('physics', {}).get('domain', {})
        bounds: List[Tuple[float, float]] = []
        for axis in ['x', 'y', 'z']:
            rng = domain.get(f'{axis}_range')
            if rng is not None:
                bounds.append((float(rng[0]), float(rng[1])))
        if bounds:
            bounds_tensor = torch.tensor(bounds, dtype=torch.float32, device=device)

    feature_range = tuple(scaling_cfg.get('input_norm_range', [-1.0, 1.0]))
    config_obj = NormalizationConfig(
        norm_type=norm_type,
        feature_range=(float(feature_range[0]), float(feature_range[1])),
        bounds=bounds_tensor
    )
    normalizer = InputNormalizer(config_obj)

    coord_tensors = _collect_coordinate_tensors(training_data)
    if coord_tensors:
        samples = torch.cat(coord_tensors, dim=0)
        if normalizer.bounds is not None and normalizer.bounds.shape[0] > samples.shape[1]:
            normalizer.bounds = normalizer.bounds[:samples.shape[1], :]
        normalizer.fit(samples)
    else:
        return None

    normalizer.to(device)
    return normalizer


# ============================================================================
# Warmup + CosineAnnealing å­¸ç¿’ç‡èª¿åº¦å™¨
# ============================================================================
class WarmupCosineScheduler:
    """
    Warmup + CosineAnnealing å­¸ç¿’ç‡èª¿åº¦å™¨
    
    å‰ warmup_epochs å€‹ epoch ç·šæ€§å¢åŠ å­¸ç¿’ç‡å¾ 0 åˆ° base_lr
    ä¹‹å¾Œä½¿ç”¨ CosineAnnealing è¡°æ¸›åˆ° min_lr
    """
    
    def __init__(self, optimizer, warmup_epochs: int, max_epochs: int, 
                 base_lr: float, min_lr: float = 0.0):
        """
        Args:
            optimizer: PyTorch å„ªåŒ–å™¨
            warmup_epochs: Warmup éšæ®µçš„ epoch æ•¸é‡
            max_epochs: ç¸½è¨“ç·´ epoch æ•¸é‡
            base_lr: åŸºç¤å­¸ç¿’ç‡ï¼ˆWarmup å¾Œçš„å³°å€¼ï¼‰
            min_lr: æœ€å°å­¸ç¿’ç‡ï¼ˆCosineAnnealing çš„ä¸‹é™ï¼‰
        """
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.current_epoch = 0
        
        # å»ºç«‹å…§éƒ¨çš„ CosineAnnealing èª¿åº¦å™¨ï¼ˆç”¨æ–¼ Warmup å¾Œéšæ®µï¼‰
        self.cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=max_epochs - warmup_epochs,
            eta_min=min_lr
        )
        
        logging.info(f"âœ… WarmupCosineScheduler initialized:")
        logging.info(f"   Warmup epochs: {warmup_epochs}")
        logging.info(f"   Max epochs: {max_epochs}")
        logging.info(f"   Base LR: {base_lr:.6f}")
        logging.info(f"   Min LR: {min_lr:.6f}")
    
    def step(self):
        """æ›´æ–°å­¸ç¿’ç‡"""
        if self.current_epoch < self.warmup_epochs:
            # Warmup éšæ®µï¼šç·šæ€§å¢åŠ 
            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
        else:
            # CosineAnnealing éšæ®µ
            self.cosine_scheduler.step()
        
        self.current_epoch += 1
    
    def get_last_lr(self):
        """è¿”å›ç•¶å‰å­¸ç¿’ç‡ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰"""
        return [param_group['lr'] for param_group in self.optimizer.param_groups]


# ============================================================================
# éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨
# ============================================================================
class StagedWeightScheduler:
    """éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨ - æ ¹æ“š epoch åˆ‡æ›ä¸åŒè¨“ç·´éšæ®µçš„æ¬Šé‡"""
    
    def __init__(self, phases: list):
        """
        Args:
            phases: éšæ®µé…ç½®åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«:
                - name: éšæ®µåç¨±
                - epoch_range: [start, end] epoch ç¯„åœ
                - weights: è©²éšæ®µçš„æ¬Šé‡å­—å…¸
        """
        self.phases = phases
        self.current_phase_idx = 0
        self.current_phase_name = phases[0]['name'] if phases else "default"
        
        # æŒ‰ epoch_range æ’åº
        self.phases.sort(key=lambda p: p['epoch_range'][0])
        
        logging.info(f"âœ… StagedWeightScheduler initialized with {len(phases)} phases:")
        for p in self.phases:
            logging.info(f"   {p['name']}: Epoch {p['epoch_range'][0]}-{p['epoch_range'][1]}")
    
    def get_phase_weights(self, epoch: int) -> tuple:
        """
        ç²å–ç•¶å‰ epoch å°æ‡‰çš„æ¬Šé‡
        
        Returns:
            (weights_dict, phase_name, is_transition)
        """
        # æ‰¾åˆ°ç•¶å‰ epoch æ‰€å±¬éšæ®µ
        for idx, phase in enumerate(self.phases):
            start, end = phase['epoch_range']
            if start <= epoch < end:
                # æª¢æ¸¬æ˜¯å¦ç‚ºéšæ®µåˆ‡æ›é»
                is_transition = (idx != self.current_phase_idx)
                self.current_phase_idx = idx
                self.current_phase_name = phase['name']
                
                return phase['weights'], phase['name'], is_transition
        
        # å¦‚æœè¶…å‡ºæ‰€æœ‰éšæ®µï¼Œè¿”å›æœ€å¾Œéšæ®µ
        last_phase = self.phases[-1]
        return last_phase['weights'], last_phase['name'], False


# èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ - æ”¯æ´é›·è«¾æ•¸å‹•æ…‹è®ŠåŒ–
class CurriculumScheduler:
    """
    èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ - é€æ­¥æå‡é›·è«¾æ•¸ï¼Œå¾å±¤æµåˆ°æ¹æµ
    
    ç‰¹æ€§ï¼š
    - å‹•æ…‹èª¿æ•´ Re_tau, nu, pressure_gradient
    - éšæ®µå¼æ¬Šé‡åˆ‡æ›
    - éšæ®µå¼å­¸ç¿’ç‡èª¿æ•´
    - éšæ®µå¼æ¡æ¨£ç­–ç•¥èª¿æ•´
    """
    
    def __init__(self, stages: list, physics_module):
        """
        Args:
            stages: èª²ç¨‹éšæ®µåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«:
                - name: éšæ®µåç¨±
                - epoch_range: [start, end]
                - Re_tau: é›·è«¾æ•¸
                - nu: é»åº¦
                - pressure_gradient: å£“åŠ›æ¢¯åº¦
                - weights: æå¤±æ¬Šé‡å­—å…¸
                - sampling: æ¡æ¨£é…ç½®
                - lr: å­¸ç¿’ç‡
            physics_module: ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆç”¨æ–¼æ›´æ–°åƒæ•¸ï¼‰
        """
        self.stages = stages
        self.physics = physics_module
        self.current_stage_idx = 0
        self.current_stage = stages[0] if stages else None
        
        # æŒ‰ epoch_range æ’åº
        self.stages.sort(key=lambda s: s['epoch_range'][0])
        
        logging.info("="*80)
        logging.info("ğŸš€ CurriculumScheduler initialized - Progressive Reynolds Number Training")
        logging.info("="*80)
        for i, s in enumerate(self.stages, 1):
            logging.info(f"Stage {i}: {s['name']}")
            logging.info(f"  Epochs: {s['epoch_range'][0]}-{s['epoch_range'][1]}")
            logging.info(f"  Re_tau: {s['Re_tau']:.1f}, nu: {s['nu']:.6f}, dP/dx: {s['pressure_gradient']:.3f}")
            logging.info(f"  PDE points: {s['sampling']['pde_points']}, BC points: {s['sampling']['boundary_points']}")
            logging.info(f"  Learning rate: {s['lr']:.6f}")
        logging.info("="*80)
    
    def get_stage_config(self, epoch: int) -> Dict[str, Any]:
        """
        ç²å–ç•¶å‰ epoch å°æ‡‰çš„éšæ®µé…ç½®
        
        Returns:
            {
                'stage_name': str,
                'is_transition': bool,
                'weights': dict,
                'Re_tau': float,
                'nu': float,
                'pressure_gradient': float,
                'sampling': dict,
                'lr': float
            }
        """
        # æ‰¾åˆ°ç•¶å‰éšæ®µ
        for idx, stage in enumerate(self.stages):
            start, end = stage['epoch_range']
            if start <= epoch < end:
                # æª¢æ¸¬éšæ®µåˆ‡æ›
                is_transition = (idx != self.current_stage_idx)
                
                if is_transition:
                    self.current_stage_idx = idx
                    self.current_stage = stage
                    
                    # æ›´æ–°ç‰©ç†åƒæ•¸
                    self._update_physics_parameters(stage)
                    
                    logging.info("="*80)
                    logging.info(f"ğŸ¯ CURRICULUM STAGE TRANSITION at Epoch {epoch}")
                    logging.info(f"ğŸ“š New Stage: {stage['name']}")
                    logging.info(f"ğŸ”¬ Re_tau: {stage['Re_tau']:.1f}, nu: {stage['nu']:.6f}")
                    logging.info(f"âš™ï¸  PDE/BC points: {stage['sampling']['pde_points']}/{stage['sampling']['boundary_points']}")
                    logging.info(f"ğŸ“Š Weights: {stage['weights']}")
                    logging.info(f"ğŸ“‰ Learning rate: {stage['lr']:.6f}")
                    logging.info("="*80)
                
                return {
                    'stage_name': stage['name'],
                    'is_transition': is_transition,
                    'weights': stage['weights'],
                    'Re_tau': stage['Re_tau'],
                    'nu': stage['nu'],
                    'pressure_gradient': stage['pressure_gradient'],
                    'sampling': stage['sampling'],
                    'lr': stage['lr']
                }
        
        # è¶…å‡ºç¯„åœï¼Œè¿”å›æœ€å¾Œéšæ®µ
        last_stage = self.stages[-1]
        return {
            'stage_name': last_stage['name'],
            'is_transition': False,
            'weights': last_stage['weights'],
            'Re_tau': last_stage['Re_tau'],
            'nu': last_stage['nu'],
            'pressure_gradient': last_stage['pressure_gradient'],
            'sampling': last_stage['sampling'],
            'lr': last_stage['lr']
        }
    
    def _update_physics_parameters(self, stage: Dict[str, Any]):
        """æ›´æ–°ç‰©ç†æ–¹ç¨‹æ¨¡çµ„çš„åƒæ•¸"""
        if hasattr(self.physics, 'nu'):
            self.physics.nu = stage['nu']
        if hasattr(self.physics, 'Re_tau'):
            self.physics.Re_tau = stage['Re_tau']
        if hasattr(self.physics, 'pressure_gradient'):
            self.physics.pressure_gradient = stage['pressure_gradient']
        
        logging.debug(f"âœ… Physics parameters updated: Re_tau={stage['Re_tau']}, nu={stage['nu']}")

# å…¨åŸŸå¿«å–ï¼Œç”¨æ–¼å­˜å„² Channel Flow è³‡æ–™å’Œçµ±è¨ˆè³‡è¨Š
_channel_data_cache: Optional[Dict[str, Any]] = None

# ============================================================================
# è¨“ç·´å°ˆç”¨è¼”åŠ©å‡½æ•¸ï¼ˆä¿ç•™ï¼Œæœªåœ¨æ¨¡çµ„ä¸­å¯¦ç¾ï¼‰
# ============================================================================


def create_loss_functions(config: Dict[str, Any], device: torch.device) -> Dict[str, nn.Module]:
    """å»ºç«‹æå¤±å‡½æ•¸"""
    loss_cfg = config.get('losses', {})
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_cfg = physics_type == 'vs_pinn_channel_flow'
    base_weight_template, default_adaptive_terms = derive_loss_weights(
        loss_cfg,
        loss_cfg.get('prior_weight', 0.3),
        is_vs_cfg
    )
    
    losses = {
        'residual': NSResidualLoss(
            nu=loss_cfg.get('nu', 1e-3),
            density=loss_cfg.get('rho', 1.0)
        ),
        'boundary': BoundaryConditionLoss(),  # ğŸ†• é‚Šç•Œæ¢ä»¶æå¤±ï¼ˆå« inletï¼‰
        'prior': PriorLossManager(
            consistency_weight=loss_cfg['prior_weight']
        )
    }
    
    # â­ Phase 6C: å‡å€¼ç´„æŸæå¤±ï¼ˆå¯é¸ï¼‰
    mean_constraint_cfg = loss_cfg.get('mean_constraint', {})
    if mean_constraint_cfg.get('enabled', False):
        losses['mean_constraint'] = MeanConstraintLoss()
        logging.info(f"âœ… MeanConstraintLoss enabled with weight={mean_constraint_cfg.get('weight', 10.0)}")
        logging.info(f"   Target means: {mean_constraint_cfg.get('target_means', {})}")
    
    return losses


def create_weighters(config: Dict[str, Any], model: nn.Module, device: torch.device, physics=None) -> Dict[str, Any]:
    """å»ºç«‹å‹•æ…‹æ¬Šé‡å™¨ (éœ€è¦æ¨¡å‹å¯¦ä¾‹)"""
    loss_cfg = config.get('losses', {})
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_cfg = physics_type == 'vs_pinn_channel_flow'
    base_weight_template, default_adaptive_terms = derive_loss_weights(
        loss_cfg,
        loss_cfg.get('prior_weight', 0.3),
        is_vs_cfg
    )
    weighters = {}
    
    # ğŸš€ èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
    curriculum_cfg = config.get('curriculum', {})
    if curriculum_cfg.get('enable', False):
        stages = curriculum_cfg.get('stages', [])
        if stages and physics is not None:
            weighters['curriculum'] = CurriculumScheduler(stages, physics)
            logging.info(f"âœ… Curriculum scheduler enabled with {len(stages)} stages")
            # èª²ç¨‹è¨“ç·´å•Ÿç”¨æ™‚ï¼Œç¦ç”¨å…¶ä»–èª¿åº¦å™¨
            weighters['staged'] = None
            weighters['gradnorm'] = None
            weighters['scheduler'] = None
            weighters['causal'] = None
            logging.info("âš ï¸  Other schedulers disabled (curriculum mode active)")
            return weighters
        else:
            weighters['curriculum'] = None
            if not stages:
                logging.warning("âš ï¸  curriculum.enable=true but no stages defined")
            if physics is None:
                logging.warning("âš ï¸  curriculum requires physics module, falling back to staged weights")
    else:
        weighters['curriculum'] = None
    
    # éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨ï¼ˆå„ªå…ˆç´šç¬¬äºŒï¼‰
    if 'staged_weights' in loss_cfg and loss_cfg['staged_weights'].get('enable', False):
        phases = loss_cfg['staged_weights'].get('phases', [])
        if phases:
            weighters['staged'] = StagedWeightScheduler(phases)
            logging.info(f"âœ… Staged weight scheduler enabled with {len(phases)} phases")
        else:
            weighters['staged'] = None
            logging.warning("âš ï¸  staged_weights.enable=true but no phases defined")
    else:
        weighters['staged'] = None
    
    # GradNorm æ¬Šé‡å™¨ï¼ˆèˆ‡éšæ®µå¼æ¬Šé‡äº’æ–¥ï¼‰
    configured_terms = loss_cfg.get('adaptive_loss_terms')
    if configured_terms is not None:
        adaptive_terms = [name for name in configured_terms if name in base_weight_template]
    else:
        adaptive_terms = default_adaptive_terms
    if loss_cfg.get('adaptive_weighting', False) and weighters['staged'] is None and adaptive_terms:
        initial_weights = {name: base_weight_template.get(name, 1.0) for name in adaptive_terms}
        weighters['gradnorm'] = GradNormWeighter(
            model=model,
            loss_names=adaptive_terms,
            alpha=loss_cfg.get('grad_norm_alpha', 0.12),
            update_frequency=loss_cfg.get('weight_update_freq', 100),
            initial_weights=initial_weights,
            device=str(device),
            min_weight=loss_cfg.get('grad_norm_min_weight', 0.1),
            max_weight=loss_cfg.get('grad_norm_max_weight', 10.0)
        )
        logging.info("GradNorm adaptive weighting enabled")
    else:
        weighters['gradnorm'] = None
        if loss_cfg.get('adaptive_weighting', False) and weighters['staged'] is not None:
            logging.info("âš ï¸  adaptive_weighting disabled (using staged_weights)")
    
    # å› æœæ¬Šé‡å™¨
    if loss_cfg.get('causal_weighting', False):
        weighters['causal'] = CausalWeighter(
            causality_strength=loss_cfg.get('causal_eps', 1.0)
        )
        logging.info("Causal weighting enabled")
    else:
        weighters['causal'] = None
    
    # è‡ªé©æ‡‰æ¬Šé‡èª¿åº¦å™¨
    # ğŸ”§ ä¿®å¾©ï¼šåƒ…åœ¨æ˜ç¢ºè¦æ±‚ phase_scheduling æ™‚å•Ÿç”¨ï¼ˆèˆ‡ GradNorm è¡çªï¼‰
    if loss_cfg.get('phase_scheduling', False) and weighters['staged'] is None and adaptive_terms:
        weighters['scheduler'] = AdaptiveWeightScheduler(
            loss_names=adaptive_terms
        )
        logging.info("Adaptive weight scheduler created")
    else:
        weighters['scheduler'] = None
        if not loss_cfg.get('phase_scheduling', False) and weighters['staged'] is None:
            logging.info("Adaptive weight scheduler disabled (use 'phase_scheduling: true' to enable)")
    
    return weighters


def prepare_training_data(config: Dict[str, Any], device: torch.device, config_path: Optional[str] = None) -> Dict[str, torch.Tensor]:
    """æº–å‚™è¨“ç·´è³‡æ–™ - æ”¯æ´ JHTDB Channel Flow æˆ– Mock è³‡æ–™
    
    Args:
        config: é…ç½®å­—å…¸
        device: PyTorch è¨­å‚™
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼ˆç”¨æ–¼ ChannelFlowLoaderï¼‰
    """
    
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ JHTDB Channel Flow è¼‰å…¥å™¨
    jhtdb_enabled = config.get('data', {}).get('jhtdb_config', {}).get('enabled', False)
    channel_flow_enabled = 'channel_flow' in config and config['channel_flow'].get('enabled', False)
    
    if jhtdb_enabled or channel_flow_enabled:
        return prepare_channel_flow_training_data(config, device, config_path)
    else:
        return prepare_mock_training_data(config, device)


def _apply_validation_split(
    training_dict: Dict[str, torch.Tensor],
    validation_split: float,
    is_vs_pinn: bool
) -> Dict[str, torch.Tensor]:
    """
    ä¾æ“š validation_split å°‡æ„Ÿæ¸¬é»è³‡æ–™åˆ‡åˆ†æˆè¨“ç·´/é©—è­‰é›†åˆï¼Œä¸¦åœ¨ training_dict ä¸­æ–°å¢
    'validation' ç´¢å¼•ã€‚
    """
    if validation_split is None or validation_split <= 0.0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    x_sensors = training_dict.get('x_sensors')
    if x_sensors is None or x_sensors.shape[0] == 0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    n_total = x_sensors.shape[0]
    if n_total < 2:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    n_val = max(1, int(round(n_total * validation_split)))
    if n_val >= n_total:
        # è‡³å°‘ä¿ç•™ä¸€å€‹è¨“ç·´æ„Ÿæ¸¬é»
        n_val = max(1, n_total - 1)
    if n_val <= 0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    device = x_sensors.device
    perm = torch.randperm(n_total, device=device)
    val_idx = perm[:n_val]
    train_idx = perm[n_val:]
    
    if train_idx.numel() == 0:
        # ç•¶ split å¹¾ä¹ç‚º 1.0 æ™‚ï¼Œç¢ºä¿ä»æœ‰è¨“ç·´è³‡æ–™
        train_idx = val_idx[-1:].clone()
        val_idx = val_idx[:-1]
    
    def split_tensor(tensor: Optional[torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        if tensor is None:
            return None, None
        if tensor.shape[0] == 0:
            return tensor, tensor
        return tensor[train_idx], tensor[val_idx]
    
    x_train, x_val = split_tensor(training_dict.get('x_sensors'))
    y_train, y_val = split_tensor(training_dict.get('y_sensors'))
    z_train, z_val = split_tensor(training_dict.get('z_sensors'))
    t_train, t_val = split_tensor(training_dict.get('t_sensors'))
    u_train, u_val = split_tensor(training_dict.get('u_sensors'))
    v_train, v_val = split_tensor(training_dict.get('v_sensors'))
    w_train, w_val = split_tensor(training_dict.get('w_sensors'))
    p_train, p_val = split_tensor(training_dict.get('p_sensors'))
    
    # æ›´æ–°è¨“ç·´æ„Ÿæ¸¬è³‡æ–™
    if x_train is not None:
        training_dict['x_sensors'] = x_train
    if y_train is not None:
        training_dict['y_sensors'] = y_train
    if z_train is not None:
        training_dict['z_sensors'] = z_train
    if t_train is not None:
        training_dict['t_sensors'] = t_train
    if u_train is not None:
        training_dict['u_sensors'] = u_train
    if v_train is not None:
        training_dict['v_sensors'] = v_train
    if w_train is not None:
        training_dict['w_sensors'] = w_train
    if p_train is not None:
        training_dict['p_sensors'] = p_train
    
    # å»ºç«‹é©—è­‰è³‡æ–™
    if x_val is None or x_val.shape[0] == 0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    coord_parts = [x_val, y_val]
    if is_vs_pinn and z_val is not None and z_val.shape[0] > 0:
        coord_parts.append(z_val)
    validation_coords = torch.cat(coord_parts, dim=1).detach()
    
    target_parts = [u_val, v_val]
    component_order = ['u', 'v']
    if is_vs_pinn:
        if w_val is not None and w_val.shape[0] == validation_coords.shape[0]:
            target_parts.append(w_val)
        else:
            target_parts.append(torch.zeros_like(u_val))
        component_order.append('w')
    target_parts.append(p_val)
    component_order.append('p')
    validation_targets = torch.cat(target_parts, dim=1).detach()
    
    if t_val is not None:
        validation_time = t_val.detach()
    else:
        validation_time = None
    
    training_dict['validation'] = {
        'coords': validation_coords,
        'targets': validation_targets,
        'time': validation_time,
        'components': component_order,
        'size': int(validation_coords.shape[0])
    }
    
    return training_dict


def prepare_mock_training_data(config: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:
    """å»ºç«‹ Mock è¨“ç·´è³‡æ–™ç”¨æ–¼æ¸¬è©¦æ•´åˆ"""
    
    # å¾é…ç½®ä¸­è®€å–åƒæ•¸
    K = config['sensors']['K']
    sampling = config['training']['sampling']
    physics_cfg = config['physics']
    domain = physics_cfg['domain']
    
    # å®šç¾©åŸŸç¯„åœ
    x_range = domain['x_range']
    y_range = domain['y_range']
    
    # ç”Ÿæˆæ„Ÿæ¸¬å™¨é» (å‡å‹»åˆ†ä½ˆ)
    x_sensors = torch.rand(K, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y_sensors = torch.rand(K, 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
    t_sensors = torch.zeros_like(x_sensors)  # å‡è¨­ç©©æ…‹
    
    # ç”Ÿæˆ Mock é€Ÿåº¦å’Œå£“åŠ›è³‡æ–™ (åŸºæ–¼è§£æè§£æˆ–ç°¡å–®æ¨¡å¼)
    # ç°¡å–®çš„é€šé“æµæ¨¡å¼: u = U_max * (1 - (2y/H - 1)^2), v = 0, p = ç·šæ€§åˆ†ä½ˆ
    y_norm = (y_sensors - y_range[0]) / (y_range[1] - y_range[0])  # æ­¸ä¸€åŒ–åˆ° [0,1]
    y_centered = 2 * y_norm - 1  # æ­¸ä¸€åŒ–åˆ° [-1,1]
    
    u_max = 1.0  # æœ€å¤§é€Ÿåº¦
    u_sensors = u_max * (1 - y_centered**2)  # æ‹‹ç‰©ç·šå‹é€Ÿåº¦åˆ†ä½ˆ
    v_sensors = torch.zeros_like(u_sensors)   # å‚ç›´é€Ÿåº¦ç‚ºé›¶
    p_sensors = torch.ones_like(u_sensors) * 0.1  # ç°¡å–®çš„å£“åŠ›å ´
    
    # ç”Ÿæˆ PDE æ®˜å·®é»
    x_pde = torch.rand(sampling['pde_points'], 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y_pde = torch.rand(sampling['pde_points'], 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
    t_pde = torch.zeros_like(x_pde)  # ç©©æ…‹å‡è¨­
    
    # ç”Ÿæˆé‚Šç•Œé» (ä¸Šä¸‹å£é¢)
    n_bc = sampling['boundary_points']
    x_bc = torch.rand(n_bc, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y_bc_bottom = torch.full((n_bc//2, 1), y_range[0], device=device)  # ä¸‹å£é¢
    y_bc_top = torch.full((n_bc - n_bc//2, 1), y_range[1], device=device)  # ä¸Šå£é¢
    y_bc = torch.cat([y_bc_bottom, y_bc_top], dim=0)
    x_bc = torch.cat([x_bc[:n_bc//2], x_bc[n_bc//2:]], dim=0)
    t_bc = torch.zeros_like(x_bc)
    
    logging.info(f"Mock training data generated: K={K} sensors, {sampling['pde_points']} PDE points, {n_bc} BC points")
    
    training_dict = {
        'x_pde': x_pde, 'y_pde': y_pde, 't_pde': t_pde,
        'x_bc': x_bc, 'y_bc': y_bc, 't_bc': t_bc,
        'x_sensors': x_sensors, 'y_sensors': y_sensors, 't_sensors': t_sensors,
        'u_sensors': u_sensors, 'v_sensors': v_sensors, 'p_sensors': p_sensors
    }
    
    validation_split = config.get('training', {}).get('validation_split', 0.0)
    training_dict = _apply_validation_split(training_dict, validation_split, is_vs_pinn=False)
    
    return training_dict


def sample_boundary_points(
    n_points: int,
    domain_bounds: Dict[str, Tuple[float, float]],
    device: torch.device,
    distribution: Optional[Dict[str, int]] = None
) -> torch.Tensor:
    """
    åœ¨é‚Šç•Œä¸Šå‡å‹»æ¡æ¨£é»
    
    Args:
        n_points: ç¸½é‚Šç•Œé»æ•¸
        domain_bounds: åŸŸé‚Šç•Œ {'x': (x_min, x_max), 'y': (y_min, y_max), 'z': (z_min, z_max)}
        device: PyTorch device
        distribution: é‚Šç•Œé»åˆ†ä½ˆ {'wall': int, 'periodic': int, 'inlet': int}
                     å¦‚æœç‚º Noneï¼Œé è¨­ç‚º {'wall': 1000, 'periodic': 800, 'inlet': 200}
    
    Returns:
        é‚Šç•Œé»åº§æ¨™ [n_points, 3] (x, y, z)
    """
    if distribution is None:
        distribution = {'wall': 1000, 'periodic': 800, 'inlet': 200}
    
    # é©—è­‰ç¸½é»æ•¸
    total_requested = sum(distribution.values())
    if total_requested != n_points:
        logging.warning(f"âš ï¸ Boundary distribution sum ({total_requested}) != n_points ({n_points}), è‡ªå‹•èª¿æ•´æ¯”ä¾‹")
        # æŒ‰æ¯”ä¾‹èª¿æ•´
        scale = n_points / total_requested
        distribution = {k: int(v * scale) for k, v in distribution.items()}
        # ä¿®æ­£èˆå…¥èª¤å·®
        diff = n_points - sum(distribution.values())
        distribution['wall'] += diff
    
    x_min, x_max = domain_bounds['x']
    y_min, y_max = domain_bounds['y']
    z_min, z_max = domain_bounds['z']
    
    boundary_points = []
    
    # 1. å£é¢é» (y = y_min å’Œ y = y_max)
    n_wall = distribution['wall']
    n_wall_bottom = n_wall // 2
    n_wall_top = n_wall - n_wall_bottom
    
    # ä¸‹å£é¢ (y = y_min)
    x_wall_bottom = torch.rand(n_wall_bottom, 1, device=device) * (x_max - x_min) + x_min
    y_wall_bottom = torch.full((n_wall_bottom, 1), y_min, device=device)
    z_wall_bottom = torch.rand(n_wall_bottom, 1, device=device) * (z_max - z_min) + z_min
    wall_bottom = torch.cat([x_wall_bottom, y_wall_bottom, z_wall_bottom], dim=1)
    
    # ä¸Šå£é¢ (y = y_max)
    x_wall_top = torch.rand(n_wall_top, 1, device=device) * (x_max - x_min) + x_min
    y_wall_top = torch.full((n_wall_top, 1), y_max, device=device)
    z_wall_top = torch.rand(n_wall_top, 1, device=device) * (z_max - z_min) + z_min
    wall_top = torch.cat([x_wall_top, y_wall_top, z_wall_top], dim=1)
    
    boundary_points.extend([wall_bottom, wall_top])
    
    # 2. é€±æœŸæ€§é‚Šç•Œé» (x = x_min/x_max, z = z_min/z_max)
    n_periodic = distribution['periodic']
    n_per_face = n_periodic // 4  # 4 å€‹é¢ï¼šx_min, x_max, z_min, z_max
    
    # x = x_min
    x_left = torch.full((n_per_face, 1), x_min, device=device)
    y_left = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_left = torch.rand(n_per_face, 1, device=device) * (z_max - z_min) + z_min
    periodic_left = torch.cat([x_left, y_left, z_left], dim=1)
    
    # x = x_max
    x_right = torch.full((n_per_face, 1), x_max, device=device)
    y_right = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_right = torch.rand(n_per_face, 1, device=device) * (z_max - z_min) + z_min
    periodic_right = torch.cat([x_right, y_right, z_right], dim=1)
    
    # z = z_min
    x_front = torch.rand(n_per_face, 1, device=device) * (x_max - x_min) + x_min
    y_front = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_front = torch.full((n_per_face, 1), z_min, device=device)
    periodic_front = torch.cat([x_front, y_front, z_front], dim=1)
    
    # z = z_max
    x_back = torch.rand(n_per_face, 1, device=device) * (x_max - x_min) + x_min
    y_back = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_back = torch.full((n_per_face, 1), z_max, device=device)
    periodic_back = torch.cat([x_back, y_back, z_back], dim=1)
    
    boundary_points.extend([periodic_left, periodic_right, periodic_front, periodic_back])
    
    # 3. Inlet é» (x = x_minï¼Œç‰¹åˆ¥è™•ç†)
    n_inlet = distribution['inlet']
    x_inlet = torch.full((n_inlet, 1), x_min, device=device)
    y_inlet = torch.rand(n_inlet, 1, device=device) * (y_max - y_min) + y_min
    z_inlet = torch.rand(n_inlet, 1, device=device) * (z_max - z_min) + z_min
    inlet = torch.cat([x_inlet, y_inlet, z_inlet], dim=1)
    
    boundary_points.append(inlet)
    
    # åˆä½µæ‰€æœ‰é‚Šç•Œé»
    all_boundary_points = torch.cat(boundary_points, dim=0)
    
    return all_boundary_points


def sample_interior_points(
    n_points: int,
    domain_bounds: Dict[str, Tuple[float, float]],
    device: torch.device,
    exclude_boundary_tol: float = 0.01,
    use_sobol: bool = True
) -> torch.Tensor:
    """
    åœ¨å…§éƒ¨å‡å‹»æ¡æ¨£é»ï¼Œæ’é™¤é‚Šç•Œå€åŸŸ
    
    Args:
        n_points: å…§éƒ¨é»æ•¸
        domain_bounds: åŸŸé‚Šç•Œ {'x': (x_min, x_max), 'y': (y_min, y_max), 'z': (z_min, z_max)}
        device: PyTorch device
        exclude_boundary_tol: é‚Šç•Œæ’é™¤å®¹å·®ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        use_sobol: æ˜¯å¦ä½¿ç”¨ Sobol åºåˆ—ï¼ˆæ›´å‡å‹»ï¼‰
    
    Returns:
        å…§éƒ¨é»åº§æ¨™ [n_points, 3] (x, y, z)
    """
    x_min, x_max = domain_bounds['x']
    y_min, y_max = domain_bounds['y']
    z_min, z_max = domain_bounds['z']
    
    # èª¿æ•´å…§éƒ¨åŸŸç¯„åœï¼ˆæ’é™¤é‚Šç•Œå®¹å·®ï¼‰
    x_min_inner = x_min + exclude_boundary_tol
    x_max_inner = x_max - exclude_boundary_tol
    y_min_inner = y_min + exclude_boundary_tol
    y_max_inner = y_max - exclude_boundary_tol
    z_min_inner = z_min + exclude_boundary_tol
    z_max_inner = z_max - exclude_boundary_tol
    
    if use_sobol:
        # ä½¿ç”¨ Sobol åºåˆ—ï¼ˆæº–å‡å‹»åˆ†ä½ˆï¼‰
        sobol = torch.quasirandom.SobolEngine(dimension=3, scramble=True)
        samples = sobol.draw(n_points).to(device)
        
        # ç¸®æ”¾åˆ°å…§éƒ¨åŸŸ
        x_interior = samples[:, 0:1] * (x_max_inner - x_min_inner) + x_min_inner
        y_interior = samples[:, 1:2] * (y_max_inner - y_min_inner) + y_min_inner
        z_interior = samples[:, 2:3] * (z_max_inner - z_min_inner) + z_min_inner
    else:
        # ä½¿ç”¨å‡å‹»éš¨æ©Ÿæ¡æ¨£
        x_interior = torch.rand(n_points, 1, device=device) * (x_max_inner - x_min_inner) + x_min_inner
        y_interior = torch.rand(n_points, 1, device=device) * (y_max_inner - y_min_inner) + y_min_inner
        z_interior = torch.rand(n_points, 1, device=device) * (z_max_inner - z_min_inner) + z_min_inner
    
    interior_points = torch.cat([x_interior, y_interior, z_interior], dim=1)
    
    return interior_points


def prepare_channel_flow_training_data(config: Dict[str, Any], device: torch.device, config_path: Optional[str] = None) -> Dict[str, torch.Tensor]:
    """ä½¿ç”¨ Channel Flow è¼‰å…¥å™¨æº–å‚™è¨“ç·´è³‡æ–™
    
    Args:
        config: é…ç½®å­—å…¸
        device: PyTorch è¨­å‚™
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼ˆå‚³éçµ¦ ChannelFlowLoaderï¼‰
    """
    from pinnx.dataio.channel_flow_loader import prepare_training_data as load_channel_flow
    
    # è¼‰å…¥ Channel Flow è³‡æ–™ - æ”¯æ´å…©ç¨®é…ç½®æ ¼å¼
    if 'channel_flow' in config:
        cf_config = config['channel_flow']
        strategy = cf_config.get('strategy', 'qr_pivot')
    else:
        # ä½¿ç”¨ JHTDB é…ç½®æ ¼å¼ - è®€å– sensors.selection_method
        sensors_cfg = config.get('sensors', {})
        strategy = sensors_cfg.get('selection_method', 'qr_pivot')  # æ”¯æ´å¾é…ç½®è®€å–ç­–ç•¥
    
    K = config['sensors']['K']
    
    # ğŸ†• è®€å–è‡ªå®šç¾©æ„Ÿæ¸¬é»æ–‡ä»¶åï¼ˆå¦‚æœæœ‰ï¼‰
    sensor_file = config.get('sensors', {}).get('sensor_file', None)
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚º 3D æ¡ˆä¾‹ï¼ˆæ±ºå®šæ˜¯å¦è«‹æ±‚ w åˆ†é‡ï¼‰
    is_3d = config.get('physics', {}).get('type') == 'vs_pinn_channel_flow'
    target_fields = ['u', 'v', 'w', 'p'] if is_3d else ['u', 'v', 'p']
    
    channel_data = load_channel_flow(
        config_path=config_path,  # â­ å‚³éé…ç½®è·¯å¾‘çµ¦ ChannelFlowLoader
        strategy=strategy,
        K=K,
        target_fields=target_fields,
        sensor_file=sensor_file  # å‚³éè‡ªå®šç¾©æ–‡ä»¶å
    )
    
    # æå–æ„Ÿæ¸¬å™¨åº§æ¨™å’Œè³‡æ–™
    coords = channel_data['coordinates']  # (K, 2 or 3) numpy array
    sensor_data = channel_data['sensor_data']  # dict with 'u', 'v', ('w',) 'p'
    domain_bounds = channel_data['domain_bounds']
    
    # è¨ˆç®—æ¨™æº–åŒ–åƒæ•¸ï¼ˆVS-PINN é¢¨æ ¼ï¼šæ˜ å°„åˆ° [-1, 1]ï¼‰
    x_range = domain_bounds['x']
    y_range = domain_bounds['y']
    x_min, x_max = x_range[0], x_range[1]
    y_min, y_max = y_range[0], y_range[1]
    
    def normalize_coord(coord, c_min, c_max):
        """å°‡åº§æ¨™æ¨™æº–åŒ–åˆ° [-1, 1]"""
        return 2.0 * (coord - c_min) / (c_max - c_min) - 1.0
    
    def denormalize_coord(coord_norm, c_min, c_max):
        """å¾ [-1, 1] åæ¨™æº–åŒ–"""
        return (coord_norm + 1.0) / 2.0 * (c_max - c_min) + c_min
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚º VS-PINNï¼ˆéœ€è¦ 3D åæ¨™ï¼‰
    is_vs_pinn = config.get('physics', {}).get('type') == 'vs_pinn_channel_flow'
    
    # è½‰æ›ç‚º PyTorch tensor
    x_sensors_raw = torch.from_numpy(coords[:, 0:1]).float().to(device)  # (K, 1)
    y_sensors_raw = torch.from_numpy(coords[:, 1:2]).float().to(device)  # (K, 1)
    
    # ğŸ†• å¦‚æœæ˜¯ VS-PINN ä¸”æœ‰çœŸå¯¦ z åº§æ¨™ï¼Œä½¿ç”¨å®ƒï¼›å¦å‰‡ç‚º 0
    if is_vs_pinn and coords.shape[1] >= 3:
        z_sensors_raw = torch.from_numpy(coords[:, 2:3]).float().to(device)  # (K, 1)
        # å¾é…ç½®è®€å– z ç¯„åœ
        z_domain = config['physics'].get('domain', {})
        z_min = z_domain.get('z_range', [0.0, 9.42])[0]
        z_max = z_domain.get('z_range', [0.0, 9.42])[1]
    else:
        z_sensors_raw = torch.zeros_like(x_sensors_raw)
        z_min = z_max = 0.0  # 2D æƒ…æ³
    
    # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
    x_sensors = x_sensors_raw
    y_sensors = y_sensors_raw
    z_sensors = z_sensors_raw if is_vs_pinn else torch.zeros_like(x_sensors)
    t_sensors = torch.zeros_like(x_sensors)  # æš«æ™‚å‡è¨­ t=0
    
    u_sensors = torch.from_numpy(sensor_data['u'].reshape(-1, 1)).float().to(device)
    v_sensors = torch.from_numpy(sensor_data['v'].reshape(-1, 1)).float().to(device)
    p_sensors = torch.from_numpy(sensor_data['p'].reshape(-1, 1)).float().to(device)
    
    # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  w åˆ†é‡ï¼ˆå‡è¨­ç‚º 0 æˆ–å¾æ•¸æ“šä¸­ç²å–ï¼‰
    if is_vs_pinn:
        if 'w' in sensor_data:
            w_sensors = torch.from_numpy(sensor_data['w'].reshape(-1, 1)).float().to(device)
        else:
            # 2D åˆ‡ç‰‡å‡è¨­ w=0
            w_sensors = torch.zeros_like(u_sensors)
    else:
        w_sensors = None  # 2D ä¸éœ€è¦ w
    
    # ç”Ÿæˆ PDE æ®˜å·®é»å’Œé‚Šç•Œé»
    sampling = config['training']['sampling']
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å±¤æ¡æ¨£ç­–ç•¥
    use_stratified = sampling.get('strategy', 'uniform') == 'stratified'
    
    if use_stratified:
        # === åˆ†å±¤æ¡æ¨£ç­–ç•¥ ===
        logging.info("ğŸ“Š ä½¿ç”¨åˆ†å±¤æ¡æ¨£ç­–ç•¥ (stratified sampling)")
        
        # æ§‹å»ºåŸŸé‚Šç•Œå­—å…¸ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        bounds_dict = {
            'x': (x_min, x_max),
            'y': (y_min, y_max),
            'z': (z_min, z_max) if is_vs_pinn else (0.0, 0.0)
        }
        
        # ç²å–é‚Šç•Œé»åˆ†ä½ˆé…ç½®
        boundary_dist = sampling.get('boundary_distribution', {
            'wall': 1000, 
            'periodic': 800, 
            'inlet': 200
        })
        
        # ç”Ÿæˆé‚Šç•Œé»ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        n_bc = sampling.get('boundary_points', 2000)
        boundary_points_raw = sample_boundary_points(
            n_points=n_bc,
            domain_bounds=bounds_dict,
            device=device,
            distribution=boundary_dist
        )
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_bc = boundary_points_raw[:, 0:1]
        y_bc = boundary_points_raw[:, 1:2]
        z_bc = boundary_points_raw[:, 2:3] if is_vs_pinn else torch.zeros_like(x_bc)
        t_bc = torch.zeros_like(x_bc)
        
        # ç”Ÿæˆå…§éƒ¨ PDE é»ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        n_pde = sampling.get('interior_points', 10000)
        use_sobol = sampling.get('use_sobol', True)
        exclude_tol = sampling.get('boundary_tolerance', 0.01)
        
        interior_points_raw = sample_interior_points(
            n_points=n_pde,
            domain_bounds=bounds_dict,
            device=device,
            exclude_boundary_tol=exclude_tol,
            use_sobol=use_sobol
        )
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_pde = interior_points_raw[:, 0:1]
        y_pde = interior_points_raw[:, 1:2]
        z_pde = interior_points_raw[:, 2:3] if is_vs_pinn else torch.zeros_like(x_pde)
        t_pde = torch.zeros_like(x_pde)
        
        logging.info(f"âœ… åˆ†å±¤æ¡æ¨£å®Œæˆ: {n_bc} é‚Šç•Œé» + {n_pde} å…§éƒ¨é»")
        logging.info(f"   - é‚Šç•Œåˆ†ä½ˆ: {boundary_dist}")
        logging.info(f"   - Sobol æ¡æ¨£: {use_sobol}, é‚Šç•Œå®¹å·®: {exclude_tol}")
        
    else:
        # === åŸå§‹å‡å‹»éš¨æ©Ÿæ¡æ¨£ ===
        logging.info("ğŸ“Š ä½¿ç”¨å‡å‹»éš¨æ©Ÿæ¡æ¨£ç­–ç•¥ (uniform sampling)")
        
        # PDE æ®˜å·®é»ï¼ˆåŸå§‹åº§æ¨™ï¼‰
        x_pde_raw = torch.rand(sampling['pde_points'], 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
        y_pde_raw = torch.rand(sampling['pde_points'], 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_pde = x_pde_raw
        y_pde = y_pde_raw
        t_pde = torch.zeros_like(x_pde)  # ç©©æ…‹å‡è¨­
        
        # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  z åº§æ¨™åˆ° PDE é»
        if is_vs_pinn:
            z_pde_raw = torch.rand(sampling['pde_points'], 1, device=device) * (z_max - z_min) + z_min
            z_pde = z_pde_raw
        else:
            z_pde = torch.zeros_like(x_pde)  # 2D æƒ…æ³ä¸‹ z=0
        
        # é‚Šç•Œé»ï¼ˆåŸå§‹åº§æ¨™ï¼‰
        n_bc = sampling['boundary_points']
        x_bc_raw = torch.rand(n_bc, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
        y_bc_bottom_raw = torch.full((n_bc//2, 1), y_range[0], device=device)  # ä¸‹å£é¢
        y_bc_top_raw = torch.full((n_bc - n_bc//2, 1), y_range[1], device=device)  # ä¸Šå£é¢
        y_bc_raw = torch.cat([y_bc_bottom_raw, y_bc_top_raw], dim=0)
        x_bc_raw = torch.cat([x_bc_raw[:n_bc//2], x_bc_raw[n_bc//2:]], dim=0)
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_bc = x_bc_raw
        y_bc = y_bc_raw
        t_bc = torch.zeros_like(x_bc)
        
        # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  z åº§æ¨™åˆ°é‚Šç•Œé»
        if is_vs_pinn:
            z_bc_raw = torch.rand(n_bc, 1, device=device) * (z_max - z_min) + z_min
            z_bc = z_bc_raw
        else:
            z_bc = torch.zeros_like(x_bc)  # 2D æƒ…æ³ä¸‹ z=0
    
    # å­˜å„²é¡å¤–è³‡è¨Šåˆ°å…¨å±€è®Šé‡ï¼ˆåŒ…å«æ¨™æº–åŒ–åƒæ•¸ï¼‰
    global _channel_data_cache
    _channel_data_cache = {
        'domain_bounds': domain_bounds,
        'channel_data': channel_data,
        'normalization': {
            'x_min': x_min, 'x_max': x_max,
            'y_min': y_min, 'y_max': y_max,
            'z_min': z_min, 'z_max': z_max,  # ğŸ†• æ·»åŠ  z ç¯„åœ
            'normalize_fn': normalize_coord,
            'denormalize_fn': denormalize_coord
        },
        'is_vs_pinn': is_vs_pinn  # ğŸ†• æ¨™è¨˜æ˜¯å¦ç‚º VS-PINN
    }
    
    # ğŸ†• ç”Ÿæˆåˆå§‹æ¢ä»¶é»ï¼ˆt=0ï¼‰- ä½¿ç”¨æ„Ÿæ¸¬å™¨æ•¸æ“šä½œç‚º IC
    ic_config = config.get('initial_condition', {})
    if ic_config.get('enabled', False):
        n_ic = ic_config.get('n_points', 256)
        # å¾æ„Ÿæ¸¬å™¨æ•¸æ“šä¸­éš¨æ©Ÿæ¡æ¨£ä½œç‚º ICï¼ˆæˆ–ä½¿ç”¨å®Œæ•´æ„Ÿæ¸¬å™¨æ•¸æ“šï¼‰
        if n_ic >= len(x_sensors):
            # ä½¿ç”¨æ‰€æœ‰æ„Ÿæ¸¬å™¨æ•¸æ“š
            x_ic = x_sensors.clone()
            y_ic = y_sensors.clone()
            z_ic = z_sensors.clone()  # ğŸ†• æ·»åŠ  z_ic
            t_ic = torch.zeros_like(x_ic)
            u_ic = u_sensors.clone()
            v_ic = v_sensors.clone()
            p_ic = p_sensors.clone()
            if is_vs_pinn:
                w_ic = w_sensors.clone() if w_sensors is not None else torch.zeros_like(u_ic)
            else:
                w_ic = torch.empty(0, 1, device=device)
        else:
            # éš¨æ©Ÿæ¡æ¨£
            ic_indices = torch.randperm(len(x_sensors), device=device)[:n_ic]
            x_ic = x_sensors[ic_indices]
            y_ic = y_sensors[ic_indices]
            z_ic = z_sensors[ic_indices]  # ğŸ†• æ·»åŠ  z_ic
            t_ic = torch.zeros_like(x_ic)
            u_ic = u_sensors[ic_indices]
            v_ic = v_sensors[ic_indices]
            p_ic = p_sensors[ic_indices]
            if is_vs_pinn:
                w_ic = w_sensors[ic_indices] if w_sensors is not None else torch.zeros_like(u_ic)
            else:
                w_ic = torch.empty(0, 1, device=device)
    else:
        # IC ç¦ç”¨æ™‚ï¼Œä½¿ç”¨ç©ºå¼µé‡
        x_ic = torch.empty(0, 1, device=device)
        y_ic = torch.empty(0, 1, device=device)
        z_ic = torch.empty(0, 1, device=device)  # ğŸ†• æ·»åŠ  z_ic
        t_ic = torch.empty(0, 1, device=device)
        u_ic = torch.empty(0, 1, device=device)
        v_ic = torch.empty(0, 1, device=device)
        w_ic = torch.empty(0, 1, device=device)  # ğŸ†• æ·»åŠ  w_ic
        p_ic = torch.empty(0, 1, device=device)
    
    # æå–ä½ä¿çœŸå…ˆé©—è³‡æ–™ (å¦‚æœæœ‰)
    training_dict = {
        'x_pde': x_pde, 'y_pde': y_pde, 'z_pde': z_pde, 't_pde': t_pde,  # ğŸ†• æ·»åŠ  z_pde
        'x_bc': x_bc, 'y_bc': y_bc, 'z_bc': z_bc, 't_bc': t_bc,  # ğŸ†• æ·»åŠ  z_bc
        'x_sensors': x_sensors, 'y_sensors': y_sensors, 'z_sensors': z_sensors, 't_sensors': t_sensors,  # ğŸ†• æ·»åŠ  z_sensors
        'u_sensors': u_sensors, 'v_sensors': v_sensors, 'p_sensors': p_sensors,
        'x_ic': x_ic, 'y_ic': y_ic, 'z_ic': z_ic, 't_ic': t_ic,  # ğŸ†• æ·»åŠ  z_ic
        'u_ic': u_ic, 'v_ic': v_ic, 'p_ic': p_ic
    }
    
    # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  w åˆ†é‡åˆ°è¨“ç·´å­—å…¸
    if is_vs_pinn:
        training_dict['w_sensors'] = w_sensors if w_sensors is not None else torch.zeros_like(u_sensors)
        training_dict['w_ic'] = w_ic
    else:
        # 2D æƒ…æ³ä¸‹ä¸éœ€è¦ wï¼Œä½†ç‚ºäº†çµ±ä¸€æ€§å¯ä»¥æ·»åŠ ç©ºå¼µé‡
        training_dict['w_sensors'] = torch.empty(0, 1, device=device)
        training_dict['w_ic'] = torch.empty(0, 1, device=device)
    
    # æ·»åŠ ä½ä¿çœŸå…ˆé©—è³‡æ–™åˆ°æ‰¹æ¬¡ (å¦‚æœå¯ç”¨)
    if 'lowfi_prior' in channel_data and channel_data['lowfi_prior']:
        lowfi = channel_data['lowfi_prior']
        if 'u' in lowfi:
            training_dict['u_prior'] = torch.from_numpy(lowfi['u'].reshape(-1, 1)).float().to(device)
        if 'v' in lowfi:
            training_dict['v_prior'] = torch.from_numpy(lowfi['v'].reshape(-1, 1)).float().to(device)
        if 'p' in lowfi:
            training_dict['p_prior'] = torch.from_numpy(lowfi['p'].reshape(-1, 1)).float().to(device)
        training_dict['has_prior'] = True
    else:
        training_dict['has_prior'] = False
    
    validation_split = config.get('training', {}).get('validation_split', 0.0)
    training_dict = _apply_validation_split(training_dict, validation_split, is_vs_pinn=is_vs_pinn)
    
    return training_dict





def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description='PINNs Inverse Training Script')
    parser.add_argument('--cfg', type=str, default='configs/defaults.yml',
                       help='Path to configuration file')
    parser.add_argument('--ensemble', action='store_true',
                       help='Run ensemble training for UQ')
    parser.add_argument('--resume', type=str, default=None,
                       help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    # è¼‰å…¥é…ç½®
    config = load_config(args.cfg)
    
    # ğŸ†• æ ¹æ“šç‰©ç†é¡å‹è‡ªå‹•è¨­ç½®æ¨¡å‹è¼¸å…¥è¼¸å‡ºç¶­åº¦
    if 'in_dim' not in config['model'] or 'out_dim' not in config['model']:
        physics_type = config.get('physics', {}).get('type', 'ns_2d')
        
        if physics_type == 'vs_pinn_channel_flow':
            # VS-PINN 3D: è¼¸å…¥ (x, y, z)ï¼Œè¼¸å‡º (u, v, w, p)
            config['model']['in_dim'] = 3
            config['model']['out_dim'] = 4
            logger_msg = "VS-PINN 3D: in_dim=3 (x,y,z), out_dim=4 (u,v,w,p)"
        else:
            # æ¨™æº– PINN 2D: è¼¸å…¥ (x, y)ï¼Œè¼¸å‡º (u, v, p)
            config['model']['in_dim'] = 2
            config['model']['out_dim'] = 3
            logger_msg = "Standard PINN 2D: in_dim=2 (x,y), out_dim=3 (u,v,p)"
        
        # æš«æ™‚ç”¨ printï¼ˆå› ç‚º logger å°šæœªè¨­ç½®ï¼‰
        print(f"ğŸ”§ Auto-configured model dimensions: {logger_msg}")
    
    # è¨­ç½®æ—¥èªŒ
    logger = setup_logging(config['logging']['level'])
    logger.info("=" * 60)
    logger.info("PINNs Inverse Reconstruction Training")
    logger.info("=" * 60)
    
    # è¨­ç½®é‡ç¾æ€§
    set_random_seed(
        config['experiment']['seed'],
        config['reproducibility']['deterministic']
    )
    
    # è¨­ç½®è¨­å‚™
    device = get_device(config['experiment']['device'])
    
    # æå‰æº–å‚™è³‡æ–™ä»¥æå–çµ±è¨ˆè³‡è¨Šï¼ˆç”¨æ–¼è‡ªå‹•è¼¸å‡ºç¯„åœï¼‰
    logger.info("Preparing training data to extract statistics...")
    training_data_sample = prepare_training_data(config, device, args.cfg)
    
    # å¾å¿«å–ä¸­æå–çµ±è¨ˆè³‡è¨Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
    statistics = None
    if '_channel_data_cache' in globals() and _channel_data_cache is not None:
        channel_data = _channel_data_cache.get('channel_data', {})
        if 'statistics' in channel_data:
            statistics = channel_data['statistics']
            logger.info(f"âœ… Extracted statistics for auto output ranges:")
            logger.info(f"   u: {statistics.get('u', {}).get('range', 'N/A')}")
            logger.info(f"   v: {statistics.get('v', {}).get('range', 'N/A')}")
            logger.info(f"   p: {statistics.get('p', {}).get('range', 'N/A')}")
        else:
            logger.warning("âš ï¸  No statistics found in channel_data")
    else:
        logger.warning("âš ï¸  Channel data cache not available, will use hardcoded ranges")
    
    # å»ºç«‹æ¨¡å‹å’Œç‰©ç†æ¨¡çµ„
    model = create_model(config, device, statistics=statistics)
    physics = create_physics(config, device)
    losses = create_loss_functions(config, device)
    
    logger.info(f"Model architecture: {config['model']['type']}")
    logger.info(f"Input dimension: {config['model']['in_dim']}")
    logger.info(f"Output dimension: {config['model']['out_dim']}")
    
    # å®‰å…¨è®€å–ç‰©ç†åƒæ•¸
    physics_type = config.get('physics', {}).get('type', 'unknown')
    if physics_type == 'vs_pinn_channel_flow':
        physics_params = config.get('physics', {}).get('physics_params', {})
        logger.info(f"Physics: VS-PINN Channel Flow with nu={physics_params.get('nu', 'N/A')}")
    else:
        nu = config.get('physics', {}).get('nu', config.get('physics', {}).get('physics_params', {}).get('nu', 'N/A'))
        logger.info(f"Physics: NS-2D with nu={nu}")
    
    if args.ensemble:
        logger.info("Running ensemble training...")
        ensemble_cfg = config['ensemble']
        
        models = []
        for i, seed in enumerate(ensemble_cfg['seeds']):
            logger.info(f"Training ensemble member {i+1}/{len(ensemble_cfg['seeds'])} (seed={seed})")
            
            # é‡ç½®éš¨æ©Ÿç¨®å­
            set_random_seed(seed, config['reproducibility']['deterministic'])
            
            # å»ºç«‹æ–°æ¨¡å‹ï¼ˆä½¿ç”¨ç›¸åŒçš„çµ±è¨ˆè³‡è¨Šï¼‰
            member_model = create_model(config, device, statistics=statistics)
            
            # å‰µå»ºå‹•æ…‹æ¬Šé‡å™¨ï¼ˆGradNorm/Causal/Curriculumï¼‰
            member_weighters = create_weighters(config, member_model, device, physics=physics)
            
            # ä½¿ç”¨ Trainer è¨“ç·´
            trainer = Trainer(member_model, physics, losses, config, device, weighters=member_weighters)
            trainer.training_data = training_data_sample
            
            # âœ… å¾è¨“ç·´è³‡æ–™è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡ï¼ˆè‹¥é…ç½®è¦æ±‚ä½† params ç‚ºç©ºï¼‰
            if config.get('data', {}).get('normalize', False):
                norm_cfg = config.get('normalization', {})
                if norm_cfg.get('type') == 'training_data_norm' and not norm_cfg.get('params'):
                    # å¾æ„Ÿæ¸¬é»æ•¸æ“šè¨ˆç®— Z-score
                    sensor_data = {
                        'u': training_data_sample['u_sensors'],
                        'v': training_data_sample['v_sensors'],
                        'p': training_data_sample['p_sensors']
                    }
                    if 'w_sensors' in training_data_sample:
                        sensor_data['w'] = training_data_sample['w_sensors']
                    
                    from pinnx.utils.normalization import DataNormalizer
                    trainer.data_normalizer = DataNormalizer.from_data(
                        sensor_data, 
                        norm_type='training_data_norm'
                    )
                    logger.info(f"âœ… å·²å¾è¨“ç·´è³‡æ–™è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡ï¼ˆEnsemble member {i+1}ï¼‰: {trainer.data_normalizer}")
            
            train_result = trainer.train()
            models.append(member_model)
            
            logger.info(f"Member {i+1} final loss: {train_result['final_loss']:.6f}")
        
        # å„²å­˜æ¨¡å‹åˆ—è¡¨ï¼ˆæš«æ™‚ä¸ä½¿ç”¨ EnsembleWrapperï¼‰
        logger.info(f"Ensemble training completed with {len(models)} members")
        logger.info("Note: EnsembleWrapper not implemented yet - models stored as list")
        
    else:
        logger.info("Running single model training...")
        weighters = create_weighters(config, model, device, physics=physics)
        trainer = Trainer(model, physics, losses, config, device, weighters=weighters)
        trainer.training_data = training_data_sample
        
        # âœ… å¾è¨“ç·´è³‡æ–™è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡ï¼ˆè‹¥é…ç½®è¦æ±‚ä½† params ç‚ºç©ºï¼‰
        if config.get('data', {}).get('normalize', False):
            norm_cfg = config.get('normalization', {})
            if norm_cfg.get('type') == 'training_data_norm' and not norm_cfg.get('params'):
                # å¾æ„Ÿæ¸¬é»æ•¸æ“šè¨ˆç®— Z-score
                sensor_data = {
                    'u': training_data_sample['u_sensors'],
                    'v': training_data_sample['v_sensors'],
                    'p': training_data_sample['p_sensors']
                }
                if 'w_sensors' in training_data_sample:
                    sensor_data['w'] = training_data_sample['w_sensors']
                
                from pinnx.utils.normalization import DataNormalizer
                trainer.data_normalizer = DataNormalizer.from_data(
                    sensor_data, 
                    norm_type='training_data_norm'
                )
                logger.info(f"âœ… å·²å¾è¨“ç·´è³‡æ–™è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡: {trainer.data_normalizer}")
        
        train_result = trainer.train()
        logger.info(f"Training completed. Final loss: {train_result['final_loss']:.6f}")
    
    logger.info("Training script finished successfully!")


if __name__ == "__main__":
    main()
