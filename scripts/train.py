#!/usr/bin/env python3
"""
PINNs é€†é‡å»ºä¸»è¨“ç·´è…³æœ¬
è² è²¬å”èª¿è³‡æ–™è¼‰å…¥ã€æ¨¡å‹å»ºç«‹ã€è¨“ç·´è¿´åœˆèˆ‡è©•ä¼°è¼¸å‡º
"""

import argparse
import logging
import os
import random
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List

import numpy as np
import torch
import torch.nn as nn
import yaml

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from pinnx.models.fourier_mlp import PINNNet, create_enhanced_pinn, init_siren_weights
from pinnx.models.wrappers import ScaledPINNWrapper
from pinnx.physics.scaling import VSScaler
from pinnx.physics.ns_2d import NSEquations2D
from pinnx.physics import create_vs_pinn_channel_flow, VSPINNChannelFlow  # VS-PINN
from pinnx.losses.residuals import NSResidualLoss, BoundaryConditionLoss
from pinnx.losses.priors import PriorLossManager
from pinnx.losses.weighting import GradNormWeighter, CausalWeighter, AdaptiveWeightScheduler
from pinnx.train.loop import TrainingLoopManager, apply_point_weights_to_loss  # è‡ªé©æ‡‰æ¡æ¨£ç®¡ç†å™¨
from pinnx.utils.normalization import InputNormalizer, NormalizationConfig
from pinnx.evals.metrics import relative_L2

_LOSS_KEY_MAP: Dict[str, str] = {
    'data': 'data_weight',
    'momentum_x': 'momentum_x_weight',
    'momentum_y': 'momentum_y_weight',
    'momentum_z': 'momentum_z_weight',
    'continuity': 'continuity_weight',
    'wall_constraint': 'wall_constraint_weight',
    'periodicity': 'periodicity_weight',
    'inlet': 'inlet_weight',
    'initial_condition': 'initial_condition_weight',
    'bulk_velocity': 'bulk_velocity_weight',
    'centerline_dudy': 'centerline_dudy_weight',
    'centerline_v': 'centerline_v_weight',
    'pressure_reference': 'pressure_reference_weight',
}

_DEFAULT_WEIGHTS: Dict[str, float] = {
    'data': 10.0,
    'momentum_x': 2.0,
    'momentum_y': 2.0,
    'momentum_z': 2.0,
    'continuity': 2.0,
    'wall_constraint': 10.0,
    'periodicity': 5.0,
    'inlet': 10.0,
    'initial_condition': 100.0,
    'bulk_velocity': 2.0,
    'centerline_dudy': 1.0,
    'centerline_v': 1.0,
    'pressure_reference': 0.0,
}

_VS_ONLY_LOSSES = {
    'momentum_z',
    'bulk_velocity',
    'centerline_dudy',
    'centerline_v',
    'pressure_reference',
}


def derive_loss_weights(
    loss_cfg: Dict[str, Any],
    prior_weight: float,
    is_vs_pinn: bool
) -> Tuple[Dict[str, float], List[str]]:
    """
    æ ¹æ“šé…ç½®æ¨å°åŸºç¤æ¬Šé‡èˆ‡å¯èª¿æ•´çš„æå¤±é …åˆ—è¡¨ã€‚
    """
    base_weights: Dict[str, float] = {}

    for name, default_val in _DEFAULT_WEIGHTS.items():
        if not is_vs_pinn and name in _VS_ONLY_LOSSES:
            continue

        cfg_key = _LOSS_KEY_MAP.get(name)
        if cfg_key is not None:
            if name == 'periodicity' and not is_vs_pinn and cfg_key not in loss_cfg:
                continue
            val = loss_cfg.get(cfg_key, default_val)
        else:
            val = default_val

        base_weights[name] = float(val)

    base_weights['prior'] = float(loss_cfg.get('prior_weight', prior_weight))

    if 'boundary_weight' in loss_cfg:
        base_weights['wall_constraint'] = base_weights.get('wall_constraint', 0.0) + float(loss_cfg['boundary_weight'])

    adaptive_terms = [name for name in base_weights if name != 'prior']
    if base_weights.get('prior', 0.0) > 0.0:
        adaptive_terms.append('prior')

    return base_weights, adaptive_terms


def _collect_coordinate_tensors(training_data: Dict[str, torch.Tensor]) -> List[torch.Tensor]:
    prefixes = ['sensors', 'pde', 'bc', 'ic']
    coords: List[torch.Tensor] = []
    axes = ['x', 'y', 'z']
    for prefix in prefixes:
        components = []
        for axis in axes:
            key = f'{axis}_{prefix}'
            if key in training_data and training_data[key].numel() > 0:
                components.append(training_data[key])
        if components:
            coords.append(torch.cat(components, dim=1))
    return coords


def create_input_normalizer(
    config: Dict[str, Any],
    training_data: Dict[str, torch.Tensor],
    is_vs_pinn: bool,
    device: torch.device
) -> Optional[InputNormalizer]:
    scaling_cfg = config.get('model', {}).get('scaling', {})
    norm_type = scaling_cfg.get('input_norm', 'none')
    if norm_type is None:
        norm_type = 'none'

    norm_type = norm_type.lower()

    if norm_type in ('none', 'identity'):
        return None

    if is_vs_pinn and norm_type in ('vs_pinn', 'channel_flow'):
        # VS-PINN already applies dedicated scaling; avoid double normalization.
        return None

    bounds_tensor: Optional[torch.Tensor] = None
    if norm_type in ('channel_flow',):
        domain = config.get('physics', {}).get('domain', {})
        bounds: List[Tuple[float, float]] = []
        for axis in ['x', 'y', 'z']:
            rng = domain.get(f'{axis}_range')
            if rng is not None:
                bounds.append((float(rng[0]), float(rng[1])))
        if bounds:
            bounds_tensor = torch.tensor(bounds, dtype=torch.float32, device=device)

    feature_range = tuple(scaling_cfg.get('input_norm_range', [-1.0, 1.0]))
    config_obj = NormalizationConfig(
        norm_type=norm_type,
        feature_range=(float(feature_range[0]), float(feature_range[1])),
        bounds=bounds_tensor
    )
    normalizer = InputNormalizer(config_obj)

    coord_tensors = _collect_coordinate_tensors(training_data)
    if coord_tensors:
        samples = torch.cat(coord_tensors, dim=0)
        if normalizer.bounds is not None and normalizer.bounds.shape[0] > samples.shape[1]:
            normalizer.bounds = normalizer.bounds[:samples.shape[1], :]
        normalizer.fit(samples)
    else:
        return None

    normalizer.to(device)
    return normalizer


# ============================================================================
# Warmup + CosineAnnealing å­¸ç¿’ç‡èª¿åº¦å™¨
# ============================================================================
class WarmupCosineScheduler:
    """
    Warmup + CosineAnnealing å­¸ç¿’ç‡èª¿åº¦å™¨
    
    å‰ warmup_epochs å€‹ epoch ç·šæ€§å¢åŠ å­¸ç¿’ç‡å¾ 0 åˆ° base_lr
    ä¹‹å¾Œä½¿ç”¨ CosineAnnealing è¡°æ¸›åˆ° min_lr
    """
    
    def __init__(self, optimizer, warmup_epochs: int, max_epochs: int, 
                 base_lr: float, min_lr: float = 0.0):
        """
        Args:
            optimizer: PyTorch å„ªåŒ–å™¨
            warmup_epochs: Warmup éšæ®µçš„ epoch æ•¸é‡
            max_epochs: ç¸½è¨“ç·´ epoch æ•¸é‡
            base_lr: åŸºç¤å­¸ç¿’ç‡ï¼ˆWarmup å¾Œçš„å³°å€¼ï¼‰
            min_lr: æœ€å°å­¸ç¿’ç‡ï¼ˆCosineAnnealing çš„ä¸‹é™ï¼‰
        """
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr
        self.current_epoch = 0
        
        # å»ºç«‹å…§éƒ¨çš„ CosineAnnealing èª¿åº¦å™¨ï¼ˆç”¨æ–¼ Warmup å¾Œéšæ®µï¼‰
        self.cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, 
            T_max=max_epochs - warmup_epochs,
            eta_min=min_lr
        )
        
        logging.info(f"âœ… WarmupCosineScheduler initialized:")
        logging.info(f"   Warmup epochs: {warmup_epochs}")
        logging.info(f"   Max epochs: {max_epochs}")
        logging.info(f"   Base LR: {base_lr:.6f}")
        logging.info(f"   Min LR: {min_lr:.6f}")
    
    def step(self):
        """æ›´æ–°å­¸ç¿’ç‡"""
        if self.current_epoch < self.warmup_epochs:
            # Warmup éšæ®µï¼šç·šæ€§å¢åŠ 
            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr
        else:
            # CosineAnnealing éšæ®µ
            self.cosine_scheduler.step()
        
        self.current_epoch += 1
    
    def get_last_lr(self):
        """è¿”å›ç•¶å‰å­¸ç¿’ç‡ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰"""
        return [param_group['lr'] for param_group in self.optimizer.param_groups]


# ============================================================================
# éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨
# ============================================================================
class StagedWeightScheduler:
    """éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨ - æ ¹æ“š epoch åˆ‡æ›ä¸åŒè¨“ç·´éšæ®µçš„æ¬Šé‡"""
    
    def __init__(self, phases: list):
        """
        Args:
            phases: éšæ®µé…ç½®åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«:
                - name: éšæ®µåç¨±
                - epoch_range: [start, end] epoch ç¯„åœ
                - weights: è©²éšæ®µçš„æ¬Šé‡å­—å…¸
        """
        self.phases = phases
        self.current_phase_idx = 0
        self.current_phase_name = phases[0]['name'] if phases else "default"
        
        # æŒ‰ epoch_range æ’åº
        self.phases.sort(key=lambda p: p['epoch_range'][0])
        
        logging.info(f"âœ… StagedWeightScheduler initialized with {len(phases)} phases:")
        for p in self.phases:
            logging.info(f"   {p['name']}: Epoch {p['epoch_range'][0]}-{p['epoch_range'][1]}")
    
    def get_phase_weights(self, epoch: int) -> tuple:
        """
        ç²å–ç•¶å‰ epoch å°æ‡‰çš„æ¬Šé‡
        
        Returns:
            (weights_dict, phase_name, is_transition)
        """
        # æ‰¾åˆ°ç•¶å‰ epoch æ‰€å±¬éšæ®µ
        for idx, phase in enumerate(self.phases):
            start, end = phase['epoch_range']
            if start <= epoch < end:
                # æª¢æ¸¬æ˜¯å¦ç‚ºéšæ®µåˆ‡æ›é»
                is_transition = (idx != self.current_phase_idx)
                self.current_phase_idx = idx
                self.current_phase_name = phase['name']
                
                return phase['weights'], phase['name'], is_transition
        
        # å¦‚æœè¶…å‡ºæ‰€æœ‰éšæ®µï¼Œè¿”å›æœ€å¾Œéšæ®µ
        last_phase = self.phases[-1]
        return last_phase['weights'], last_phase['name'], False


# èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ - æ”¯æ´é›·è«¾æ•¸å‹•æ…‹è®ŠåŒ–
class CurriculumScheduler:
    """
    èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ - é€æ­¥æå‡é›·è«¾æ•¸ï¼Œå¾å±¤æµåˆ°æ¹æµ
    
    ç‰¹æ€§ï¼š
    - å‹•æ…‹èª¿æ•´ Re_tau, nu, pressure_gradient
    - éšæ®µå¼æ¬Šé‡åˆ‡æ›
    - éšæ®µå¼å­¸ç¿’ç‡èª¿æ•´
    - éšæ®µå¼æ¡æ¨£ç­–ç•¥èª¿æ•´
    """
    
    def __init__(self, stages: list, physics_module):
        """
        Args:
            stages: èª²ç¨‹éšæ®µåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«:
                - name: éšæ®µåç¨±
                - epoch_range: [start, end]
                - Re_tau: é›·è«¾æ•¸
                - nu: é»åº¦
                - pressure_gradient: å£“åŠ›æ¢¯åº¦
                - weights: æå¤±æ¬Šé‡å­—å…¸
                - sampling: æ¡æ¨£é…ç½®
                - lr: å­¸ç¿’ç‡
            physics_module: ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆç”¨æ–¼æ›´æ–°åƒæ•¸ï¼‰
        """
        self.stages = stages
        self.physics = physics_module
        self.current_stage_idx = 0
        self.current_stage = stages[0] if stages else None
        
        # æŒ‰ epoch_range æ’åº
        self.stages.sort(key=lambda s: s['epoch_range'][0])
        
        logging.info("="*80)
        logging.info("ğŸš€ CurriculumScheduler initialized - Progressive Reynolds Number Training")
        logging.info("="*80)
        for i, s in enumerate(self.stages, 1):
            logging.info(f"Stage {i}: {s['name']}")
            logging.info(f"  Epochs: {s['epoch_range'][0]}-{s['epoch_range'][1]}")
            logging.info(f"  Re_tau: {s['Re_tau']:.1f}, nu: {s['nu']:.6f}, dP/dx: {s['pressure_gradient']:.3f}")
            logging.info(f"  PDE points: {s['sampling']['pde_points']}, BC points: {s['sampling']['boundary_points']}")
            logging.info(f"  Learning rate: {s['lr']:.6f}")
        logging.info("="*80)
    
    def get_stage_config(self, epoch: int) -> Dict[str, Any]:
        """
        ç²å–ç•¶å‰ epoch å°æ‡‰çš„éšæ®µé…ç½®
        
        Returns:
            {
                'stage_name': str,
                'is_transition': bool,
                'weights': dict,
                'Re_tau': float,
                'nu': float,
                'pressure_gradient': float,
                'sampling': dict,
                'lr': float
            }
        """
        # æ‰¾åˆ°ç•¶å‰éšæ®µ
        for idx, stage in enumerate(self.stages):
            start, end = stage['epoch_range']
            if start <= epoch < end:
                # æª¢æ¸¬éšæ®µåˆ‡æ›
                is_transition = (idx != self.current_stage_idx)
                
                if is_transition:
                    self.current_stage_idx = idx
                    self.current_stage = stage
                    
                    # æ›´æ–°ç‰©ç†åƒæ•¸
                    self._update_physics_parameters(stage)
                    
                    logging.info("="*80)
                    logging.info(f"ğŸ¯ CURRICULUM STAGE TRANSITION at Epoch {epoch}")
                    logging.info(f"ğŸ“š New Stage: {stage['name']}")
                    logging.info(f"ğŸ”¬ Re_tau: {stage['Re_tau']:.1f}, nu: {stage['nu']:.6f}")
                    logging.info(f"âš™ï¸  PDE/BC points: {stage['sampling']['pde_points']}/{stage['sampling']['boundary_points']}")
                    logging.info(f"ğŸ“Š Weights: {stage['weights']}")
                    logging.info(f"ğŸ“‰ Learning rate: {stage['lr']:.6f}")
                    logging.info("="*80)
                
                return {
                    'stage_name': stage['name'],
                    'is_transition': is_transition,
                    'weights': stage['weights'],
                    'Re_tau': stage['Re_tau'],
                    'nu': stage['nu'],
                    'pressure_gradient': stage['pressure_gradient'],
                    'sampling': stage['sampling'],
                    'lr': stage['lr']
                }
        
        # è¶…å‡ºç¯„åœï¼Œè¿”å›æœ€å¾Œéšæ®µ
        last_stage = self.stages[-1]
        return {
            'stage_name': last_stage['name'],
            'is_transition': False,
            'weights': last_stage['weights'],
            'Re_tau': last_stage['Re_tau'],
            'nu': last_stage['nu'],
            'pressure_gradient': last_stage['pressure_gradient'],
            'sampling': last_stage['sampling'],
            'lr': last_stage['lr']
        }
    
    def _update_physics_parameters(self, stage: Dict[str, Any]):
        """æ›´æ–°ç‰©ç†æ–¹ç¨‹æ¨¡çµ„çš„åƒæ•¸"""
        if hasattr(self.physics, 'nu'):
            self.physics.nu = stage['nu']
        if hasattr(self.physics, 'Re_tau'):
            self.physics.Re_tau = stage['Re_tau']
        if hasattr(self.physics, 'pressure_gradient'):
            self.physics.pressure_gradient = stage['pressure_gradient']
        
        logging.debug(f"âœ… Physics parameters updated: Re_tau={stage['Re_tau']}, nu={stage['nu']}")

# å…¨åŸŸå¿«å–ï¼Œç”¨æ–¼å­˜å„² Channel Flow è³‡æ–™å’Œçµ±è¨ˆè³‡è¨Š
_channel_data_cache: Optional[Dict[str, Any]] = None

# æ¨¡å‹æª¢æŸ¥é»ä¿å­˜èˆ‡è¼‰å…¥åŠŸèƒ½
def save_checkpoint(model, optimizer, epoch, loss, config, checkpoint_dir="checkpoints"):
    """ä¿å­˜æ¨¡å‹æª¢æŸ¥é»"""
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'config': config
    }
    
    experiment_name = config['experiment']['name']
    checkpoint_path = os.path.join(checkpoint_dir, f"{experiment_name}_epoch_{epoch}.pth")
    torch.save(checkpoint, checkpoint_path)
    
    # åŒæ™‚ä¿å­˜æœ€æ–°çš„æª¢æŸ¥é»
    latest_path = os.path.join(checkpoint_dir, f"{experiment_name}_latest.pth")
    torch.save(checkpoint, latest_path)
    
    return checkpoint_path

def load_checkpoint(checkpoint_path, model, optimizer=None):
    """è¼‰å…¥æ¨¡å‹æª¢æŸ¥é»"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    model.load_state_dict(checkpoint['model_state_dict'])
    
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    return checkpoint['epoch'], checkpoint['loss'], checkpoint.get('config')


def setup_logging(level: str = "info") -> logging.Logger:
    """è¨­ç½®æ—¥èªŒç³»çµ±"""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    
    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('pinnx.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)


def set_random_seed(seed: int, deterministic: bool = True) -> None:
    """è¨­ç½®éš¨æ©Ÿç¨®å­ç¢ºä¿é‡ç¾æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    if deterministic:
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def load_config(config_path: str) -> Dict[str, Any]:
    """è¼‰å…¥YAMLé…ç½®æª”æ¡ˆ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return normalize_config_structure(config)


def normalize_config_structure(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    æ¨™æº–åŒ–é…ç½®çµæ§‹ï¼Œæ”¯æŒåµŒå¥—å’Œæ‰å¹³å…©ç¨®æ ¼å¼
    
    è™•ç†ä»¥ä¸‹å…¼å®¹æ€§å•é¡Œï¼š
    1. model.fourier.{enabled, m, sigma} â†’ model.{use_fourier, fourier_m, fourier_sigma}
    2. ç¢ºä¿æ‰€æœ‰å¿…è¦å­—æ®µéƒ½æœ‰é»˜èªå€¼
    """
    model_cfg = config.get('model', {})
    
    # è™•ç† Fourier é…ç½®ï¼ˆåµŒå¥—æ ¼å¼ â†’ æ‰å¹³æ ¼å¼ï¼‰
    if 'fourier' in model_cfg and isinstance(model_cfg['fourier'], dict):
        fourier_cfg = model_cfg['fourier']
        
        # æ˜ å°„ enabled â†’ use_fourier
        if 'enabled' in fourier_cfg and 'use_fourier' not in model_cfg:
            model_cfg['use_fourier'] = fourier_cfg['enabled']
        
        # æ˜ å°„ m â†’ fourier_m
        if 'm' in fourier_cfg and 'fourier_m' not in model_cfg:
            model_cfg['fourier_m'] = fourier_cfg['m']
        
        # æ˜ å°„ sigma â†’ fourier_sigma
        if 'sigma' in fourier_cfg and 'fourier_sigma' not in model_cfg:
            model_cfg['fourier_sigma'] = fourier_cfg['sigma']
        
        # æ˜ å°„ trainable â†’ fourier_trainable
        if 'trainable' in fourier_cfg and 'fourier_trainable' not in model_cfg:
            model_cfg['fourier_trainable'] = fourier_cfg['trainable']
        
        logging.info("âœ… Normalized nested fourier config to flat structure")
    
    # è¨­ç½®é»˜èªå€¼ï¼ˆå¦‚æœæœªè¨­ç½®ï¼‰
    model_cfg.setdefault('use_fourier', True)  # é»˜èªå•Ÿç”¨ Fourier
    model_cfg.setdefault('fourier_m', 32)
    model_cfg.setdefault('fourier_sigma', 1.0)
    model_cfg.setdefault('fourier_trainable', False)
    
    config['model'] = model_cfg
    return config


def get_device(device_name: str) -> torch.device:
    """ç²å–é‹ç®—è¨­å‚™"""
    if device_name == "auto":
        # è‡ªå‹•é¸æ“‡æœ€ä½³å¯ç”¨è¨­å‚™
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logging.info(f"Auto-selected CUDA: {torch.cuda.get_device_name()}")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            logging.info("Auto-selected Apple Metal Performance Shaders")
        else:
            device = torch.device("cpu")
            logging.info("Auto-selected CPU (no GPU available)")
    elif device_name == "cuda" and torch.cuda.is_available():
        device = torch.device("cuda")
        logging.info(f"Using CUDA: {torch.cuda.get_device_name()}")
    elif device_name == "mps" and torch.backends.mps.is_available():
        device = torch.device("mps")
        logging.info("Using Apple Metal Performance Shaders")
    else:
        device = torch.device("cpu")
        logging.info("Using CPU")
    
    return device


def create_model(config: Dict[str, Any], device: torch.device, statistics: Optional[Dict[str, Dict[str, float]]] = None) -> nn.Module:
    """å»ºç«‹ PINN æ¨¡å‹
    
    Args:
        config: é…ç½®å­—å…¸
        device: è¨ˆç®—è¨­å‚™
        statistics: è³‡æ–™çµ±è¨ˆè³‡è¨Šï¼ˆå¯é¸ï¼‰ï¼Œç”¨æ–¼è‡ªå‹•è¨­å®šè¼¸å‡ºç¯„åœ
    """
    model_cfg = config['model']
    
    # ğŸ”§ å¾é…ç½®è®€å– Fourier é–‹é—œï¼ˆæ”¯æŒæ¶ˆèå¯¦é©—ï¼‰
    use_fourier = model_cfg.get('use_fourier', True)  # é»˜èªå•Ÿç”¨
    
    # ğŸ”§ æª¢æŸ¥æ˜¯å¦ç‚º VS-PINNï¼Œè‹¥æ˜¯å‰‡æº–å‚™ç¸®æ”¾å› å­ï¼ˆç”¨æ–¼ Fourier æ¨™æº–åŒ–ä¿®å¾©ï¼‰
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_pinn = (physics_type == 'vs_pinn_channel_flow')
    
    # æå– VS-PINN ç¸®æ”¾å› å­ï¼ˆå¦‚æœæ˜¯ VS-PINNï¼‰
    input_scale_factors = None
    fourier_normalize_input = False
    if is_vs_pinn and use_fourier:
        vs_pinn_cfg = config.get('physics', {}).get('vs_pinn', {})
        scaling_cfg = vs_pinn_cfg.get('scaling_factors', {})
        N_x = scaling_cfg.get('N_x', 2.0)
        N_y = scaling_cfg.get('N_y', 12.0)
        N_z = scaling_cfg.get('N_z', 2.0)
        input_scale_factors = torch.tensor([N_x, N_y, N_z], dtype=torch.float32)
        fourier_normalize_input = True  # ğŸ”§ å•Ÿç”¨ Fourier å‰æ¨™æº–åŒ–ä¿®å¾©
        logging.info(f"ğŸ”§ VS-PINN + Fourier ä¿®å¾©å•Ÿç”¨ï¼šç¸®æ”¾å› å­ N=[{N_x}, {N_y}, {N_z}]")
    
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨å¢å¼·ç‰ˆæ¨¡å‹
    if model_cfg.get('type') == 'enhanced_fourier_mlp':
        # ä½¿ç”¨å¢å¼·ç‰ˆ PINN ç¶²è·¯ (çµ±ä¸€åˆ° PINNNetï¼Œé€éåƒæ•¸æ§åˆ¶)
        base_model = create_enhanced_pinn(
            in_dim=model_cfg['in_dim'],
            out_dim=model_cfg['out_dim'],
            width=model_cfg['width'],
            depth=model_cfg['depth'],
            activation=model_cfg['activation'],
            use_fourier=use_fourier,  # âœ… éµå®ˆé…ç½®é–‹é—œ
            fourier_m=model_cfg.get('fourier_m', 32),
            fourier_sigma=model_cfg.get('fourier_sigma', 1.0),
            use_rwf=model_cfg.get('use_rwf', False),
            rwf_scale_std=model_cfg.get('rwf_scale_std', 0.1),
            fourier_normalize_input=fourier_normalize_input,  # ğŸ”§ æ–°åƒæ•¸
            input_scale_factors=input_scale_factors  # ğŸ”§ æ–°åƒæ•¸
        ).to(device)
    else:
        # ä½¿ç”¨åŸºç¤ PINN ç¶²è·¯
        base_model = PINNNet(
            in_dim=model_cfg['in_dim'],
            out_dim=model_cfg['out_dim'],
            width=model_cfg['width'],
            depth=model_cfg['depth'],
            activation=model_cfg['activation'],
            use_fourier=use_fourier,  # âœ… éµå®ˆé…ç½®é–‹é—œ
            fourier_m=model_cfg.get('fourier_m', 32),
            fourier_sigma=model_cfg.get('fourier_sigma', 1.0),
            fourier_normalize_input=fourier_normalize_input,  # ğŸ”§ æ–°åƒæ•¸
            input_scale_factors=input_scale_factors  # ğŸ”§ æ–°åƒæ•¸
        ).to(device)
    
    # æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ VS-PINN å°ºåº¦åŒ–æˆ–æ‰‹å‹•æ¨™æº–åŒ–
    scaling_cfg = model_cfg.get('scaling', {})
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_pinn = (physics_type == 'vs_pinn_channel_flow')
    
    # ğŸ”§ ä¿®å¾©ï¼šVS-PINN ä½¿ç”¨è‡ªå·±çš„ scale_coordinatesï¼Œè·³é ManualScalingWrapper
    #    é¿å…é›™é‡æ¨™æº–åŒ–ç ´å£ Fourier features å’Œ VS-PINN ç¸®æ”¾
    scaling_enabled = bool(scaling_cfg) and not is_vs_pinn
    
    if scaling_enabled:
        # æš«æ™‚ä½¿ç”¨æ‰‹å‹•æ¨™æº–åŒ–åŒ…è£å™¨ (é¿å… Fourier feature é£½å’Œ)
        from pinnx.models.wrappers import ManualScalingWrapper
        
        # è¼¸å…¥ç¯„åœï¼šå„ªå…ˆä½¿ç”¨é…ç½®æ–‡ä»¶çš„åŸŸç¯„åœï¼Œç¢ºä¿æ¨¡å‹èƒ½æ³›åŒ–åˆ°å®Œæ•´åŸŸ
        # ğŸ”¥ ä¿®å¾©ï¼šä½¿ç”¨ physics.domain è€Œéæ„Ÿæ¸¬é»çµ±è¨ˆï¼Œé¿å…è¨“ç·´ç¯„åœéå°
        domain_cfg = config.get('physics', {}).get('domain', {})
        
        if domain_cfg and 'x_range' in domain_cfg:
            # å„ªå…ˆï¼šå¾é…ç½®æ–‡ä»¶è®€å–å®Œæ•´åŸŸç¯„åœ
            input_x_range = tuple(domain_cfg['x_range'])  # type: ignore
            input_y_range = tuple(domain_cfg['y_range'])  # type: ignore
            logging.info(f"âœ… Using domain ranges from config: x={input_x_range}, y={input_y_range}")
        elif statistics and 'x' in statistics and 'range' in statistics['x']:
            # å›é€€ï¼šä½¿ç”¨æ„Ÿæ¸¬é»çµ±è¨ˆï¼ˆåƒ…ç”¨æ–¼ç„¡é…ç½®çš„ legacy æƒ…æ³ï¼‰
            input_x_range = tuple(statistics['x']['range'])  # type: ignore
            input_y_range = tuple(statistics['y']['range'])  # type: ignore
            logging.warning(f"âš ï¸ Falling back to statistics-based ranges: x={input_x_range}, y={input_y_range}")
            logging.warning(f"âš ï¸ This may cause generalization issues if sensors don't cover full domain!")
        else:
            # æœ€çµ‚å›é€€ï¼šç¡¬ç·¨ç¢¼ï¼ˆJHTDB Channel Re1000ï¼‰
            input_x_range = (0.0, 25.13)
            input_y_range = (-1.0, 1.0)
            logging.warning(f"âš ï¸ Using hardcoded domain ranges: x={input_x_range}, y={input_y_range}")
        
        input_scales: Dict[str, Tuple[float, float]] = {
            'x': input_x_range,
            'y': input_y_range
        }
        
        # ğŸ”¥ 3D è‡ªå‹•æª¢æ¸¬ï¼šå„ªå…ˆä½¿ç”¨é…ç½®ï¼Œå…¶æ¬¡çµ±è¨ˆ
        if domain_cfg and 'z_range' in domain_cfg:
            input_z_range = tuple(domain_cfg['z_range'])  # type: ignore
            input_scales['z'] = input_z_range
            logging.info(f"âœ… 3D mode: z={input_z_range}")
        elif statistics and 'z' in statistics and 'range' in statistics['z']:
            input_z_range = tuple(statistics['z']['range'])  # type: ignore
            input_scales['z'] = input_z_range
            logging.warning(f"âš ï¸ 3D mode using statistics: z={input_z_range}")
        
        # ğŸ”¥ ä¿®å¾©ï¼šå„ªå…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆç”¨æˆ¶é¡¯å¼æŒ‡å®šï¼‰ï¼Œç„¶å¾Œæ‰æ˜¯çµ±è¨ˆè³‡è¨Š
        # å…¼å®¹æ€§è™•ç†ï¼šæ”¯æŒå­—å…¸æ ¼å¼å’Œå­—ç¬¦ä¸²æ ¼å¼çš„ output_norm
        output_scales: Dict[str, Tuple[float, float]] | None = None
        
        if 'output_norm' in scaling_cfg:
            output_norm_raw = scaling_cfg['output_norm']
            
            if isinstance(output_norm_raw, dict):
                # å­—å…¸æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
                output_scales = {
                    'u': tuple(output_norm_raw.get('u', [0.0, 20.0])),   # type: ignore
                    'v': tuple(output_norm_raw.get('v', [-1.0, 1.0])),   # type: ignore
                    'w': tuple(output_norm_raw.get('w', [-5.0, 5.0])),   # type: ignore  # ğŸ”¥ ä¿®å¾©ï¼šæ·»åŠ  w
                    'p': tuple(output_norm_raw.get('p', [-100.0, 10.0])) # type: ignore
                }
                logging.info(f"âœ… Using output ranges from config file (dict format):")
                logging.info(f"   u: {output_scales['u']}")
                logging.info(f"   v: {output_scales['v']}")
                logging.info(f"   w: {output_scales['w']}")  # ğŸ”¥ æ·»åŠ æ—¥èªŒ
                logging.info(f"   p: {output_scales['p']}")
            elif isinstance(output_norm_raw, str):
                # å­—ç¬¦ä¸²æ ¼å¼ï¼šä½¿ç”¨çµ±è¨ˆä¿¡æ¯è‡ªå‹•æ¨å°ï¼ˆå¦‚ "friction_velocity"ï¼‰
                logging.info(f"âš ï¸  output_norm is string '{output_norm_raw}', falling back to statistics")
        
        # å¦‚æœæ²’æœ‰å¾é…ç½®ç²å–åˆ°æœ‰æ•ˆçš„è¼¸å‡ºç¯„åœï¼Œä½¿ç”¨çµ±è¨ˆè³‡è¨Š
        if output_scales is None:
            if statistics:
                # å›é€€ï¼šçµ±è¨ˆä¿¡æ¯ï¼ˆå¯èƒ½å—é™æ–¼å‚³æ„Ÿå™¨é»æ•¸ï¼‰
                if 'u' in statistics and 'range' in statistics['u']:
                    output_u_range = tuple(statistics['u']['range'])  # type: ignore
                else:
                    output_u_range = (0.0, 20.0)
                    
                if 'v' in statistics and 'range' in statistics['v']:
                    output_v_range = tuple(statistics['v']['range'])  # type: ignore
                else:
                    output_v_range = (-1.0, 1.0)
                
                # ğŸ”¥ ä¿®å¾©ï¼šæ·»åŠ  w çš„è™•ç†
                if 'w' in statistics and 'range' in statistics['w']:
                    output_w_range = tuple(statistics['w']['range'])  # type: ignore
                else:
                    output_w_range = (-5.0, 5.0)
                    
                if 'p' in statistics and 'range' in statistics['p']:
                    output_p_range = tuple(statistics['p']['range'])  # type: ignore
                else:
                    output_p_range = (-100.0, 10.0)
                
                output_scales = {
                    'u': output_u_range,
                    'v': output_v_range,
                    'w': output_w_range,  # ğŸ”¥ æ·»åŠ  w
                    'p': output_p_range
                }
                logging.info(f"âœ… Using data-driven output ranges from statistics:")
                logging.info(f"   u: {output_scales['u']}")
                logging.info(f"   v: {output_scales['v']}")
                logging.info(f"   w: {output_scales['w']}")  # ğŸ”¥ æ·»åŠ æ—¥èªŒ
                logging.info(f"   p: {output_scales['p']}")
            else:
                # æœ€çµ‚å›é€€åˆ°ç¡¬ç·¨ç¢¼ç¯„åœ
                output_scales = {
                    'u': (0.0, 20.0),      # é€Ÿåº¦ç¯„åœ (åŸºæ–¼ JHTDB Channel Re1000 å¯¦éš›è³‡æ–™)
                    'v': (-1.0, 1.0),      # æ³•å‘é€Ÿåº¦ç¯„åœ
                    'w': (-5.0, 5.0),      # ğŸ”¥ æ·»åŠ  wï¼šå±•å‘é€Ÿåº¦ç¯„åœ
                    'p': (-100.0, 10.0)    # å£“åŠ›ç¯„åœ (JHTDB å¯¦éš›ç¯„åœç´„ -78 åˆ° 0.3)
                }
                logging.warning("âš ï¸  No statistics or config output_norm provided, using hardcoded ranges (may cause NaN)")
        
        # ğŸ”¥ é‡è¦ï¼šæ ¹æ“šæ¨¡å‹è¼¸å‡ºç¶­åº¦è£œå……ç¼ºå¤±çš„ç¯„åœï¼ˆå¦‚æºé … Sï¼‰
        expected_out_dim = model_cfg.get('out_dim', 3)
        # ğŸ”¥ ä¿®å¾©ï¼šåªæœ‰ç•¶ç¢ºå¯¦ç¼ºå°‘è¼¸å‡ºè®Šé‡æ™‚æ‰è£œå……
        # 3D æ¨¡å‹è¼¸å‡ºé †åºï¼šu, v, w, p (out_dim=4)
        # å¦‚æœæœ‰æºé …å‰‡ç‚ºï¼šu, v, w, p, S (out_dim=5)
        if expected_out_dim == 5 and len(output_scales) == 4:
            # è£œå……æºé … S çš„ç¯„åœï¼ˆé€šå¸¸ç”¨æ–¼é€†å•é¡Œï¼‰
            output_scales['S'] = (-1.0, 1.0)  # åˆå§‹ä¼°è¨ˆï¼Œå¯æ ¹æ“šå¯¦éš›èª¿æ•´
            logging.info("âœ… Added source term 'S' range: (-1.0, 1.0)")
        
        try:
            model = ManualScalingWrapper(
                base_model, 
                input_ranges=input_scales,
                output_ranges=output_scales
            ).to(device)
            logging.info(f"âœ… Manual scaling wrapper applied: inputs {input_scales}, outputs {output_scales}")
        except ImportError:
            logging.warning("ManualScalingWrapper not found, using base model. Consider implementing it.")
            model = base_model
    else:
        # ç›´æ¥ä½¿ç”¨åŸºç¤æ¨¡å‹
        model = base_model
        logging.info("Using base model without scaling")
    
    # ğŸ”¥ æ‡‰ç”¨ SIREN åˆå§‹åŒ–ï¼ˆè‹¥ä½¿ç”¨ Sine æ¿€æ´»å‡½æ•¸ï¼‰
    if model_cfg['activation'] == 'sine':
        # è‹¥æœ‰ wrapperï¼Œéœ€è¦æ‰¾åˆ°åº•å±¤çš„ PINNNet
        target_model = base_model
        if hasattr(model, 'model'):  # ManualScalingWrapper æˆ–å…¶ä»–åŒ…è£å™¨
            target_model = model.model  # type: ignore
        
        if isinstance(target_model, PINNNet):
            init_siren_weights(target_model)
            logging.info("âœ… Applied SIREN weight initialization for Sine activation")
        else:
            logging.warning(f"âš ï¸  Cannot apply SIREN initialization: base model type is {type(target_model)}")
    
    # ğŸ”¥ è¨“ç·´å‰é©—è­‰ï¼šæª¢æŸ¥ç¸®æ”¾åƒæ•¸æ˜¯å¦èˆ‡é…ç½®ä¸€è‡´
    if hasattr(model, 'input_min') and hasattr(model, 'input_max'):
        logging.info("=" * 60)
        logging.info("ğŸ“ Model Scaling Parameters Verification:")
        logging.info(f"   Input min:  {model.input_min.cpu().numpy()}")
        logging.info(f"   Input max:  {model.input_max.cpu().numpy()}")
        logging.info(f"   Output min: {model.output_min.cpu().numpy()}")
        logging.info(f"   Output max: {model.output_max.cpu().numpy()}")
        
        # é©—è­‰è¼¸å…¥ç¯„åœæ˜¯å¦åŒ¹é…é…ç½®
        domain_cfg = config.get('physics', {}).get('domain', {})
        if domain_cfg and 'x_range' in domain_cfg:
            expected_x_range = domain_cfg['x_range']
            expected_y_range = domain_cfg['y_range']
            
            actual_x_min = model.input_min[0].item()
            actual_x_max = model.input_max[0].item()
            actual_y_min = model.input_min[1].item()
            actual_y_max = model.input_max[1].item()
            
            # å®¹å·®æª¢æŸ¥ï¼ˆ1e-3ï¼‰
            x_match = abs(actual_x_min - expected_x_range[0]) < 1e-3 and \
                      abs(actual_x_max - expected_x_range[1]) < 1e-3
            y_match = abs(actual_y_min - expected_y_range[0]) < 1e-3 and \
                      abs(actual_y_max - expected_y_range[1]) < 1e-3
            
            if x_match and y_match:
                logging.info(f"âœ… Input ranges match config: x={expected_x_range}, y={expected_y_range}")
            else:
                logging.error("=" * 60)
                logging.error("âŒ CRITICAL: Input range mismatch detected!")
                logging.error(f"   Expected x: {expected_x_range}, got: [{actual_x_min:.4f}, {actual_x_max:.4f}]")
                logging.error(f"   Expected y: {expected_y_range}, got: [{actual_y_min:.4f}, {actual_y_max:.4f}]")
                logging.error("   This will cause generalization failure outside sensor coverage!")
                logging.error("=" * 60)
                raise ValueError("Input scaling range configuration error - model cannot generalize to full domain")
            
            # 3D æª¢æŸ¥
            if 'z_range' in domain_cfg and len(model.input_min) > 2:
                expected_z_range = domain_cfg['z_range']
                actual_z_min = model.input_min[2].item()
                actual_z_max = model.input_max[2].item()
                z_match = abs(actual_z_min - expected_z_range[0]) < 1e-3 and \
                          abs(actual_z_max - expected_z_range[1]) < 1e-3
                if z_match:
                    logging.info(f"âœ… 3D z-range matches config: {expected_z_range}")
                else:
                    logging.error(f"âŒ Expected z: {expected_z_range}, got: [{actual_z_min:.4f}, {actual_z_max:.4f}]")
                    raise ValueError("Z-axis scaling range configuration error")
        logging.info("=" * 60)
    
    logging.info(f"Created model with {sum(p.numel() for p in model.parameters())} parameters")
    return model


def create_physics(config: Dict[str, Any], device: torch.device):
    """å»ºç«‹ç‰©ç†æ–¹ç¨‹å¼æ¨¡çµ„ï¼ˆæ”¯æ´ VS-PINNï¼‰"""
    physics_cfg = config['physics']
    physics_type = physics_cfg.get('type', 'ns_2d')  # é»˜èªä½¿ç”¨ NS 2D
    
    if physics_type == 'vs_pinn_channel_flow':
        # VS-PINN é€šé“æµæ±‚è§£å™¨
        # ğŸ”§ ä¿®å¾©ï¼šå¾æ­£ç¢ºçš„å·¢ç‹€è·¯å¾‘è®€å–é…ç½®ï¼ˆphysics.vs_pinn.scaling_factorsï¼‰
        vs_pinn_cfg = physics_cfg.get('vs_pinn', {})
        scaling_cfg = vs_pinn_cfg.get('scaling_factors', {})
        
        # ç‰©ç†åƒæ•¸å¯èƒ½åœ¨å¤šå€‹ä½ç½®ï¼ˆå…¼å®¹èˆŠé…ç½®ï¼‰
        channel_flow_cfg = physics_cfg.get('channel_flow', {})
        
        # åŸŸé…ç½®ï¼šå„ªå…ˆä½¿ç”¨ physics.domainï¼ˆæ–°æ ¼å¼ï¼‰ï¼Œå›é€€åˆ° vs_pinn å…§éƒ¨
        domain_cfg = physics_cfg.get('domain', {})
        
        # ğŸ”§ ä¿®å¾©ï¼šè§£æåŸŸé‚Šç•Œï¼ˆä½¿ç”¨ç‰©ç†åº§æ¨™ç¯„åœï¼Œéæ¨™æº–åŒ–åº§æ¨™ï¼‰
        domain_bounds = {
            'x': domain_cfg.get('x_range', [0.0, 25.13]),  # ç‰©ç†åº§æ¨™
            'y': domain_cfg.get('y_range', [-1.0, 1.0]),
            'z': domain_cfg.get('z_range', [0.0, 9.42]),
        }
        
        physics = create_vs_pinn_channel_flow(
            N_x=scaling_cfg.get('N_x', 2.0),
            N_y=scaling_cfg.get('N_y', 12.0),
            N_z=scaling_cfg.get('N_z', 2.0),
            nu=physics_cfg.get('nu', channel_flow_cfg.get('u_tau', 5e-5)),  # å…¼å®¹å¤šç¨®æ ¼å¼
            dP_dx=channel_flow_cfg.get('pressure_gradient', physics_cfg.get('dP_dx', 0.0025)),
            rho=physics_cfg.get('rho', 1.0),
            domain_bounds=domain_bounds,
            loss_config=config.get('losses', {}),  # ğŸ”´ å‚³éæå¤±é…ç½®ï¼ˆåŒ…å« warmup_epochsï¼‰
        )
        logging.info(f"âœ… ä½¿ç”¨ VS-PINN æ±‚è§£å™¨ (N_x={scaling_cfg.get('N_x', 2.0)}, N_y={scaling_cfg.get('N_y', 12.0)}, N_z={scaling_cfg.get('N_z', 2.0)})")
        logging.info(f"   åŸŸé‚Šç•Œ: x={domain_bounds['x']}, y={domain_bounds['y']}, z={domain_bounds['z']}")
    else:
        # æ¨™æº– NS 2D æ±‚è§£å™¨
        physics = NSEquations2D(
            viscosity=physics_cfg.get('nu', 1e-3),
            density=physics_cfg.get('rho', 1.0)
        )
        logging.info(f"âœ… ä½¿ç”¨æ¨™æº– NS 2D æ±‚è§£å™¨")
    
    # ğŸ”§ ä¿®å¾©ï¼šå°‡ physics æ¨¡çµ„ç§»å‹•åˆ°æ­£ç¢ºçš„è¨­å‚™ï¼ˆé¿å… CPU/CUDA å¼µé‡æ··åˆï¼‰
    physics = physics.to(device)
    logging.info(f"   Physics module moved to device: {device}")
    
    return physics


def create_loss_functions(config: Dict[str, Any], device: torch.device) -> Dict[str, nn.Module]:
    """å»ºç«‹æå¤±å‡½æ•¸"""
    loss_cfg = config.get('losses', {})
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_cfg = physics_type == 'vs_pinn_channel_flow'
    base_weight_template, default_adaptive_terms = derive_loss_weights(
        loss_cfg,
        loss_cfg.get('prior_weight', 0.3),
        is_vs_cfg
    )
    
    losses = {
        'residual': NSResidualLoss(
            nu=loss_cfg.get('nu', 1e-3),
            density=loss_cfg.get('rho', 1.0)
        ),
        'boundary': BoundaryConditionLoss(),  # ğŸ†• é‚Šç•Œæ¢ä»¶æå¤±ï¼ˆå« inletï¼‰
        'prior': PriorLossManager(
            consistency_weight=loss_cfg['prior_weight']
        )
    }
    
    return losses


def create_weighters(config: Dict[str, Any], model: nn.Module, device: torch.device, physics=None) -> Dict[str, Any]:
    """å»ºç«‹å‹•æ…‹æ¬Šé‡å™¨ (éœ€è¦æ¨¡å‹å¯¦ä¾‹)"""
    loss_cfg = config.get('losses', {})
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_cfg = physics_type == 'vs_pinn_channel_flow'
    base_weight_template, default_adaptive_terms = derive_loss_weights(
        loss_cfg,
        loss_cfg.get('prior_weight', 0.3),
        is_vs_cfg
    )
    weighters = {}
    
    # ğŸš€ èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
    curriculum_cfg = config.get('curriculum', {})
    if curriculum_cfg.get('enable', False):
        stages = curriculum_cfg.get('stages', [])
        if stages and physics is not None:
            weighters['curriculum'] = CurriculumScheduler(stages, physics)
            logging.info(f"âœ… Curriculum scheduler enabled with {len(stages)} stages")
            # èª²ç¨‹è¨“ç·´å•Ÿç”¨æ™‚ï¼Œç¦ç”¨å…¶ä»–èª¿åº¦å™¨
            weighters['staged'] = None
            weighters['gradnorm'] = None
            weighters['scheduler'] = None
            weighters['causal'] = None
            logging.info("âš ï¸  Other schedulers disabled (curriculum mode active)")
            return weighters
        else:
            weighters['curriculum'] = None
            if not stages:
                logging.warning("âš ï¸  curriculum.enable=true but no stages defined")
            if physics is None:
                logging.warning("âš ï¸  curriculum requires physics module, falling back to staged weights")
    else:
        weighters['curriculum'] = None
    
    # éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨ï¼ˆå„ªå…ˆç´šç¬¬äºŒï¼‰
    if 'staged_weights' in loss_cfg and loss_cfg['staged_weights'].get('enable', False):
        phases = loss_cfg['staged_weights'].get('phases', [])
        if phases:
            weighters['staged'] = StagedWeightScheduler(phases)
            logging.info(f"âœ… Staged weight scheduler enabled with {len(phases)} phases")
        else:
            weighters['staged'] = None
            logging.warning("âš ï¸  staged_weights.enable=true but no phases defined")
    else:
        weighters['staged'] = None
    
    # GradNorm æ¬Šé‡å™¨ï¼ˆèˆ‡éšæ®µå¼æ¬Šé‡äº’æ–¥ï¼‰
    configured_terms = loss_cfg.get('adaptive_loss_terms')
    if configured_terms is not None:
        adaptive_terms = [name for name in configured_terms if name in base_weight_template]
    else:
        adaptive_terms = default_adaptive_terms
    if loss_cfg.get('adaptive_weighting', False) and weighters['staged'] is None and adaptive_terms:
        initial_weights = {name: base_weight_template.get(name, 1.0) for name in adaptive_terms}
        weighters['gradnorm'] = GradNormWeighter(
            model=model,
            loss_names=adaptive_terms,
            alpha=loss_cfg.get('grad_norm_alpha', 0.12),
            update_frequency=loss_cfg.get('weight_update_freq', 100),
            initial_weights=initial_weights,
            device=str(device),
            min_weight=loss_cfg.get('grad_norm_min_weight', 0.1),
            max_weight=loss_cfg.get('grad_norm_max_weight', 10.0)
        )
        logging.info("GradNorm adaptive weighting enabled")
    else:
        weighters['gradnorm'] = None
        if loss_cfg.get('adaptive_weighting', False) and weighters['staged'] is not None:
            logging.info("âš ï¸  adaptive_weighting disabled (using staged_weights)")
    
    # å› æœæ¬Šé‡å™¨
    if loss_cfg.get('causal_weighting', False):
        weighters['causal'] = CausalWeighter(
            causality_strength=loss_cfg.get('causal_eps', 1.0)
        )
        logging.info("Causal weighting enabled")
    else:
        weighters['causal'] = None
    
    # è‡ªé©æ‡‰æ¬Šé‡èª¿åº¦å™¨
    # ğŸ”§ ä¿®å¾©ï¼šåƒ…åœ¨æ˜ç¢ºè¦æ±‚ phase_scheduling æ™‚å•Ÿç”¨ï¼ˆèˆ‡ GradNorm è¡çªï¼‰
    if loss_cfg.get('phase_scheduling', False) and weighters['staged'] is None and adaptive_terms:
        weighters['scheduler'] = AdaptiveWeightScheduler(
            loss_names=adaptive_terms
        )
        logging.info("Adaptive weight scheduler created")
    else:
        weighters['scheduler'] = None
        if not loss_cfg.get('phase_scheduling', False) and weighters['staged'] is None:
            logging.info("Adaptive weight scheduler disabled (use 'phase_scheduling: true' to enable)")
    
    return weighters


def prepare_training_data(config: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:
    """æº–å‚™è¨“ç·´è³‡æ–™ - æ”¯æ´ JHTDB Channel Flow æˆ– Mock è³‡æ–™"""
    
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ JHTDB Channel Flow è¼‰å…¥å™¨
    jhtdb_enabled = config.get('data', {}).get('jhtdb_config', {}).get('enabled', False)
    channel_flow_enabled = 'channel_flow' in config and config['channel_flow'].get('enabled', False)
    
    if jhtdb_enabled or channel_flow_enabled:
        return prepare_channel_flow_training_data(config, device)
    else:
        return prepare_mock_training_data(config, device)


def _apply_validation_split(
    training_dict: Dict[str, torch.Tensor],
    validation_split: float,
    is_vs_pinn: bool
) -> Dict[str, torch.Tensor]:
    """
    ä¾æ“š validation_split å°‡æ„Ÿæ¸¬é»è³‡æ–™åˆ‡åˆ†æˆè¨“ç·´/é©—è­‰é›†åˆï¼Œä¸¦åœ¨ training_dict ä¸­æ–°å¢
    'validation' ç´¢å¼•ã€‚
    """
    if validation_split is None or validation_split <= 0.0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    x_sensors = training_dict.get('x_sensors')
    if x_sensors is None or x_sensors.shape[0] == 0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    n_total = x_sensors.shape[0]
    if n_total < 2:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    n_val = max(1, int(round(n_total * validation_split)))
    if n_val >= n_total:
        # è‡³å°‘ä¿ç•™ä¸€å€‹è¨“ç·´æ„Ÿæ¸¬é»
        n_val = max(1, n_total - 1)
    if n_val <= 0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    device = x_sensors.device
    perm = torch.randperm(n_total, device=device)
    val_idx = perm[:n_val]
    train_idx = perm[n_val:]
    
    if train_idx.numel() == 0:
        # ç•¶ split å¹¾ä¹ç‚º 1.0 æ™‚ï¼Œç¢ºä¿ä»æœ‰è¨“ç·´è³‡æ–™
        train_idx = val_idx[-1:].clone()
        val_idx = val_idx[:-1]
    
    def split_tensor(tensor: Optional[torch.Tensor]) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        if tensor is None:
            return None, None
        if tensor.shape[0] == 0:
            return tensor, tensor
        return tensor[train_idx], tensor[val_idx]
    
    x_train, x_val = split_tensor(training_dict.get('x_sensors'))
    y_train, y_val = split_tensor(training_dict.get('y_sensors'))
    z_train, z_val = split_tensor(training_dict.get('z_sensors'))
    t_train, t_val = split_tensor(training_dict.get('t_sensors'))
    u_train, u_val = split_tensor(training_dict.get('u_sensors'))
    v_train, v_val = split_tensor(training_dict.get('v_sensors'))
    w_train, w_val = split_tensor(training_dict.get('w_sensors'))
    p_train, p_val = split_tensor(training_dict.get('p_sensors'))
    
    # æ›´æ–°è¨“ç·´æ„Ÿæ¸¬è³‡æ–™
    if x_train is not None:
        training_dict['x_sensors'] = x_train
    if y_train is not None:
        training_dict['y_sensors'] = y_train
    if z_train is not None:
        training_dict['z_sensors'] = z_train
    if t_train is not None:
        training_dict['t_sensors'] = t_train
    if u_train is not None:
        training_dict['u_sensors'] = u_train
    if v_train is not None:
        training_dict['v_sensors'] = v_train
    if w_train is not None:
        training_dict['w_sensors'] = w_train
    if p_train is not None:
        training_dict['p_sensors'] = p_train
    
    # å»ºç«‹é©—è­‰è³‡æ–™
    if x_val is None or x_val.shape[0] == 0:
        training_dict['validation'] = {'size': 0}
        return training_dict
    
    coord_parts = [x_val, y_val]
    if is_vs_pinn and z_val is not None and z_val.shape[0] > 0:
        coord_parts.append(z_val)
    validation_coords = torch.cat(coord_parts, dim=1).detach()
    
    target_parts = [u_val, v_val]
    component_order = ['u', 'v']
    if is_vs_pinn:
        if w_val is not None and w_val.shape[0] == validation_coords.shape[0]:
            target_parts.append(w_val)
        else:
            target_parts.append(torch.zeros_like(u_val))
        component_order.append('w')
    target_parts.append(p_val)
    component_order.append('p')
    validation_targets = torch.cat(target_parts, dim=1).detach()
    
    if t_val is not None:
        validation_time = t_val.detach()
    else:
        validation_time = None
    
    training_dict['validation'] = {
        'coords': validation_coords,
        'targets': validation_targets,
        'time': validation_time,
        'components': component_order,
        'size': int(validation_coords.shape[0])
    }
    
    return training_dict


def prepare_mock_training_data(config: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:
    """å»ºç«‹ Mock è¨“ç·´è³‡æ–™ç”¨æ–¼æ¸¬è©¦æ•´åˆ"""
    
    # å¾é…ç½®ä¸­è®€å–åƒæ•¸
    K = config['sensors']['K']
    sampling = config['training']['sampling']
    physics_cfg = config['physics']
    domain = physics_cfg['domain']
    
    # å®šç¾©åŸŸç¯„åœ
    x_range = domain['x_range']
    y_range = domain['y_range']
    
    # ç”Ÿæˆæ„Ÿæ¸¬å™¨é» (å‡å‹»åˆ†ä½ˆ)
    x_sensors = torch.rand(K, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y_sensors = torch.rand(K, 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
    t_sensors = torch.zeros_like(x_sensors)  # å‡è¨­ç©©æ…‹
    
    # ç”Ÿæˆ Mock é€Ÿåº¦å’Œå£“åŠ›è³‡æ–™ (åŸºæ–¼è§£æè§£æˆ–ç°¡å–®æ¨¡å¼)
    # ç°¡å–®çš„é€šé“æµæ¨¡å¼: u = U_max * (1 - (2y/H - 1)^2), v = 0, p = ç·šæ€§åˆ†ä½ˆ
    y_norm = (y_sensors - y_range[0]) / (y_range[1] - y_range[0])  # æ­¸ä¸€åŒ–åˆ° [0,1]
    y_centered = 2 * y_norm - 1  # æ­¸ä¸€åŒ–åˆ° [-1,1]
    
    u_max = 1.0  # æœ€å¤§é€Ÿåº¦
    u_sensors = u_max * (1 - y_centered**2)  # æ‹‹ç‰©ç·šå‹é€Ÿåº¦åˆ†ä½ˆ
    v_sensors = torch.zeros_like(u_sensors)   # å‚ç›´é€Ÿåº¦ç‚ºé›¶
    p_sensors = torch.ones_like(u_sensors) * 0.1  # ç°¡å–®çš„å£“åŠ›å ´
    
    # ç”Ÿæˆ PDE æ®˜å·®é»
    x_pde = torch.rand(sampling['pde_points'], 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y_pde = torch.rand(sampling['pde_points'], 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
    t_pde = torch.zeros_like(x_pde)  # ç©©æ…‹å‡è¨­
    
    # ç”Ÿæˆé‚Šç•Œé» (ä¸Šä¸‹å£é¢)
    n_bc = sampling['boundary_points']
    x_bc = torch.rand(n_bc, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
    y_bc_bottom = torch.full((n_bc//2, 1), y_range[0], device=device)  # ä¸‹å£é¢
    y_bc_top = torch.full((n_bc - n_bc//2, 1), y_range[1], device=device)  # ä¸Šå£é¢
    y_bc = torch.cat([y_bc_bottom, y_bc_top], dim=0)
    x_bc = torch.cat([x_bc[:n_bc//2], x_bc[n_bc//2:]], dim=0)
    t_bc = torch.zeros_like(x_bc)
    
    logging.info(f"Mock training data generated: K={K} sensors, {sampling['pde_points']} PDE points, {n_bc} BC points")
    
    training_dict = {
        'x_pde': x_pde, 'y_pde': y_pde, 't_pde': t_pde,
        'x_bc': x_bc, 'y_bc': y_bc, 't_bc': t_bc,
        'x_sensors': x_sensors, 'y_sensors': y_sensors, 't_sensors': t_sensors,
        'u_sensors': u_sensors, 'v_sensors': v_sensors, 'p_sensors': p_sensors
    }
    
    validation_split = config.get('training', {}).get('validation_split', 0.0)
    training_dict = _apply_validation_split(training_dict, validation_split, is_vs_pinn=False)
    
    return training_dict


def sample_boundary_points(
    n_points: int,
    domain_bounds: Dict[str, Tuple[float, float]],
    device: torch.device,
    distribution: Optional[Dict[str, int]] = None
) -> torch.Tensor:
    """
    åœ¨é‚Šç•Œä¸Šå‡å‹»æ¡æ¨£é»
    
    Args:
        n_points: ç¸½é‚Šç•Œé»æ•¸
        domain_bounds: åŸŸé‚Šç•Œ {'x': (x_min, x_max), 'y': (y_min, y_max), 'z': (z_min, z_max)}
        device: PyTorch device
        distribution: é‚Šç•Œé»åˆ†ä½ˆ {'wall': int, 'periodic': int, 'inlet': int}
                     å¦‚æœç‚º Noneï¼Œé è¨­ç‚º {'wall': 1000, 'periodic': 800, 'inlet': 200}
    
    Returns:
        é‚Šç•Œé»åº§æ¨™ [n_points, 3] (x, y, z)
    """
    if distribution is None:
        distribution = {'wall': 1000, 'periodic': 800, 'inlet': 200}
    
    # é©—è­‰ç¸½é»æ•¸
    total_requested = sum(distribution.values())
    if total_requested != n_points:
        logging.warning(f"âš ï¸ Boundary distribution sum ({total_requested}) != n_points ({n_points}), è‡ªå‹•èª¿æ•´æ¯”ä¾‹")
        # æŒ‰æ¯”ä¾‹èª¿æ•´
        scale = n_points / total_requested
        distribution = {k: int(v * scale) for k, v in distribution.items()}
        # ä¿®æ­£èˆå…¥èª¤å·®
        diff = n_points - sum(distribution.values())
        distribution['wall'] += diff
    
    x_min, x_max = domain_bounds['x']
    y_min, y_max = domain_bounds['y']
    z_min, z_max = domain_bounds['z']
    
    boundary_points = []
    
    # 1. å£é¢é» (y = y_min å’Œ y = y_max)
    n_wall = distribution['wall']
    n_wall_bottom = n_wall // 2
    n_wall_top = n_wall - n_wall_bottom
    
    # ä¸‹å£é¢ (y = y_min)
    x_wall_bottom = torch.rand(n_wall_bottom, 1, device=device) * (x_max - x_min) + x_min
    y_wall_bottom = torch.full((n_wall_bottom, 1), y_min, device=device)
    z_wall_bottom = torch.rand(n_wall_bottom, 1, device=device) * (z_max - z_min) + z_min
    wall_bottom = torch.cat([x_wall_bottom, y_wall_bottom, z_wall_bottom], dim=1)
    
    # ä¸Šå£é¢ (y = y_max)
    x_wall_top = torch.rand(n_wall_top, 1, device=device) * (x_max - x_min) + x_min
    y_wall_top = torch.full((n_wall_top, 1), y_max, device=device)
    z_wall_top = torch.rand(n_wall_top, 1, device=device) * (z_max - z_min) + z_min
    wall_top = torch.cat([x_wall_top, y_wall_top, z_wall_top], dim=1)
    
    boundary_points.extend([wall_bottom, wall_top])
    
    # 2. é€±æœŸæ€§é‚Šç•Œé» (x = x_min/x_max, z = z_min/z_max)
    n_periodic = distribution['periodic']
    n_per_face = n_periodic // 4  # 4 å€‹é¢ï¼šx_min, x_max, z_min, z_max
    
    # x = x_min
    x_left = torch.full((n_per_face, 1), x_min, device=device)
    y_left = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_left = torch.rand(n_per_face, 1, device=device) * (z_max - z_min) + z_min
    periodic_left = torch.cat([x_left, y_left, z_left], dim=1)
    
    # x = x_max
    x_right = torch.full((n_per_face, 1), x_max, device=device)
    y_right = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_right = torch.rand(n_per_face, 1, device=device) * (z_max - z_min) + z_min
    periodic_right = torch.cat([x_right, y_right, z_right], dim=1)
    
    # z = z_min
    x_front = torch.rand(n_per_face, 1, device=device) * (x_max - x_min) + x_min
    y_front = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_front = torch.full((n_per_face, 1), z_min, device=device)
    periodic_front = torch.cat([x_front, y_front, z_front], dim=1)
    
    # z = z_max
    x_back = torch.rand(n_per_face, 1, device=device) * (x_max - x_min) + x_min
    y_back = torch.rand(n_per_face, 1, device=device) * (y_max - y_min) + y_min
    z_back = torch.full((n_per_face, 1), z_max, device=device)
    periodic_back = torch.cat([x_back, y_back, z_back], dim=1)
    
    boundary_points.extend([periodic_left, periodic_right, periodic_front, periodic_back])
    
    # 3. Inlet é» (x = x_minï¼Œç‰¹åˆ¥è™•ç†)
    n_inlet = distribution['inlet']
    x_inlet = torch.full((n_inlet, 1), x_min, device=device)
    y_inlet = torch.rand(n_inlet, 1, device=device) * (y_max - y_min) + y_min
    z_inlet = torch.rand(n_inlet, 1, device=device) * (z_max - z_min) + z_min
    inlet = torch.cat([x_inlet, y_inlet, z_inlet], dim=1)
    
    boundary_points.append(inlet)
    
    # åˆä½µæ‰€æœ‰é‚Šç•Œé»
    all_boundary_points = torch.cat(boundary_points, dim=0)
    
    return all_boundary_points


def sample_interior_points(
    n_points: int,
    domain_bounds: Dict[str, Tuple[float, float]],
    device: torch.device,
    exclude_boundary_tol: float = 0.01,
    use_sobol: bool = True
) -> torch.Tensor:
    """
    åœ¨å…§éƒ¨å‡å‹»æ¡æ¨£é»ï¼Œæ’é™¤é‚Šç•Œå€åŸŸ
    
    Args:
        n_points: å…§éƒ¨é»æ•¸
        domain_bounds: åŸŸé‚Šç•Œ {'x': (x_min, x_max), 'y': (y_min, y_max), 'z': (z_min, z_max)}
        device: PyTorch device
        exclude_boundary_tol: é‚Šç•Œæ’é™¤å®¹å·®ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        use_sobol: æ˜¯å¦ä½¿ç”¨ Sobol åºåˆ—ï¼ˆæ›´å‡å‹»ï¼‰
    
    Returns:
        å…§éƒ¨é»åº§æ¨™ [n_points, 3] (x, y, z)
    """
    x_min, x_max = domain_bounds['x']
    y_min, y_max = domain_bounds['y']
    z_min, z_max = domain_bounds['z']
    
    # èª¿æ•´å…§éƒ¨åŸŸç¯„åœï¼ˆæ’é™¤é‚Šç•Œå®¹å·®ï¼‰
    x_min_inner = x_min + exclude_boundary_tol
    x_max_inner = x_max - exclude_boundary_tol
    y_min_inner = y_min + exclude_boundary_tol
    y_max_inner = y_max - exclude_boundary_tol
    z_min_inner = z_min + exclude_boundary_tol
    z_max_inner = z_max - exclude_boundary_tol
    
    if use_sobol:
        # ä½¿ç”¨ Sobol åºåˆ—ï¼ˆæº–å‡å‹»åˆ†ä½ˆï¼‰
        sobol = torch.quasirandom.SobolEngine(dimension=3, scramble=True)
        samples = sobol.draw(n_points).to(device)
        
        # ç¸®æ”¾åˆ°å…§éƒ¨åŸŸ
        x_interior = samples[:, 0:1] * (x_max_inner - x_min_inner) + x_min_inner
        y_interior = samples[:, 1:2] * (y_max_inner - y_min_inner) + y_min_inner
        z_interior = samples[:, 2:3] * (z_max_inner - z_min_inner) + z_min_inner
    else:
        # ä½¿ç”¨å‡å‹»éš¨æ©Ÿæ¡æ¨£
        x_interior = torch.rand(n_points, 1, device=device) * (x_max_inner - x_min_inner) + x_min_inner
        y_interior = torch.rand(n_points, 1, device=device) * (y_max_inner - y_min_inner) + y_min_inner
        z_interior = torch.rand(n_points, 1, device=device) * (z_max_inner - z_min_inner) + z_min_inner
    
    interior_points = torch.cat([x_interior, y_interior, z_interior], dim=1)
    
    return interior_points


def prepare_channel_flow_training_data(config: Dict[str, Any], device: torch.device) -> Dict[str, torch.Tensor]:
    """ä½¿ç”¨ Channel Flow è¼‰å…¥å™¨æº–å‚™è¨“ç·´è³‡æ–™"""
    from pinnx.dataio.channel_flow_loader import prepare_training_data as load_channel_flow
    
    # è¼‰å…¥ Channel Flow è³‡æ–™ - æ”¯æ´å…©ç¨®é…ç½®æ ¼å¼
    if 'channel_flow' in config:
        cf_config = config['channel_flow']
        strategy = cf_config.get('strategy', 'qr_pivot')
    else:
        # ä½¿ç”¨ JHTDB é…ç½®æ ¼å¼ - è®€å– sensors.selection_method
        sensors_cfg = config.get('sensors', {})
        strategy = sensors_cfg.get('selection_method', 'qr_pivot')  # æ”¯æ´å¾é…ç½®è®€å–ç­–ç•¥
    
    K = config['sensors']['K']
    
    # ğŸ†• è®€å–è‡ªå®šç¾©æ„Ÿæ¸¬é»æ–‡ä»¶åï¼ˆå¦‚æœæœ‰ï¼‰
    sensor_file = config.get('sensors', {}).get('sensor_file', None)
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚º 3D æ¡ˆä¾‹ï¼ˆæ±ºå®šæ˜¯å¦è«‹æ±‚ w åˆ†é‡ï¼‰
    is_3d = config.get('physics', {}).get('type') == 'vs_pinn_channel_flow'
    target_fields = ['u', 'v', 'w', 'p'] if is_3d else ['u', 'v', 'p']
    
    channel_data = load_channel_flow(
        strategy=strategy,
        K=K,
        target_fields=target_fields,
        sensor_file=sensor_file  # å‚³éè‡ªå®šç¾©æ–‡ä»¶å
    )
    
    # æå–æ„Ÿæ¸¬å™¨åº§æ¨™å’Œè³‡æ–™
    coords = channel_data['coordinates']  # (K, 2 or 3) numpy array
    sensor_data = channel_data['sensor_data']  # dict with 'u', 'v', ('w',) 'p'
    domain_bounds = channel_data['domain_bounds']
    
    # è¨ˆç®—æ¨™æº–åŒ–åƒæ•¸ï¼ˆVS-PINN é¢¨æ ¼ï¼šæ˜ å°„åˆ° [-1, 1]ï¼‰
    x_range = domain_bounds['x']
    y_range = domain_bounds['y']
    x_min, x_max = x_range[0], x_range[1]
    y_min, y_max = y_range[0], y_range[1]
    
    def normalize_coord(coord, c_min, c_max):
        """å°‡åº§æ¨™æ¨™æº–åŒ–åˆ° [-1, 1]"""
        return 2.0 * (coord - c_min) / (c_max - c_min) - 1.0
    
    def denormalize_coord(coord_norm, c_min, c_max):
        """å¾ [-1, 1] åæ¨™æº–åŒ–"""
        return (coord_norm + 1.0) / 2.0 * (c_max - c_min) + c_min
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚º VS-PINNï¼ˆéœ€è¦ 3D åæ¨™ï¼‰
    is_vs_pinn = config.get('physics', {}).get('type') == 'vs_pinn_channel_flow'
    
    # è½‰æ›ç‚º PyTorch tensor
    x_sensors_raw = torch.from_numpy(coords[:, 0:1]).float().to(device)  # (K, 1)
    y_sensors_raw = torch.from_numpy(coords[:, 1:2]).float().to(device)  # (K, 1)
    
    # ğŸ†• å¦‚æœæ˜¯ VS-PINN ä¸”æœ‰çœŸå¯¦ z åº§æ¨™ï¼Œä½¿ç”¨å®ƒï¼›å¦å‰‡ç‚º 0
    if is_vs_pinn and coords.shape[1] >= 3:
        z_sensors_raw = torch.from_numpy(coords[:, 2:3]).float().to(device)  # (K, 1)
        # å¾é…ç½®è®€å– z ç¯„åœ
        z_domain = config['physics'].get('domain', {})
        z_min = z_domain.get('z_range', [0.0, 9.42])[0]
        z_max = z_domain.get('z_range', [0.0, 9.42])[1]
    else:
        z_sensors_raw = torch.zeros_like(x_sensors_raw)
        z_min = z_max = 0.0  # 2D æƒ…æ³
    
    # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
    x_sensors = x_sensors_raw
    y_sensors = y_sensors_raw
    z_sensors = z_sensors_raw if is_vs_pinn else torch.zeros_like(x_sensors)
    t_sensors = torch.zeros_like(x_sensors)  # æš«æ™‚å‡è¨­ t=0
    
    u_sensors = torch.from_numpy(sensor_data['u'].reshape(-1, 1)).float().to(device)
    v_sensors = torch.from_numpy(sensor_data['v'].reshape(-1, 1)).float().to(device)
    p_sensors = torch.from_numpy(sensor_data['p'].reshape(-1, 1)).float().to(device)
    
    # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  w åˆ†é‡ï¼ˆå‡è¨­ç‚º 0 æˆ–å¾æ•¸æ“šä¸­ç²å–ï¼‰
    if is_vs_pinn:
        if 'w' in sensor_data:
            w_sensors = torch.from_numpy(sensor_data['w'].reshape(-1, 1)).float().to(device)
        else:
            # 2D åˆ‡ç‰‡å‡è¨­ w=0
            w_sensors = torch.zeros_like(u_sensors)
    else:
        w_sensors = None  # 2D ä¸éœ€è¦ w
    
    # ç”Ÿæˆ PDE æ®˜å·®é»å’Œé‚Šç•Œé»
    sampling = config['training']['sampling']
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ä½¿ç”¨åˆ†å±¤æ¡æ¨£ç­–ç•¥
    use_stratified = sampling.get('strategy', 'uniform') == 'stratified'
    
    if use_stratified:
        # === åˆ†å±¤æ¡æ¨£ç­–ç•¥ ===
        logging.info("ğŸ“Š ä½¿ç”¨åˆ†å±¤æ¡æ¨£ç­–ç•¥ (stratified sampling)")
        
        # æ§‹å»ºåŸŸé‚Šç•Œå­—å…¸ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        bounds_dict = {
            'x': (x_min, x_max),
            'y': (y_min, y_max),
            'z': (z_min, z_max) if is_vs_pinn else (0.0, 0.0)
        }
        
        # ç²å–é‚Šç•Œé»åˆ†ä½ˆé…ç½®
        boundary_dist = sampling.get('boundary_distribution', {
            'wall': 1000, 
            'periodic': 800, 
            'inlet': 200
        })
        
        # ç”Ÿæˆé‚Šç•Œé»ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        n_bc = sampling.get('boundary_points', 2000)
        boundary_points_raw = sample_boundary_points(
            n_points=n_bc,
            domain_bounds=bounds_dict,
            device=device,
            distribution=boundary_dist
        )
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_bc = boundary_points_raw[:, 0:1]
        y_bc = boundary_points_raw[:, 1:2]
        z_bc = boundary_points_raw[:, 2:3] if is_vs_pinn else torch.zeros_like(x_bc)
        t_bc = torch.zeros_like(x_bc)
        
        # ç”Ÿæˆå…§éƒ¨ PDE é»ï¼ˆç‰©ç†åº§æ¨™ï¼‰
        n_pde = sampling.get('interior_points', 10000)
        use_sobol = sampling.get('use_sobol', True)
        exclude_tol = sampling.get('boundary_tolerance', 0.01)
        
        interior_points_raw = sample_interior_points(
            n_points=n_pde,
            domain_bounds=bounds_dict,
            device=device,
            exclude_boundary_tol=exclude_tol,
            use_sobol=use_sobol
        )
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_pde = interior_points_raw[:, 0:1]
        y_pde = interior_points_raw[:, 1:2]
        z_pde = interior_points_raw[:, 2:3] if is_vs_pinn else torch.zeros_like(x_pde)
        t_pde = torch.zeros_like(x_pde)
        
        logging.info(f"âœ… åˆ†å±¤æ¡æ¨£å®Œæˆ: {n_bc} é‚Šç•Œé» + {n_pde} å…§éƒ¨é»")
        logging.info(f"   - é‚Šç•Œåˆ†ä½ˆ: {boundary_dist}")
        logging.info(f"   - Sobol æ¡æ¨£: {use_sobol}, é‚Šç•Œå®¹å·®: {exclude_tol}")
        
    else:
        # === åŸå§‹å‡å‹»éš¨æ©Ÿæ¡æ¨£ ===
        logging.info("ğŸ“Š ä½¿ç”¨å‡å‹»éš¨æ©Ÿæ¡æ¨£ç­–ç•¥ (uniform sampling)")
        
        # PDE æ®˜å·®é»ï¼ˆåŸå§‹åº§æ¨™ï¼‰
        x_pde_raw = torch.rand(sampling['pde_points'], 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
        y_pde_raw = torch.rand(sampling['pde_points'], 1, device=device) * (y_range[1] - y_range[0]) + y_range[0]
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_pde = x_pde_raw
        y_pde = y_pde_raw
        t_pde = torch.zeros_like(x_pde)  # ç©©æ…‹å‡è¨­
        
        # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  z åº§æ¨™åˆ° PDE é»
        if is_vs_pinn:
            z_pde_raw = torch.rand(sampling['pde_points'], 1, device=device) * (z_max - z_min) + z_min
            z_pde = z_pde_raw
        else:
            z_pde = torch.zeros_like(x_pde)  # 2D æƒ…æ³ä¸‹ z=0
        
        # é‚Šç•Œé»ï¼ˆåŸå§‹åº§æ¨™ï¼‰
        n_bc = sampling['boundary_points']
        x_bc_raw = torch.rand(n_bc, 1, device=device) * (x_range[1] - x_range[0]) + x_range[0]
        y_bc_bottom_raw = torch.full((n_bc//2, 1), y_range[0], device=device)  # ä¸‹å£é¢
        y_bc_top_raw = torch.full((n_bc - n_bc//2, 1), y_range[1], device=device)  # ä¸Šå£é¢
        y_bc_raw = torch.cat([y_bc_bottom_raw, y_bc_top_raw], dim=0)
        x_bc_raw = torch.cat([x_bc_raw[:n_bc//2], x_bc_raw[n_bc//2:]], dim=0)
        
        # ğŸ”§ ä¿æŒç‰©ç†åº§æ¨™ï¼ˆç”± ManualScalingWrapper è² è²¬æ¨™æº–åŒ–ï¼‰
        x_bc = x_bc_raw
        y_bc = y_bc_raw
        t_bc = torch.zeros_like(x_bc)
        
        # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  z åº§æ¨™åˆ°é‚Šç•Œé»
        if is_vs_pinn:
            z_bc_raw = torch.rand(n_bc, 1, device=device) * (z_max - z_min) + z_min
            z_bc = z_bc_raw
        else:
            z_bc = torch.zeros_like(x_bc)  # 2D æƒ…æ³ä¸‹ z=0
    
    # å­˜å„²é¡å¤–è³‡è¨Šåˆ°å…¨å±€è®Šé‡ï¼ˆåŒ…å«æ¨™æº–åŒ–åƒæ•¸ï¼‰
    global _channel_data_cache
    _channel_data_cache = {
        'domain_bounds': domain_bounds,
        'channel_data': channel_data,
        'normalization': {
            'x_min': x_min, 'x_max': x_max,
            'y_min': y_min, 'y_max': y_max,
            'z_min': z_min, 'z_max': z_max,  # ğŸ†• æ·»åŠ  z ç¯„åœ
            'normalize_fn': normalize_coord,
            'denormalize_fn': denormalize_coord
        },
        'is_vs_pinn': is_vs_pinn  # ğŸ†• æ¨™è¨˜æ˜¯å¦ç‚º VS-PINN
    }
    
    # ğŸ†• ç”Ÿæˆåˆå§‹æ¢ä»¶é»ï¼ˆt=0ï¼‰- ä½¿ç”¨æ„Ÿæ¸¬å™¨æ•¸æ“šä½œç‚º IC
    ic_config = config.get('initial_condition', {})
    if ic_config.get('enabled', False):
        n_ic = ic_config.get('n_points', 256)
        # å¾æ„Ÿæ¸¬å™¨æ•¸æ“šä¸­éš¨æ©Ÿæ¡æ¨£ä½œç‚º ICï¼ˆæˆ–ä½¿ç”¨å®Œæ•´æ„Ÿæ¸¬å™¨æ•¸æ“šï¼‰
        if n_ic >= len(x_sensors):
            # ä½¿ç”¨æ‰€æœ‰æ„Ÿæ¸¬å™¨æ•¸æ“š
            x_ic = x_sensors.clone()
            y_ic = y_sensors.clone()
            z_ic = z_sensors.clone()  # ğŸ†• æ·»åŠ  z_ic
            t_ic = torch.zeros_like(x_ic)
            u_ic = u_sensors.clone()
            v_ic = v_sensors.clone()
            p_ic = p_sensors.clone()
            if is_vs_pinn:
                w_ic = w_sensors.clone() if w_sensors is not None else torch.zeros_like(u_ic)
            else:
                w_ic = torch.empty(0, 1, device=device)
        else:
            # éš¨æ©Ÿæ¡æ¨£
            ic_indices = torch.randperm(len(x_sensors), device=device)[:n_ic]
            x_ic = x_sensors[ic_indices]
            y_ic = y_sensors[ic_indices]
            z_ic = z_sensors[ic_indices]  # ğŸ†• æ·»åŠ  z_ic
            t_ic = torch.zeros_like(x_ic)
            u_ic = u_sensors[ic_indices]
            v_ic = v_sensors[ic_indices]
            p_ic = p_sensors[ic_indices]
            if is_vs_pinn:
                w_ic = w_sensors[ic_indices] if w_sensors is not None else torch.zeros_like(u_ic)
            else:
                w_ic = torch.empty(0, 1, device=device)
    else:
        # IC ç¦ç”¨æ™‚ï¼Œä½¿ç”¨ç©ºå¼µé‡
        x_ic = torch.empty(0, 1, device=device)
        y_ic = torch.empty(0, 1, device=device)
        z_ic = torch.empty(0, 1, device=device)  # ğŸ†• æ·»åŠ  z_ic
        t_ic = torch.empty(0, 1, device=device)
        u_ic = torch.empty(0, 1, device=device)
        v_ic = torch.empty(0, 1, device=device)
        w_ic = torch.empty(0, 1, device=device)  # ğŸ†• æ·»åŠ  w_ic
        p_ic = torch.empty(0, 1, device=device)
    
    # æå–ä½ä¿çœŸå…ˆé©—è³‡æ–™ (å¦‚æœæœ‰)
    training_dict = {
        'x_pde': x_pde, 'y_pde': y_pde, 'z_pde': z_pde, 't_pde': t_pde,  # ğŸ†• æ·»åŠ  z_pde
        'x_bc': x_bc, 'y_bc': y_bc, 'z_bc': z_bc, 't_bc': t_bc,  # ğŸ†• æ·»åŠ  z_bc
        'x_sensors': x_sensors, 'y_sensors': y_sensors, 'z_sensors': z_sensors, 't_sensors': t_sensors,  # ğŸ†• æ·»åŠ  z_sensors
        'u_sensors': u_sensors, 'v_sensors': v_sensors, 'p_sensors': p_sensors,
        'x_ic': x_ic, 'y_ic': y_ic, 'z_ic': z_ic, 't_ic': t_ic,  # ğŸ†• æ·»åŠ  z_ic
        'u_ic': u_ic, 'v_ic': v_ic, 'p_ic': p_ic
    }
    
    # ğŸ†• å¦‚æœæ˜¯ VS-PINNï¼Œæ·»åŠ  w åˆ†é‡åˆ°è¨“ç·´å­—å…¸
    if is_vs_pinn:
        training_dict['w_sensors'] = w_sensors if w_sensors is not None else torch.zeros_like(u_sensors)
        training_dict['w_ic'] = w_ic
    else:
        # 2D æƒ…æ³ä¸‹ä¸éœ€è¦ wï¼Œä½†ç‚ºäº†çµ±ä¸€æ€§å¯ä»¥æ·»åŠ ç©ºå¼µé‡
        training_dict['w_sensors'] = torch.empty(0, 1, device=device)
        training_dict['w_ic'] = torch.empty(0, 1, device=device)
    
    # æ·»åŠ ä½ä¿çœŸå…ˆé©—è³‡æ–™åˆ°æ‰¹æ¬¡ (å¦‚æœå¯ç”¨)
    if 'lowfi_prior' in channel_data and channel_data['lowfi_prior']:
        lowfi = channel_data['lowfi_prior']
        if 'u' in lowfi:
            training_dict['u_prior'] = torch.from_numpy(lowfi['u'].reshape(-1, 1)).float().to(device)
        if 'v' in lowfi:
            training_dict['v_prior'] = torch.from_numpy(lowfi['v'].reshape(-1, 1)).float().to(device)
        if 'p' in lowfi:
            training_dict['p_prior'] = torch.from_numpy(lowfi['p'].reshape(-1, 1)).float().to(device)
        training_dict['has_prior'] = True
    else:
        training_dict['has_prior'] = False
    
    validation_split = config.get('training', {}).get('validation_split', 0.0)
    training_dict = _apply_validation_split(training_dict, validation_split, is_vs_pinn=is_vs_pinn)
    
    return training_dict





def train_step(model: nn.Module, 
               physics: NSEquations2D,
               losses: Dict[str, nn.Module],
               data_batch: Dict[str, torch.Tensor],
               optimizer: torch.optim.Optimizer,
               weighters: Dict[str, Any],
               epoch: int,
               device: torch.device,
               config: Optional[Dict[str, Any]] = None,
               input_normalizer: Optional[InputNormalizer] = None) -> Dict[str, Any]:
    """åŸ·è¡Œä¸€å€‹è¨“ç·´æ­¥é©Ÿï¼Œæ”¯æ´å‹•æ…‹æ¬Šé‡èª¿æ•´"""
    optimizer.zero_grad()
    
    # ğŸ†• æª¢æŸ¥æ˜¯å¦ç‚º VS-PINNï¼ˆ3Dï¼‰é‚„æ˜¯æ¨™æº– PINNï¼ˆ2Dï¼‰
    is_vs_pinn = 'z_pde' in data_batch and hasattr(physics, 'compute_momentum_residuals')
    loss_cfg = config['losses'] if config and 'losses' in config else {}
    
    def prepare_model_coords(coord_tensor: torch.Tensor, require_grad: bool = False) -> torch.Tensor:
        """ç”¢ç”Ÿæ¨¡å‹è¼¸å…¥åº§æ¨™"""
        coords = coord_tensor
        if input_normalizer is not None:
            coords = input_normalizer.transform(coords)
        if is_vs_pinn and hasattr(physics, 'scale_coordinates'):
            coords = physics.scale_coordinates(coord_tensor)
        # ğŸ”§ ä¿®å¾©ï¼šä¸è¦ä½¿ç”¨ .detach()ï¼Œå¦å‰‡æœƒæ–·é–‹æ¢¯åº¦éˆæ¢
        # if require_grad:
        #     coords = coords.clone().detach().requires_grad_(True)
        if require_grad and not coords.requires_grad:
            coords.requires_grad_(True)
        return coords
    
    # PDE æ®˜å·®è¨ˆç®— - æ”¯æŒ 2D å’Œ 3D
    if is_vs_pinn:
        # VS-PINN 3D: ä½¿ç”¨ (x, y, z) åº§æ¨™
        coords_pde = torch.cat([data_batch['x_pde'], data_batch['y_pde'], data_batch['z_pde']], dim=1)
    else:
        # æ¨™æº– PINN 2D: åªä½¿ç”¨ (x, y) åº§æ¨™
        coords_pde = torch.cat([data_batch['x_pde'], data_batch['y_pde']], dim=1)
    
    coords_pde.requires_grad_(True)  # ç‰©ç†åº§æ¨™ä¿ç•™æ¢¯åº¦ä»¥æ”¯æ´éç¸®æ”¾æƒ…æ³
    model_coords_pde = prepare_model_coords(coords_pde, require_grad=True)
    
    u_pred = model(model_coords_pde)
    
    # æ ¹æ“šæ¨¡å¼æå–é€Ÿåº¦å’Œå£“åŠ›åˆ†é‡
    if is_vs_pinn:
        # VS-PINN: é æ¸¬ [u, v, w, p]
        if u_pred.shape[1] == 3:
            # æ¨¡å‹åªè¼¸å‡º [u, v, p]ï¼Œæ·»åŠ  w=0
            velocity = u_pred[:, :2]  # u, v
            pressure = u_pred[:, 2:3]  # p
            w_component = torch.zeros_like(pressure)
            predictions = torch.cat([velocity, w_component, pressure], dim=1)
        elif u_pred.shape[1] == 4:
            # æ¨¡å‹è¼¸å‡ºå®Œæ•´ [u, v, w, p]
            predictions = u_pred
            velocity = u_pred[:, :2]  # u, v
            pressure = u_pred[:, 3:4]  # p
        else:
            raise ValueError(f"VS-PINN æ¨¡å‹è¼¸å‡ºç¶­åº¦éŒ¯èª¤ï¼š{u_pred.shape[1]}ï¼ŒæœŸæœ› 3 æˆ– 4")
    else:
        # æ¨™æº– PINN: æ ¹æ“šè¼¸å‡ºç¶­åº¦è™•ç†
        if u_pred.shape[1] == 3:
            # 3D è¼¸å‡º [u, v, p]
            velocity = u_pred[:, :2]  # u, v
            pressure = u_pred[:, 2:3]  # p
        elif u_pred.shape[1] == 4:
            # 4D è¼¸å‡º [u, v, w, p] - 2D æµå ´ wâ‰ˆ0
            velocity = u_pred[:, :2]  # u, v
            pressure = u_pred[:, 3:4]  # p (ç¬¬4å€‹è¼¸å‡º)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨™æº– PINN è¼¸å‡ºç¶­åº¦: {u_pred.shape[1]}ï¼ŒæœŸæœ› 3 æˆ– 4")
        predictions = None  # æ¨™æº– PINN ä¸éœ€è¦ predictions
    
    try:
        # æª¢æŸ¥æ˜¯å¦ç‚º VS-PINNï¼ˆæœ‰å°ˆç”¨çš„æ®˜å·®è¨ˆç®—æ–¹æ³•ï¼‰
        if is_vs_pinn and hasattr(physics, 'compute_momentum_residuals'):
            # VS-PINN 3D - è¨ˆç®—å‹•é‡å’Œé€£çºŒæ€§æ®˜å·®
            residuals_mom = physics.compute_momentum_residuals(
                coords_pde,
                predictions,
                scaled_coords=model_coords_pde
            )
            continuity_residual = physics.compute_continuity_residual(
                coords_pde,
                predictions,
                scaled_coords=model_coords_pde
            )
            
            # æå–æ®˜å·®ï¼ˆVS-PINN è¿”å›å­—å…¸ï¼‰
            residuals = {
                'momentum_x': residuals_mom['momentum_x'],
                'momentum_y': residuals_mom['momentum_y'],
                'momentum_z': residuals_mom['momentum_z'],
                'continuity': continuity_residual,
            }
        else:
            # æ¨™æº– NS 2D - ä½¿ç”¨åŸæœ‰æ¥å£
            residuals = physics.residual(coords_pde, velocity, pressure)
        
        # ğŸ”§ ç²å–é»æ¬Šé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        pde_point_weights = data_batch.get('pde_point_weights', None)
        
        # ğŸ”§ åˆ†é›¢å„ PDE é …ä»¥æ”¯æ´ç¨ç«‹æ¬Šé‡ï¼ˆæ‡‰ç”¨é»æ¬Šé‡ï¼‰
        if pde_point_weights is not None:
            # æ‡‰ç”¨é»æ¬Šé‡åˆ°æ¯å€‹æ®˜å·®é …
            momentum_x_loss = apply_point_weights_to_loss(residuals['momentum_x']**2, pde_point_weights)
            momentum_y_loss = apply_point_weights_to_loss(residuals['momentum_y']**2, pde_point_weights)
            momentum_z_loss = apply_point_weights_to_loss(residuals['momentum_z']**2, pde_point_weights) if is_vs_pinn else torch.tensor(0.0, device=device)
            continuity_loss = apply_point_weights_to_loss(residuals['continuity']**2, pde_point_weights)
        else:
            # æ¨™æº–å‡å€¼è¨ˆç®—
            momentum_x_loss = torch.mean(residuals['momentum_x']**2)
            momentum_y_loss = torch.mean(residuals['momentum_y']**2)
            momentum_z_loss = torch.mean(residuals['momentum_z']**2) if is_vs_pinn else torch.tensor(0.0, device=device)
            continuity_loss = torch.mean(residuals['continuity']**2)
    except Exception as e:
        # ğŸš¨ ç‰©ç†æ®˜å·®è¨ˆç®—å¤±æ•— - è©³ç´°éŒ¯èª¤æ—¥èªŒ
        logging.error("=" * 60)
        logging.error("ğŸš¨ Physics residual computation FAILED!")
        logging.error("=" * 60)
        logging.error(f"Exception: {e}")
        logging.error(f"coords_pde shape: {coords_pde.shape}")
        logging.error(f"u_pred shape: {u_pred.shape}")
        logging.error(f"velocity shape: {velocity.shape}")
        logging.error(f"pressure shape: {pressure.shape}")
        logging.error(f"is_vs_pinn: {is_vs_pinn}")
        logging.error("Traceback:")
        logging.error(traceback.format_exc())
        logging.error("=" * 60)
        
        # ğŸ”´ åœ¨é–‹ç™¼éšæ®µç›´æ¥æ‹‹å‡ºç•°å¸¸ï¼Œé¿å…éœé»˜å¤±æ•—
        if os.getenv('PINNS_DEV_MODE', 'true').lower() == 'true':
            raise
        
        # ç”Ÿç”¢ç’°å¢ƒå‚™ç”¨ï¼ˆæ‡‰è©²æ¥µå°‘è§¸ç™¼ï¼‰
        logging.warning("âš ï¸ Using fallback L2 regularization (PHYSICS CONSTRAINTS DISABLED!)")
        momentum_x_loss = torch.mean(u_pred**2) * 0.001
        momentum_y_loss = torch.mean(u_pred**2) * 0.001
        momentum_z_loss = torch.mean(u_pred**2) * 0.001 if is_vs_pinn else torch.tensor(0.0, device=device)
        continuity_loss = torch.mean(u_pred**2) * 0.001
    
    # é‚Šç•Œæ¢ä»¶æå¤± - æ”¯æŒ 2D/3D
    if is_vs_pinn:
        # VS-PINN 3D: ä½¿ç”¨ (x, y, z) åº§æ¨™
        coords_bc = torch.cat([data_batch['x_bc'], data_batch['y_bc'], data_batch['z_bc']], dim=1)
    else:
        # æ¨™æº– PINN 2D: åªä½¿ç”¨ (x, y) åº§æ¨™
        coords_bc = torch.cat([data_batch['x_bc'], data_batch['y_bc']], dim=1)
    model_coords_bc = prepare_model_coords(coords_bc, require_grad=False)
    u_bc_pred = model(model_coords_bc)
    
    # ğŸ”§ åˆ†é›¢å£é¢ç´„æŸå’Œé€±æœŸæ€§é‚Šç•Œ
    # å£é¢ç´„æŸï¼šu=v=0 at y=Â±1ï¼ˆæ¨™æº–åŒ–åº§æ¨™ï¼‰
    # æ¨™æº–åŒ–å¾Œçš„ y åº§æ¨™ç¯„åœç‚º [-1, 1]ï¼Œå£é¢ä¹Ÿåœ¨ y=Â±1
    y_bc = data_batch['y_bc']  # å½¢ç‹€ [N_bc, 1]
    
    # ğŸ”¥ æ˜ç¢ºè­˜åˆ¥å£é¢é»ï¼šå®¹å·®è¨­ç‚º 1e-3ï¼ˆæ¨™æº–åŒ–åº§æ¨™ï¼‰
    wall_mask = (torch.abs(y_bc - 1.0) < 1e-3) | (torch.abs(y_bc + 1.0) < 1e-3)
    wall_mask = wall_mask.squeeze()  # å½¢ç‹€ [N_bc]
    
    if wall_mask.sum() > 0:
        # åªåœ¨å£é¢é»æ‡‰ç”¨ç„¡æ»‘ç§»æ¢ä»¶
        u_wall = u_bc_pred[wall_mask, 0]  # u åˆ†é‡
        v_wall = u_bc_pred[wall_mask, 1]  # v åˆ†é‡
        wall_loss = torch.mean(u_wall**2 + v_wall**2)
    else:
        wall_loss = torch.tensor(0.0, device=device)
        if epoch == 0:
            logging.warning(f"âš ï¸ No wall boundary points detected! y_bc range: [{y_bc.min():.6f}, {y_bc.max():.6f}]")
    
    # é€±æœŸæ€§é‚Šç•Œï¼šu(x=0) = u(x=2Ï€)
    # å¾ PDE é»ä¸­æå–å·¦å³é‚Šç•Œï¼ˆæ¨™æº–åŒ–åº§æ¨™ x=-1 å’Œ x=+1ï¼‰
    x_pde_denorm = data_batch['x_pde']  # æ¨™æº–åŒ–åº§æ¨™ [-1, 1]
    left_mask = (x_pde_denorm < -0.95).squeeze()  # å·¦é‚Šç•Œé™„è¿‘
    right_mask = (x_pde_denorm > 0.95).squeeze()  # å³é‚Šç•Œé™„è¿‘
    
    if left_mask.sum() > 0 and right_mask.sum() > 0:
        # åœ¨é‚Šç•Œé»è©•ä¼°é€Ÿåº¦å ´
        coords_left = coords_pde[left_mask]
        coords_right = coords_pde[right_mask]
        model_left = prepare_model_coords(coords_left, require_grad=False)
        model_right = prepare_model_coords(coords_right, require_grad=False)
        u_left = model(model_left)
        u_right = model(model_right)
        
        # é€±æœŸæ€§æå¤±ï¼šå–æœ€å°æ•¸é‡å°é½Šï¼ˆä¿®å¾©é¡å‹éŒ¯èª¤ï¼‰
        n_min = min(left_mask.sum().item(), right_mask.sum().item())
        periodicity_loss = torch.mean((u_left[:n_min] - u_right[:n_min])**2)
    else:
        periodicity_loss = torch.tensor(0.0, device=device)
    
    # ğŸ†• Inlet é€Ÿåº¦å‰–é¢é‚Šç•Œæ¢ä»¶ï¼šæŒ‡å°æ¨¡å‹å­¸ç¿’ä¸»æµæ–¹å‘
    # å¾å…¨åŸŸé…ç½®ä¸­è®€å– inlet è¨­å®š
    inlet_enabled = False
    if config is not None and 'inlet' in config:
        inlet_config = config['inlet']
        inlet_enabled = inlet_config.get('enabled', False)
        n_inlet = inlet_config.get('n_points', 64)
        x_inlet_pos = inlet_config.get('x_position', -1.0)  # æ¨™æº–åŒ–åº§æ¨™ï¼Œé è¨­å·¦é‚Šç•Œ
        
        # ğŸš€ å¾èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ç²å–ç•¶å‰éšæ®µé…ç½®ï¼ˆå„ªå…ˆï¼‰
        stage_config = None
        if weighters.get('curriculum') is not None:
            stage_config = weighters['curriculum'].get_stage_config(epoch)
        
        # å¾éšæ®µç‰¹å®šé…ç½®è®€å–ï¼ˆå„ªå…ˆï¼‰ï¼Œå¦å‰‡ä½¿ç”¨å…¨åŸŸé…ç½®
        if stage_config is not None and 'inlet' in stage_config:
            stage_inlet = stage_config['inlet']
            profile_type = stage_inlet.get('profile_type', inlet_config.get('profile_type', 'log_law'))
            u_max = stage_inlet.get('u_max', None)  # éšæ®µç‰¹å®šçš„æœ€å¤§é€Ÿåº¦
        else:
            profile_type = inlet_config.get('profile_type', 'log_law')
            u_max = None
    else:
        # é è¨­é…ç½®ï¼ˆå¦‚æœæœªåœ¨ config ä¸­è¨­å®šï¼‰
        n_inlet = 64
        profile_type = 'log_law'
        x_inlet_pos = -1.0
        u_max = None
        stage_config = None
    
    if inlet_enabled:
        # ç”Ÿæˆ inlet é‚Šç•Œé»ï¼ˆx=x_inlet_pos, y âˆˆ [-1, 1]ï¼‰
        y_inlet = torch.linspace(-1.0, 1.0, n_inlet, device=device).unsqueeze(1)
        x_inlet_coords = torch.full_like(y_inlet, x_inlet_pos)
        
        # ğŸ†• å¦‚æœæ˜¯ VS-PINN 3Dï¼Œæ·»åŠ  z åº§æ¨™ï¼ˆä¸­é–“å€¼ï¼‰
        if is_vs_pinn:
            z_inlet = torch.zeros_like(y_inlet)  # z=0 (æ¨™æº–åŒ–åº§æ¨™ä¸­é–“å€¼)
            inlet_coords = torch.cat([x_inlet_coords, y_inlet, z_inlet], dim=1)
        else:
            inlet_coords = torch.cat([x_inlet_coords, y_inlet], dim=1)
        
        # æ¨¡å‹é æ¸¬ï¼ˆåœ¨ç¸®æ”¾ç©ºé–“è¨ˆç®—ï¼‰
        inlet_model_coords = prepare_model_coords(inlet_coords, require_grad=False)
        inlet_pred = model(inlet_model_coords)
        
        # å¾èª²ç¨‹éšæ®µè®€å–ç‰©ç†åƒæ•¸ï¼ˆå„ªå…ˆï¼‰ï¼Œå¦å‰‡ä½¿ç”¨å…¨åŸŸé…ç½®
        if stage_config is not None:
            Re_tau = stage_config.get('Re_tau', 1000.0)
            # å¦‚æœéšæ®µæ²’æœ‰è¨­å®š u_maxï¼Œä½¿ç”¨é è¨­å€¼
            if u_max is None:
                u_max = 16.5  # é è¨­å€¼
        elif config is not None and 'physics' in config:
            Re_tau = config['physics'].get('Re_tau', 1000.0)
            if u_max is None:
                u_max = config['physics'].get('u_bulk', 16.5)
        else:
            # é è¨­å€¼ï¼ˆèˆ‡èª²ç¨‹è¨“ç·´å°é½Šï¼‰
            Re_tau = 1000.0
            if u_max is None:
                u_max = 16.5
        
        # è¨ˆç®— inlet æå¤±
        # ğŸ”§ ç›´æ¥å¯¦ä¾‹åŒ–é‚Šç•Œæ¢ä»¶æå¤±æ¨¡çµ„ï¼ˆé¿å…é¡å‹æ¨æ–·å•é¡Œï¼‰
        bc_loss_module = BoundaryConditionLoss()
        
        inlet_loss = bc_loss_module.inlet_velocity_profile_loss(
            inlet_coords=inlet_coords,
            inlet_predictions=inlet_pred,
            profile_type=profile_type,
            Re_tau=Re_tau,
            u_max=u_max,
            y_range=(-1.0, 1.0)
        )
    else:
        inlet_loss = torch.tensor(0.0, device=device)
    
    # ğŸ†• åˆå§‹æ¢ä»¶æå¤±ï¼ˆt=0 æµå ´ç´„æŸï¼‰
    ic_config = config.get('initial_condition', {}) if config is not None else {}
    if ic_config.get('enabled', False) and len(data_batch.get('x_ic', [])) > 0:
        # çµ„åˆåˆå§‹æ¢ä»¶åº§æ¨™
        if is_vs_pinn:
            coords_ic = torch.cat([data_batch['x_ic'], data_batch['y_ic'], data_batch['z_ic']], dim=1)
        else:
            coords_ic = torch.cat([data_batch['x_ic'], data_batch['y_ic']], dim=1)
        
        # æ¨¡å‹åœ¨ t=0 çš„é æ¸¬
        ic_model_coords = prepare_model_coords(coords_ic, require_grad=False)
        ic_pred = model(ic_model_coords)
        
        # çœŸå¯¦åˆå§‹æ¢ä»¶
        u_ic_true = data_batch['u_ic']
        v_ic_true = data_batch['v_ic']
        p_ic_true = data_batch['p_ic']
        ic_true = torch.cat([u_ic_true, v_ic_true, p_ic_true], dim=1)
        
        # è¨ˆç®— IC æå¤±ï¼ˆMSEï¼‰
        from pinnx.losses.residuals import InitialConditionLoss
        ic_loss_module = InitialConditionLoss()
        ic_loss_result = ic_loss_module(
            initial_coords=coords_ic,
            initial_predictions=ic_pred,
            initial_data=ic_true,
            weights=None
        )
        ic_loss = ic_loss_result['total_ic']  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„éµå
    else:
        ic_loss = torch.tensor(0.0, device=device)
    
    # è³‡æ–™åŒ¹é…æå¤±
    # æ„Ÿæ¸¬é»è³‡æ–™æå¤± - æ”¯æŒ 2D/3D (åŒ…å« u, v, [w,] p å ´)
    if is_vs_pinn:
        # VS-PINN 3D: ä½¿ç”¨ (x, y, z) åº§æ¨™
        coords_sensors = torch.cat([data_batch['x_sensors'], data_batch['y_sensors'], data_batch['z_sensors']], dim=1)
    else:
        # æ¨™æº– PINN 2D: åªä½¿ç”¨ (x, y) åº§æ¨™
        coords_sensors = torch.cat([data_batch['x_sensors'], data_batch['y_sensors']], dim=1)
    model_coords_sensors = prepare_model_coords(coords_sensors, require_grad=False)
    u_sensors_pred = model(model_coords_sensors)
    
    # é€Ÿåº¦å ´æå¤± (u, v) - ä½¿ç”¨å®Œæ•´å ´çµ±è¨ˆæ¨™æº–åŒ–ä»¥å¹³è¡¡ä¸åŒå ´çš„å°ºåº¦
    u_true = data_batch['u_sensors']
    v_true = data_batch['v_sensors']
    p_true = data_batch['p_sensors']
    
    # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨å®Œæ•´ DNS å ´çµ±è¨ˆä½œç‚ºæ¨™æº–åŒ–å› å­ï¼ˆéæ„Ÿæ¸¬é»çµ±è¨ˆï¼‰
    # é€™äº›å€¼å¾ 128Ã—64 å®Œæ•´å ´æå–ï¼Œç¢ºä¿æ­£ç¢ºçš„å°ºåº¦å¹³è¡¡
    u_scale = 9.841839   # DNS å®Œæ•´å ´ U å‡å€¼
    v_scale = 0.188766   # DNS å®Œæ•´å ´ V æ¨™æº–å·®ï¼ˆv å‡å€¼æ¥è¿‘ 0ï¼‰
    p_scale = 35.655934  # DNS å®Œæ•´å ´ P å‡å€¼çµ•å°å€¼
    
    # æ¨™æº–åŒ–å¾Œçš„ MSEï¼ˆç›¸å°èª¤å·®ï¼‰
    u_loss = torch.mean(((u_sensors_pred[:, 0:1] - u_true) / u_scale)**2)
    v_loss = torch.mean(((u_sensors_pred[:, 1:2] - v_true) / v_scale)**2)
    pressure_loss = torch.mean(((u_sensors_pred[:, 2:3] - p_true) / p_scale)**2)
    
    # çµ„åˆè³‡æ–™æå¤± (å„å ´æ¬Šé‡å¹³è¡¡)
    velocity_loss = u_loss + v_loss
    data_loss = velocity_loss + pressure_loss  # ç¾åœ¨å„å ´çš„å°ºåº¦å·²æ­£ç¢ºå¹³è¡¡
    
    # å…ˆé©—ä¸€è‡´æ€§æå¤± (èˆ‡ä½ä¿çœŸå ´çš„è»Ÿç´„æŸ)
    if data_batch.get('has_prior', False):
        # åœ¨æ„Ÿæ¸¬é»ä½ç½®è¨ˆç®— PINNs é æ¸¬ï¼ˆå·²åœ¨ä¸Šæ–¹è¨ˆç®—ï¼Œé‡è¤‡ä½¿ç”¨ï¼‰
        # æ³¨æ„ï¼šx_sensors å·²ç¶“åœ¨ä¸Šæ–¹æ ¹æ“š is_vs_pinn æ­£ç¢ºæ‹¼æ¥
        
        # è¨ˆç®—èˆ‡ä½ä¿çœŸå…ˆé©—çš„å·®ç•°
        prior_loss = torch.tensor(0.0, device=device)
        
        if 'u_prior' in data_batch:
            prior_loss += torch.mean((u_sensors_pred[:, 0:1] - data_batch['u_prior'])**2)
        if 'v_prior' in data_batch:
            prior_loss += torch.mean((u_sensors_pred[:, 1:2] - data_batch['v_prior'])**2)
        if 'p_prior' in data_batch:
            prior_loss += torch.mean((u_sensors_pred[:, 2:3] - data_batch['p_prior'])**2)
    else:
        # æ²’æœ‰å…ˆé©—è³‡æ–™æ™‚ï¼Œä½¿ç”¨é›¶æå¤±ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        prior_loss = torch.tensor(0.0, device=device)
    
    # ğŸ”§ æ–°å¢ç‰©ç†ç´„æŸæå¤±ï¼ˆTask-10ï¼‰
    # 1. æµé‡ç´„æŸï¼ˆé«”ç©æµé‡å®ˆæ†ï¼‰
    bulk_velocity_loss = torch.tensor(0.0, device=device)
    if is_vs_pinn and hasattr(physics, 'compute_bulk_velocity_constraint'):
        try:
            bulk_velocity_loss = physics.compute_bulk_velocity_constraint(coords_pde, predictions)
        except Exception as e:
            logging.debug(f"Bulk velocity constraint failed: {e}")
    
    # 2. ä¸­å¿ƒç·šå°ç¨±ç´„æŸ
    centerline_dudy_loss = torch.tensor(0.0, device=device)
    centerline_v_loss = torch.tensor(0.0, device=device)
    if is_vs_pinn and hasattr(physics, 'compute_centerline_symmetry'):
        try:
            centerline_losses = physics.compute_centerline_symmetry(
                coords_pde,
                predictions,
                scaled_coords=model_coords_pde
            )
            centerline_dudy_loss = centerline_losses['centerline_dudy']
            centerline_v_loss = centerline_losses['centerline_v']
        except Exception as e:
            logging.debug(f"Centerline symmetry constraint failed: {e}")
    
    # 3. å£“åŠ›åƒè€ƒé»ç´„æŸï¼ˆå¯é¸ï¼‰
    pressure_reference_loss = torch.tensor(0.0, device=device)
    if is_vs_pinn and hasattr(physics, 'compute_pressure_reference'):
        try:
            pressure_reference_loss = physics.compute_pressure_reference(coords_pde, predictions)
        except Exception as e:
            logging.debug(f"Pressure reference constraint failed: {e}")
    
    # ğŸ”§ æ”¶é›†æ‰€æœ‰æå¤±åˆ°å­—å…¸ä¸­ï¼ˆ12 å€‹ç¨ç«‹é … + priorï¼ŒåŒ…å« Task-10 æ–°å¢é …ï¼‰
    loss_dict = {
        'data': data_loss,
        'momentum_x': momentum_x_loss,
        'momentum_y': momentum_y_loss,
        'momentum_z': momentum_z_loss,
        'continuity': continuity_loss,
        'wall_constraint': wall_loss,
        'periodicity': periodicity_loss,
        'inlet': inlet_loss,  # ğŸ†• Inlet é€Ÿåº¦å‰–é¢æå¤±
        'initial_condition': ic_loss,  # ğŸ”¥ Initial condition æå¤±
        'bulk_velocity': bulk_velocity_loss,  # ğŸ†• Task-10: æµé‡ç´„æŸ
        'centerline_dudy': centerline_dudy_loss,  # ğŸ†• Task-10: ä¸­å¿ƒç·šå°ç¨±ï¼ˆæ¢¯åº¦ï¼‰
        'centerline_v': centerline_v_loss,  # ğŸ†• Task-10: ä¸­å¿ƒç·šå°ç¨±ï¼ˆé€Ÿåº¦ï¼‰
        'pressure_reference': pressure_reference_loss,  # ğŸ†• Task-10: å£“åŠ›åƒè€ƒé»
        'prior': prior_loss
    }
    
    # ğŸ†•ğŸ†•ğŸ†• æå¤±æ­¸ä¸€åŒ–ï¼ˆVS-PINN åŠŸèƒ½ï¼‰
    # åœ¨æ‡‰ç”¨æ¬Šé‡å‰ï¼Œå…ˆå°‡æ‰€æœ‰æå¤±æ­¸ä¸€åŒ–åˆ°åŒä¸€æ•¸é‡ç´š
    if hasattr(physics, 'normalize_loss_dict'):
        # è¨˜éŒ„åŸå§‹æå¤±ï¼ˆåƒ…åœ¨å‰ 5 å€‹ epochï¼‰
        if epoch < 5 and epoch == 0:
            logging.info(f"[NORMALIZATION] Raw losses at epoch {epoch}:")
            for key, loss in loss_dict.items():
                logging.info(f"  {key}: {loss.item():.6f}")
        
        # åŸ·è¡Œæ­¸ä¸€åŒ–
        loss_dict = physics.normalize_loss_dict(loss_dict, epoch)
        
        # è¨˜éŒ„æ­¸ä¸€åŒ–å¾Œçš„æå¤±ï¼ˆåƒ…åœ¨é–‹å§‹æ­¸ä¸€åŒ–å¾Œçš„å‰å¹¾å€‹ epochï¼‰
        if epoch >= physics.warmup_epochs and epoch < physics.warmup_epochs + 3:
            logging.info(f"[NORMALIZATION] Normalized losses at epoch {epoch}:")
            for key, loss in loss_dict.items():
                logging.info(f"  {key}: {loss.item():.6f}")
            
            # é¡¯ç¤ºæ­¸ä¸€åŒ–åƒè€ƒå€¼
            norm_info = physics.get_normalization_info()
            logging.info(f"[NORMALIZATION] Reference values (normalizers):")
            for key, val in norm_info['normalizers'].items():
                logging.info(f"  {key}: {val:.6f}")
    
    # æ‡‰ç”¨å‹•æ…‹æ¬Šé‡
    # åˆå§‹æ¬Šé‡ï¼ˆå¾é…ç½®æ–‡ä»¶è®€å–æˆ–ä½¿ç”¨é è¨­ï¼‰
    prior_weight_from_config = getattr(losses['prior'], 'consistency_weight', 0.3)

    base_weights, adaptive_terms = derive_loss_weights(loss_cfg, prior_weight_from_config, is_vs_pinn)
    if epoch == 0 and loss_cfg:
        logging.info("[CONFIG] Using weights from config file:")
        for key, val in base_weights.items():
            logging.info(f"  {key}_weight: {val:.6f}")
    elif epoch == 0 and not loss_cfg:
        logging.warning("[FALLBACK] No loss config provided, using default hardcoded weights")

    configured_terms = loss_cfg.get('adaptive_loss_terms') if isinstance(loss_cfg, dict) else None
    if configured_terms is not None:
        adaptive_terms = [name for name in configured_terms if name in base_weights]
    
    # ğŸ”§ DEBUG: è¨˜éŒ„è®€å–çš„ prior_weightï¼ˆä¿ç•™åŸæœ‰é‚è¼¯ï¼‰
    if epoch == 0:
        logging.info(f"[DEBUG] Prior weight from config: {prior_weight_from_config}")
    
    weights = base_weights.copy()
    
    # ğŸš€ğŸš€ğŸš€ èª²ç¨‹è¨“ç·´èª¿åº¦å™¨ï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
    if weighters.get('curriculum') is not None:
        stage_config = weighters['curriculum'].get_stage_config(epoch)
        
        # éšæ®µåˆ‡æ›æ™‚è¨˜éŒ„ï¼ˆå·²åœ¨ CurriculumScheduler å…§éƒ¨è™•ç†ï¼‰
        # æ‡‰ç”¨èª²ç¨‹æ¬Šé‡ï¼ˆå®Œå…¨è¦†è“‹ base_weightsï¼‰
        weights.update(stage_config['weights'])
        
        # ğŸ”¥ è¿”å›é¡å¤–çš„é…ç½®è³‡è¨Šä¾›è¨“ç·´è¿´åœˆä½¿ç”¨
        # æ³¨æ„ï¼šé€™è£¡åªæ›´æ–°æ¬Šé‡ï¼Œç‰©ç†åƒæ•¸å’Œå­¸ç¿’ç‡éœ€è¦åœ¨è¨“ç·´è¿´åœˆä¸­è™•ç†
        curriculum_info = {
            'stage_name': stage_config['stage_name'],
            'is_transition': stage_config['is_transition'],
            'sampling': stage_config['sampling'],
            'lr': stage_config['lr']
        }
    else:
        curriculum_info = None
    
    # ğŸš€ğŸš€ğŸš€ éšæ®µå¼æ¬Šé‡èª¿åº¦å™¨ï¼ˆå„ªå…ˆç´šç¬¬äºŒï¼Œèˆ‡èª²ç¨‹è¨“ç·´äº’æ–¥ï¼‰
    if weighters.get('staged') is not None and weighters.get('curriculum') is None:
        staged_weights, phase_name, is_transition = weighters['staged'].get_phase_weights(epoch)
        
        # ğŸ”§ éšæ®µåˆ‡æ›æ™‚è¨˜éŒ„
        if is_transition:
            logging.info(f"=" * 80)
            logging.info(f"ğŸ¯ PHASE TRANSITION at Epoch {epoch}: {phase_name}")
            logging.info(f"   New weights: {staged_weights}")
            logging.info(f"=" * 80)
        
        # æ‡‰ç”¨éšæ®µå¼æ¬Šé‡ï¼ˆè¦†è“‹ base_weightsï¼‰
        weights.update(staged_weights)
    
    # ä½¿ç”¨å‹•æ…‹æ¬Šé‡å™¨ (å¦‚æœå·²å•Ÿç”¨ä¸”æ²’æœ‰éšæ®µå¼èª¿åº¦å™¨)
    if weighters['gradnorm'] is not None and weighters.get('staged') is None:
        # GradNorm æœƒæ ¹æ“šå„æå¤±é …çš„æ¢¯åº¦å‹•æ…‹èª¿æ•´æ¬Šé‡
        try:
            updated_weights = weighters['gradnorm'].update_weights(loss_dict)
            if updated_weights is not None:
                adaptive_base_sum = sum(base_weights.get(name, 0.0) for name in adaptive_terms)
                total_updated = sum(updated_weights.values()) if updated_weights else 1.0
                for key in base_weights:
                    if key in adaptive_terms and key in updated_weights:
                        normalized_factor = updated_weights[key] / (total_updated + 1e-12)
                        weights[key] = adaptive_base_sum * normalized_factor
                    elif key not in adaptive_terms:
                        weights[key] = base_weights[key]
        except Exception as e:
            logging.warning(f"GradNorm update failed: {e}, using base weights")
    
    # ä½¿ç”¨æ¬Šé‡èª¿åº¦å™¨ (éšæ®µæ€§èª¿æ•´)
    if weighters['scheduler'] is not None:
        try:
            if hasattr(weighters['scheduler'], 'get_phase_weights'):
                phase_weights = weighters['scheduler'].get_phase_weights(epoch, 15000)
                # èˆ‡åŸºç¤æ¬Šé‡ç›¸ä¹˜
                for key in weights:
                    if key in phase_weights:
                        weights[key] *= phase_weights[key]
            elif hasattr(weighters['scheduler'], 'get_weights'):
                scheduled_weights = weighters['scheduler'].get_weights()
                for key in weights:
                    if key in scheduled_weights:
                        weights[key] *= scheduled_weights[key]
        except Exception as e:
            logging.warning(f"Scheduler update failed: {e}, using current weights")
    
    # å› æœæ¬Šé‡å™¨ (æ™‚é–“åºåˆ—å•é¡Œæ‰éœ€è¦ï¼Œ2D ç©©æ…‹è·³é)
    if weighters['causal'] is not None and hasattr(weighters['causal'], 'compute_causal_weights'):
        try:
            time_losses = [loss_dict.get('data', torch.tensor(0.0)), 
                          loss_dict.get('residual', torch.tensor(0.0))]
            causal_weights_list = weighters['causal'].compute_causal_weights(time_losses)
            causal_weights = dict(zip(['data', 'residual'], causal_weights_list[:2]))
            
            for key in weights:
                if key in causal_weights:
                    weights[key] *= causal_weights[key]
        except Exception as e:
            logging.warning(f"Causal weighting failed: {e}, skipping")
    
    # ğŸ”§ å¼·åˆ¶è¦†è“‹ï¼šå¦‚æœé…ç½®ä¸­ prior_weight=0ï¼Œç¢ºä¿æ¬Šé‡è¢«å®Œå…¨ç¦ç”¨
    if base_weights.get('prior', 0.0) == 0.0 and 'prior' in weights:
        weights['prior'] = 0.0
        if epoch == 0:
            logging.info("[FIX] Prior weight forced to 0.0 (config: prior_weight=0.0)")
    
    # è¨ˆç®—åŠ æ¬Šç¸½æå¤± - ä½¿ç”¨ç¬¬ä¸€å€‹æå¤±é …åˆå§‹åŒ–ï¼ˆä¿ç•™è¨ˆç®—åœ–ï¼‰
    loss_keys = list(loss_dict.keys())
    if len(loss_keys) > 0:
        # ğŸ” é™¤éŒ¯ï¼šæª¢æŸ¥æ¯å€‹æå¤±é …å’Œæ¬Šé‡
        if epoch == 0:
            logging.info(f"[DEBUG] Loss dict at epoch 0:")
            for key in loss_keys:
                loss_val = loss_dict[key]
                weight_val = weights.get(key, 1.0)
                logging.info(f"  {key}: loss={loss_val.item():.6f}, weight={weight_val}, "
                           f"has_nan={torch.isnan(loss_val).any()}, has_inf={torch.isinf(loss_val).any()}")
        
        # ä½¿ç”¨ç¬¬ä¸€å€‹æå¤±é …åˆå§‹åŒ–ï¼ˆå¸¶è¨ˆç®—åœ–ï¼‰
        first_key = loss_keys[0]
        weight_0 = weights.get(first_key, 1.0)
        weight_tensor_0 = torch.tensor(weight_0, device=device, dtype=torch.float32) if not torch.is_tensor(weight_0) else weight_0.to(dtype=torch.float32)
        total_loss = weight_tensor_0 * loss_dict[first_key]
        
        if epoch == 0:
            logging.info(f"[DEBUG] Initial total_loss from '{first_key}': {total_loss.item():.6f}, has_nan={torch.isnan(total_loss).any()}")
        
        # ç´¯åŠ å…¶é¤˜æå¤±é …
        for key in loss_keys[1:]:
            weight = weights.get(key, 1.0)
            weight_tensor = torch.tensor(weight, device=device, dtype=torch.float32) if not torch.is_tensor(weight) else weight.to(dtype=torch.float32)
            weighted_term = weight_tensor * loss_dict[key]
            
            if epoch == 0:
                logging.info(f"[DEBUG] Adding '{key}': weighted_term={weighted_term.item():.6f}, has_nan={torch.isnan(weighted_term).any()}")
            
            total_loss = total_loss + weighted_term
            
            if epoch == 0:
                logging.info(f"[DEBUG] Cumulative total_loss after '{key}': {total_loss.item() if not torch.isnan(total_loss).any() else 'NaN'}")
    else:
        # å‚™ç”¨ï¼šå¦‚æœæ²’æœ‰æå¤±é …ï¼Œå‰µå»ºé›¶æå¤±
        total_loss = torch.tensor(0.0, device=device, dtype=torch.float32, requires_grad=True)
    
    total_loss.backward()
    optimizer.step()
    
    # ğŸ”§ æº–å‚™è¿”å›å€¼ï¼ˆåŒ…å«æ‰€æœ‰ç¨ç«‹æå¤±é …ï¼‰
    result = {
        'total_loss': float(total_loss.item()),
        'momentum_x_loss': float(momentum_x_loss.item()),
        'momentum_y_loss': float(momentum_y_loss.item()),
        'momentum_z_loss': float(momentum_z_loss.item()),
        'continuity_loss': float(continuity_loss.item()),
        'wall_loss': float(wall_loss.item()),
        'periodicity_loss': float(periodicity_loss.item()),
        'inlet_loss': float(inlet_loss.item()),  # ğŸ†• Inlet æå¤±
        'data_loss': float(data_loss.item()),
        'prior_loss': float(prior_loss.item()),
        # å‘å¾Œå…¼å®¹ï¼ˆèšåˆæå¤±ï¼‰
        'residual_loss': float(momentum_x_loss.item() + momentum_y_loss.item() + momentum_z_loss.item() + continuity_loss.item()),
        'bc_loss': float(wall_loss.item() + inlet_loss.item())  # ğŸ”§ åŒ…å« inlet æå¤±
    }
    
    # æ·»åŠ æ¬Šé‡è³‡è¨Š
    for k, v in weights.items():
        result[f'weight_{k}'] = float(v.item() if torch.is_tensor(v) else v)
    
    # ğŸ”§ ç›£æ§æ¬Šé‡ç¸½å’Œï¼ˆé©—è­‰æ¨™æº–åŒ–ï¼‰
    adaptive_weight_sum = sum(float(weights.get(name, 0.0)) for name in adaptive_terms if name in weights)
    fixed_weight_sum = sum(float(weights.get(name, 0.0)) for name in weights if name not in adaptive_terms)
    result['weight_sum_adaptive'] = adaptive_weight_sum
    result['weight_sum_fixed'] = fixed_weight_sum
    
    # ğŸš€ æ·»åŠ èª²ç¨‹è¨“ç·´è³‡è¨Šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰- ä½¿ç”¨ç‰¹æ®Šå‰ç¶´é¿å…é¡å‹è¡çª
    if curriculum_info is not None:
        result['_curriculum_stage'] = curriculum_info['stage_name']  # type: ignore
        result['_curriculum_transition'] = 1.0 if curriculum_info['is_transition'] else 0.0
        result['_curriculum_lr'] = curriculum_info['lr']
    
    return result


def compute_validation_metrics(
    model: nn.Module,
    validation_info: Optional[Dict[str, torch.Tensor]],
    device: torch.device,
    physics: Optional[Any] = None,
    input_normalizer: Optional[InputNormalizer] = None
) -> Optional[Dict[str, float]]:
    """
    è¨ˆç®—é©—è­‰è³‡æ–™ä¸Šçš„èª¤å·®æŒ‡æ¨™ï¼ˆç›¸å° L2 èˆ‡ MSEï¼‰ã€‚
    """
    if not validation_info or validation_info.get('size', 0) == 0:
        return None
    
    coords = validation_info.get('coords')
    targets = validation_info.get('targets')
    
    if coords is None or targets is None or coords.numel() == 0 or targets.numel() == 0:
        return None
    
    coords = coords.to(device)
    targets = targets.to(device)
    
    training_mode = model.training
    model.eval()
    
    with torch.no_grad():
        coords_for_model = coords
        if input_normalizer is not None:
            coords_for_model = input_normalizer.transform(coords_for_model)
        if physics is not None and hasattr(physics, 'scale_coordinates'):
            coords_for_model = physics.scale_coordinates(coords)
        preds = model(coords_for_model)
        # åƒ…æ¯”è¼ƒå¯ç”¨çš„å ´åˆ†é‡
        n_pred = preds.shape[1]
        n_targets = targets.shape[1]
        n_common = min(n_pred, n_targets)
        if n_pred != n_targets:
            logging.debug(
                f"[Validation] Output dimension mismatch (pred={n_pred}, target={n_targets}); "
                f"comparing first {n_common} components."
            )
        preds = preds[:, :n_common]
        targets = targets[:, :n_common]
        diff = preds - targets
        mse = torch.mean(diff**2).item()
        rel_l2 = relative_L2(preds, targets).mean().item()
    
    if training_mode:
        model.train()
    
    return {
        'mse': mse,
        'relative_l2': rel_l2
    }


def train_model(model: nn.Module,
                physics: NSEquations2D,
                losses: Dict[str, nn.Module],
                config: Dict[str, Any],
                device: torch.device) -> Dict[str, Any]:
    """ä¸»è¦è¨“ç·´è¿´åœˆ"""
    train_cfg = config['training']
    loss_cfg = config.get('losses', {})
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_cfg = physics_type == 'vs_pinn_channel_flow'
    
    # å»ºç«‹å„ªåŒ–å™¨
    optimizer_cfg = train_cfg.get('optimizer', {})
    lr = optimizer_cfg.get('lr', train_cfg.get('lr', 1e-3))
    weight_decay = optimizer_cfg.get('weight_decay', train_cfg.get('weight_decay', 0.0))
    
    optimizer_name = optimizer_cfg.get('type', train_cfg.get('optimizer', 'adam')).lower()
    if optimizer_name == 'soap':
        try:
            from torch_optimizer import SOAP
        except ImportError as exc:
            raise ImportError("Requested optimizer 'SOAP' but torch_optimizer is not installed") from exc
        soap_kwargs = optimizer_cfg.get('soap', train_cfg.get('soap', {}))
        optimizer = SOAP(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            **soap_kwargs
        )
        logging.info("âœ… Using SOAP optimizer")
    else:
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        logging.info("âœ… Using Adam optimizer")
    
    # å­¸ç¿’ç‡èª¿åº¦å™¨
    scheduler_cfg = train_cfg.get('scheduler', {})
    scheduler_type = scheduler_cfg.get('type', train_cfg.get('lr_scheduler', 'none'))
    max_epochs = train_cfg.get('epochs', train_cfg.get('max_epochs', 1000))
    
    if scheduler_type == 'warmup_cosine':
        # Warmup + CosineAnnealing èª¿åº¦å™¨
        warmup_epochs = scheduler_cfg.get('warmup_epochs', 10)
        min_lr = scheduler_cfg.get('min_lr', 1e-6)
        scheduler = WarmupCosineScheduler(
            optimizer, 
            warmup_epochs=warmup_epochs,
            max_epochs=max_epochs,
            base_lr=lr,
            min_lr=min_lr
        )
    elif scheduler_type == 'cosine_warm_restarts':
        # CosineAnnealingWarmRestarts èª¿åº¦å™¨ï¼ˆé€±æœŸæ€§é‡å•Ÿï¼‰
        T_0 = scheduler_cfg.get('T_0', 100)  # ç¬¬ä¸€å€‹é€±æœŸé•·åº¦
        T_mult = scheduler_cfg.get('T_mult', 1)  # é€±æœŸé•·åº¦å€å¢å› å­
        eta_min = scheduler_cfg.get('eta_min', 1e-6)  # æœ€å°å­¸ç¿’ç‡
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=T_0,
            T_mult=T_mult,
            eta_min=eta_min
        )
    elif scheduler_type == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=max_epochs
        )
    elif scheduler_type == 'exponential':
        gamma = scheduler_cfg.get('gamma', 0.999)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
    else:
        scheduler = None
    
    
    # å»ºç«‹æ¬Šé‡å™¨ï¼ˆå‚³å…¥ physics ä»¥æ”¯æ´èª²ç¨‹è¨“ç·´ï¼‰- éœ€è¦åœ¨ early stopping ä¹‹å‰åˆå§‹åŒ–
    weighters = create_weighters(config, model, device, physics)

    # ğŸš€ Early Stopping è¨­å®š
    early_stopping_cfg = train_cfg.get('early_stopping', {})
    adaptive_terms = loss_cfg.get('adaptive_loss_terms')
    if adaptive_terms is None:
        adaptive_terms = [
            'data',
            'momentum_x',
            'momentum_y',
            'momentum_z',
            'continuity',
            'wall_constraint',
            'periodicity',
            'inlet',
            'bulk_velocity',
            'centerline_dudy',
            'centerline_v',
            'pressure_reference',
            'initial_condition',
        ]
        if loss_cfg.get('prior_weight', 0.0) > 0.0:
            adaptive_terms.append('prior')

    if loss_cfg.get('adaptive_weighting', False) and weighters['staged'] is None:
        early_stopping_enabled = early_stopping_cfg.get('enabled', False)
        patience = early_stopping_cfg.get('patience', 400)
        min_delta = early_stopping_cfg.get('min_delta', 0.001)
        metric_name = early_stopping_cfg.get('monitor', 'conservation_error')
        restore_best = early_stopping_cfg.get('restore_best_weights', True)
    else:
        # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯å¸ƒå°”å€¼
        early_stopping_enabled = bool(early_stopping_cfg)
        patience = train_cfg.get('patience', 400)
        min_delta = train_cfg.get('min_delta', 0.001)
        metric_name = train_cfg.get('early_stopping_metric', 'conservation_error')
        restore_best = train_cfg.get('restore_best_weights', True)
    
    best_metric = float('inf')
    best_epoch = 0
    best_model_state = None
    patience_counter = 0
    
    if early_stopping_enabled:
        logging.info(f"âœ… Early stopping enabled: metric={metric_name}, patience={patience}, min_delta={min_delta}")
    else:
        logging.info("âš ï¸  Early stopping disabled")
    
    # æº–å‚™è¨“ç·´è³‡æ–™
    training_data = prepare_training_data(config, device)
    input_normalizer = create_input_normalizer(config, training_data, is_vs_cfg, device)
    
    validation_freq = train_cfg.get('validation_freq', train_cfg.get('checkpoint_interval', 1000))
    if validation_freq <= 0:
        validation_freq = 1
    last_val_metrics: Optional[Dict[str, float]] = None
    
    # è¨“ç·´å¾ªç’°
    logger = logging.getLogger(__name__)
    
    # ğŸ”§ åˆå§‹åŒ–è‡ªé©æ‡‰æ¡æ¨£ç®¡ç†å™¨
    loop_manager = None
    adaptive_cfg = config.get('adaptive_collocation', {})
    if adaptive_cfg.get('enabled', False):
        try:
            loop_manager = TrainingLoopManager(config)
            # åˆå§‹åŒ– PDE é»åˆ°ç®¡ç†å™¨
            if 'x_pde' in training_data and 'y_pde' in training_data:
                # åˆä½µ x, y æˆä¸€å€‹å¼µé‡ [N, 2]
                pde_points = torch.cat([training_data['x_pde'], training_data['y_pde']], dim=1)
                loop_manager.setup_initial_points(pde_points)
                n_pde = len(loop_manager.current_pde_points) if loop_manager.current_pde_points is not None else 0
                logger.info(f"âœ… Adaptive collocation enabled: {n_pde} initial PDE points")
            else:
                logger.warning("âš ï¸ No PDE points found, adaptive collocation disabled")
                loop_manager = None
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize adaptive collocation: {e}")
            loop_manager = None
    
    start_time = time.time()
    loss_dict = {'total_loss': 0.0, 'residual_loss': 0.0, 'bc_loss': 0.0, 'data_loss': 0.0}
    epoch = -1  # åˆå§‹åŒ– epoch è®Šæ•¸
    max_epochs = train_cfg.get('epochs', train_cfg.get('max_epochs', 1000))
    
    for epoch in range(max_epochs):
        # ğŸ”§ è‡ªé©æ‡‰æ®˜å·®é»æ¡æ¨£
        if loop_manager is not None:
            # 1. æ›´æ–°è¨“ç·´æ‰¹æ¬¡ï¼ˆæ‡‰ç”¨æ–°é»èˆ‡æ¬Šé‡è¡°æ¸›ï¼‰
            training_data = loop_manager.update_training_batch(training_data, epoch)
            
            # 2. æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ¡æ¨£
            if loop_manager.should_resample_collocation_points(
                epoch, 
                loss_dict.get('total_loss', float('inf')), 
                loss_dict
            ):
                # æå–åŸŸé‚Šç•Œ
                domain_bounds = {
                    'x': (config['domain']['x_min'], config['domain']['x_max']),
                    'y': (config['domain']['y_min'], config['domain']['y_max'])
                }
                if 'z_min' in config['domain']:
                    domain_bounds['z'] = (config['domain']['z_min'], config['domain']['z_max'])
                if 't_min' in config['domain']:
                    domain_bounds['t'] = (config['domain']['t_min'], config['domain']['t_max'])
                
                try:
                    new_points, metrics = loop_manager.resample_collocation_points(
                        model, physics, domain_bounds, epoch, device
                    )
                    logger.info(f"ğŸ”„ Resampled {len(new_points)} collocation points at epoch {epoch}")
                    logger.info(f"   Metrics: {metrics}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Resampling failed at epoch {epoch}: {e}")
        
        # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
        loss_dict = train_step(
            model, physics, losses, training_data, optimizer, weighters, epoch, device, config, input_normalizer
        )
        
        val_info = training_data.get('validation')
        if val_info and val_info.get('size', 0) > 0:
            if epoch % validation_freq == 0:
                validation_metrics = compute_validation_metrics(
                    model,
                    val_info,
                    device,
                    physics,
                    input_normalizer
                )
                if validation_metrics is not None:
                    last_val_metrics = validation_metrics
            if last_val_metrics is not None:
                loss_dict['val_loss'] = last_val_metrics['relative_l2']
                loss_dict['val_mse'] = last_val_metrics['mse']
        
        # ğŸš€ğŸš€ğŸš€ èª²ç¨‹è¨“ç·´ï¼šè™•ç†éšæ®µåˆ‡æ›
        if '_curriculum_transition' in loss_dict and loss_dict['_curriculum_transition'] > 0.5:
            # éšæ®µåˆ‡æ›æ™‚æ›´æ–°å­¸ç¿’ç‡
            new_lr = loss_dict.get('_curriculum_lr', train_cfg['lr'])
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lr
            logger.info(f"ğŸ“‰ Curriculum: Updated learning rate to {new_lr:.6f}")
            
            # ä¿å­˜éšæ®µæª¢æŸ¥é»ï¼ˆå¦‚æœé…ç½®å•Ÿç”¨ï¼‰
            if config.get('logging', {}).get('save_stage_checkpoints', False):
                stage_name = loss_dict.get('_curriculum_stage', f'stage_{epoch}')
                checkpoint_dir = config.get('logging', {}).get('stage_checkpoint_dir', './checkpoints/curriculum')
                os.makedirs(checkpoint_dir, exist_ok=True)
                checkpoint_path = os.path.join(checkpoint_dir, f"{stage_name}_epoch{epoch}.pth")
                
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': loss_dict['total_loss'],
                    'config': config,
                    'stage_name': stage_name
                }
                torch.save(checkpoint, checkpoint_path)
                logger.info(f"ğŸ’¾ Stage checkpoint saved: {checkpoint_path}")
        
        # æ›´æ–°å­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆåƒ…åœ¨éèª²ç¨‹è¨“ç·´æ¨¡å¼ï¼‰
        if scheduler and weighters.get('curriculum') is None:
            scheduler.step()
        
        # æ—¥èªŒè¼¸å‡º
        log_freq = config.get('training', {}).get('log_interval', config.get('logging', {}).get('log_freq', 50))
        if epoch % log_freq == 0:
            elapsed = time.time() - start_time
            log_msg = (
                f"Epoch {epoch:6d} | "
                f"Total: {loss_dict['total_loss']:.6f} | "
                f"Residual: {loss_dict['residual_loss']:.6f} | "
                f"BC: {loss_dict['bc_loss']:.6f} | "
                f"Data: {loss_dict['data_loss']:.6f} | "
                f"Time: {elapsed:.1f}s"
            )
            if 'val_loss' in loss_dict:
                log_msg += f" | Val(relL2): {loss_dict['val_loss']:.6f}"
            if 'val_mse' in loss_dict:
                log_msg += f" | Val(MSE): {loss_dict['val_mse']:.6f}"
            
            # æ·»åŠ æ¬Šé‡è³‡è¨Šåˆ°æ—¥èªŒä¸­ (å¦‚æœæœ‰çš„è©±)
            if 'weight_data' in loss_dict:
                log_msg += f" | W_data: {loss_dict['weight_data']:.3f}"
            if 'weight_residual' in loss_dict:
                log_msg += f" | W_residual: {loss_dict['weight_residual']:.3f}"
            if 'weight_boundary' in loss_dict:
                log_msg += f" | W_boundary: {loss_dict['weight_boundary']:.3f}"
            
            # ğŸ”§ ç›£æ§æ¬Šé‡ç¸½å’Œï¼ˆé©—è­‰æ¨™æº–åŒ–ï¼‰
            if 'weight_sum' in loss_dict:
                log_msg += f" | Î£W: {loss_dict['weight_sum']:.3f}"
            
            # ğŸ”§ è‡ªé©æ‡‰æ¡æ¨£çµ±è¨ˆ
            if loop_manager is not None:
                stats = loop_manager.get_summary()
                if stats.get('total_resamples', 0) > 0:
                    log_msg += f" | Resamples: {stats['total_resamples']}"
                    if 'new_points_count' in stats:
                        log_msg += f" | NewPts: {stats['new_points_count']}"
                
            logger.info(log_msg)
        
        # ğŸ”§ æ”¶é›†è‡ªé©æ‡‰æ¡æ¨£çµ±è¨ˆ
        if loop_manager is not None:
            loop_manager.collect_epoch_stats(epoch, loss_dict)
        
        # æª¢æŸ¥é»ä¿å­˜
        if epoch % train_cfg.get('checkpoint_freq', 2000) == 0 and epoch > 0:
            save_checkpoint(model, optimizer, epoch, loss_dict['total_loss'], config)
            logger.info(f"Checkpoint saved at epoch {epoch}")
        
        # ğŸš€ Early Stopping æª¢æŸ¥
        if early_stopping_enabled:
            # ğŸ”§ æ ¹æ“šé…ç½®é¸æ“‡ç›£æ§æŒ‡æ¨™
            if metric_name == 'val_loss':
                if 'val_loss' in loss_dict:
                    current_metric = loss_dict['val_loss']
                else:
                    current_metric = loss_dict['total_loss']
                    if epoch == 0:
                        logger.warning("[Early Stopping] 'val_loss' requested but validation data unavailable; falling back to 'total_loss'")
            elif metric_name == 'total_loss':
                current_metric = loss_dict['total_loss']
            elif metric_name == 'conservation_error' or metric_name == 'continuity_loss':
                current_metric = loss_dict['continuity_loss']
            elif metric_name in loss_dict:
                # å˜—è©¦å¾ loss_dict ç›´æ¥è®€å–
                current_metric = loss_dict[metric_name]
            else:
                # å‚™ç”¨ï¼šä½¿ç”¨ total_loss
                current_metric = loss_dict['total_loss']
                if epoch == 0:
                    logger.warning(f"[Early Stopping] Metric '{metric_name}' not found, using 'total_loss'")
            
            # æª¢æŸ¥æ˜¯å¦æ”¹å–„
            if current_metric < best_metric - min_delta:
                best_metric = current_metric
                best_epoch = epoch
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹ç‹€æ…‹
                if restore_best:
                    best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                
                logger.info(f"ğŸ¯ New best {metric_name}: {best_metric:.6f} at epoch {epoch}")
            else:
                patience_counter += 1
            
            # æª¢æŸ¥æ˜¯å¦è§¸ç™¼ Early Stopping
            if patience_counter >= patience:
                logger.info(f"=" * 80)
                logger.info(f"ğŸ›‘ Early stopping triggered at epoch {epoch}")
                logger.info(f"   Best {metric_name}: {best_metric:.6f} at epoch {best_epoch}")
                logger.info(f"   Patience counter: {patience_counter}/{patience}")
                logger.info(f"=" * 80)
                
                # æ¢å¾©æœ€ä½³æ¨¡å‹
                if restore_best and best_model_state is not None:
                    model.load_state_dict(best_model_state)
                    logger.info(f"âœ… Restored best model from epoch {best_epoch}")
                
                break
        
        # é©—è­‰å’Œæª¢æŸ¥é»
        if epoch % validation_freq == 0 and epoch > 0:
            logger.info("=== Validation checkpoint ===")
            # é€™è£¡å¯ä»¥æ·»åŠ é©—è­‰é‚è¼¯
        
        # æ—©åœæª¢æŸ¥ï¼ˆåŸæœ‰çš„å¿«é€Ÿæ”¶æ–‚æª¢æŸ¥ï¼‰
        if loss_dict['total_loss'] < 1e-6:
            logger.info(f"Early convergence at epoch {epoch}")
            break
    
    total_time = time.time() - start_time
    logger.info(f"Training completed in {total_time:.1f}s")
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    final_checkpoint_path = save_checkpoint(model, optimizer, epoch + 1, loss_dict['total_loss'], config)
    logger.info(f"Final model saved: {final_checkpoint_path}")
    
    return {
        'final_loss': loss_dict['total_loss'],
        'training_time': total_time,
        'epochs_completed': epoch + 1,
        'checkpoint_path': final_checkpoint_path
    }


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description='PINNs Inverse Training Script')
    parser.add_argument('--cfg', type=str, default='configs/defaults.yml',
                       help='Path to configuration file')
    parser.add_argument('--ensemble', action='store_true',
                       help='Run ensemble training for UQ')
    parser.add_argument('--resume', type=str, default=None,
                       help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    # è¼‰å…¥é…ç½®
    config = load_config(args.cfg)
    
    # ğŸ†• æ ¹æ“šç‰©ç†é¡å‹è‡ªå‹•è¨­ç½®æ¨¡å‹è¼¸å…¥è¼¸å‡ºç¶­åº¦
    if 'in_dim' not in config['model'] or 'out_dim' not in config['model']:
        physics_type = config.get('physics', {}).get('type', 'ns_2d')
        
        if physics_type == 'vs_pinn_channel_flow':
            # VS-PINN 3D: è¼¸å…¥ (x, y, z)ï¼Œè¼¸å‡º (u, v, w, p)
            config['model']['in_dim'] = 3
            config['model']['out_dim'] = 4
            logger_msg = "VS-PINN 3D: in_dim=3 (x,y,z), out_dim=4 (u,v,w,p)"
        else:
            # æ¨™æº– PINN 2D: è¼¸å…¥ (x, y)ï¼Œè¼¸å‡º (u, v, p)
            config['model']['in_dim'] = 2
            config['model']['out_dim'] = 3
            logger_msg = "Standard PINN 2D: in_dim=2 (x,y), out_dim=3 (u,v,p)"
        
        # æš«æ™‚ç”¨ printï¼ˆå› ç‚º logger å°šæœªè¨­ç½®ï¼‰
        print(f"ğŸ”§ Auto-configured model dimensions: {logger_msg}")
    
    # è¨­ç½®æ—¥èªŒ
    logger = setup_logging(config['logging']['level'])
    logger.info("=" * 60)
    logger.info("PINNs Inverse Reconstruction Training")
    logger.info("=" * 60)
    
    # è¨­ç½®é‡ç¾æ€§
    set_random_seed(
        config['experiment']['seed'],
        config['reproducibility']['deterministic']
    )
    
    # è¨­ç½®è¨­å‚™
    device = get_device(config['experiment']['device'])
    
    # æå‰æº–å‚™è³‡æ–™ä»¥æå–çµ±è¨ˆè³‡è¨Šï¼ˆç”¨æ–¼è‡ªå‹•è¼¸å‡ºç¯„åœï¼‰
    logger.info("Preparing training data to extract statistics...")
    training_data_sample = prepare_training_data(config, device)
    
    # å¾å¿«å–ä¸­æå–çµ±è¨ˆè³‡è¨Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
    statistics = None
    if '_channel_data_cache' in globals() and _channel_data_cache is not None:
        channel_data = _channel_data_cache.get('channel_data', {})
        if 'statistics' in channel_data:
            statistics = channel_data['statistics']
            logger.info(f"âœ… Extracted statistics for auto output ranges:")
            logger.info(f"   u: {statistics.get('u', {}).get('range', 'N/A')}")
            logger.info(f"   v: {statistics.get('v', {}).get('range', 'N/A')}")
            logger.info(f"   p: {statistics.get('p', {}).get('range', 'N/A')}")
        else:
            logger.warning("âš ï¸  No statistics found in channel_data")
    else:
        logger.warning("âš ï¸  Channel data cache not available, will use hardcoded ranges")
    
    # å»ºç«‹æ¨¡å‹å’Œç‰©ç†æ¨¡çµ„
    model = create_model(config, device, statistics=statistics)
    physics = create_physics(config, device)
    losses = create_loss_functions(config, device)
    
    logger.info(f"Model architecture: {config['model']['type']}")
    logger.info(f"Input dimension: {config['model']['in_dim']}")
    logger.info(f"Output dimension: {config['model']['out_dim']}")
    
    # å®‰å…¨è®€å–ç‰©ç†åƒæ•¸
    physics_type = config.get('physics', {}).get('type', 'unknown')
    if physics_type == 'vs_pinn_channel_flow':
        physics_params = config.get('physics', {}).get('physics_params', {})
        logger.info(f"Physics: VS-PINN Channel Flow with nu={physics_params.get('nu', 'N/A')}")
    else:
        nu = config.get('physics', {}).get('nu', config.get('physics', {}).get('physics_params', {}).get('nu', 'N/A'))
        logger.info(f"Physics: NS-2D with nu={nu}")
    
    if args.ensemble:
        logger.info("Running ensemble training...")
        ensemble_cfg = config['ensemble']
        
        models = []
        for i, seed in enumerate(ensemble_cfg['seeds']):
            logger.info(f"Training ensemble member {i+1}/{len(ensemble_cfg['seeds'])} (seed={seed})")
            
            # é‡ç½®éš¨æ©Ÿç¨®å­
            set_random_seed(seed, config['reproducibility']['deterministic'])
            
            # å»ºç«‹æ–°æ¨¡å‹ï¼ˆä½¿ç”¨ç›¸åŒçš„çµ±è¨ˆè³‡è¨Šï¼‰
            member_model = create_model(config, device, statistics=statistics)
            
            # è¨“ç·´
            train_result = train_model(member_model, physics, losses, config, device)
            models.append(member_model)
            
            logger.info(f"Member {i+1} final loss: {train_result['final_loss']:.6f}")
        
        # å„²å­˜æ¨¡å‹åˆ—è¡¨ï¼ˆæš«æ™‚ä¸ä½¿ç”¨ EnsembleWrapperï¼‰
        logger.info(f"Ensemble training completed with {len(models)} members")
        logger.info("Note: EnsembleWrapper not implemented yet - models stored as list")
        
    else:
        logger.info("Running single model training...")
        train_result = train_model(model, physics, losses, config, device)
        logger.info(f"Training completed. Final loss: {train_result['final_loss']:.6f}")
    
    logger.info("Training script finished successfully!")


if __name__ == "__main__":
    main()
