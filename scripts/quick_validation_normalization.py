#!/usr/bin/env python3
"""
å¿«é€Ÿé©—è­‰ï¼šæ¨™æº–åŒ–å°è¨“ç·´ç©©å®šæ€§çš„å½±éŸ¿

æ¸¬è©¦å…§å®¹ï¼š
1. Baselineï¼ˆç„¡æ¨™æº–åŒ–ï¼‰vs Normalizedï¼ˆZ-Score æ¨™æº–åŒ–ï¼‰
2. è¨“ç·´ 200 epochsï¼ˆå¿«é€Ÿé©—è­‰ï¼‰
3. æå¤±æ”¶æ–‚æ›²ç·šå°æ¯”
4. æª¢æŸ¥é»ä¿å­˜/è¼‰å…¥ä¸€è‡´æ€§
5. æ—©åœæ©Ÿåˆ¶é©—è­‰

é è¨ˆæ™‚é–“ï¼š30-45 åˆ†é˜
"""

import sys
import os
import json
import time
from pathlib import Path
from typing import Dict, Any, List

import torch
import numpy as np
import matplotlib.pyplot as plt

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pinnx.train.config_loader import load_config
from pinnx.train.factory import create_model, create_physics, create_optimizer
from pinnx.train.trainer import Trainer


def create_test_config(enable_normalization: bool) -> Dict[str, Any]:
    """å‰µå»ºæ¸¬è©¦é…ç½®ï¼ˆåŸºæ–¼ normalization_baseline_test_fix_v1.ymlï¼‰"""
    
    config = {
        'experiment': {
            'name': f'quick_val_{"normalized" if enable_normalization else "baseline"}',
            'version': 'v1.0',
            'seed': 42,
            'device': 'auto',
            'precision': 'float32'
        },
        
        'data': {
            'source': 'synthetic',  # ä½¿ç”¨åˆæˆè³‡æ–™åŠ é€Ÿæ¸¬è©¦
            'dataset': 'channel',
            'domain': {
                'x': [0, 25.13],
                'y': [-1.0, 1.0],
                'z': [0, 9.42]
            }
        },
        
        'normalization': {
            'type': 'training_data_norm' if enable_normalization else 'none',
            'noise_sigma': 0.01,
            'dropout_prob': 0.0  # é—œé–‰ dropout ä»¥æ¸›å°‘éš¨æ©Ÿæ€§
        },
        
        'model': {
            'type': 'fourier_mlp',
            'in_dim': 3,
            'out_dim': 4,
            'width': 128,
            'depth': 4,
            'activation': 'sine',
            'use_fourier': True,
            'fourier_m': 16,
            'fourier_sigma': 1.0
        },
        
        'training': {
            'epochs': 200,  # å¿«é€Ÿæ¸¬è©¦
            'batch_size': 512,
            'optimizer': {
                'type': 'adam',
                'lr': 1.0e-3,
                'betas': [0.9, 0.999]
            },
            'scheduler': {
                'type': 'exponential',
                'decay_rate': 0.95,
                'decay_epochs': 50
            },
            'sampling': {
                'pde_points': 1024,
                'boundary_points': 128,
                'sensor_points': 50
            },
            'validation_freq': 10,
            'checkpoint_freq': 50,
            'log_interval': 5,
            'gradient_clip': 1.0,
            'early_stopping': {
                'enabled': True,
                'patience': 50,
                'min_delta': 1.0e-4
            }
        },
        
        'losses': {
            'data_loss_weight': 100.0,
            'pde_loss_weight': 1.0,
            'wall_loss_weight': 50.0,
            'initial_loss_weight': 0.0,
            'adaptive_weights': {
                'enabled': False
            }
        },
        
        'physics': {
            'type': 'vs_pinn_channel_flow',
            'nu': 5.0e-5,
            're_tau': 1000,
            'u_tau': 0.04997,
            'domain': {
                'x_min': 0.0,
                'x_max': 25.13,
                'y_min': -1.0,
                'y_max': 1.0,
                'z_min': 0.0,
                'z_max': 9.42
            },
            'scaling': {
                'use_scaling': True,
                'N_x': 2.0,
                'N_y': 8.0,
                'N_z': 2.0
            }
        },
        
        'output': {
            'checkpoint_dir': f'./checkpoints/quick_val_{"normalized" if enable_normalization else "baseline"}',
            'results_dir': f'./results/quick_val_{"normalized" if enable_normalization else "baseline"}',
            'visualization_dir': f'./results/quick_val_{"normalized" if enable_normalization else "baseline"}/visualizations'
        }
    }
    
    return config


def generate_synthetic_data(config: Dict[str, Any], n_sensors: int = 50, n_pde: int = 500, n_bc: int = 100) -> Dict[str, torch.Tensor]:
    """
    ç”Ÿæˆåˆæˆè¨“ç·´è³‡æ–™ï¼ˆé€šé“æµè§£æè§£è¿‘ä¼¼ï¼‰
    
    è¿”å›æ ¼å¼ç¬¦åˆ Trainer æœŸæœ›ï¼š
    - æ„Ÿæ¸¬é»è³‡æ–™ï¼šx_sensors, y_sensors, z_sensors, u_sensors, v_sensors, w_sensors, p_sensors
    - PDE é»ï¼šx_pde, y_pde, z_pde
    - é‚Šç•Œé»ï¼šx_bc, y_bc, z_bc
    """
    
    domain = config['physics']['domain']
    u_tau = config['physics']['u_tau']
    nu = config['physics']['nu']
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    np.random.seed(42)
    
    # ==================== 1. ç”Ÿæˆæ„Ÿæ¸¬é»è³‡æ–™ ====================
    x_sensors = np.random.uniform(domain['x_min'], domain['x_max'], n_sensors)
    y_sensors = np.random.uniform(domain['y_min'], domain['y_max'], n_sensors)
    z_sensors = np.random.uniform(domain['z_min'], domain['z_max'], n_sensors)
    
    # è§£æè§£è¿‘ä¼¼ï¼ˆå±¤æµ Poiseuille æµï¼‰
    # u = u_max * (1 - (y/h)^2)ï¼ˆæ‹‹ç‰©ç·šåˆ†ä½ˆï¼‰
    u_sensors = 1.0 * (1 - (y_sensors / 1.0)**2)
    v_sensors = 1e-3 * np.random.randn(n_sensors)  # å¾®å°æ“¾å‹•
    w_sensors = 1e-3 * np.random.randn(n_sensors)
    p_sensors = -2.0 * nu * 1.0 * x_sensors / (1.0**2)  # ç·šæ€§å£“é™
    
    # ==================== 2. ç”Ÿæˆ PDE é»ï¼ˆç„¡éœ€çœŸå¯¦å€¼ï¼‰====================
    x_pde = np.random.uniform(domain['x_min'], domain['x_max'], n_pde)
    y_pde = np.random.uniform(domain['y_min'], domain['y_max'], n_pde)
    z_pde = np.random.uniform(domain['z_min'], domain['z_max'], n_pde)
    
    # ==================== 3. ç”Ÿæˆé‚Šç•Œé» ====================
    # ç‰†é¢é‚Šç•Œï¼ˆy = Â±1ï¼‰
    n_wall = n_bc // 2
    x_bc_wall = np.random.uniform(domain['x_min'], domain['x_max'], n_wall)
    y_bc_wall = np.random.choice([-1.0, 1.0], n_wall)
    z_bc_wall = np.random.uniform(domain['z_min'], domain['z_max'], n_wall)
    
    # é€±æœŸé‚Šç•Œï¼ˆx, zï¼‰
    n_periodic = n_bc - n_wall
    x_bc_periodic = np.random.choice([domain['x_min'], domain['x_max']], n_periodic)
    y_bc_periodic = np.random.uniform(domain['y_min'], domain['y_max'], n_periodic)
    z_bc_periodic = np.random.choice([domain['z_min'], domain['z_max']], n_periodic)
    
    x_bc = np.concatenate([x_bc_wall, x_bc_periodic])
    y_bc = np.concatenate([y_bc_wall, y_bc_periodic])
    z_bc = np.concatenate([z_bc_wall, z_bc_periodic])
    
    # ==================== 4. è½‰æ›ç‚º PyTorch å¼µé‡ ====================
    data = {
        # æ„Ÿæ¸¬é»è³‡æ–™
        'x_sensors': torch.tensor(x_sensors, dtype=torch.float32).unsqueeze(1),
        'y_sensors': torch.tensor(y_sensors, dtype=torch.float32).unsqueeze(1),
        'z_sensors': torch.tensor(z_sensors, dtype=torch.float32).unsqueeze(1),
        'u_sensors': torch.tensor(u_sensors, dtype=torch.float32).unsqueeze(1),
        'v_sensors': torch.tensor(v_sensors, dtype=torch.float32).unsqueeze(1),
        'w_sensors': torch.tensor(w_sensors, dtype=torch.float32).unsqueeze(1),
        'p_sensors': torch.tensor(p_sensors, dtype=torch.float32).unsqueeze(1),
        
        # PDE é»
        'x_pde': torch.tensor(x_pde, dtype=torch.float32).unsqueeze(1),
        'y_pde': torch.tensor(y_pde, dtype=torch.float32).unsqueeze(1),
        'z_pde': torch.tensor(z_pde, dtype=torch.float32).unsqueeze(1),
        
        # é‚Šç•Œé»
        'x_bc': torch.tensor(x_bc, dtype=torch.float32).unsqueeze(1),
        'y_bc': torch.tensor(y_bc, dtype=torch.float32).unsqueeze(1),
        'z_bc': torch.tensor(z_bc, dtype=torch.float32).unsqueeze(1),
    }
    
    return data


def run_training(config: Dict[str, Any], training_data: Dict[str, torch.Tensor]) -> Dict[str, Any]:
    """åŸ·è¡Œè¨“ç·´ä¸¦è¿”å›çµæœ"""
    
    device = torch.device('cuda' if torch.cuda.is_available() and config['experiment']['device'] == 'auto' else 'cpu')
    print(f"\n{'='*60}")
    print(f"è¨“ç·´é…ç½®: {config['experiment']['name']}")
    print(f"æ¨™æº–åŒ–: {config['normalization']['type']}")
    print(f"è¨­å‚™: {device}")
    print(f"{'='*60}\n")
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    torch.manual_seed(config['experiment']['seed'])
    np.random.seed(config['experiment']['seed'])
    
    # å‰µå»ºæ¨¡å‹ã€ç‰©ç†
    model = create_model(config, device=device)
    physics = create_physics(config, device=device)
    
    # å‰µå»ºæå¤±å‡½æ•¸å­—å…¸ï¼ˆTrainer éœ€è¦å¯¦ä¾‹åŒ–å°è±¡ï¼Œè€Œéé…ç½®ï¼‰
    # æ³¨æ„ï¼šè³‡æ–™æå¤±ï¼ˆMSEï¼‰ç”± Trainer å…§éƒ¨è¨ˆç®—ï¼Œé€™è£¡åªéœ€è¦ç‰©ç†æå¤±é¡åˆ¥
    from pinnx.losses.residuals import NSResidualLoss, BoundaryConditionLoss
    from pinnx.losses.priors import PriorLossManager
    
    loss_cfg = config.get('losses', {})
    losses_dict = {
        'residual': NSResidualLoss(
            nu=config['physics'].get('nu', 5.0e-5),
            density=1.0
        ),
        'boundary': BoundaryConditionLoss(),
        'prior': PriorLossManager(
            consistency_weight=loss_cfg.get('prior_weight', 0.0)  # å¿«é€Ÿæ¸¬è©¦ä¸ä½¿ç”¨å…ˆé©—
        )
    }
    
    # å‰µå»ºè¨“ç·´å™¨ï¼ˆå…§éƒ¨æœƒå¾ config å‰µå»ºå„ªåŒ–å™¨ï¼‰
    # æ³¨æ„ï¼šä¸éœ€è¦å‚³é optimizer åƒæ•¸ï¼ŒTrainer å…§éƒ¨æœƒè‡ªå‹•å‰µå»º
    trainer = Trainer(
        model=model,
        physics=physics,
        losses=losses_dict,
        config=config,
        device=device,
        training_data=training_data  # å‚³éè¨“ç·´è³‡æ–™ä»¥è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡
    )
    
    # â­ æ‰‹å‹•è¨­ç½®è¨“ç·´è³‡æ–™ï¼ˆTrainer éœ€è¦ self.training_data ç”¨æ–¼ step() æ–¹æ³•ï¼‰
    # æ³¨æ„ï¼šåˆå§‹åŒ–æ™‚çš„ training_data åƒ…ç”¨æ–¼è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡ï¼Œä¸æœƒè‡ªå‹•è¨­ç‚º self.training_data
    trainer.training_data = training_data
    
    # é–‹å§‹è¨“ç·´
    start_time = time.time()
    result = trainer.train()
    training_time = time.time() - start_time
    
    print(f"\nâœ… è¨“ç·´å®Œæˆï¼è€—æ™‚ {training_time:.2f} ç§’")
    
    # è¿”å›çµæœ
    return {
        'config': config,
        'history': result['history'],
        'best_epoch': result['best_epoch'],
        'best_loss': result['best_metric'],  # â­ ä¿®æ­£ï¼štrain() è¿”å› 'best_metric' è€Œé 'best_loss'
        'training_time': training_time,
        'model_checkpoint': result.get('checkpoint_path', None),
        'final_model_state': model.state_dict()  # â­ æ·»åŠ æœ€çµ‚æ¨¡å‹ç‹€æ…‹
    }


def test_checkpoint_consistency(config: Dict[str, Any], original_state: Dict[str, Any]) -> bool:
    """æ¸¬è©¦æª¢æŸ¥é»ä¿å­˜/è¼‰å…¥ä¸€è‡´æ€§"""
    
    print(f"\n{'='*60}")
    print("æ¸¬è©¦æª¢æŸ¥é»ä¸€è‡´æ€§")
    print(f"{'='*60}\n")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    checkpoint_dir = Path(config['output']['checkpoint_dir'])
    
    # å°‹æ‰¾æœ€æ–°æª¢æŸ¥é»
    checkpoints = list(checkpoint_dir.glob('epoch_*.pth'))
    if not checkpoints:
        print("âŒ æœªæ‰¾åˆ°æª¢æŸ¥é»æ–‡ä»¶")
        return False
    
    latest_checkpoint = max(checkpoints, key=lambda p: int(p.stem.split('_')[1]))
    print(f"è¼‰å…¥æª¢æŸ¥é»: {latest_checkpoint}")
    
    # è¼‰å…¥æª¢æŸ¥é»
    checkpoint = torch.load(latest_checkpoint, map_location=device)
    
    # æª¢æŸ¥é—œéµæ¬„ä½
    required_keys = ['epoch', 'model_state_dict', 'optimizer_state_dict', 'history', 'config']
    missing_keys = [k for k in required_keys if k not in checkpoint]
    
    if missing_keys:
        print(f"âŒ æª¢æŸ¥é»ç¼ºå°‘æ¬„ä½: {missing_keys}")
        return False
    
    # å¾ history ä¸­æå–æœ€çµ‚æå¤±
    final_loss = checkpoint['history']['total_loss'][-1]
    
    print(f"âœ… æª¢æŸ¥é»å®Œæ•´æ€§é©—è­‰é€šé")
    print(f"  - Epoch: {checkpoint['epoch']}")
    print(f"  - Final Total Loss: {final_loss:.6e}")
    
    # æ¯”è¼ƒæ¨¡å‹åƒæ•¸ï¼ˆèˆ‡è¨“ç·´çµæŸæ™‚çš„ç‹€æ…‹ï¼‰
    loaded_state = checkpoint['model_state_dict']
    param_diff = []
    
    for key in original_state.keys():
        if key in loaded_state:
            diff = torch.abs(original_state[key] - loaded_state[key]).max().item()
            param_diff.append(diff)
    
    max_diff = max(param_diff) if param_diff else float('inf')
    print(f"  - æ¨¡å‹åƒæ•¸æœ€å¤§å·®ç•°: {max_diff:.6e}")
    
    if max_diff < 1e-5:
        print("âœ… æ¨¡å‹åƒæ•¸ä¸€è‡´æ€§é©—è­‰é€šé")
        return True
    else:
        print(f"âŒ æ¨¡å‹åƒæ•¸ä¸ä¸€è‡´ï¼ˆå·®ç•°: {max_diff:.6e} > 1e-5ï¼‰")
        return False


def plot_comparison(baseline_history: Dict[str, List], normalized_history: Dict[str, List], output_dir: Path):
    """ç¹ªè£½å°æ¯”åœ–"""
    
    print(f"\n{'='*60}")
    print("ç”Ÿæˆå°æ¯”åœ–è¡¨")
    print(f"{'='*60}\n")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # 1. Total Lossï¼ˆå°æ•¸å°ºåº¦ï¼‰
    ax = axes[0]
    ax.plot(baseline_history['total_loss'], label='Baseline', linewidth=2, alpha=0.8, color='tab:blue')
    ax.plot(normalized_history['total_loss'], label='Normalized', linewidth=2, alpha=0.8, color='tab:orange')
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Total Loss (log scale)', fontsize=12)
    ax.set_yscale('log')
    ax.set_title('Total Loss Convergence', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    # 2. Total Lossï¼ˆç·šæ€§å°ºåº¦ - æœ€å¾Œ 50 epochsï¼‰
    ax = axes[1]
    start_epoch = max(0, len(baseline_history['total_loss']) - 50)
    ax.plot(
        range(start_epoch, len(baseline_history['total_loss'])),
        baseline_history['total_loss'][start_epoch:],
        label='Baseline',
        linewidth=2,
        alpha=0.8,
        color='tab:blue'
    )
    ax.plot(
        range(start_epoch, len(normalized_history['total_loss'])),
        normalized_history['total_loss'][start_epoch:],
        label='Normalized',
        linewidth=2,
        alpha=0.8,
        color='tab:orange'
    )
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Total Loss (linear scale)', fontsize=12)
    ax.set_title('Final Convergence (Last 50 Epochs)', fontsize=14, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    output_file = output_dir / 'training_comparison.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"âœ… å°æ¯”åœ–è¡¨å·²ä¿å­˜: {output_file}")
    
    plt.close()


def generate_report(baseline_result: Dict, normalized_result: Dict, 
                   baseline_checkpoint_ok: bool, normalized_checkpoint_ok: bool,
                   output_dir: Path):
    """ç”Ÿæˆé©—è­‰å ±å‘Š"""
    
    print(f"\n{'='*60}")
    print("ç”Ÿæˆé©—è­‰å ±å‘Š")
    print(f"{'='*60}\n")
    
    report = {
        'test_name': 'Quick Validation: Normalization Impact on Training Stability',
        'test_date': time.strftime('%Y-%m-%d %H:%M:%S'),
        'baseline': {
            'config_name': baseline_result['config']['experiment']['name'],
            'normalization_type': baseline_result['config']['normalization']['type'],
            'best_epoch': baseline_result['best_epoch'],
            'best_loss': float(baseline_result['best_loss']),
            'final_total_loss': float(baseline_result['history']['total_loss'][-1]),
            'training_time_sec': baseline_result['training_time'],
            'checkpoint_consistency': baseline_checkpoint_ok
        },
        'normalized': {
            'config_name': normalized_result['config']['experiment']['name'],
            'normalization_type': normalized_result['config']['normalization']['type'],
            'best_epoch': normalized_result['best_epoch'],
            'best_loss': float(normalized_result['best_loss']),
            'final_total_loss': float(normalized_result['history']['total_loss'][-1]),
            'training_time_sec': normalized_result['training_time'],
            'checkpoint_consistency': normalized_checkpoint_ok
        },
        'comparison': {
            'best_loss_ratio': float(normalized_result['best_loss'] / baseline_result['best_loss']),
            'final_total_loss_ratio': float(normalized_result['history']['total_loss'][-1] / baseline_result['history']['total_loss'][-1]),
            'convergence_speed_comparison': f"Normalized reached best at epoch {normalized_result['best_epoch']}, Baseline at epoch {baseline_result['best_epoch']}",
            'training_time_ratio': normalized_result['training_time'] / baseline_result['training_time']
        },
        'test_results': {
            'training_stability': {
                'baseline_has_nan': any(np.isnan(baseline_result['history']['total_loss'])),
                'normalized_has_nan': any(np.isnan(normalized_result['history']['total_loss'])),
                'status': 'âœ… PASS' if not any(np.isnan(normalized_result['history']['total_loss'])) else 'âŒ FAIL'
            },
            'checkpoint_consistency': {
                'baseline': 'âœ… PASS' if baseline_checkpoint_ok else 'âŒ FAIL',
                'normalized': 'âœ… PASS' if normalized_checkpoint_ok else 'âŒ FAIL',
                'status': 'âœ… PASS' if (baseline_checkpoint_ok and normalized_checkpoint_ok) else 'âŒ FAIL'
            },
            'performance_impact': {
                'normalized_improves_loss': normalized_result['best_loss'] < baseline_result['best_loss'],
                'normalized_faster_convergence': normalized_result['best_epoch'] < baseline_result['best_epoch'],
                'status': 'âœ… PASS' if normalized_result['best_loss'] < baseline_result['best_loss'] * 1.5 else 'âš ï¸ WARNING'
            }
        }
    }
    
    # ä¿å­˜ JSON å ±å‘Š
    output_file = output_dir / 'quick_validation_report.json'
    with open(output_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… JSON å ±å‘Šå·²ä¿å­˜: {output_file}")
    
    # æ‰“å°æ‘˜è¦
    print(f"\n{'='*60}")
    print("é©—è­‰çµæœæ‘˜è¦")
    print(f"{'='*60}\n")
    
    print("ğŸ“Š è¨“ç·´ç©©å®šæ€§:")
    print(f"  Baseline - NaN æå¤±: {'æ˜¯' if report['test_results']['training_stability']['baseline_has_nan'] else 'å¦'}")
    print(f"  Normalized - NaN æå¤±: {'æ˜¯' if report['test_results']['training_stability']['normalized_has_nan'] else 'å¦'}")
    print(f"  ç‹€æ…‹: {report['test_results']['training_stability']['status']}")
    
    print("\nğŸ“¦ æª¢æŸ¥é»ä¸€è‡´æ€§:")
    print(f"  Baseline: {report['test_results']['checkpoint_consistency']['baseline']}")
    print(f"  Normalized: {report['test_results']['checkpoint_consistency']['normalized']}")
    print(f"  ç‹€æ…‹: {report['test_results']['checkpoint_consistency']['status']}")
    
    print("\nğŸ¯ æ€§èƒ½å½±éŸ¿:")
    print(f"  æœ€ä½³æå¤± (Normalized/Baseline): {report['comparison']['best_loss_ratio']:.3f}Ã—")
    print(f"  æœ€çµ‚ç¸½æå¤± (Normalized/Baseline): {report['comparison']['final_total_loss_ratio']:.3f}Ã—")
    print(f"  è¨“ç·´æ™‚é–“ (Normalized/Baseline): {report['comparison']['training_time_ratio']:.3f}Ã—")
    print(f"  ç‹€æ…‹: {report['test_results']['performance_impact']['status']}")
    
    print(f"\n{'='*60}\n")
    
    return report


def main():
    """ä¸»ç¨‹å¼"""
    
    print("\n" + "="*60)
    print("ğŸš€ å¿«é€Ÿé©—è­‰ï¼šæ¨™æº–åŒ–å°è¨“ç·´ç©©å®šæ€§çš„å½±éŸ¿")
    print("="*60 + "\n")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    output_dir = Path('./results/quick_validation_normalization')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. ç”Ÿæˆåˆæˆè¨“ç·´è³‡æ–™
    print("ğŸ“¦ ç”Ÿæˆåˆæˆè¨“ç·´è³‡æ–™...")
    baseline_config = create_test_config(enable_normalization=False)
    training_data = generate_synthetic_data(
        baseline_config, 
        n_sensors=50,    # æ„Ÿæ¸¬é»æ•¸é‡
        n_pde=500,       # PDE é»æ•¸é‡
        n_bc=100         # é‚Šç•Œé»æ•¸é‡
    )
    print(f"  - æ„Ÿæ¸¬é»æ•¸é‡: {len(training_data['x_sensors'])}")
    print(f"  - PDE é»æ•¸é‡: {len(training_data['x_pde'])}")
    print(f"  - é‚Šç•Œé»æ•¸é‡: {len(training_data['x_bc'])}")
    print(f"  - åº§æ¨™ç¯„åœ: x âˆˆ [{training_data['x_sensors'].min():.2f}, {training_data['x_sensors'].max():.2f}]")
    print(f"  - é€Ÿåº¦ç¯„åœ: u âˆˆ [{training_data['u_sensors'].min():.3f}, {training_data['u_sensors'].max():.3f}]")
    
    # 2. è¨“ç·´ Baselineï¼ˆç„¡æ¨™æº–åŒ–ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ æ¸¬è©¦ 1/2: Baselineï¼ˆç„¡æ¨™æº–åŒ–ï¼‰")
    print("="*60)
    baseline_result = run_training(baseline_config, training_data)
    
    # 3. è¨“ç·´ Normalizedï¼ˆZ-Score æ¨™æº–åŒ–ï¼‰
    print("\n" + "="*60)
    print("ğŸ”§ æ¸¬è©¦ 2/2: Normalizedï¼ˆZ-Score æ¨™æº–åŒ–ï¼‰")
    print("="*60)
    normalized_config = create_test_config(enable_normalization=True)
    normalized_result = run_training(normalized_config, training_data)
    
    # 4. æ¸¬è©¦æª¢æŸ¥é»ä¸€è‡´æ€§
    baseline_checkpoint_ok = test_checkpoint_consistency(
        baseline_config, 
        baseline_result['final_model_state']
    )
    normalized_checkpoint_ok = test_checkpoint_consistency(
        normalized_config, 
        normalized_result['final_model_state']
    )
    
    # 5. ç”Ÿæˆå°æ¯”åœ–
    plot_comparison(
        baseline_result['history'],
        normalized_result['history'],
        output_dir
    )
    
    # 6. ç”Ÿæˆé©—è­‰å ±å‘Š
    report = generate_report(
        baseline_result,
        normalized_result,
        baseline_checkpoint_ok,
        normalized_checkpoint_ok,
        output_dir
    )
    
    # 7. æœ€çµ‚ç¸½çµ
    print(f"\n{'='*60}")
    print("âœ… å¿«é€Ÿé©—è­‰å®Œæˆï¼")
    print(f"{'='*60}\n")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    print(f"ğŸ“Š å ±å‘Šæ–‡ä»¶: {output_dir}/quick_validation_report.json")
    print(f"ğŸ“ˆ å°æ¯”åœ–è¡¨: {output_dir}/training_comparison.png")
    print(f"\n{'='*60}\n")
    
    # è¿”å›ç‹€æ…‹ç¢¼
    all_pass = all([
        report['test_results']['training_stability']['status'] == 'âœ… PASS',
        report['test_results']['checkpoint_consistency']['status'] == 'âœ… PASS'
    ])
    
    return 0 if all_pass else 1


if __name__ == '__main__':
    exit(main())
