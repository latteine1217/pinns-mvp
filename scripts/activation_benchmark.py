#!/usr/bin/env python3
"""
æ¿€æ´»å‡½æ•¸æ¯”è¼ƒå¯¦é©—
æ¸¬è©¦ sineã€swishã€tanh åœ¨çœŸå¯¦ JHTDB Channel Flow æ•¸æ“šä¸Šçš„æ•ˆæœ
è¨“ç·´ 5000 epochsï¼Œç”Ÿæˆå®Œæ•´è©•ä¼°å ±å‘Š
"""

import argparse
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, Any, List
import json
from datetime import datetime

import numpy as np
import torch
import yaml
import matplotlib.pyplot as plt

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from scripts.train import (
    setup_logging, set_random_seed, load_config, get_device,
    create_model, create_physics, create_loss_functions,
    create_weighters, prepare_training_data, train_step
)


class ActivationBenchmark:
    """æ¿€æ´»å‡½æ•¸åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self, base_config_path: str, output_dir: str = "activation_benchmark"):
        self.base_config = load_config(base_config_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        # å®šç¾©æ¸¬è©¦çš„æ¿€æ´»å‡½æ•¸
        self.activations = ['sine', 'swish', 'tanh']
        
        # çµæœå­˜å„²
        self.results = {}
        
        # è¨­ç½®æ—¥èªŒ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.output_dir / f"benchmark_{timestamp}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def create_config_variant(self, activation: str) -> Dict[str, Any]:
        """ç‚ºç‰¹å®šæ¿€æ´»å‡½æ•¸å‰µå»ºé…ç½®è®Šé«”"""
        config = self.base_config.copy()
        
        # ä¿®æ”¹æ¨¡å‹é…ç½®
        config['model']['activation'] = activation
        
        # ä¿®æ”¹è¨“ç·´é…ç½®ç‚º 5000 epochs
        config['training']['max_epochs'] = 5000
        config['training']['checkpoint_freq'] = 1000
        config['training']['validation_freq'] = 500
        
        # ä¿®æ”¹å¯¦é©—åç¨±
        config['experiment']['name'] = f"activation_benchmark_{activation}"
        config['experiment']['version'] = f"5000ep_{activation}"
        
        # ç¦ç”¨ early stopping ä»¥ç¢ºä¿å®Œæ•´è¨“ç·´
        config['training']['early_stopping'] = False
        
        return config
    
    def train_single_activation(self, activation: str, device: torch.device) -> Dict[str, Any]:
        """è¨“ç·´å–®å€‹æ¿€æ´»å‡½æ•¸é…ç½®"""
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸš€ é–‹å§‹è¨“ç·´: activation = {activation}")
        self.logger.info("=" * 80)
        
        # å‰µå»ºé…ç½®
        config = self.create_config_variant(activation)
        
        # Sine æ¿€æ´»éœ€è¦ç‰¹æ®Šèª¿æ•´ä»¥ä¿è­‰ç©©å®šæ€§
        if activation == 'sine':
            config['training']['lr'] = 5e-4  # é™ä½å­¸ç¿’ç‡
            config['model']['sine_omega_0'] = 1.0  # ä½¿ç”¨ä¿å®ˆçš„ omega_0
            self.logger.info(f"âš™ï¸  Sine æ¿€æ´»ç‰¹æ®Šè¨­ç½®: lr={config['training']['lr']}, omega_0={config['model']['sine_omega_0']}")
        
        # è¨­ç½®éš¨æ©Ÿç¨®å­
        set_random_seed(config['experiment']['seed'], True)
        
        # æº–å‚™è¨“ç·´æ•¸æ“š
        training_data = prepare_training_data(config, device)
        
        # å‰µå»ºæ¨¡å‹
        model = create_model(config, device)
        physics = create_physics(config, device)
        losses = create_loss_functions(config, device)
        weighters = create_weighters(config, model, device)
        
        # å‰µå»ºå„ªåŒ–å™¨
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config['training']['lr'],
            weight_decay=config['training']['weight_decay']
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['training']['max_epochs']
        )
        
        # è¨“ç·´æ­·å²è¨˜éŒ„
        history = {
            'epoch': [],
            'total_loss': [],
            'residual_loss': [],
            'bc_loss': [],
            'data_loss': [],
            'momentum_x_loss': [],
            'momentum_y_loss': [],
            'continuity_loss': []
        }
        
        # è¨“ç·´å¾ªç’°
        start_time = time.time()
        
        for epoch in range(config['training']['max_epochs']):
            # è¨“ç·´æ­¥é©Ÿ
            loss_dict = train_step(
                model, physics, losses, training_data,
                optimizer, weighters, epoch, device, config
            )
            
            # æ›´æ–°å­¸ç¿’ç‡
            scheduler.step()
            
            # è¨˜éŒ„æ­·å²
            history['epoch'].append(epoch)
            for key in ['total_loss', 'residual_loss', 'bc_loss', 'data_loss',
                       'momentum_x_loss', 'momentum_y_loss', 'continuity_loss']:
                if key in loss_dict:
                    history[key].append(loss_dict[key])
            
            # æ—¥èªŒè¼¸å‡º
            if epoch % 100 == 0:
                elapsed = time.time() - start_time
                self.logger.info(
                    f"[{activation.upper()}] Epoch {epoch:5d} | "
                    f"Total: {loss_dict['total_loss']:.6f} | "
                    f"Residual: {loss_dict['residual_loss']:.6f} | "
                    f"BC: {loss_dict['bc_loss']:.6f} | "
                    f"Data: {loss_dict['data_loss']:.6f} | "
                    f"Time: {elapsed:.1f}s"
                )
        
        training_time = time.time() - start_time
        
        # ä¿å­˜æ¨¡å‹
        model_path = self.output_dir / f"model_{activation}.pth"
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config,
            'history': history,
            'training_time': training_time
        }, model_path)
        
        self.logger.info(f"âœ… {activation.upper()} è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {training_time:.1f}s")
        self.logger.info(f"   æœ€çµ‚ loss: {history['total_loss'][-1]:.6f}")
        self.logger.info(f"   æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        return {
            'model': model,
            'config': config,
            'history': history,
            'training_time': training_time,
            'model_path': str(model_path)
        }
    
    def evaluate_model(self, model: torch.nn.Module, config: Dict[str, Any], 
                      activation: str, device: torch.device) -> Dict[str, Any]:
        """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
        self.logger.info(f"ğŸ“Š è©•ä¼°æ¨¡å‹: {activation}")
        
        # æº–å‚™è©•ä¼°ç¶²æ ¼
        eval_cfg = config.get('evaluation', {})
        grid_res = eval_cfg.get('grid_resolution', [256, 128])
        
        # è¼‰å…¥ Channel Flow æ•¸æ“šç”¨æ–¼å°æ¯”
        from pinnx.dataio.channel_flow_loader import prepare_training_data as load_channel_flow
        
        channel_data = load_channel_flow(
            strategy=config['sensors'].get('selection_method', 'qr_pivot'),
            K=config['sensors']['K'],
            target_fields=['u', 'v', 'p']
        )
        
        full_field = channel_data['full_field']
        domain_bounds = channel_data['domain_bounds']
        
        # å‰µå»ºè©•ä¼°ç¶²æ ¼
        x_range = domain_bounds['x']
        y_range = domain_bounds['y']
        
        x = np.linspace(x_range[0], x_range[1], grid_res[0])
        y = np.linspace(y_range[0], y_range[1], grid_res[1])
        X, Y = np.meshgrid(x, y)
        
        # æ¨™æº–åŒ–åº§æ¨™ (èˆ‡è¨“ç·´ä¸€è‡´)
        x_min, x_max = x_range[0], x_range[1]
        y_min, y_max = y_range[0], y_range[1]
        X_norm = 2.0 * (X - x_min) / (x_max - x_min) - 1.0
        Y_norm = 2.0 * (Y - y_min) / (y_max - y_min) - 1.0
        
        # æº–å‚™è¼¸å…¥
        coords = np.stack([X_norm.flatten(), Y_norm.flatten()], axis=1)
        coords_tensor = torch.from_numpy(coords).float().to(device)
        
        # é æ¸¬
        model.eval()
        with torch.no_grad():
            pred = model(coords_tensor).cpu().numpy()
        
        u_pred = pred[:, 0].reshape(grid_res[1], grid_res[0])
        v_pred = pred[:, 1].reshape(grid_res[1], grid_res[0])
        p_pred = pred[:, 2].reshape(grid_res[1], grid_res[0])
        
        # æå–çœŸå¯¦å ´ (éœ€è¦æ’å€¼åˆ°è©•ä¼°ç¶²æ ¼)
        u_true = full_field['u']
        v_true = full_field['v']
        p_true = full_field['p']
        
        # è¨ˆç®—èª¤å·®æŒ‡æ¨™
        def relative_l2_error(pred, true):
            return np.linalg.norm(pred - true) / np.linalg.norm(true)
        
        def rmse(pred, true):
            return np.sqrt(np.mean((pred - true)**2))
        
        u_l2_error = relative_l2_error(u_pred, u_true)
        v_l2_error = relative_l2_error(v_pred, v_true)
        p_l2_error = relative_l2_error(p_pred, p_true)
        
        u_rmse = rmse(u_pred, u_true)
        v_rmse = rmse(v_pred, v_true)
        p_rmse = rmse(p_pred, p_true)
        
        # è¨ˆç®—çµ±è¨ˆé‡
        u_mean_pred = np.mean(u_pred)
        u_mean_true = np.mean(u_true)
        u_mean_error = abs(u_mean_pred - u_mean_true) / u_mean_true
        
        metrics = {
            'u_l2_error': float(u_l2_error),
            'v_l2_error': float(v_l2_error),
            'p_l2_error': float(p_l2_error),
            'u_rmse': float(u_rmse),
            'v_rmse': float(v_rmse),
            'p_rmse': float(p_rmse),
            'u_mean_error': float(u_mean_error),
            'u_mean_pred': float(u_mean_pred),
            'u_mean_true': float(u_mean_true)
        }
        
        self.logger.info(f"   U - L2 ç›¸å°èª¤å·®: {u_l2_error:.4f} ({u_l2_error*100:.2f}%)")
        self.logger.info(f"   V - L2 ç›¸å°èª¤å·®: {v_l2_error:.4f} ({v_l2_error*100:.2f}%)")
        self.logger.info(f"   P - L2 ç›¸å°èª¤å·®: {p_l2_error:.4f} ({p_l2_error*100:.2f}%)")
        self.logger.info(f"   U å‡å€¼èª¤å·®: {u_mean_error:.4f} ({u_mean_error*100:.2f}%)")
        
        # ä¿å­˜é æ¸¬å ´
        predictions = {
            'u': u_pred,
            'v': v_pred,
            'p': p_pred,
            'X': X,
            'Y': Y
        }
        
        return {
            'metrics': metrics,
            'predictions': predictions,
            'ground_truth': {
                'u': u_true,
                'v': v_true,
                'p': p_true
            }
        }
    
    def plot_comparison(self):
        """ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨"""
        self.logger.info("ğŸ“ˆ ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨...")
        
        # 1. Loss æ›²ç·šæ¯”è¼ƒ
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        for activation in self.activations:
            result = self.results[activation]
            history = result['history']
            
            # Total Loss
            axes[0, 0].plot(history['epoch'], history['total_loss'], 
                           label=activation, linewidth=2)
            
            # Residual Loss
            axes[0, 1].plot(history['epoch'], history['residual_loss'], 
                           label=activation, linewidth=2)
            
            # BC Loss
            axes[1, 0].plot(history['epoch'], history['bc_loss'], 
                           label=activation, linewidth=2)
            
            # Data Loss
            axes[1, 1].plot(history['epoch'], history['data_loss'], 
                           label=activation, linewidth=2)
        
        axes[0, 0].set_title('Total Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_yscale('log')
        
        axes[0, 1].set_title('Residual Loss (PDE)')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_yscale('log')
        
        axes[1, 0].set_title('Boundary Condition Loss')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_yscale('log')
        
        axes[1, 1].set_title('Data Fitting Loss')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Loss')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_yscale('log')
        
        plt.tight_layout()
        loss_plot_path = self.output_dir / "loss_comparison.png"
        plt.savefig(loss_plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"   Loss æ¯”è¼ƒåœ–å·²ä¿å­˜: {loss_plot_path}")
        
        # 2. èª¤å·®æŒ‡æ¨™æ¯”è¼ƒ (æŸ±ç‹€åœ–)
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        activations_list = list(self.activations)
        
        # U èª¤å·®
        u_l2_errors = [self.results[act]['evaluation']['metrics']['u_l2_error'] 
                       for act in activations_list]
        axes[0].bar(activations_list, u_l2_errors, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0].set_title('U Velocity - Relative L2 Error', fontsize=14, fontweight='bold')
        axes[0].set_ylabel('Relative L2 Error', fontsize=12)
        axes[0].grid(True, alpha=0.3, axis='y')
        for i, v in enumerate(u_l2_errors):
            axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')
        
        # V èª¤å·®
        v_l2_errors = [self.results[act]['evaluation']['metrics']['v_l2_error'] 
                       for act in activations_list]
        axes[1].bar(activations_list, v_l2_errors, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[1].set_title('V Velocity - Relative L2 Error', fontsize=14, fontweight='bold')
        axes[1].set_ylabel('Relative L2 Error', fontsize=12)
        axes[1].grid(True, alpha=0.3, axis='y')
        for i, v in enumerate(v_l2_errors):
            axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')
        
        # P èª¤å·®
        p_l2_errors = [self.results[act]['evaluation']['metrics']['p_l2_error'] 
                       for act in activations_list]
        axes[2].bar(activations_list, p_l2_errors, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[2].set_title('Pressure - Relative L2 Error', fontsize=14, fontweight='bold')
        axes[2].set_ylabel('Relative L2 Error', fontsize=12)
        axes[2].grid(True, alpha=0.3, axis='y')
        for i, v in enumerate(p_l2_errors):
            axes[2].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')
        
        plt.tight_layout()
        error_plot_path = self.output_dir / "error_comparison.png"
        plt.savefig(error_plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"   èª¤å·®æ¯”è¼ƒåœ–å·²ä¿å­˜: {error_plot_path}")
        
        # 3. é æ¸¬å ´å¯è¦–åŒ– (æ¯å€‹æ¿€æ´»å‡½æ•¸ä¸€è¡Œ)
        fig, axes = plt.subplots(len(self.activations), 3, figsize=(18, 5*len(self.activations)))
        
        for i, activation in enumerate(self.activations):
            eval_result = self.results[activation]['evaluation']
            pred = eval_result['predictions']
            true = eval_result['ground_truth']
            
            # U å ´
            im0 = axes[i, 0].contourf(pred['X'], pred['Y'], pred['u'], levels=20, cmap='RdBu_r')
            axes[i, 0].set_title(f'{activation.upper()} - U Velocity', fontsize=12, fontweight='bold')
            axes[i, 0].set_xlabel('x')
            axes[i, 0].set_ylabel('y')
            plt.colorbar(im0, ax=axes[i, 0])
            
            # V å ´
            im1 = axes[i, 1].contourf(pred['X'], pred['Y'], pred['v'], levels=20, cmap='RdBu_r')
            axes[i, 1].set_title(f'{activation.upper()} - V Velocity', fontsize=12, fontweight='bold')
            axes[i, 1].set_xlabel('x')
            axes[i, 1].set_ylabel('y')
            plt.colorbar(im1, ax=axes[i, 1])
            
            # P å ´
            im2 = axes[i, 2].contourf(pred['X'], pred['Y'], pred['p'], levels=20, cmap='RdBu_r')
            axes[i, 2].set_title(f'{activation.upper()} - Pressure', fontsize=12, fontweight='bold')
            axes[i, 2].set_xlabel('x')
            axes[i, 2].set_ylabel('y')
            plt.colorbar(im2, ax=axes[i, 2])
        
        plt.tight_layout()
        field_plot_path = self.output_dir / "field_comparison.png"
        plt.savefig(field_plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"   å ´æ¯”è¼ƒåœ–å·²ä¿å­˜: {field_plot_path}")
    
    def generate_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        self.logger.info("ğŸ“ ç”Ÿæˆæ¸¬è©¦å ±å‘Š...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'base_config': str(self.base_config['experiment']['name']),
            'epochs': 5000,
            'activations_tested': self.activations,
            'results': {}
        }
        
        # å½™ç¸½çµæœ
        for activation in self.activations:
            result = self.results[activation]
            
            report['results'][activation] = {
                'training_time': result['training_time'],
                'final_total_loss': result['history']['total_loss'][-1],
                'final_residual_loss': result['history']['residual_loss'][-1],
                'final_data_loss': result['history']['data_loss'][-1],
                'metrics': result['evaluation']['metrics'],
                'model_path': result['model_path']
            }
        
        # ä¿å­˜ JSON å ±å‘Š
        report_path = self.output_dir / "benchmark_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"   JSON å ±å‘Šå·²ä¿å­˜: {report_path}")
        
        # ç”Ÿæˆ Markdown å ±å‘Š
        md_report = self._generate_markdown_report(report)
        md_path = self.output_dir / "BENCHMARK_REPORT.md"
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_report)
        
        self.logger.info(f"   Markdown å ±å‘Šå·²ä¿å­˜: {md_path}")
        
        return report
    
    def _generate_markdown_report(self, report: Dict[str, Any]) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
        md = f"""# æ¿€æ´»å‡½æ•¸åŸºæº–æ¸¬è©¦å ±å‘Š

## ğŸ“‹ æ¸¬è©¦é…ç½®

- **æ¸¬è©¦æ™‚é–“**: {report['timestamp']}
- **åŸºç¤é…ç½®**: {report['base_config']}
- **è¨“ç·´è¼ªæ•¸**: {report['epochs']} epochs
- **æ¸¬è©¦æ¿€æ´»å‡½æ•¸**: {', '.join(report['activations_tested'])}

## ğŸ¯ å¯¦é©—ç›®æ¨™

è©•ä¼° **sine, swish, tanh** æ¿€æ´»å‡½æ•¸åœ¨ JHTDB Channel Flow Re=1000 æ•¸æ“šä¸Šçš„è¡¨ç¾ï¼š
- è¨“ç·´ç©©å®šæ€§
- æ”¶æ–‚é€Ÿåº¦
- æœ€çµ‚ç²¾åº¦ï¼ˆL2 ç›¸å°èª¤å·®ï¼‰
- çµ±è¨ˆé‡æº–ç¢ºæ€§

> âš ï¸ **æ³¨æ„**: Loss å¤§å°ä¸ç­‰æ–¼ç²¾åº¦ï¼å¿…é ˆä»¥è©•ä¼°æŒ‡æ¨™ç‚ºæº–ã€‚

---

## ğŸ“Š çµæœç¸½çµ

### è¨“ç·´æ™‚é–“å°æ¯”

| æ¿€æ´»å‡½æ•¸ | è¨“ç·´æ™‚é–“ (ç§’) | è¨“ç·´æ™‚é–“ (åˆ†é˜) |
|---------|-------------|---------------|
"""
        
        for activation in report['activations_tested']:
            t = report['results'][activation]['training_time']
            md += f"| **{activation.upper()}** | {t:.1f} | {t/60:.1f} |\n"
        
        md += "\n### æœ€çµ‚ Loss å°æ¯”\n\n"
        md += "| æ¿€æ´»å‡½æ•¸ | Total Loss | Residual Loss | Data Loss |\n"
        md += "|---------|-----------|--------------|----------|\n"
        
        for activation in report['activations_tested']:
            res = report['results'][activation]
            md += f"| **{activation.upper()}** | {res['final_total_loss']:.6f} | {res['final_residual_loss']:.6f} | {res['final_data_loss']:.6f} |\n"
        
        md += "\n### ğŸ¯ æº–ç¢ºåº¦æŒ‡æ¨™ (é‡è¦ï¼)\n\n"
        md += "| æ¿€æ´»å‡½æ•¸ | U - L2 èª¤å·® | V - L2 èª¤å·® | P - L2 èª¤å·® | U å‡å€¼èª¤å·® |\n"
        md += "|---------|------------|------------|------------|----------|\n"
        
        for activation in report['activations_tested']:
            metrics = report['results'][activation]['metrics']
            md += f"| **{activation.upper()}** | {metrics['u_l2_error']:.4f} ({metrics['u_l2_error']*100:.2f}%) | {metrics['v_l2_error']:.4f} ({metrics['v_l2_error']*100:.2f}%) | {metrics['p_l2_error']:.4f} ({metrics['p_l2_error']*100:.2f}%) | {metrics['u_mean_error']:.4f} ({metrics['u_mean_error']*100:.2f}%) |\n"
        
        md += "\n### RMSE å°æ¯”\n\n"
        md += "| æ¿€æ´»å‡½æ•¸ | U - RMSE | V - RMSE | P - RMSE |\n"
        md += "|---------|---------|---------|----------|\n"
        
        for activation in report['activations_tested']:
            metrics = report['results'][activation]['metrics']
            md += f"| **{activation.upper()}** | {metrics['u_rmse']:.4f} | {metrics['v_rmse']:.4f} | {metrics['p_rmse']:.4f} |\n"
        
        md += "\n---\n\n## ğŸ† æœ€ä½³æ¿€æ´»å‡½æ•¸æ¨è–¦\n\n"
        
        # æ‰¾å‡ºæœ€ä½³æ¿€æ´»å‡½æ•¸
        best_u = min(report['activations_tested'], 
                     key=lambda a: report['results'][a]['metrics']['u_l2_error'])
        best_v = min(report['activations_tested'], 
                     key=lambda a: report['results'][a]['metrics']['v_l2_error'])
        best_p = min(report['activations_tested'], 
                     key=lambda a: report['results'][a]['metrics']['p_l2_error'])
        
        md += f"- **U é€Ÿåº¦å ´**: `{best_u}` (L2 èª¤å·®: {report['results'][best_u]['metrics']['u_l2_error']:.4f})\n"
        md += f"- **V é€Ÿåº¦å ´**: `{best_v}` (L2 èª¤å·®: {report['results'][best_v]['metrics']['v_l2_error']:.4f})\n"
        md += f"- **å£“åŠ›å ´**: `{best_p}` (L2 èª¤å·®: {report['results'][best_p]['metrics']['p_l2_error']:.4f})\n"
        
        md += "\n---\n\n## ğŸ“ˆ å¯è¦–åŒ–åœ–è¡¨\n\n"
        md += "1. **Loss æ›²ç·šæ¯”è¼ƒ**: `loss_comparison.png`\n"
        md += "2. **èª¤å·®æŒ‡æ¨™æ¯”è¼ƒ**: `error_comparison.png`\n"
        md += "3. **é æ¸¬å ´å¯è¦–åŒ–**: `field_comparison.png`\n"
        
        md += "\n---\n\n## ğŸ’¾ æ¨¡å‹æ–‡ä»¶\n\n"
        
        for activation in report['activations_tested']:
            md += f"- **{activation.upper()}**: `{report['results'][activation]['model_path']}`\n"
        
        md += "\n---\n\n## ğŸ”¬ åˆ†æèˆ‡å»ºè­°\n\n"
        md += "### è§€å¯Ÿè¦é»\n\n"
        md += "1. **æ”¶æ–‚ç©©å®šæ€§**: è§€å¯Ÿ loss æ›²ç·šæ˜¯å¦å¹³ç©©ä¸‹é™\n"
        md += "2. **è¨“ç·´æ•ˆç‡**: æ¯”è¼ƒé”åˆ°ç›¸åŒç²¾åº¦æ‰€éœ€çš„ epochs\n"
        md += "3. **æœ€çµ‚ç²¾åº¦**: ä»¥ L2 ç›¸å°èª¤å·®ç‚ºæº–ï¼ˆé loss å€¼ï¼‰\n"
        md += "4. **çµ±è¨ˆæº–ç¢ºæ€§**: U å‡å€¼èª¤å·®åæ˜ å®è§€çµ±è¨ˆä¿æŒèƒ½åŠ›\n"
        
        md += "\n### å»ºè­°\n\n"
        md += "- è‹¥ **sine** è¡¨ç¾æœ€ä½³ï¼šè€ƒæ…®ç”¨æ–¼é«˜é »ç‰¹å¾µæ•æ‰\n"
        md += "- è‹¥ **swish** è¡¨ç¾æœ€ä½³ï¼šå¹³è¡¡æ€§èƒ½èˆ‡ç©©å®šæ€§çš„å¥½é¸æ“‡\n"
        md += "- è‹¥ **tanh** è¡¨ç¾æœ€ä½³ï¼šç¶“å…¸é¸æ“‡ï¼Œç©©å®šå¯é \n"
        
        return md
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦"""
        self.logger.info("ğŸš€ æ¿€æ´»å‡½æ•¸åŸºæº–æ¸¬è©¦é–‹å§‹")
        self.logger.info(f"   æ¸¬è©¦æ¿€æ´»å‡½æ•¸: {', '.join(self.activations)}")
        self.logger.info(f"   è¨“ç·´è¼ªæ•¸: 5000 epochs")
        self.logger.info(f"   è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        
        # ç²å–è¨­å‚™
        device = get_device(self.base_config['experiment']['device'])
        self.logger.info(f"   ä½¿ç”¨è¨­å‚™: {device}")
        
        # è¨“ç·´æ‰€æœ‰æ¿€æ´»å‡½æ•¸
        for activation in self.activations:
            result = self.train_single_activation(activation, device)
            self.results[activation] = result
        
        # è©•ä¼°æ‰€æœ‰æ¨¡å‹
        for activation in self.activations:
            eval_result = self.evaluate_model(
                self.results[activation]['model'],
                self.results[activation]['config'],
                activation,
                device
            )
            self.results[activation]['evaluation'] = eval_result
        
        # ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨
        self.plot_comparison()
        
        # ç”Ÿæˆå ±å‘Š
        report = self.generate_report()
        
        self.logger.info("=" * 80)
        self.logger.info("âœ… æ¿€æ´»å‡½æ•¸åŸºæº–æ¸¬è©¦å®Œæˆï¼")
        self.logger.info(f"   æ‰€æœ‰çµæœå·²ä¿å­˜è‡³: {self.output_dir}")
        self.logger.info("=" * 80)
        
        return report


def main():
    parser = argparse.ArgumentParser(description='æ¿€æ´»å‡½æ•¸åŸºæº–æ¸¬è©¦')
    parser.add_argument('--config', type=str, 
                       default='configs/channel_flow_re1000_K80_wall_balanced.yml',
                       help='åŸºç¤é…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output', type=str, 
                       default='activation_benchmark',
                       help='è¼¸å‡ºç›®éŒ„')
    
    args = parser.parse_args()
    
    # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
    benchmark = ActivationBenchmark(args.config, args.output)
    benchmark.run()


if __name__ == "__main__":
    main()
