#!/usr/bin/env python3
"""
Colab A100 ç’°å¢ƒé…ç½®æª¢æŸ¥
ç”¨æ–¼é©—è­‰ Colab ç’°å¢ƒæ˜¯å¦æ»¿è¶³è¨“ç·´éœ€æ±‚
"""

import sys
import torch
import numpy as np
from pathlib import Path

def check_cuda():
    """æª¢æŸ¥ CUDA å¯ç”¨æ€§"""
    print("=" * 60)
    print("1. CUDA æª¢æŸ¥")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼")
        return False
    
    print(f"âœ… CUDA å¯ç”¨")
    # é¡å‹å¿½ç•¥ï¼štorch.version.cuda åœ¨é‹è¡Œæ™‚å­˜åœ¨ä½†éœæ…‹æª¢æŸ¥ç„¡æ³•è­˜åˆ¥
    cuda_version = getattr(torch.version, 'cuda', 'N/A')  # type: ignore
    print(f"   CUDA ç‰ˆæœ¬: {cuda_version}")
    print(f"   GPU æ•¸é‡: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i}: {props.name}")
        print(f"      è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
        print(f"      è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    return True

def check_memory():
    """æª¢æŸ¥è¨˜æ†¶é«”"""
    print("\n" + "=" * 60)
    print("2. è¨˜æ†¶é«”æª¢æŸ¥")
    print("=" * 60)
    
    if torch.cuda.is_available():
        total = torch.cuda.get_device_properties(0).total_memory
        reserved = torch.cuda.memory_reserved(0)
        allocated = torch.cuda.memory_allocated(0)
        
        print(f"âœ… GPU è¨˜æ†¶é«”:")
        print(f"   ç¸½é‡: {total / 1024**3:.2f} GB")
        print(f"   å·²ä¿ç•™: {reserved / 1024**3:.2f} GB")
        print(f"   å·²åˆ†é…: {allocated / 1024**3:.2f} GB")
        print(f"   å¯ç”¨: {(total - reserved) / 1024**3:.2f} GB")

def check_project_structure():
    """æª¢æŸ¥å°ˆæ¡ˆçµæ§‹"""
    print("\n" + "=" * 60)
    print("3. å°ˆæ¡ˆçµæ§‹æª¢æŸ¥")
    print("=" * 60)
    
    required_dirs = [
        'configs',
        'scripts',
        'pinnx',
        'data/jhtdb/channel_flow_re1000',
        'checkpoints',
        'log',
    ]
    
    all_ok = True
    for dir_path in required_dirs:
        path = Path(dir_path)
        if path.exists():
            print(f"âœ… {dir_path}")
        else:
            print(f"âŒ {dir_path} (ç¼ºå¤±)")
            all_ok = False
    
    return all_ok

def check_config_file():
    """æª¢æŸ¥é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("4. é…ç½®æ–‡ä»¶æª¢æŸ¥")
    print("=" * 60)
    
    config_path = Path('configs/test_normalization_main_2000epochs.yml')
    if config_path.exists():
        print(f"âœ… {config_path}")
        
        # æª¢æŸ¥æ„Ÿæ¸¬é»æ–‡ä»¶
        sensor_path = Path('data/jhtdb/channel_flow_re1000/sensors_K50_qr_pivot.npz')
        if sensor_path.exists():
            data = np.load(sensor_path)
            print(f"âœ… æ„Ÿæ¸¬é»æ–‡ä»¶: {sensor_path}")
            print(f"   åŒ…å«éµ: {list(data.keys())}")
            if 'coords' in data:
                print(f"   æ„Ÿæ¸¬é»æ•¸é‡: {data['coords'].shape[0]}")
        else:
            print(f"âŒ æ„Ÿæ¸¬é»æ–‡ä»¶ç¼ºå¤±: {sensor_path}")
            return False
    else:
        print(f"âŒ é…ç½®æ–‡ä»¶ç¼ºå¤±: {config_path}")
        return False
    
    return True

def check_dependencies():
    """æª¢æŸ¥ä¾è³´å¥—ä»¶"""
    print("\n" + "=" * 60)
    print("5. ä¾è³´å¥—ä»¶æª¢æŸ¥")
    print("=" * 60)
    
    required_packages = {
        'torch': torch.__version__,
        'numpy': np.__version__,
    }
    
    try:
        import yaml
        required_packages['yaml'] = yaml.__version__
    except ImportError:
        print("âŒ PyYAML æœªå®‰è£")
        return False
    
    try:
        import h5py
        required_packages['h5py'] = h5py.__version__
    except ImportError:
        print("âš ï¸  h5py æœªå®‰è£ï¼ˆè‹¥ä½¿ç”¨ HDF5 è³‡æ–™éœ€å®‰è£ï¼‰")
    
    for pkg, ver in required_packages.items():
        print(f"âœ… {pkg}: {ver}")
    
    return True

def estimate_training_resources():
    """ä¼°ç®—è¨“ç·´è³‡æºéœ€æ±‚"""
    print("\n" + "=" * 60)
    print("6. è¨“ç·´è³‡æºä¼°ç®—")
    print("=" * 60)
    
    # æ¨¡å‹åƒæ•¸é‡
    model_params = 296_804
    param_size_mb = model_params * 4 / 1024**2  # float32
    
    # æ‰¹æ¬¡è³‡æ–™
    batch_size = 10_000
    pde_points = 20_000
    boundary_points = 5_000
    
    # ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚ï¼ˆç²—ç•¥ï¼‰
    data_size_mb = (batch_size + pde_points + boundary_points) * 3 * 4 / 1024**2
    gradient_size_mb = param_size_mb * 2  # æ¢¯åº¦ + å„ªåŒ–å™¨ç‹€æ…‹
    
    total_estimate_mb = param_size_mb + data_size_mb + gradient_size_mb
    
    print(f"ğŸ“Š ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚:")
    print(f"   æ¨¡å‹åƒæ•¸: {param_size_mb:.2f} MB ({model_params:,} params)")
    print(f"   æ‰¹æ¬¡è³‡æ–™: {data_size_mb:.2f} MB")
    print(f"   æ¢¯åº¦/å„ªåŒ–å™¨: {gradient_size_mb:.2f} MB")
    print(f"   ç¸½è¨ˆï¼ˆç²—ä¼°ï¼‰: {total_estimate_mb:.2f} MB")
    print(f"   å»ºè­° GPU è¨˜æ†¶é«”: â‰¥ {total_estimate_mb * 4:.0f} MB (å«å®‰å…¨é¤˜é‡)")
    
    if torch.cuda.is_available():
        gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        if gpu_mem_gb * 1024 >= total_estimate_mb * 4:
            print(f"âœ… GPU è¨˜æ†¶é«”å……è¶³ ({gpu_mem_gb:.1f} GB)")
        else:
            print(f"âš ï¸  GPU è¨˜æ†¶é«”å¯èƒ½ä¸è¶³ ({gpu_mem_gb:.1f} GB)")

def main():
    """ä¸»æª¢æŸ¥æµç¨‹"""
    print("ğŸ” Colab A100 ç’°å¢ƒé…ç½®æª¢æŸ¥")
    print()
    
    checks = [
        ("CUDA", check_cuda),
        ("è¨˜æ†¶é«”", check_memory),
        ("å°ˆæ¡ˆçµæ§‹", check_project_structure),
        ("é…ç½®æ–‡ä»¶", check_config_file),
        ("ä¾è³´å¥—ä»¶", check_dependencies),
    ]
    
    results = {}
    for name, check_func in checks:
        try:
            result = check_func()
            results[name] = result if result is not None else True
        except Exception as e:
            print(f"âŒ {name} æª¢æŸ¥å¤±æ•—: {e}")
            results[name] = False
    
    # è³‡æºä¼°ç®—
    estimate_training_resources()
    
    # ç¸½çµ
    print("\n" + "=" * 60)
    print("æª¢æŸ¥ç¸½çµ")
    print("=" * 60)
    
    all_passed = all(results.values())
    for name, passed in results.items():
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {name}")
    
    if all_passed:
        print("\nâœ… æ‰€æœ‰æª¢æŸ¥é€šéï¼å¯ä»¥é–‹å§‹è¨“ç·´ã€‚")
        print("\nğŸ“ å•Ÿå‹•è¨“ç·´æŒ‡ä»¤:")
        print("   python scripts/train.py --cfg configs/test_normalization_main_2000epochs.yml")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æª¢æŸ¥æœªé€šéï¼Œè«‹å…ˆä¿®æ­£å•é¡Œã€‚")
        return 1

if __name__ == "__main__":
    sys.exit(main())
