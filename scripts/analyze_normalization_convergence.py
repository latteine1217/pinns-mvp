#!/usr/bin/env python3
"""
é€²éšåˆ†æï¼šæ¨™æº–åŒ–å°è¨“ç·´æ”¶æ–‚ç‰¹æ€§çš„å½±éŸ¿

åˆ†æå…§å®¹ï¼š
1. æ”¶æ–‚é€Ÿåº¦å°æ¯”ï¼ˆé”åˆ°ç‰¹å®šæå¤±é–¾å€¼æ‰€éœ€ epochï¼‰
2. æå¤±æ›²ç·šå¹³æ»‘åº¦ï¼ˆéœ‡ç›ªå¼·åº¦ï¼‰
3. æ”¶æ–‚ç‡ï¼ˆæå¤±ä¸‹é™æ–œç‡ï¼‰
4. ç”Ÿæˆé¡å¤–è¦–è¦ºåŒ–åœ–è¡¨

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/analyze_normalization_convergence.py
    
è¼¸å‡ºï¼š
    - results/normalization_analysis/convergence_speed.png
    - results/normalization_analysis/smoothness_comparison.png
    - results/normalization_analysis/convergence_rate.png
    - results/normalization_analysis/detailed_analysis_report.json
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from typing import Dict, List, Tuple
import sys

# è¨­ç½®ä¸­æ–‡å­—é«”ï¼ˆmacOSï¼‰
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Heiti TC', 'Songti SC']
plt.rcParams['axes.unicode_minus'] = False


def load_checkpoint_history(checkpoint_path: Path) -> Dict:
    """è¼‰å…¥æª¢æŸ¥é»ä¸­çš„è¨“ç·´æ­·å²"""
    ckpt = torch.load(str(checkpoint_path), map_location='cpu')
    return ckpt['history']


def compute_convergence_epoch(loss_history: List[float], threshold: float) -> int:
    """
    è¨ˆç®—é”åˆ°ç‰¹å®šæå¤±é–¾å€¼æ‰€éœ€çš„ epoch
    
    Args:
        loss_history: æå¤±æ­·å²åˆ—è¡¨
        threshold: æå¤±é–¾å€¼
        
    Returns:
        é”åˆ°é–¾å€¼çš„ epochï¼ˆè‹¥æœªé”åˆ°å‰‡è¿”å› -1ï¼‰
    """
    for epoch, loss in enumerate(loss_history):
        if loss <= threshold:
            return epoch
    return -1


def compute_smoothness_score(loss_history: List[float], window: int = 10) -> float:
    """
    è¨ˆç®—æå¤±æ›²ç·šå¹³æ»‘åº¦ï¼ˆéœ‡ç›ªå¼·åº¦ï¼‰
    
    ä½¿ç”¨æ»‘å‹•çª—å£å…§çš„æ¨™æº–å·®ä½œç‚ºéœ‡ç›ªæŒ‡æ¨™
    
    Args:
        loss_history: æå¤±æ­·å²åˆ—è¡¨
        window: æ»‘å‹•çª—å£å¤§å°
        
    Returns:
        å¹³å‡éœ‡ç›ªå¼·åº¦ï¼ˆè¶Šå°è¶Šå¹³æ»‘ï¼‰
    """
    smoothness_scores = []
    for i in range(window, len(loss_history)):
        window_data = loss_history[i-window:i]
        std = np.std(window_data)
        smoothness_scores.append(std)
    
    return float(np.mean(smoothness_scores))


def compute_convergence_rate(loss_history: List[float], start: int = 0, end: int = -1) -> float:
    """
    è¨ˆç®—æ”¶æ–‚ç‡ï¼ˆæå¤±ä¸‹é™æ–œç‡ï¼‰
    
    ä½¿ç”¨ç·šæ€§æ“¬åˆè¨ˆç®—å°æ•¸ç©ºé–“çš„æ–œç‡
    
    Args:
        loss_history: æå¤±æ­·å²åˆ—è¡¨
        start: èµ·å§‹ epoch
        end: çµæŸ epochï¼ˆ-1 = ä½¿ç”¨å…¨éƒ¨ï¼‰
        
    Returns:
        æ”¶æ–‚ç‡ï¼ˆè² å€¼è¡¨ç¤ºä¸‹é™ï¼‰
    """
    if end == -1:
        end = len(loss_history)
    
    epochs = np.arange(start, end)
    losses = np.array(loss_history[start:end])
    
    # é¿å… log(0) éŒ¯èª¤
    losses = np.maximum(losses, 1e-10)
    log_losses = np.log(losses)
    
    # ç·šæ€§æ“¬åˆ
    coeffs = np.polyfit(epochs, log_losses, 1)
    return float(coeffs[0])  # æ–œç‡


def analyze_convergence_phases(loss_history: List[float]) -> Dict:
    """
    åˆ†æè¨“ç·´çš„ä¸åŒéšæ®µ
    
    Returns:
        Dict with keys: 'early', 'mid', 'late' (å„éšæ®µçš„çµ±è¨ˆè³‡è¨Š)
    """
    total_epochs = len(loss_history)
    early_end = total_epochs // 3
    mid_end = 2 * total_epochs // 3
    
    phases = {
        'early': loss_history[:early_end],
        'mid': loss_history[early_end:mid_end],
        'late': loss_history[mid_end:]
    }
    
    stats = {}
    for phase_name, phase_losses in phases.items():
        stats[phase_name] = {
            'mean': float(np.mean(phase_losses)),
            'std': float(np.std(phase_losses)),
            'min': float(np.min(phase_losses)),
            'max': float(np.max(phase_losses)),
            'rate': float(compute_convergence_rate(phase_losses))
        }
    
    return stats


def plot_convergence_speed(baseline_history: List[float], 
                          normalized_history: List[float],
                          output_path: Path):
    """ç¹ªè£½æ”¶æ–‚é€Ÿåº¦å°æ¯”åœ–ï¼ˆé”åˆ°ä¸åŒé–¾å€¼æ‰€éœ€ epochï¼‰"""
    
    # å®šç¾©å¤šå€‹æå¤±é–¾å€¼
    thresholds = [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005]
    
    baseline_epochs = []
    normalized_epochs = []
    
    for threshold in thresholds:
        baseline_ep = compute_convergence_epoch(baseline_history, threshold)
        normalized_ep = compute_convergence_epoch(normalized_history, threshold)
        baseline_epochs.append(baseline_ep if baseline_ep != -1 else 200)  # æœªé”åˆ°å‰‡ç”¨æœ€å¤§å€¼
        normalized_epochs.append(normalized_ep if normalized_ep != -1 else 200)
    
    # ç¹ªåœ–
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(thresholds))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, baseline_epochs, width, label='Baseline (ç„¡æ¨™æº–åŒ–)', color='#e74c3c')
    bars2 = ax.bar(x + width/2, normalized_epochs, width, label='Normalized (Z-Score)', color='#3498db')
    
    ax.set_xlabel('Loss Threshold', fontsize=12, fontweight='bold')
    ax.set_ylabel('Epochs to Converge', fontsize=12, fontweight='bold')
    ax.set_title('Convergence Speed Comparison\n(Lower is Better)', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([f'{t:.4f}' for t in thresholds], rotation=45)
    ax.legend(fontsize=11)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            if height < 200:  # åªæ¨™è¨˜é”åˆ°é–¾å€¼çš„æƒ…æ³
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{int(height)}',
                       ha='center', va='bottom', fontsize=9)
            else:
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       'N/A',
                       ha='center', va='bottom', fontsize=9, color='red')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Saved: {output_path}")


def plot_smoothness_comparison(baseline_history: List[float], 
                               normalized_history: List[float],
                               output_path: Path):
    """ç¹ªè£½æå¤±æ›²ç·šå¹³æ»‘åº¦å°æ¯”"""
    
    window_sizes = [5, 10, 20, 50]
    baseline_smoothness = []
    normalized_smoothness = []
    
    for window in window_sizes:
        baseline_smoothness.append(compute_smoothness_score(baseline_history, window))
        normalized_smoothness.append(compute_smoothness_score(normalized_history, window))
    
    # ç¹ªåœ–
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(window_sizes))
    width = 0.35
    
    ax.bar(x - width/2, baseline_smoothness, width, label='Baseline (ç„¡æ¨™æº–åŒ–)', color='#e74c3c')
    ax.bar(x + width/2, normalized_smoothness, width, label='Normalized (Z-Score)', color='#3498db')
    
    ax.set_xlabel('Smoothing Window Size (epochs)', fontsize=12, fontweight='bold')
    ax.set_ylabel('Oscillation Intensity (Std Dev)', fontsize=12, fontweight='bold')
    ax.set_title('Training Stability Comparison\n(Lower is More Stable)', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(window_sizes)
    ax.legend(fontsize=11)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_yscale('log')  # å°æ•¸å°ºåº¦æ›´æ¸…æ¥š
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Saved: {output_path}")


def plot_convergence_rate(baseline_history: List[float], 
                          normalized_history: List[float],
                          output_path: Path):
    """ç¹ªè£½ä¸åŒéšæ®µçš„æ”¶æ–‚ç‡å°æ¯”"""
    
    baseline_phases = analyze_convergence_phases(baseline_history)
    normalized_phases = analyze_convergence_phases(normalized_history)
    
    phases = ['early', 'mid', 'late']
    baseline_rates = [baseline_phases[p]['rate'] for p in phases]
    normalized_rates = [normalized_phases[p]['rate'] for p in phases]
    
    # ç¹ªåœ–
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(phases))
    width = 0.35
    
    ax.bar(x - width/2, baseline_rates, width, label='Baseline (ç„¡æ¨™æº–åŒ–)', color='#e74c3c')
    ax.bar(x + width/2, normalized_rates, width, label='Normalized (Z-Score)', color='#3498db')
    
    ax.set_xlabel('Training Phase', fontsize=12, fontweight='bold')
    ax.set_ylabel('Convergence Rate (log loss / epoch)', fontsize=12, fontweight='bold')
    ax.set_title('Phase-wise Convergence Rate Comparison\n(More Negative = Faster Convergence)', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(['Early (0-33%)', 'Mid (33-66%)', 'Late (66-100%)'])
    ax.legend(fontsize=11)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… Saved: {output_path}")


def generate_detailed_report(baseline_history: List[float], 
                            normalized_history: List[float],
                            output_path: Path):
    """ç”Ÿæˆè©³ç´°åˆ†æå ±å‘Šï¼ˆJSON æ ¼å¼ï¼‰"""
    
    report = {
        "analysis_date": "2025-10-17",
        "baseline": {
            "total_epochs": len(baseline_history),
            "final_loss": float(baseline_history[-1]),
            "best_loss": float(min(baseline_history)),
            "smoothness_score_10ep": float(compute_smoothness_score(baseline_history, 10)),
            "convergence_rate_overall": float(compute_convergence_rate(baseline_history)),
            "phases": analyze_convergence_phases(baseline_history),
            "epochs_to_threshold": {
                "0.01": compute_convergence_epoch(baseline_history, 0.01),
                "0.005": compute_convergence_epoch(baseline_history, 0.005),
                "0.001": compute_convergence_epoch(baseline_history, 0.001)
            }
        },
        "normalized": {
            "total_epochs": len(normalized_history),
            "final_loss": float(normalized_history[-1]),
            "best_loss": float(min(normalized_history)),
            "smoothness_score_10ep": float(compute_smoothness_score(normalized_history, 10)),
            "convergence_rate_overall": float(compute_convergence_rate(normalized_history)),
            "phases": analyze_convergence_phases(normalized_history),
            "epochs_to_threshold": {
                "0.01": compute_convergence_epoch(normalized_history, 0.01),
                "0.005": compute_convergence_epoch(normalized_history, 0.005),
                "0.001": compute_convergence_epoch(normalized_history, 0.001)
            }
        },
        "comparison": {
            "final_loss_improvement": f"{(1 - normalized_history[-1] / baseline_history[-1]) * 100:.1f}%",
            "smoothness_improvement": f"{(1 - compute_smoothness_score(normalized_history, 10) / compute_smoothness_score(baseline_history, 10)) * 100:.1f}%",
            "convergence_rate_ratio": float(compute_convergence_rate(normalized_history) / compute_convergence_rate(baseline_history))
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Saved: {output_path}")
    return report


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    
    # é…ç½®è·¯å¾‘
    baseline_ckpt = Path("checkpoints/quick_val_baseline/best_model.pth")
    normalized_ckpt = Path("checkpoints/quick_val_normalized/best_model.pth")
    output_dir = Path("results/normalization_analysis")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è¼‰å…¥è¨“ç·´æ­·å²
    print("ğŸ“‚ Loading training histories...")
    baseline_history = load_checkpoint_history(baseline_ckpt)['total_loss']
    normalized_history = load_checkpoint_history(normalized_ckpt)['total_loss']
    
    print(f"   - Baseline: {len(baseline_history)} epochs")
    print(f"   - Normalized: {len(normalized_history)} epochs")
    
    # ç”Ÿæˆè¦–è¦ºåŒ–
    print("\nğŸ“Š Generating visualizations...")
    
    plot_convergence_speed(
        baseline_history, 
        normalized_history,
        output_dir / "convergence_speed.png"
    )
    
    plot_smoothness_comparison(
        baseline_history,
        normalized_history,
        output_dir / "smoothness_comparison.png"
    )
    
    plot_convergence_rate(
        baseline_history,
        normalized_history,
        output_dir / "convergence_rate.png"
    )
    
    # ç”Ÿæˆè©³ç´°å ±å‘Š
    print("\nğŸ“ Generating detailed report...")
    report = generate_detailed_report(
        baseline_history,
        normalized_history,
        output_dir / "detailed_analysis_report.json"
    )
    
    # æ‰“å°é—œéµçµè«–
    print("\n" + "="*60)
    print("ğŸ¯ Key Findings:")
    print("="*60)
    print(f"âœ… Final Loss Improvement: {report['comparison']['final_loss_improvement']}")
    print(f"âœ… Smoothness Improvement: {report['comparison']['smoothness_improvement']}")
    print(f"âœ… Convergence Rate Ratio: {report['comparison']['convergence_rate_ratio']:.2f}x")
    print(f"\nâœ… Epochs to reach 0.001 loss:")
    print(f"   - Baseline: {report['baseline']['epochs_to_threshold']['0.001']} epochs")
    print(f"   - Normalized: {report['normalized']['epochs_to_threshold']['0.001']} epochs")
    print("\nğŸ“ All outputs saved to: results/normalization_analysis/")
    print("="*60)


if __name__ == "__main__":
    main()
