#!/usr/bin/env python3
"""
è©•ä¼°èª²ç¨‹è¨“ç·´å®Œæˆçš„æª¢æŸ¥é»
æ­£ç¢ºè¼‰å…¥ ManualScalingWrapper åŒ…è£çš„æ¨¡å‹ä¸¦è¨ˆç®—èª¤å·®
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Tuple

import numpy as np
import torch
import torch.nn as nn
import yaml
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # ç„¡é¡¯ç¤ºæ¨¡å¼

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from pinnx.models import PINNNet, create_pinn_model
from pinnx.models.wrappers import ManualScalingWrapper


def setup_logging(level: str = "info") -> logging.Logger:
    """è¨­ç½®æ—¥èªŒç³»çµ±"""
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f'Invalid log level: {level}')
    
    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    return logging.getLogger(__name__)


def load_config(config_path: str) -> Dict[str, Any]:
    """è¼‰å…¥YAMLé…ç½®æª”æ¡ˆ"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def create_model_from_config(config: Dict[str, Any], device: torch.device) -> nn.Module:
    """å¾é…ç½®å‰µå»ºæ¨¡å‹ï¼ˆè¤‡è£½è‡ª train.pyï¼‰
    
    æ³¨æ„ï¼šå¿…é ˆèˆ‡è¨“ç·´æ™‚çš„æ¨¡å‹å»ºç«‹é‚è¼¯å®Œå…¨ä¸€è‡´ï¼ŒåŒ…æ‹¬ wrapper
    """
    model_cfg = config['model']
    
    # å»ºç«‹åŸºç¤æ¨¡å‹
    fourier_cfg = model_cfg.get('fourier_features', {})
    fourier_m = fourier_cfg.get('fourier_m', model_cfg.get('fourier_m', 32))
    fourier_sigma = fourier_cfg.get('fourier_sigma', model_cfg.get('fourier_sigma', 1.0))
    
    if model_cfg.get('type') == 'fourier_vs_mlp':
        base_model_cfg = {
            **model_cfg,
            'fourier_m': fourier_m,
            'fourier_sigma': fourier_sigma,
            'use_fourier': model_cfg.get('use_fourier', True)
        }
        base_model = create_pinn_model(base_model_cfg).to(device)
    else:
        base_model = PINNNet(
            in_dim=model_cfg['in_dim'],
            out_dim=model_cfg['out_dim'],
            width=model_cfg['width'],
            depth=model_cfg['depth'],
            activation=model_cfg['activation'],
            use_fourier=True,
            fourier_m=fourier_m,
            fourier_sigma=fourier_sigma
        ).to(device)
    
    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨ scaling wrapper
    scaling_cfg = model_cfg.get('scaling', {})
    scaling_enabled = bool(scaling_cfg)
    
    if scaling_enabled:
        # å¾é…ç½®ä¸­æå–è¼¸å…¥è¼¸å‡ºç¯„åœ
        input_x_range = tuple(scaling_cfg.get('input_norm', {}).get('x', [0.0, 25.13]))
        input_y_range = tuple(scaling_cfg.get('input_norm', {}).get('y', [-1.0, 1.0]))
        
        input_scales = {
            'x': input_x_range,
            'y': input_y_range
        }
        
        # è¼¸å‡ºç¯„åœ
        output_norm = scaling_cfg.get('output_norm', {})
        output_scales = {
            'u': tuple(output_norm.get('u', [0.0, 16.5])),
            'v': tuple(output_norm.get('v', [-0.6, 0.6])),
            'p': tuple(output_norm.get('p', [-85.0, 3.0]))
        }
        
        # å»ºç«‹åŒ…è£æ¨¡å‹
        model = ManualScalingWrapper(
            base_model, 
            input_ranges=input_scales,
            output_ranges=output_scales
        ).to(device)
        
        logging.info(f"âœ… Created ManualScalingWrapper")
        logging.info(f"   Input ranges: {input_scales}")
        logging.info(f"   Output ranges: {output_scales}")
    else:
        model = base_model
        logging.info("Using base model without wrapper")
    
    return model


def load_checkpoint(checkpoint_path: str, model: nn.Module) -> Tuple[int, float, Dict]:
    """è¼‰å…¥æª¢æŸ¥é»åˆ°æ¨¡å‹"""
    logging.info(f"Loading checkpoint from: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # è¼‰å…¥æ¨¡å‹æ¬Šé‡
    model.load_state_dict(checkpoint['model_state_dict'])
    
    epoch = checkpoint['epoch']
    loss = checkpoint['loss']
    config = checkpoint.get('config', {})
    
    logging.info(f"âœ… Checkpoint loaded successfully")
    logging.info(f"   Epoch: {epoch}")
    logging.info(f"   Training Loss: {loss:.6f}")
    
    return epoch, loss, config


def load_sensor_data(sensor_file: str, device: torch.device) -> Dict[str, torch.Tensor]:
    """è¼‰å…¥æ„Ÿæ¸¬å™¨æ•¸æ“š"""
    logging.info(f"Loading sensor data from: {sensor_file}")
    
    data = np.load(sensor_file)
    
    # æå–åº§æ¨™å’ŒçœŸå¯¦å€¼
    coords = data['sensor_points']  # (K, 2)
    u_true = data['sensor_u'].reshape(-1, 1)  # (K, 1)
    v_true = data['sensor_v'].reshape(-1, 1)
    p_true = data['sensor_p'].reshape(-1, 1)
    
    # è½‰æ›ç‚º tensorï¼ˆä¸æ¨™æº–åŒ–ï¼Œæ¨¡å‹æœƒè™•ç†ï¼‰
    x = torch.from_numpy(coords[:, 0:1]).float().to(device)
    y = torch.from_numpy(coords[:, 1:2]).float().to(device)
    
    sensor_data = {
        'x': x,
        'y': y,
        'coords': torch.cat([x, y], dim=1),  # (K, 2)
        'u_true': torch.from_numpy(u_true).float().to(device),
        'v_true': torch.from_numpy(v_true).float().to(device),
        'p_true': torch.from_numpy(p_true).float().to(device)
    }
    
    logging.info(f"âœ… Sensor data loaded: {len(x)} points")
    logging.info(f"   X range: [{x.min():.6f}, {x.max():.6f}]")
    logging.info(f"   Y range: [{y.min():.6f}, {y.max():.6f}]")
    logging.info(f"   U range: [{u_true.min():.6f}, {u_true.max():.6f}]")
    logging.info(f"   V range: [{v_true.min():.6f}, {v_true.max():.6f}]")
    logging.info(f"   P range: [{p_true.min():.6f}, {p_true.max():.6f}]")
    
    return sensor_data


def evaluate_model(model: nn.Module, sensor_data: Dict[str, torch.Tensor]) -> Dict[str, float]:
    """è©•ä¼°æ¨¡å‹åœ¨æ„Ÿæ¸¬é»ä¸Šçš„èª¤å·®"""
    model.eval()
    
    with torch.no_grad():
        # æ¨¡å‹é æ¸¬ï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹åº§æ¨™ï¼Œwrapper æœƒè™•ç†æ¨™æº–åŒ–ï¼‰
        coords = sensor_data['coords']  # (K, 2)
        pred = model(coords)  # (K, 3)
        
        u_pred = pred[:, 0:1]
        v_pred = pred[:, 1:2]
        p_pred = pred[:, 2:3]
    
    # æå–çœŸå¯¦å€¼
    u_true = sensor_data['u_true']
    v_true = sensor_data['v_true']
    p_true = sensor_data['p_true']
    
    # è¨ˆç®—ç›¸å° L2 èª¤å·®
    def relative_l2_error(pred, true):
        numerator = torch.sqrt(torch.mean((pred - true)**2))
        denominator = torch.sqrt(torch.mean(true**2))
        return (numerator / (denominator + 1e-12)).item()
    
    u_error = relative_l2_error(u_pred, u_true)
    v_error = relative_l2_error(v_pred, v_true)
    p_error = relative_l2_error(p_pred, p_true)
    
    # è¨ˆç®—çµ•å°èª¤å·®ï¼ˆRMSEï¼‰
    u_rmse = torch.sqrt(torch.mean((u_pred - u_true)**2)).item()
    v_rmse = torch.sqrt(torch.mean((v_pred - v_true)**2)).item()
    p_rmse = torch.sqrt(torch.mean((p_pred - p_true)**2)).item()
    
    # è¨ˆç®—æœ€å¤§èª¤å·®
    u_max_error = torch.max(torch.abs(u_pred - u_true)).item()
    v_max_error = torch.max(torch.abs(v_pred - v_true)).item()
    p_max_error = torch.max(torch.abs(p_pred - p_true)).item()
    
    results = {
        'u_rel_l2': u_error * 100,  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
        'v_rel_l2': v_error * 100,
        'p_rel_l2': p_error * 100,
        'u_rmse': u_rmse,
        'v_rmse': v_rmse,
        'p_rmse': p_rmse,
        'u_max_error': u_max_error,
        'v_max_error': v_max_error,
        'p_max_error': p_max_error
    }
    
    return results


def visualize_predictions(model: nn.Module, 
                         sensor_data: Dict[str, torch.Tensor],
                         output_dir: str = "./evaluation_results"):
    """å¯è¦–åŒ–é æ¸¬çµæœ"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    model.eval()
    
    with torch.no_grad():
        coords = sensor_data['coords']
        pred = model(coords)
        
        u_pred = pred[:, 0].cpu().numpy()
        v_pred = pred[:, 1].cpu().numpy()
        p_pred = pred[:, 2].cpu().numpy()
    
    # æå–çœŸå¯¦å€¼
    u_true = sensor_data['u_true'].cpu().numpy().flatten()
    v_true = sensor_data['v_true'].cpu().numpy().flatten()
    p_true = sensor_data['p_true'].cpu().numpy().flatten()
    
    # å‰µå»ºå°æ¯”åœ–
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    # U é€Ÿåº¦
    axes[0].scatter(u_true, u_pred, alpha=0.6, s=50, label='Predictions')
    u_min, u_max = u_true.min(), u_true.max()
    axes[0].plot([u_min, u_max], [u_min, u_max], 'r--', lw=2, label='Perfect fit')
    axes[0].set_xlabel('True U', fontsize=12)
    axes[0].set_ylabel('Predicted U', fontsize=12)
    axes[0].set_title('U Velocity Comparison', fontsize=14)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # V é€Ÿåº¦
    axes[1].scatter(v_true, v_pred, alpha=0.6, s=50, label='Predictions')
    v_min, v_max = v_true.min(), v_true.max()
    axes[1].plot([v_min, v_max], [v_min, v_max], 'r--', lw=2, label='Perfect fit')
    axes[1].set_xlabel('True V', fontsize=12)
    axes[1].set_ylabel('Predicted V', fontsize=12)
    axes[1].set_title('V Velocity Comparison', fontsize=14)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # å£“åŠ›
    axes[2].scatter(p_true, p_pred, alpha=0.6, s=50, label='Predictions')
    p_min, p_max = p_true.min(), p_true.max()
    axes[2].plot([p_min, p_max], [p_min, p_max], 'r--', lw=2, label='Perfect fit')
    axes[2].set_xlabel('True P', fontsize=12)
    axes[2].set_ylabel('Predicted P', fontsize=12)
    axes[2].set_title('Pressure Comparison', fontsize=14)
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = f"{output_dir}/prediction_comparison.png"
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    logging.info(f"âœ… Visualization saved to: {plot_path}")
    plt.close()


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description='Evaluate Curriculum Training Checkpoint')
    parser.add_argument('--checkpoint', type=str, 
                       default='checkpoints/pinnx_channel_flow_curriculum_ic_latest.pth',
                       help='Path to checkpoint file')
    parser.add_argument('--config', type=str,
                       default='configs/channel_flow_curriculum_4stage_ic.yml',
                       help='Path to config file')
    parser.add_argument('--sensor-file', type=str,
                       default='data/jhtdb/channel_flow_re1000/sensors_K80_wall_balanced.npz',
                       help='Path to sensor data file')
    parser.add_argument('--output-dir', type=str,
                       default='./evaluation_results',
                       help='Output directory for results')
    parser.add_argument('--visualize', action='store_true',
                       help='Generate visualization plots')
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    logger = setup_logging('info')
    
    logger.info("=" * 80)
    logger.info("ğŸ“Š èª²ç¨‹è¨“ç·´æª¢æŸ¥é»è©•ä¼°")
    logger.info("=" * 80)
    
    # è¼‰å…¥é…ç½®
    config = load_config(args.config)
    device = torch.device('cpu')  # è©•ä¼°ä½¿ç”¨ CPU å³å¯
    
    # å‰µå»ºæ¨¡å‹ï¼ˆå¿…é ˆèˆ‡è¨“ç·´æ™‚ä¸€è‡´ï¼‰
    model = create_model_from_config(config, device)
    
    # è¼‰å…¥æª¢æŸ¥é»
    epoch, loss, ckpt_config = load_checkpoint(args.checkpoint, model)
    
    # è¼‰å…¥æ„Ÿæ¸¬å™¨æ•¸æ“š
    sensor_data = load_sensor_data(args.sensor_file, device)
    
    # è©•ä¼°æ¨¡å‹
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ” é–‹å§‹è©•ä¼°...")
    logger.info("=" * 80)
    
    results = evaluate_model(model, sensor_data)
    
    # è¼¸å‡ºçµæœ
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“ˆ è©•ä¼°çµæœ")
    logger.info("=" * 80)
    logger.info(f"è¨“ç·´è¼ªæ•¸: {epoch}")
    logger.info(f"è¨“ç·´æå¤±: {loss:.6f}")
    logger.info("")
    logger.info("ç›¸å° L2 èª¤å·® (%)ï¼š")
    logger.info(f"  U é€Ÿåº¦: {results['u_rel_l2']:.2f}%")
    logger.info(f"  V é€Ÿåº¦: {results['v_rel_l2']:.2f}%")
    logger.info(f"  å£“åŠ›:   {results['p_rel_l2']:.2f}%")
    logger.info("")
    logger.info("RMSE èª¤å·®ï¼š")
    logger.info(f"  U é€Ÿåº¦: {results['u_rmse']:.6f}")
    logger.info(f"  V é€Ÿåº¦: {results['v_rmse']:.6f}")
    logger.info(f"  å£“åŠ›:   {results['p_rmse']:.6f}")
    logger.info("")
    logger.info("æœ€å¤§çµ•å°èª¤å·®ï¼š")
    logger.info(f"  U é€Ÿåº¦: {results['u_max_error']:.6f}")
    logger.info(f"  V é€Ÿåº¦: {results['v_max_error']:.6f}")
    logger.info(f"  å£“åŠ›:   {results['p_max_error']:.6f}")
    logger.info("=" * 80)
    
    # æˆæ•—åˆ¤å®šï¼ˆç›®æ¨™: < 15%ï¼‰
    threshold = 15.0
    all_pass = all([
        results['u_rel_l2'] < threshold,
        results['v_rel_l2'] < threshold,
        results['p_rel_l2'] < threshold
    ])
    
    if all_pass:
        logger.info("âœ… è©•ä¼°é€šéï¼æ‰€æœ‰å ´çš„ç›¸å°èª¤å·® < 15%")
    else:
        logger.warning("âš ï¸  éƒ¨åˆ†å ´æœªé”æ¨™ï¼ˆç›®æ¨™ < 15%ï¼‰")
    
    # å¯è¦–åŒ–
    if args.visualize:
        logger.info("\nç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨...")
        visualize_predictions(model, sensor_data, args.output_dir)
    
    # ä¿å­˜çµæœåˆ° JSON
    import json
    results_file = f"{args.output_dir}/evaluation_results.json"
    with open(results_file, 'w') as f:
        json.dump({
            'checkpoint': args.checkpoint,
            'epoch': epoch,
            'training_loss': float(loss),
            'metrics': {k: float(v) for k, v in results.items()},
            'threshold': threshold,
            'passed': all_pass
        }, f, indent=2)
    logger.info(f"âœ… çµæœå·²ä¿å­˜è‡³: {results_file}")


if __name__ == "__main__":
    main()
