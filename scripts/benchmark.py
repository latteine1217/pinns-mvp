#!/usr/bin/env python3
"""
è‡ªå‹•åŒ–æ€§èƒ½åŸºæº–æ¸¬è©¦è…³æœ¬
æ”¶é›†è¨“ç·´æ€§èƒ½ã€è¨˜æ†¶é«”ä½¿ç”¨ã€æ•¸å€¼ç©©å®šæ€§ç­‰å…¨é¢æ•¸æ“š
ç”¨æ–¼å»ºç«‹å¯é‡ç¾çš„æ€§èƒ½åŸºç·š
"""

import os
import sys
import time
import json
import yaml
import logging
import platform
import tracemalloc
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Tuple
import argparse

import torch
import psutil
import numpy as np

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦é¡åˆ¥"""
    
    def __init__(self, config_path: str, task_dir: str):
        self.config_path = config_path
        self.task_dir = Path(task_dir)
        self.task_dir.mkdir(exist_ok=True)
        
        # è¼‰å…¥é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # åˆå§‹åŒ–çµæœå„²å­˜
        self.results = {
            'hardware_info': self._collect_hardware_info(),
            'environment_info': self._collect_environment_info(),
            'timestamp': datetime.now().isoformat(),
            'benchmarks': {}
        }
        
    def _collect_hardware_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç¡¬é«”è³‡è¨Š"""
        try:
            cpu_freq = psutil.cpu_freq()
            cpu_freq_max = "Unknown"
            if cpu_freq and cpu_freq.max:
                cpu_freq_max = f"{cpu_freq.max:.1f}"
            
            return {
                'platform': platform.platform(),
                'processor': platform.processor(),
                'architecture': platform.architecture()[0],
                'cpu_count': psutil.cpu_count(logical=True),
                'cpu_freq_max': cpu_freq_max,
                'memory_total_gb': psutil.virtual_memory().total / (1024**3),
                'python_version': platform.python_version(),
            }
        except Exception as e:
            logger.warning(f"Failed to collect some hardware info: {e}")
            return {'error': str(e)}
    
    def _collect_environment_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç’°å¢ƒè³‡è¨Š"""
        cuda_version = "Not available"
        if torch.cuda.is_available():
            try:
                import subprocess
                result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
                if result.returncode == 0:
                    cuda_version = "Available"
                else:
                    cuda_version = "Unknown"
            except:
                cuda_version = "Unknown"
        
        device_name = "CPU only"
        if torch.cuda.is_available():
            try:
                device_name = torch.cuda.get_device_name(0)
            except:
                device_name = "GPU available"
        
        return {
            'torch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'cuda_version': cuda_version,
            'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'device_name': device_name,
        }
    
    def benchmark_single_training(self, seed: int = 42) -> Dict[str, Any]:
        """å–®æ¬¡è¨“ç·´åŸºæº–æ¸¬è©¦"""
        logger.info(f"Running single training benchmark with seed {seed}")
        
        # è¨­å®šéš¨æ©Ÿç¨®å­
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        # é–‹å§‹è¨˜æ†¶é«”è¿½è¹¤
        tracemalloc.start()
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / (1024**2)  # MB
        
        # åŸ·è¡Œè¨“ç·´
        start_time = time.time()
        try:
            # ä¿®æ”¹é…ç½®ä»¥é€²è¡ŒåŸºæº–æ¸¬è©¦
            benchmark_config = self.config.copy()
            benchmark_config['experiment']['seed'] = seed
            benchmark_config['training']['max_epochs'] = 1000  # è¼ƒçŸ­çš„æ¸¬è©¦
            benchmark_config['logging']['log_freq'] = 50
            
            # åŸ·è¡Œè¨“ç·´
            results = self._run_training_with_monitoring(benchmark_config)
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            return {'error': str(e)}
        
        end_time = time.time()
        
        # åœæ­¢è¨˜æ†¶é«”è¿½è¹¤
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()
        
        final_memory = process.memory_info().rss / (1024**2)  # MB
        
        return {
            'seed': seed,
            'total_time': end_time - start_time,
            'memory_initial_mb': initial_memory,
            'memory_final_mb': final_memory,
            'memory_peak_mb': peak / (1024**2),
            'memory_used_mb': current / (1024**2),
            **results
        }
    
    def _run_training_with_monitoring(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œè¨“ç·´ä¸¦ç›£æ§é—œéµæŒ‡æ¨™"""
        # ç›´æ¥å°å…¥æ‰€éœ€çš„æ¨¡çµ„
        from pinnx.models.fourier_mlp import PINNNet
        from pinnx.models.wrappers import ScaledPINNWrapper
        
        # å»ºç«‹æ¨¡å‹
        device = torch.device(config['experiment']['device'] if torch.cuda.is_available() else 'cpu')
        
        # å‰µå»º PINN æ¨¡å‹
        model = PINNNet(
            in_dim=config['model']['in_dim'],
            out_dim=config['model']['out_dim'],
            width=config['model']['width'],
            depth=config['model']['depth'],
            activation=config['model']['activation'],
            fourier_m=config['model']['fourier_m'],
            fourier_sigma=config['model']['fourier_sigma']
        ).to(device)
        
        # åŒ…è£ç‚º ScaledPINNWrapper
        if config['model']['scaling']['learnable']:
            model = ScaledPINNWrapper(model).to(device)
        
        # è¨ˆç®—åƒæ•¸æ•¸é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # å‰µå»ºç‰©ç†æ¨¡çµ„ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        nu = config['physics']['nu']
        rho = config['physics']['rho']
        
        # å»ºç«‹å„ªåŒ–å™¨
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=config['training']['lr'],
            weight_decay=config['training']['weight_decay']
        )
        
        # ç”Ÿæˆè¨“ç·´è³‡æ–™
        domain = config['physics']['domain']
        n_pde = config['training']['sampling']['pde_points']
        n_bc = config['training']['sampling']['boundary_points']
        
        # PDE é»
        x_pde = torch.rand(n_pde, 1) * (domain['x_range'][1] - domain['x_range'][0]) + domain['x_range'][0]
        y_pde = torch.rand(n_pde, 1) * (domain['y_range'][1] - domain['y_range'][0]) + domain['y_range'][0]
        t_pde = torch.rand(n_pde, 1) * (domain['t_range'][1] - domain['t_range'][0]) + domain['t_range'][0]
        
        # é‚Šç•Œé»
        x_bc = torch.cat([
            torch.full((n_bc//4, 1), domain['x_range'][0]),  # å·¦é‚Šç•Œ
            torch.full((n_bc//4, 1), domain['x_range'][1]),  # å³é‚Šç•Œ
            torch.rand(n_bc//2, 1) * (domain['x_range'][1] - domain['x_range'][0]) + domain['x_range'][0]  # ä¸Šä¸‹é‚Šç•Œ
        ])
        y_bc = torch.cat([
            torch.rand(n_bc//4, 1) * (domain['y_range'][1] - domain['y_range'][0]) + domain['y_range'][0],  # å·¦é‚Šç•Œ
            torch.rand(n_bc//4, 1) * (domain['y_range'][1] - domain['y_range'][0]) + domain['y_range'][0],  # å³é‚Šç•Œ
            torch.cat([
                torch.full((n_bc//4, 1), domain['y_range'][0]),  # ä¸‹é‚Šç•Œ
                torch.full((n_bc//4, 1), domain['y_range'][1])   # ä¸Šé‚Šç•Œ
            ])
        ])
        t_bc = torch.rand(n_bc, 1) * (domain['t_range'][1] - domain['t_range'][0]) + domain['t_range'][0]
        
        # ç§»å‹•åˆ°è¨­å‚™
        x_pde, y_pde, t_pde = x_pde.to(device), y_pde.to(device), t_pde.to(device)
        x_bc, y_bc, t_bc = x_bc.to(device), y_bc.to(device), t_bc.to(device)
        
        # è¨“ç·´å¾ªç’°ç›£æ§
        epoch_times = []
        losses = []
        final_loss = None
        converged_epoch = None
        
        max_epochs = config['training']['max_epochs']
        log_freq = config['logging']['log_freq']
        
        for epoch in range(max_epochs):
            epoch_start = time.time()
            
            optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            pde_coords = torch.cat([t_pde, x_pde, y_pde], dim=1)
            bc_coords = torch.cat([t_bc, x_bc, y_bc], dim=1)
            
            # PDE æå¤±ï¼ˆç°¡åŒ–ç‰ˆ NS æ®˜å·®ï¼‰
            pde_coords.requires_grad_(True)
            pde_pred = model(pde_coords)
            u, v, p = pde_pred[:, 0], pde_pred[:, 1], pde_pred[:, 2]
            
            # è¨ˆç®—å°æ•¸
            u_t = torch.autograd.grad(u.sum(), pde_coords, create_graph=True)[0][:, 0]
            u_x = torch.autograd.grad(u.sum(), pde_coords, create_graph=True)[0][:, 1]
            u_y = torch.autograd.grad(u.sum(), pde_coords, create_graph=True)[0][:, 2]
            u_xx = torch.autograd.grad(u_x.sum(), pde_coords, create_graph=True)[0][:, 1]
            u_yy = torch.autograd.grad(u_y.sum(), pde_coords, create_graph=True)[0][:, 2]
            
            v_t = torch.autograd.grad(v.sum(), pde_coords, create_graph=True)[0][:, 0]
            v_x = torch.autograd.grad(v.sum(), pde_coords, create_graph=True)[0][:, 1]
            v_y = torch.autograd.grad(v.sum(), pde_coords, create_graph=True)[0][:, 2]
            v_xx = torch.autograd.grad(v_x.sum(), pde_coords, create_graph=True)[0][:, 1]
            v_yy = torch.autograd.grad(v_y.sum(), pde_coords, create_graph=True)[0][:, 2]
            
            p_x = torch.autograd.grad(p.sum(), pde_coords, create_graph=True)[0][:, 1]
            p_y = torch.autograd.grad(p.sum(), pde_coords, create_graph=True)[0][:, 2]
            
            # NS æ®˜å·®
            f_u = u_t + u * u_x + v * u_y + p_x / rho - nu * (u_xx + u_yy)
            f_v = v_t + u * v_x + v * v_y + p_y / rho - nu * (v_xx + v_yy)
            f_c = u_x + v_y  # é€£çºŒæ€§æ–¹ç¨‹
            
            pde_loss = torch.mean(f_u**2 + f_v**2 + f_c**2)
            
            # é‚Šç•Œæ¢ä»¶æå¤± (ç°¡å–®é›¶é€Ÿåº¦é‚Šç•Œ)
            bc_pred = model(bc_coords)
            bc_loss = torch.mean(bc_pred[:, :2]**2)  # u, v = 0 at boundaries
            
            # ç¸½æå¤±
            total_loss = (
                config['losses']['residual_weight'] * pde_loss +
                config['losses']['boundary_weight'] * bc_loss
            )
            
            # åå‘å‚³æ’­
            total_loss.backward()
            optimizer.step()
            
            epoch_time = time.time() - epoch_start
            epoch_times.append(epoch_time)
            losses.append(total_loss.item())
            
            # æª¢æŸ¥æ”¶æ–‚
            if total_loss.item() < 1e-6:
                converged_epoch = epoch
                final_loss = total_loss.item()
                break
            
            # æ—¥èªŒè¼¸å‡º
            if epoch % log_freq == 0:
                logger.info(f"Epoch {epoch:6d} | Loss: {total_loss.item():.6f} | Time: {epoch_time:.3f}s")
        
        # å¦‚æœæ²’æœ‰æ”¶æ–‚ï¼Œè¨˜éŒ„æœ€çµ‚æå¤±
        if final_loss is None:
            final_loss = losses[-1]
            converged_epoch = max_epochs
        
        return {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'final_loss': final_loss,
            'converged_epoch': converged_epoch,
            'avg_epoch_time': np.mean(epoch_times),
            'min_epoch_time': np.min(epoch_times),
            'max_epoch_time': np.max(epoch_times),
            'loss_history': losses,
            'epoch_times': epoch_times,
            'converged': converged_epoch < max_epochs
        }
    
    def benchmark_inference_performance(self, model_path: str | None = None) -> Dict[str, Any]:
        """æ¨ç†æ€§èƒ½åŸºæº–æ¸¬è©¦"""
        logger.info("Running inference performance benchmark")
        
        # ç›´æ¥å°å…¥æ‰€éœ€çš„æ¨¡çµ„
        from pinnx.models.fourier_mlp import PINNNet
        from pinnx.models.wrappers import ScaledPINNWrapper
        
        # å‰µå»ºæ¨¡å‹ï¼ˆå¦‚æœæ²’æœ‰æä¾›é è¨“ç·´æ¨¡å‹ï¼‰
        device = torch.device(self.config['experiment']['device'] if torch.cuda.is_available() else 'cpu')
        
        model = PINNNet(
            in_dim=self.config['model']['in_dim'],
            out_dim=self.config['model']['out_dim'],
            width=self.config['model']['width'],
            depth=self.config['model']['depth'],
            activation=self.config['model']['activation'],
            fourier_m=self.config['model']['fourier_m'],
            fourier_sigma=self.config['model']['fourier_sigma']
        ).to(device)
        
        if self.config['model']['scaling']['learnable']:
            model = ScaledPINNWrapper(model).to(device)
        
        model.eval()
        
        # æ¸¬è©¦ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ¨ç†æ€§èƒ½
        batch_sizes = [1, 16, 64, 256, 1024]
        inference_results = {}
        
        for batch_size in batch_sizes:
            # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
            test_input = torch.randn(batch_size, self.config['model']['in_dim']).to(device)
            
            # é ç†±
            with torch.no_grad():
                for _ in range(10):
                    _ = model(test_input)
            
            # å¯¦éš›æ¸¬è©¦
            times = []
            with torch.no_grad():
                for _ in range(100):
                    start_time = time.time()
                    output = model(test_input)
                    torch.cuda.synchronize() if torch.cuda.is_available() else None
                    end_time = time.time()
                    times.append(end_time - start_time)
            
            inference_results[f'batch_{batch_size}'] = {
                'avg_time': np.mean(times),
                'std_time': np.std(times),
                'min_time': np.min(times),
                'max_time': np.max(times),
                'throughput': batch_size / np.mean(times)  # samples/second
            }
        
        return inference_results
    
    def benchmark_multiple_runs(self, n_runs: int = 5) -> Dict[str, Any]:
        """å¤šæ¬¡é‹è¡ŒåŸºæº–æ¸¬è©¦ä»¥è©•ä¼°ç©©å®šæ€§"""
        logger.info(f"Running {n_runs} training benchmarks for stability analysis")
        
        runs = []
        seeds = [42 + i for i in range(n_runs)]
        
        for i, seed in enumerate(seeds):
            logger.info(f"Run {i+1}/{n_runs} with seed {seed}")
            run_result = self.benchmark_single_training(seed)
            runs.append(run_result)
        
        # çµ±è¨ˆåˆ†æ
        if all('error' not in run for run in runs):
            training_times = [run['total_time'] for run in runs]
            final_losses = [run['final_loss'] for run in runs]
            converged_epochs = [run['converged_epoch'] for run in runs]
            avg_epoch_times = [run['avg_epoch_time'] for run in runs]
            
            stats = {
                'n_runs': n_runs,
                'training_time': {
                    'mean': np.mean(training_times),
                    'std': np.std(training_times),
                    'min': np.min(training_times),
                    'max': np.max(training_times)
                },
                'final_loss': {
                    'mean': np.mean(final_losses),
                    'std': np.std(final_losses),
                    'min': np.min(final_losses),
                    'max': np.max(final_losses)
                },
                'converged_epochs': {
                    'mean': np.mean(converged_epochs),
                    'std': np.std(converged_epochs),
                    'min': np.min(converged_epochs),
                    'max': np.max(converged_epochs)
                },
                'avg_epoch_time': {
                    'mean': np.mean(avg_epoch_times),
                    'std': np.std(avg_epoch_times),
                    'min': np.min(avg_epoch_times),
                    'max': np.max(avg_epoch_times)
                },
                'reproducibility': {
                    'loss_cv': np.std(final_losses) / np.mean(final_losses),  # è®Šç•°ä¿‚æ•¸
                    'time_cv': np.std(training_times) / np.mean(training_times),
                    'all_converged': all(run['converged'] for run in runs)
                }
            }
        else:
            stats = {'error': 'Some runs failed'}
        
        return {
            'individual_runs': runs,
            'statistics': stats
        }
    
    def run_full_benchmark(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„åŸºæº–æ¸¬è©¦å¥—ä»¶"""
        logger.info("Starting full performance benchmark suite")
        
        # 1. å¤šæ¬¡è¨“ç·´ç©©å®šæ€§æ¸¬è©¦
        self.results['benchmarks']['stability'] = self.benchmark_multiple_runs(n_runs=5)
        
        # 2. æ¨ç†æ€§èƒ½æ¸¬è©¦
        self.results['benchmarks']['inference'] = self.benchmark_inference_performance()
        
        # 3. å–®æ¬¡è©³ç´°è¨“ç·´åˆ†æ
        self.results['benchmarks']['detailed_training'] = self.benchmark_single_training(seed=42)
        
        return self.results
    
    def save_results(self, filename: str | None = None) -> Path:
        """å„²å­˜åŸºæº–æ¸¬è©¦çµæœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"baseline_data_{timestamp}.json"
        
        filepath = self.task_dir / filename
        
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        logger.info(f"Benchmark results saved to {filepath}")
        return filepath
    
    def generate_report(self) -> str:
        """ç”ŸæˆåŸºæº–æ¸¬è©¦å ±å‘Š"""
        if 'benchmarks' not in self.results:
            return "No benchmark data available"
        
        report = []
        report.append("# æ€§èƒ½åŸºç·šåŸºæº–æ¸¬è©¦å ±å‘Š")
        report.append("")
        report.append(f"**æ¸¬è©¦æ™‚é–“**: {self.results['timestamp']}")
        report.append("")
        
        # ç¡¬é«”ç’°å¢ƒ
        hw = self.results['hardware_info']
        report.append("## ğŸ–¥ï¸ ç¡¬é«”ç’°å¢ƒåŸºç·š")
        report.append("")
        report.append(f"- **å¹³å°**: {hw.get('platform', 'Unknown')}")
        report.append(f"- **è™•ç†å™¨**: {hw.get('processor', 'Unknown')}")
        report.append(f"- **æ¶æ§‹**: {hw.get('architecture', 'Unknown')}")
        report.append(f"- **CPU æ ¸å¿ƒæ•¸**: {hw.get('cpu_count', 'Unknown')}")
        
        # è™•ç†å¯èƒ½ç‚ºå­—ç¬¦ä¸²çš„é »ç‡å€¼
        cpu_freq_str = hw.get('cpu_freq_max', 'Unknown')
        if cpu_freq_str != "Unknown":
            report.append(f"- **æœ€å¤§é »ç‡**: {cpu_freq_str} MHz")
        else:
            report.append(f"- **æœ€å¤§é »ç‡**: Unknown")
        
        # è™•ç†å¯èƒ½ç‚ºå­—ç¬¦ä¸²çš„è¨˜æ†¶é«”å€¼
        memory_gb = hw.get('memory_total_gb', 'Unknown')
        if isinstance(memory_gb, (int, float)):
            report.append(f"- **è¨˜æ†¶é«”**: {memory_gb:.1f} GB")
        else:
            report.append(f"- **è¨˜æ†¶é«”**: {memory_gb}")
        
        report.append(f"- **Python ç‰ˆæœ¬**: {hw.get('python_version', 'Unknown')}")
        report.append("")
        
        # ç’°å¢ƒé…ç½®
        env = self.results['environment_info']
        report.append("## âš™ï¸ ç’°å¢ƒé…ç½®åŸºç·š")
        report.append("")
        report.append(f"- **PyTorch ç‰ˆæœ¬**: {env.get('torch_version', 'Unknown')}")
        report.append(f"- **CUDA å¯ç”¨**: {env.get('cuda_available', False)}")
        if env.get('cuda_available', False):
            report.append(f"- **CUDA ç‰ˆæœ¬**: {env.get('cuda_version', 'Unknown')}")
            report.append(f"- **GPU è¨­å‚™**: {env.get('device_name', 'Unknown')}")
        report.append("")
        
        # è¨“ç·´æ€§èƒ½åŸºç·š
        if 'stability' in self.results['benchmarks']:
            stability = self.results['benchmarks']['stability']
            if 'statistics' in stability and 'error' not in stability['statistics']:
                stats = stability['statistics']
                report.append("## ğŸš€ è¨“ç·´æ€§èƒ½åŸºç·š")
                report.append("")
                report.append(f"- **æ¸¬è©¦æ¬¡æ•¸**: {stats['n_runs']}")
                report.append(f"- **å¹³å‡è¨“ç·´æ™‚é–“**: {stats['training_time']['mean']:.2f} Â± {stats['training_time']['std']:.2f} ç§’")
                report.append(f"- **å–® epoch æ™‚é–“**: {stats['avg_epoch_time']['mean']:.4f} Â± {stats['avg_epoch_time']['std']:.4f} ç§’")
                report.append(f"- **æ”¶æ–‚ epoch æ•¸**: {stats['converged_epochs']['mean']:.1f} Â± {stats['converged_epochs']['std']:.1f}")
                report.append(f"- **æœ€çµ‚æå¤±**: {stats['final_loss']['mean']:.2e} Â± {stats['final_loss']['std']:.2e}")
                report.append("")
                
                # ç©©å®šæ€§æŒ‡æ¨™
                repo = stats['reproducibility']
                report.append("## ğŸ“Š æ•¸å€¼ç©©å®šæ€§åŸºç·š")
                report.append("")
                report.append(f"- **æå¤±è®Šç•°ä¿‚æ•¸**: {repo['loss_cv']:.4f}")
                report.append(f"- **æ™‚é–“è®Šç•°ä¿‚æ•¸**: {repo['time_cv']:.4f}")
                report.append(f"- **å…¨éƒ¨æ”¶æ–‚**: {'âœ…' if repo['all_converged'] else 'âŒ'}")
                report.append("")
        
        # æ¨ç†æ€§èƒ½åŸºç·š
        if 'inference' in self.results['benchmarks']:
            inference = self.results['benchmarks']['inference']
            report.append("## âš¡ æ¨ç†æ€§èƒ½åŸºç·š")
            report.append("")
            report.append("| æ‰¹æ¬¡å¤§å° | å¹³å‡æ™‚é–“ (ms) | ååé‡ (samples/s) |")
            report.append("|---------|---------------|-------------------|")
            
            for batch_key, metrics in inference.items():
                batch_size = batch_key.split('_')[1]
                avg_time_ms = metrics['avg_time'] * 1000
                throughput = metrics['throughput']
                report.append(f"| {batch_size} | {avg_time_ms:.3f} | {throughput:.1f} |")
            report.append("")
        
        # æ¨¡å‹è¦æ ¼
        if 'detailed_training' in self.results['benchmarks']:
            detail = self.results['benchmarks']['detailed_training']
            if 'error' not in detail:
                report.append("## ğŸ—ï¸ æ¨¡å‹è¦æ ¼åŸºç·š")
                report.append("")
                report.append(f"- **ç¸½åƒæ•¸é‡**: {detail.get('total_params', 'Unknown'):,}")
                report.append(f"- **å¯è¨“ç·´åƒæ•¸**: {detail.get('trainable_params', 'Unknown'):,}")
                report.append(f"- **è¨˜æ†¶é«”å³°å€¼**: {detail.get('memory_peak_mb', 'Unknown'):.1f} MB")
                report.append("")
        
        # é—œéµæŒ‡æ¨™æ‘˜è¦
        report.append("## ğŸ¯ é—œéµæŒ‡æ¨™æ‘˜è¦")
        report.append("")
        if 'stability' in self.results['benchmarks'] and 'statistics' in self.results['benchmarks']['stability']:
            stats = self.results['benchmarks']['stability']['statistics']
            if 'error' not in stats:
                avg_epoch_time = stats['avg_epoch_time']['mean']
                final_loss = stats['final_loss']['mean']
                converged = stats['reproducibility']['all_converged']
                
                report.append(f"- **å–® epoch æ™‚é–“**: {avg_epoch_time:.4f}s {'âœ…' if avg_epoch_time < 0.1 else 'âŒ'} (ç›®æ¨™ < 0.1s)")
                report.append(f"- **æ”¶æ–‚æ€§**: residual loss {final_loss:.2e} {'âœ…' if final_loss < 1e-3 else 'âŒ'} (ç›®æ¨™ < 1e-3)")
                report.append(f"- **ç©©å®šæ€§**: {'âœ…' if converged else 'âŒ'} ç„¡ NaN/Inf å•é¡Œ")
        
        return "\n".join(report)


def main():
    parser = argparse.ArgumentParser(description="PINNs æ€§èƒ½åŸºæº–æ¸¬è©¦")
    parser.add_argument('--cfg', default='configs/defaults.yml', help='é…ç½®æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--task-dir', default='tasks/task-002', help='ä»»å‹™ç›®éŒ„')
    parser.add_argument('--runs', type=int, default=5, help='ç©©å®šæ€§æ¸¬è©¦æ¬¡æ•¸')
    
    args = parser.parse_args()
    
    # å»ºç«‹åŸºæº–æ¸¬è©¦å™¨
    benchmark = PerformanceBenchmark(args.cfg, args.task_dir)
    
    try:
        # åŸ·è¡Œå®Œæ•´åŸºæº–æ¸¬è©¦
        results = benchmark.run_full_benchmark()
        
        # å„²å­˜çµæœ
        data_file = benchmark.save_results()
        
        # ç”Ÿæˆä¸¦å„²å­˜å ±å‘Š
        report = benchmark.generate_report()
        report_file = Path(args.task_dir) / "perf_baseline_report.md"
        
        with open(report_file, 'w') as f:
            f.write(report)
        
        logger.info(f"Benchmark completed successfully!")
        logger.info(f"Report saved to: {report_file}")
        logger.info(f"Data saved to: {data_file}")
        
        print(f"\nåŸºæº–æ¸¬è©¦å®Œæˆï¼")
        print(f"å ±å‘Šæª”æ¡ˆ: {report_file}")
        print(f"æ•¸æ“šæª”æ¡ˆ: {data_file}")
        
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)


if __name__ == "__main__":
    main()