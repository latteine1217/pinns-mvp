#!/usr/bin/env python
"""Phase 6B è©•ä¼°è…³æœ¬ - ä¿®å¾©ç‰ˆï¼ˆæ”¯æ´è¼¸å‡ºåæ¨™æº–åŒ–ï¼‰"""
import sys
import json
import torch
import numpy as np
import yaml
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from pinnx.train.factory import create_model

def load_checkpoint_and_model(checkpoint_path, config_path, device):
    """è¼‰å…¥æª¢æŸ¥é»ä¸¦å‰µå»ºæ¨¡å‹ï¼ˆåŒ…å«æ­£ç¢ºçš„æ¨™æº–åŒ–é…ç½®ï¼‰"""
    print(f"ğŸ“‚ è¼‰å…¥æª¢æŸ¥é»: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # è¼‰å…¥é…ç½®ï¼ˆå„ªå…ˆä½¿ç”¨æª¢æŸ¥é»ä¸­çš„é…ç½®ï¼‰
    if 'config' in checkpoint:
        config = checkpoint['config']
        print("  âœ… ä½¿ç”¨æª¢æŸ¥é»å…§åµŒé…ç½®")
    else:
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        print("  âš ï¸  æª¢æŸ¥é»ç„¡é…ç½®ï¼Œä½¿ç”¨å¤–éƒ¨é…ç½®æ–‡ä»¶")
    
    # å‰µå»ºæ¨¡å‹ï¼ˆä½¿ç”¨ factory å‡½æ•¸ï¼‰
    model = create_model(config, device)
    
    # è¼‰å…¥æ¬Šé‡
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
    else:
        raise KeyError("æª¢æŸ¥é»ä¸­ç¼ºå°‘ 'model_state_dict'")
    
    model.eval()
    print(f"âœ… æ¨¡å‹å·²è¼‰å…¥ä¸¦è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼")
    
    return model, config, checkpoint


def get_denormalization_factors(config):
    """å¾é…ç½®ä¸­æå–åæ¨™æº–åŒ–å› å­"""
    scaling_cfg = config.get('model', {}).get('scaling', {})
    output_norm = scaling_cfg.get('output_norm', None)
    
    if output_norm == 'friction_velocity':
        # é€Ÿåº¦æ¨™æº–åŒ–ç‚º u_tau çš„å€æ•¸
        u_tau = config.get('physics', {}).get('channel_flow', {}).get('u_tau', 0.04997)
        velocity_scale = u_tau  # è¨“ç·´æ™‚ï¼šu_real / u_tau â†’ u_normalized
        pressure_scale = u_tau  # å£“åŠ›é€šå¸¸ä¹Ÿç”¨ç›¸åŒå°ºåº¦ï¼ˆå¾…ç¢ºèªï¼‰
        
        print(f"\nğŸ”§ æª¢æ¸¬åˆ°è¼¸å‡ºæ¨™æº–åŒ–é…ç½®:")
        print(f"  - output_norm: {output_norm}")
        print(f"  - u_tau: {u_tau}")
        print(f"  - åæ¨™æº–åŒ–å› å­: {velocity_scale} (é€Ÿåº¦), {pressure_scale} (å£“åŠ›)")
        
        return {
            'u': velocity_scale,
            'v': velocity_scale,
            'w': velocity_scale,
            'p': pressure_scale
        }
    else:
        print(f"\nâš ï¸  æœªæª¢æ¸¬åˆ°æ¨™æº–åŒ–é…ç½® (output_norm={output_norm})ï¼Œä¸é€²è¡Œåæ¨™æº–åŒ–")
        return None


def evaluate_checkpoint(checkpoint_path, config_path, data_path, output_dir=None):
    """è©•ä¼°æª¢æŸ¥é»ä¸¦è¨ˆç®—æµå ´èª¤å·®"""
    print("=" * 70)
    print("  Phase 6B è©•ä¼°è…³æœ¬ - ä¿®å¾©ç‰ˆ")
    print("=" * 70)
    
    # è¨­å‚™
    device = torch.device('mps' if torch.backends.mps.is_available() else 
                         'cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ–¥ï¸  è¨­å‚™: {device}")
    
    # è¼‰å…¥æ¨¡å‹
    model, config, checkpoint = load_checkpoint_and_model(checkpoint_path, config_path, device)
    
    # ç²å–åæ¨™æº–åŒ–å› å­
    denorm_factors = get_denormalization_factors(config)
    
    # è¼‰å…¥è©•ä¼°è³‡æ–™
    print(f"\nğŸ“ è¼‰å…¥è©•ä¼°è³‡æ–™: {data_path}")
    data = np.load(data_path)
    
    # æå–åº§æ¨™å’ŒçœŸå¯¦å ´
    x, y, z = data['x'], data['y'], data['z']
    u_true = data['u']  # (Nx, Ny, Nz)
    v_true = data['v']
    w_true = data['w']
    p_true = data['p']
    
    # å»ºç«‹ç¶²æ ¼åº§æ¨™
    X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
    coords = np.stack([X.ravel(), Y.ravel(), Z.ravel()], axis=1)
    
    print(f"  ç¶²æ ¼å°ºå¯¸: {u_true.shape}")
    print(f"  ç¸½é»æ•¸: {len(coords):,}")
    print(f"  åº§æ¨™ç¯„åœ:")
    print(f"    x âˆˆ [{x.min():.3f}, {x.max():.3f}]")
    print(f"    y âˆˆ [{y.min():.3f}, {y.max():.3f}]")
    print(f"    z âˆˆ [{z.min():.3f}, {z.max():.3f}]")
    
    # é æ¸¬ï¼ˆåˆ†æ‰¹è™•ç†é¿å…è¨˜æ†¶é«”æº¢å‡ºï¼‰
    print(f"\nğŸ”® æ¨¡å‹é æ¸¬...")
    batch_size = 8192
    n_points = len(coords)
    n_batches = (n_points + batch_size - 1) // batch_size
    
    predictions = []
    with torch.no_grad():
        for i in range(n_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, n_points)
            
            batch_coords = torch.tensor(coords[start_idx:end_idx], 
                                       dtype=torch.float32, device=device)
            batch_pred = model(batch_coords).cpu().numpy()
            predictions.append(batch_pred)
            
            if (i + 1) % 10 == 0 or (i + 1) == n_batches:
                print(f"  é€²åº¦: {i+1}/{n_batches} æ‰¹æ¬¡")
    
    pred = np.concatenate(predictions, axis=0)
    print(f"âœ… é æ¸¬å®Œæˆ: {pred.shape}")
    
    # åæ¨™æº–åŒ–é æ¸¬å€¼
    u_pred_raw = pred[:, 0]
    v_pred_raw = pred[:, 1]
    w_pred_raw = pred[:, 2]
    p_pred_raw = pred[:, 3]
    
    if denorm_factors is not None:
        print(f"\nğŸ”„ åŸ·è¡Œåæ¨™æº–åŒ–...")
        u_pred = u_pred_raw / denorm_factors['u']
        v_pred = v_pred_raw / denorm_factors['v']
        w_pred = w_pred_raw / denorm_factors['w']
        p_pred = p_pred_raw / denorm_factors['p']
        
        print(f"  u: {u_pred_raw.mean():.4f} â†’ {u_pred.mean():.4f} (Ã—{1/denorm_factors['u']:.2f})")
        print(f"  v: {v_pred_raw.mean():.4f} â†’ {v_pred.mean():.4f}")
        print(f"  w: {w_pred_raw.mean():.4f} â†’ {w_pred.mean():.4f}")
        print(f"  p: {p_pred_raw.mean():.4f} â†’ {p_pred.mean():.4f}")
    else:
        u_pred, v_pred, w_pred, p_pred = u_pred_raw, v_pred_raw, w_pred_raw, p_pred_raw
    
    # Reshape ç‚ºåŸå§‹ç¶²æ ¼å½¢ç‹€
    u_pred = u_pred.reshape(u_true.shape)
    v_pred = v_pred.reshape(v_true.shape)
    w_pred = w_pred.reshape(w_true.shape)
    p_pred = p_pred.reshape(p_true.shape)
    
    # è¨ˆç®—èª¤å·®
    def relative_l2_error(pred, true):
        return np.linalg.norm(pred - true) / np.linalg.norm(true)
    
    u_error = relative_l2_error(u_pred, u_true)
    v_error = relative_l2_error(v_pred, v_true)
    w_error = relative_l2_error(w_pred, w_true)
    p_error = relative_l2_error(p_pred, p_true)
    mean_error = (u_error + v_error + w_error) / 3
    
    # è¨ˆç®—é€é»èª¤å·®çµ±è¨ˆ
    u_pointwise = np.abs(u_pred - u_true)
    v_pointwise = np.abs(v_pred - v_true)
    w_pointwise = np.abs(w_pred - w_true)
    
    # çµ±è¨ˆè³‡è¨Š
    results = {
        'checkpoint': str(checkpoint_path),
        'epoch': checkpoint.get('epoch', -1),
        'data_file': str(data_path),
        'grid_size': list(u_true.shape),
        'total_points': int(np.prod(u_true.shape)),
        'denormalization_applied': denorm_factors is not None,
        'denorm_factors': {k: float(v) for k, v in denorm_factors.items()} if denorm_factors else None,
        'relative_l2_errors': {
            'u': float(u_error),
            'v': float(v_error),
            'w': float(w_error),
            'p': float(p_error)
        },
        'mean_l2_error': float(mean_error),
        'pointwise_errors': {
            'u': {
                'median': float(np.median(u_pointwise)),
                'p95': float(np.percentile(u_pointwise, 95)),
                'max': float(np.max(u_pointwise))
            },
            'v': {
                'median': float(np.median(v_pointwise)),
                'p95': float(np.percentile(v_pointwise, 95)),
                'max': float(np.max(v_pointwise))
            },
            'w': {
                'median': float(np.median(w_pointwise)),
                'p95': float(np.percentile(w_pointwise, 95)),
                'max': float(np.max(w_pointwise))
            }
        },
        'predictions_stats': {
            'u': {
                'min': float(u_pred.min()),
                'max': float(u_pred.max()),
                'mean': float(u_pred.mean()),
                'std': float(u_pred.std())
            },
            'v': {
                'min': float(v_pred.min()),
                'max': float(v_pred.max()),
                'mean': float(v_pred.mean()),
                'std': float(v_pred.std())
            },
            'w': {
                'min': float(w_pred.min()),
                'max': float(w_pred.max()),
                'mean': float(w_pred.mean()),
                'std': float(w_pred.std())
            },
            'p': {
                'min': float(p_pred.min()),
                'max': float(p_pred.max()),
                'mean': float(p_pred.mean()),
                'std': float(p_pred.std())
            }
        },
        'true_field_stats': {
            'u': {
                'min': float(u_true.min()),
                'max': float(u_true.max()),
                'mean': float(u_true.mean()),
                'std': float(u_true.std())
            },
            'v': {
                'min': float(v_true.min()),
                'max': float(v_true.max()),
                'mean': float(v_true.mean()),
                'std': float(v_true.std())
            },
            'w': {
                'min': float(w_true.min()),
                'max': float(w_true.max()),
                'mean': float(w_true.mean()),
                'std': float(w_true.std())
            }
        }
    }
    
    # æ‰“å°çµæœ
    print("\n" + "=" * 70)
    print("  è©•ä¼°çµæœ")
    print("=" * 70)
    print(f"\nç›¸å° L2 èª¤å·®:")
    print(f"  u: {u_error*100:.2f}%  {'âœ…' if u_error < 0.15 else 'âŒ'} (ç›®æ¨™ < 15%)")
    print(f"  v: {v_error*100:.2f}%  {'âœ…' if v_error < 0.15 else 'âŒ'}")
    print(f"  w: {w_error*100:.2f}%  {'âœ…' if w_error < 0.15 else 'âŒ'}")
    print(f"  å¹³å‡: {mean_error*100:.2f}%")
    
    print(f"\né æ¸¬å€¼çµ±è¨ˆ:")
    print(f"  u: [{u_pred.min():.3f}, {u_pred.max():.3f}], å‡å€¼ {u_pred.mean():.3f}")
    print(f"  v: [{v_pred.min():.3f}, {v_pred.max():.3f}], å‡å€¼ {v_pred.mean():.3f}")
    print(f"  w: [{w_pred.min():.3f}, {w_pred.max():.3f}], å‡å€¼ {w_pred.mean():.3f}")
    
    print(f"\nçœŸå¯¦å ´çµ±è¨ˆ:")
    print(f"  u: [{u_true.min():.3f}, {u_true.max():.3f}], å‡å€¼ {u_true.mean():.3f}")
    print(f"  v: [{v_true.min():.3f}, {v_true.max():.3f}], å‡å€¼ {v_true.mean():.3f}")
    print(f"  w: [{w_true.min():.3f}, {w_true.max():.3f}], å‡å€¼ {w_true.mean():.3f}")
    
    # ä¿å­˜çµæœ
    if output_dir is not None:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        metrics_file = output_path / 'metrics_fixed.json'
        with open(metrics_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nğŸ’¾ çµæœå·²ä¿å­˜è‡³: {metrics_file}")
    
    print("=" * 70)
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Phase 6B è©•ä¼°è…³æœ¬ - ä¿®å¾©ç‰ˆ')
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æª¢æŸ¥é»è·¯å¾‘')
    parser.add_argument('--config', type=str, required=True,
                       help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--data', type=str, required=True,
                       help='è©•ä¼°è³‡æ–™è·¯å¾‘ï¼ˆ3D cutout .npzï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='è¼¸å‡ºç›®éŒ„ï¼ˆé»˜èªä¸ä¿å­˜ï¼‰')
    
    args = parser.parse_args()
    
    evaluate_checkpoint(args.checkpoint, args.config, args.data, args.output)
