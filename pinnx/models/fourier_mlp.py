"""
Fourier ç‰¹å¾µ MLP ç¶²è·¯æ¶æ§‹æ¨¡çµ„ (çµ±ä¸€ç‰ˆ)

æ•´åˆæ¨™æº–èˆ‡å¢å¼·åŠŸèƒ½ï¼Œé€éåƒæ•¸é¸é …æ§åˆ¶ç¶²è·¯è¤‡é›œåº¦ã€‚

æ ¸å¿ƒç‰¹è‰²ï¼š
- Fourier Random Features (æ¨™æº–/å¤šå°ºåº¦)
- Random Weight Factorization (RWF) å¯é¸
- å¯é…ç½®çš„ç¶²è·¯æ·±åº¦èˆ‡å¯¬åº¦  
- æ”¯æ´å¤šç¨®æ¿€æ´»å‡½æ•¸ (tanh, swish, gelu, sine)
- æ®˜å·®é€£æ¥èˆ‡å±¤æ­¸ä¸€åŒ– (å¯é¸)
- é‡å° PINNs è‡ªå‹•å¾®åˆ†å„ªåŒ–çš„æ¬Šé‡åˆå§‹åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Union, List, Callable, Dict, Any
import numpy as np


class RWFLinear(nn.Module):
    """
    Random Weight Factorization ç·šæ€§å±¤
    
    åŸºæ–¼ arXiv 2308.08468 è«–æ–‡çš„æ¬Šé‡åˆ†è§£æŠ€è¡“ï¼š
    W^(l) = diag(exp(s^(l))) Â· V^(l)
    
    å„ªå‹¢ï¼š
    1. æ”¹å–„è¨“ç·´ç©©å®šæ€§ - é˜²æ­¢æ¬Šé‡çˆ†ç‚¸/æ¶ˆå¤±
    2. æ›´å¥½çš„æ¢¯åº¦æµå‹• - æŒ‡æ•¸ç¸®æ”¾æä¾›è‡ªé©æ‡‰å­¸ç¿’ç‡
    3. éš±å¼æ­£å‰‡åŒ– - å°æ•¸ç©ºé–“çš„å¹³æ»‘æ€§ç´„æŸ
    """
    
    def __init__(self, 
                 in_features: int, 
                 out_features: int, 
                 bias: bool = True,
                 scale_mean: float = 0.0,
                 scale_std: float = 0.1):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.scale_mean = scale_mean
        self.scale_std = scale_std
        
        # V^(l): æ¨™æº–æ¬Šé‡çŸ©é™£
        self.V = nn.Parameter(torch.empty(out_features, in_features))
        # s^(l): å°æ•¸å°ºåº¦å› å­
        self.s = nn.Parameter(torch.empty(out_features))
        
        if bias:
            self.bias = nn.Parameter(torch.empty(out_features))
        else:
            self.register_parameter('bias', None)
        
        self.reset_parameters()
    
    def reset_parameters(self) -> None:
        nn.init.xavier_uniform_(self.V)
        nn.init.normal_(self.s, mean=self.scale_mean, std=self.scale_std)
        if self.bias is not None:
            nn.init.zeros_(self.bias)
    
    def apply_siren_init(self, omega_0: float, is_first: bool) -> None:
        """
        æ‡‰ç”¨ SIREN åˆå§‹åŒ–è¦å‰‡åˆ° RWF æ¬Šé‡
        
        åŸºæ–¼è«–æ–‡: Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"
        
        Args:
            omega_0: Sine æ¿€æ´»å‡½æ•¸çš„é »ç‡åƒæ•¸
            is_first: æ˜¯å¦ç‚ºç¬¬ä¸€å±¤ï¼ˆç¬¬ä¸€å±¤ä½¿ç”¨ä¸åŒçš„åˆå§‹åŒ–è¦å‰‡ï¼‰
        """
        n_in = self.V.shape[1]
        with torch.no_grad():
            if is_first:
                # ç¬¬ä¸€å±¤: U(-1/n_in, +1/n_in)
                bound = 1.0 / n_in
            else:
                # éš±è—å±¤: U(-sqrt(6/n_in)/omega_0, +sqrt(6/n_in)/omega_0)
                bound = math.sqrt(6.0 / n_in) / omega_0
            
            nn.init.uniform_(self.V, -bound, bound)
            nn.init.zeros_(self.s)  # s åˆå§‹åŒ–ç‚º 0 (exp(0) = 1, ç„¡ç¸®æ”¾)
            if self.bias is not None:
                nn.init.zeros_(self.bias)
    
    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
                              missing_keys, unexpected_keys, error_msgs):
        weight_key = prefix + 'weight'
        v_key = prefix + 'V'
        s_key = prefix + 's'

        if weight_key in state_dict and v_key not in state_dict:
            raise KeyError(
                f"Checkpoint contains legacy parameter '{weight_key}'. "
                "Please migrate checkpoints to the new RWF format."
            )
        if s_key not in state_dict and v_key in state_dict:
            raise KeyError(
                f"Checkpoint missing required RWF parameter '{s_key}'. "
                "Ensure the checkpoint was saved after the RWF migration."
            )

        super()._load_from_state_dict(state_dict, prefix, local_metadata, strict,
                                      missing_keys, unexpected_keys, error_msgs)
    
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        scale_factors = torch.exp(self.s).unsqueeze(1)
        W = scale_factors * self.V
        return F.linear(input, W, self.bias)
    
    def extra_repr(self) -> str:
        return f'in_features={self.in_features}, out_features={self.out_features}, ' \
               f'bias={self.bias is not None}, scale_std={self.scale_std}'


class FourierFeatures(nn.Module):
    """
    Fourier Random Features ç·¨ç¢¼å±¤
    
    å°‡è¼¸å…¥åº§æ¨™é€ééš¨æ©Ÿ Fourier ç‰¹å¾µæ˜ å°„åˆ°é«˜ç¶­ç©ºé–“ï¼Œ
    æå‡ç¥ç¶“ç¶²è·¯å°é«˜é »å‡½æ•¸çš„æ“¬åˆèƒ½åŠ›ã€‚
    
    Args:
        in_dim: è¼¸å…¥ç¶­åº¦ (ä¾‹å¦‚ 3 ä»£è¡¨ t,x,y)
        m: Fourier ç‰¹å¾µæ•¸é‡ (è¼¸å‡ºç¶­åº¦ç‚º 2m)
        sigma: Fourier é »ç‡å°ºåº¦åƒæ•¸
        multiscale: æ˜¯å¦ä½¿ç”¨å¤šå°ºåº¦é »ç‡
        trainable: æ˜¯å¦è®“ Fourier ä¿‚æ•¸å¯è¨“ç·´
    """
    
    def __init__(self, in_dim: int, m: int = 32, sigma: float = 5.0, 
                 multiscale: bool = False, trainable: bool = False):
        super().__init__()
        self.in_dim = in_dim
        self.m = m
        self.sigma = sigma
        self.multiscale = multiscale
        self.out_dim = 2 * m
        
        # ç”Ÿæˆ Fourier ä¿‚æ•¸çŸ©é™£
        if multiscale:
            # å¤šå°ºåº¦ï¼šå¾ä½é »åˆ°é«˜é »çš„å°æ•¸é–“è·
            sigmas = torch.logspace(-1, math.log10(sigma), m // 4).repeat(4)[:m]
            B = torch.randn(in_dim, m) * sigmas.unsqueeze(0)
        else:
            # å–®ä¸€å°ºåº¦
            B = torch.randn(in_dim, m) * sigma
        
        if trainable:
            self.B = nn.Parameter(B)
        else:
            self.register_buffer("B", B)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch_size, in_dim] è¼¸å…¥åº§æ¨™
        Returns:
            [batch_size, 2*m] Fourier ç‰¹å¾µ [cos(z), sin(z)]
        """
        z = 2.0 * math.pi * x @ self.B
        return torch.cat([torch.cos(z), torch.sin(z)], dim=-1)
    
    def extra_repr(self) -> str:
        return f'in_dim={self.in_dim}, m={self.m}, sigma={self.sigma:.2f}, ' \
               f'multiscale={self.multiscale}, out_dim={self.out_dim}'


class SineActivation(nn.Module):
    """
    Sine æ¿€æ´»å‡½æ•¸ - é©åˆé«˜é »ç‰¹å¾µæ•æ‰
    
    åŸºæ–¼ SIREN (Implicit Neural Representations with Periodic Activation Functions)
    ä½¿ç”¨å‘¨æœŸæ€§æ¿€æ´»å‡½æ•¸å¯ä»¥æ›´å¥½åœ°è¡¨ç¤ºé«˜é »ç´°ç¯€èˆ‡å°æ•¸ã€‚
    
    Args:
        omega_0: é »ç‡åƒæ•¸ï¼Œæ§åˆ¶ sine å‡½æ•¸çš„é€±æœŸæ€§ (é è¨­ 1.0ï¼Œè¼ƒä¿å®ˆ)
    
    æ³¨æ„ï¼šèˆ‡ Fourier ç‰¹å¾µçµåˆæ™‚ä½¿ç”¨è¼ƒå°çš„ omega_0 ä»¥é¿å…è¨“ç·´ä¸ç©©å®š
    """
    def __init__(self, omega_0: float = 1.0):
        super().__init__()
        self.omega_0 = omega_0
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.sin(self.omega_0 * x)


class DenseLayer(nn.Module):
    """
    å¯†é›†é€£æ¥å±¤ï¼Œæ”¯æ´å¤šç¨®æ¿€æ´»å‡½æ•¸ã€æ®˜å·®é€£æ¥èˆ‡å±¤æ­¸ä¸€åŒ–
    """
    
    def __init__(self, in_features: int, out_features: int, 
                 activation: str = 'tanh', 
                 use_residual: bool = False,
                 use_layer_norm: bool = False,
                 dropout: float = 0.0,
                 use_rwf: bool = False,
                 rwf_scale_mean: float = 0.0,
                 rwf_scale_std: float = 0.1,
                 sine_omega_0: float = 30.0):
        super().__init__()
        
        # é¸æ“‡ç·šæ€§å±¤é¡å‹
        if use_rwf:
            self.linear = RWFLinear(in_features, out_features, bias=True, 
                                   scale_mean=rwf_scale_mean, scale_std=rwf_scale_std)
        else:
            self.linear = nn.Linear(in_features, out_features)
        
        self.use_residual = use_residual and (in_features == out_features)
        self.use_layer_norm = use_layer_norm
        self.use_rwf = use_rwf
        
        # å±¤æ­¸ä¸€åŒ–
        if use_layer_norm:
            self.layer_norm = nn.LayerNorm(out_features)
        
        # æ¿€æ´»å‡½æ•¸é¸æ“‡
        if activation == 'tanh':
            self.activation = nn.Tanh()
        elif activation == 'swish':
            self.activation = nn.SiLU()
        elif activation == 'gelu':
            self.activation = nn.GELU()
        elif activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'elu':
            self.activation = nn.ELU()
        elif activation == 'sine':
            self.activation = SineActivation(omega_0=sine_omega_0)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¿€æ´»å‡½æ•¸: {activation}")
        
        # Dropout
        self.dropout = nn.Dropout(dropout) if dropout > 0 else None
        
        # æ¬Šé‡åˆå§‹åŒ– (åƒ…å°æ¨™æº–ç·šæ€§å±¤)
        if not use_rwf and isinstance(self.linear, nn.Linear):
            if activation in ['swish', 'relu', 'elu']:
                nn.init.kaiming_normal_(self.linear.weight, nonlinearity='relu')
            elif activation == 'sine':
                # ğŸ¯ SIREN å°ˆç”¨åˆå§‹åŒ–ï¼ˆè«–æ–‡æ¨™æº–ï¼‰
                # éš±è—å±¤ï¼šU(-sqrt(6/n_in)/omega_0, +sqrt(6/n_in)/omega_0)
                # æ³¨æ„ï¼šç¬¬ä¸€å±¤åˆå§‹åŒ–ç”± PINNNet è™•ç†
                if isinstance(self.activation, SineActivation):
                    import numpy as np
                    n_in = self.linear.weight.shape[1]
                    omega_0 = self.activation.omega_0
                    bound = np.sqrt(6 / n_in) / omega_0
                    nn.init.uniform_(self.linear.weight, -bound, bound)
                else:
                    nn.init.xavier_normal_(self.linear.weight, gain=0.5)
                nn.init.zeros_(self.linear.bias)
            else:
                nn.init.xavier_normal_(self.linear.weight)
            nn.init.zeros_(self.linear.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x if self.use_residual else None
        
        out = self.linear(x)
        
        if self.use_layer_norm:
            out = self.layer_norm(out)
        
        out = self.activation(out)
        
        if self.dropout is not None:
            out = self.dropout(out)
        
        if residual is not None:
            out = out + residual
            
        return out


class PINNNet(nn.Module):
    """
    åŸºæ–¼ Fourier ç‰¹å¾µçš„ PINN ç¥ç¶“ç¶²è·¯ (çµ±ä¸€ç‰ˆ)
    
    æ•´åˆæ¨™æº–èˆ‡å¢å¼·åŠŸèƒ½ï¼Œé€éåƒæ•¸æ§åˆ¶ç¶²è·¯è¤‡é›œåº¦ï¼š
    - æ¨™æº–æ¨¡å¼ï¼šè¼ƒæ·ºè¼ƒçª„ï¼Œtanh æ¿€æ´»ï¼Œç„¡æ®˜å·®/æ­¸ä¸€åŒ–
    - å¢å¼·æ¨¡å¼ï¼šè¼ƒæ·±è¼ƒå¯¬ï¼Œswish æ¿€æ´»ï¼Œå•Ÿç”¨æ®˜å·®/æ­¸ä¸€åŒ–/RWF
    
    ç¶²è·¯æ¶æ§‹ï¼š
    è¼¸å…¥ -> Fourier ç‰¹å¾µ -> [æŠ•å½±å±¤] -> å¤šå±¤ Dense -> è¼¸å‡º
    """
    
    def __init__(self,
                 in_dim: int = 3,           # è¼¸å…¥ç¶­åº¦ (t,x,y)
                 out_dim: int = 4,          # è¼¸å‡ºç¶­åº¦ (u,v,p,S)
                 width: int = 256,          # éš±è—å±¤å¯¬åº¦
                 depth: int = 5,            # ç¶²è·¯æ·±åº¦
                 fourier_m: int = 32,       # Fourier ç‰¹å¾µæ•¸
                 fourier_sigma: float = 5.0, # Fourier é »ç‡å°ºåº¦
                 fourier_multiscale: bool = False, # å¤šå°ºåº¦ Fourier
                 activation: str = 'tanh',   # æ¿€æ´»å‡½æ•¸
                 use_fourier: bool = True,   # æ˜¯å¦ä½¿ç”¨ Fourier ç‰¹å¾µ
                 trainable_fourier: bool = False, # Fourier ä¿‚æ•¸æ˜¯å¦å¯è¨“ç·´
                 use_residual: bool = False, # æ˜¯å¦ä½¿ç”¨æ®˜å·®é€£æ¥
                 use_layer_norm: bool = False, # æ˜¯å¦ä½¿ç”¨å±¤æ­¸ä¸€åŒ–
                 use_input_projection: bool = False, # æ˜¯å¦ä½¿ç”¨è¼¸å…¥æŠ•å½±å±¤
                 dropout: float = 0.0,      # Dropout æ¯”ç‡
                 use_rwf: bool = False,     # æ˜¯å¦ä½¿ç”¨ RWF
                 rwf_scale_mean: float = 0.0, # RWF å°ºåº¦å‡å€¼ï¼ˆPirateNet: 1.0ï¼‰
                 rwf_scale_std: float = 0.1, # RWF å°ºåº¦æ¨™æº–å·®
                 sine_omega_0: float = 1.0, # Sine æ¿€æ´»å‡½æ•¸é »ç‡åƒæ•¸ (è¼ƒä¿å®ˆä»¥é…åˆ Fourier ç‰¹å¾µ)
                 fourier_normalize_input: bool = False, # ğŸ”§ æ˜¯å¦åœ¨ Fourier å‰æ¨™æº–åŒ–è¼¸å…¥ï¼ˆä¿®å¾© VS-PINN ç¸®æ”¾å•é¡Œï¼‰
                 input_scale_factors: Optional[torch.Tensor] = None): # ğŸ”§ è¼¸å…¥ç¸®æ”¾å› å­ [N_x, N_y, N_z]ï¼ˆç”¨æ–¼ VS-PINNï¼‰
        
        super().__init__()
        
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.use_fourier = use_fourier
        self.use_input_projection = use_input_projection
        self.fourier_normalize_input = fourier_normalize_input
        self._fourier_norm_type: Optional[str] = None
        self._fourier_shift: Optional[torch.Tensor] = None
        self._fourier_scale: Optional[torch.Tensor] = None
        self._fourier_feature_range: Optional[torch.Tensor] = None
        self._fourier_range: Optional[torch.Tensor] = None
        
        # ğŸ”§ è¨»å†Šè¼¸å…¥ç¸®æ”¾å› å­ï¼ˆç”¨æ–¼ VS-PINN çš„æ¨™æº–åŒ–è£œå„Ÿï¼‰
        if input_scale_factors is not None:
            self.register_buffer('input_scale_factors', input_scale_factors)
        else:
            self.input_scale_factors = None
        
        # Fourier ç‰¹å¾µç·¨ç¢¼
        if use_fourier:
            self.fourier = FourierFeatures(
                in_dim, fourier_m, fourier_sigma, 
                multiscale=fourier_multiscale,
                trainable=trainable_fourier
            )
            input_features = self.fourier.out_dim
        else:
            self.fourier = None
            input_features = in_dim
        
        # å¯é¸çš„è¼¸å…¥æŠ•å½±å±¤
        if use_input_projection:
            self.input_projection = nn.Linear(input_features, width)
            nn.init.xavier_normal_(self.input_projection.weight)
            nn.init.zeros_(self.input_projection.bias)
            current_dim = width
        else:
            self.input_projection = None
            current_dim = input_features
        
        # éš±è—å±¤
        layers = []
        for i in range(depth):
            # ç¬¬ä¸€å±¤çš„è¼¸å…¥ç¶­åº¦å¯èƒ½èˆ‡å¾ŒçºŒå±¤ä¸åŒ
            layer_in_dim = current_dim if i == 0 else width
            
            layers.append(DenseLayer(
                layer_in_dim, width, 
                activation=activation,
                use_residual=use_residual and i > 0,  # ç¬¬ä¸€å±¤ä¸ç”¨æ®˜å·®
                use_layer_norm=use_layer_norm,
                dropout=dropout,
                use_rwf=use_rwf,
                rwf_scale_mean=rwf_scale_mean,
                rwf_scale_std=rwf_scale_std,
                sine_omega_0=sine_omega_0
            ))
            current_dim = width
        
        self.hidden_layers = nn.ModuleList(layers)
        
        # è¼¸å‡ºå±¤ (ç·šæ€§ï¼Œç„¡æ¿€æ´»å‡½æ•¸)
        self.output_layer = nn.Linear(width, out_dim)
        
        # è¼¸å‡ºå±¤ç‰¹æ®Šåˆå§‹åŒ–ï¼šè¼ƒå°çš„æ¬Šé‡ï¼Œæœ‰åŠ©æ–¼è¨“ç·´ç©©å®š
        output_gain = 0.01 if use_residual else 0.1
        nn.init.xavier_normal_(self.output_layer.weight, gain=output_gain)
        nn.init.zeros_(self.output_layer.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: [batch_size, in_dim] è¼¸å…¥åº§æ¨™ï¼ˆå¯èƒ½æ˜¯ç‰©ç†åº§æ¨™æˆ– VS-PINN ç¸®æ”¾åº§æ¨™ï¼‰
        Returns:
            [batch_size, out_dim] ç¶²è·¯è¼¸å‡º
        """
        # ğŸ”§ ä¿®å¾©ï¼šè‹¥å•Ÿç”¨æ¨™æº–åŒ–ä¸”è¼¸å…¥å·²è¢« VS-PINN ç¸®æ”¾ï¼Œå…ˆé‚„åŸåˆ° [-1, 1] ç¯„åœ
        if self.use_fourier and self.fourier is not None:
            x_fourier = x
            if self._fourier_norm_type is not None:
                x_fourier = self._apply_fourier_inverse_normalizer(x_fourier)
            
            if self.fourier_normalize_input:
                if self.input_scale_factors is not None:
                    x_fourier = x_fourier / self.input_scale_factors
                else:
                    x_abs_max = x_fourier.abs().max()
                    if x_abs_max > 2.0:
                        x_min = x_fourier.min(dim=0, keepdim=True).values
                        x_max = x_fourier.max(dim=0, keepdim=True).values
                        x_range = x_max - x_min
                        x_fourier = 2.0 * (x_fourier - x_min) / (x_range + 1e-8) - 1.0
            
            h = self.fourier(x_fourier)
        else:
            h = x
        
        # å¯é¸çš„è¼¸å…¥æŠ•å½±
        if self.use_input_projection and self.input_projection is not None:
            h = self.input_projection(h)
            h = F.silu(h)  # Swish activation
        
        # éš±è—å±¤å‰å‘å‚³æ’­
        for layer in self.hidden_layers:
            h = layer(h)
        
        # è¼¸å‡ºå±¤
        output = self.output_layer(h)
        
        return output
    
    def _apply_fourier_inverse_normalizer(self, x: torch.Tensor) -> torch.Tensor:
        if self._fourier_norm_type is None:
            return x
        
        if self._fourier_norm_type == 'standard':
            if self._fourier_scale is not None:
                x = x * self._fourier_scale
            if self._fourier_shift is not None:
                x = x + self._fourier_shift
            return x
        
        if self._fourier_norm_type in ('minmax', 'channel_flow'):
            if (
                self._fourier_feature_range is not None and
                self._fourier_range is not None and
                self._fourier_shift is not None
            ):
                lo, hi = self._fourier_feature_range[0], self._fourier_feature_range[1]
                scale = hi - lo
                x = (x - lo) / (scale + 1e-8)
                x = x * self._fourier_range + self._fourier_shift
            return x
        
        return x
    
    def configure_fourier_input(self, metadata: Dict[str, Any]) -> None:
        """
        Configure inverse-normalization for Fourier features using input normalizer stats.
        """
        if not self.use_fourier or self.fourier is None:
            return
        
        norm_type = metadata.get('norm_type', 'none')
        if norm_type in ('none', 'identity', None):
            self._fourier_norm_type = None
            self._fourier_shift = None
            self._fourier_scale = None
            self._fourier_feature_range = None
            self._fourier_range = None
            return
        
        device = next(self.parameters()).device if any(p.requires_grad for p in self.parameters()) else torch.device('cpu')
        dtype = next(self.parameters()).dtype if any(p.requires_grad for p in self.parameters()) else torch.float32
        
        def _prepare(t: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
            if t is None:
                return None
            return t.to(device=device, dtype=dtype)
        
        self._fourier_norm_type = norm_type
        
        if norm_type == 'standard':
            self._fourier_shift = _prepare(metadata.get('mean'))
            self._fourier_scale = _prepare(metadata.get('std'))
            if self._fourier_scale is not None:
                self._fourier_scale = torch.clamp(self._fourier_scale, min=1e-8)
            self._fourier_feature_range = None
            self._fourier_range = None
        
        elif norm_type in ('minmax', 'channel_flow'):
            feature_range = metadata.get('feature_range')
            data_min = metadata.get('data_min')
            data_range = metadata.get('data_range')
            bounds = metadata.get('bounds')
            
            if data_min is None and bounds is not None:
                data_min = bounds[:, 0].unsqueeze(0)
            if data_range is None and bounds is not None:
                data_range = (bounds[:, 1] - bounds[:, 0]).unsqueeze(0)
            
            self._fourier_shift = _prepare(data_min)
            prepared_range = _prepare(data_range)
            if prepared_range is not None:
                self._fourier_range = torch.clamp(prepared_range, min=1e-8)
            else:
                self._fourier_range = None
            
            feature_tensor = feature_range.to(device=device, dtype=dtype) if isinstance(feature_range, torch.Tensor) else torch.tensor(feature_range, device=device, dtype=dtype)
            self._fourier_feature_range = feature_tensor
            self._fourier_scale = None
        
        else:
            # ç„¡æ³•è­˜åˆ¥çš„é¡å‹ï¼šè¦–ç‚ºç„¡éœ€è™•ç†
            self._fourier_norm_type = None
            self._fourier_shift = None
            self._fourier_scale = None
            self._fourier_feature_range = None
            self._fourier_range = None
    
    def get_num_params(self) -> int:
        """è¿”å›æ¨¡å‹åƒæ•¸ç¸½æ•¸"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_model_summary(self) -> Dict[str, Any]:
        """è¿”å›æ¨¡å‹çµæ§‹æ‘˜è¦"""
        width = 0
        if self.hidden_layers:
            layer = self.hidden_layers[0]
            if isinstance(layer.linear, nn.Linear):
                width = layer.linear.out_features
            elif isinstance(layer.linear, RWFLinear):
                width = layer.linear.out_features
        
        fourier_features = 0
        if self.use_fourier and self.fourier is not None:
            fourier_features = self.fourier.m
        
        return {
            'input_dim': self.in_dim,
            'output_dim': self.out_dim,
            'depth': len(self.hidden_layers),
            'width': width,
            'total_params': self.get_num_params(),
            'fourier_features': fourier_features,
            'use_residual': self.hidden_layers[0].use_residual if self.hidden_layers else False,
            'use_layer_norm': self.hidden_layers[0].use_layer_norm if self.hidden_layers else False,
            'use_rwf': self.hidden_layers[0].use_rwf if self.hidden_layers else False
        }
    
    def extra_repr(self) -> str:
        summary = self.get_model_summary()
        fourier_info = f", fourier_m={summary['fourier_features']}" if self.use_fourier else ""
        return (f"in_dim={summary['input_dim']}, out_dim={summary['output_dim']}, "
                f"width={summary['width']}, depth={summary['depth']}{fourier_info}, "
                f"params={summary['total_params']:,}")


class MultiScalePINNNet(nn.Module):
    """
    å¤šå°ºåº¦ PINN ç¶²è·¯ï¼šä½¿ç”¨ä¸åŒ Fourier é »ç‡çš„å­ç¶²è·¯çµ„åˆ
    
    é©ç”¨æ–¼åŒ…å«å¤šå€‹ç‰¹å¾µå°ºåº¦çš„å•é¡Œï¼ˆä¾‹å¦‚æ¹æµä¸­çš„å¤§å°ºåº¦çµæ§‹èˆ‡å°å°ºåº¦æ¸¦æ¼©ï¼‰
    """
    
    def __init__(self,
                 in_dim: int = 3,
                 out_dim: int = 4,
                 width: int = 128,
                 depth: int = 4,
                 num_scales: int = 3,
                 sigma_min: float = 1.0,
                 sigma_max: float = 10.0,
                 fourier_m: int = 16,
                 activation: str = 'tanh'):
        
        super().__init__()
        
        self.num_scales = num_scales
        self.out_dim = out_dim
        
        # ç”Ÿæˆå¤šå€‹ä¸åŒé »ç‡å°ºåº¦çš„å­ç¶²è·¯
        sigmas = np.logspace(np.log10(sigma_min), np.log10(sigma_max), num_scales)
        
        self.subnets = nn.ModuleList()
        for sigma in sigmas:
            subnet = PINNNet(
                in_dim=in_dim,
                out_dim=out_dim,
                width=width,
                depth=depth,
                fourier_m=fourier_m,
                fourier_sigma=float(sigma),
                activation=activation,
                use_fourier=True
            )
            self.subnets.append(subnet)
        
        # å°ºåº¦æ¬Šé‡ï¼ˆå¯å­¸ç¿’ï¼‰
        self.scale_weights = nn.Parameter(torch.ones(num_scales) / num_scales)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å¤šå°ºåº¦å‰å‘å‚³æ’­ï¼šå„å­ç¶²è·¯è¼¸å‡ºçš„åŠ æ¬Šçµ„åˆ
        """
        outputs = []
        for subnet in self.subnets:
            outputs.append(subnet(x))
        
        # å †ç–Š [num_scales, batch_size, out_dim]
        stacked = torch.stack(outputs, dim=0)
        
        # åŠ æ¬Šå¹³å‡ï¼š[batch_size, out_dim]
        weights = F.softmax(self.scale_weights, dim=0)
        weighted_output = torch.einsum('s,sbo->bo', weights, stacked)
        
        return weighted_output
    
    def get_num_params(self) -> int:
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


# ========== ä¾¿æ·å·¥å» å‡½æ•¸ ==========

def create_pinn_model(config: dict) -> nn.Module:
    """
    æ ¹æ“šé…ç½®å­—å…¸å»ºç«‹ PINN æ¨¡å‹
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸
    Returns:
        å»ºç«‹å¥½çš„ PINN æ¨¡å‹
    """
    model_type = config.get('type', 'standard')
    
    if model_type == 'standard' or model_type == 'enhanced_fourier_mlp':
        # ğŸ”§ è™•ç† VS-PINN ç¸®æ”¾å› å­ï¼ˆå¦‚æœæä¾›ï¼‰
        input_scale_factors = None
        if 'input_scale_factors' in config:
            scale_list = config['input_scale_factors']
            if isinstance(scale_list, (list, tuple)):
                input_scale_factors = torch.tensor(scale_list, dtype=torch.float32)
        
        return PINNNet(
            in_dim=config.get('in_dim', 3),
            out_dim=config.get('out_dim', 4),
            width=config.get('width', 256),
            depth=config.get('depth', 5),
            fourier_m=config.get('fourier_m', 32),
            fourier_sigma=config.get('fourier_sigma', 5.0),
            fourier_multiscale=config.get('fourier_multiscale', False),
            activation=config.get('activation', 'tanh'),
            use_fourier=config.get('use_fourier', True),
            trainable_fourier=config.get('trainable_fourier', False),
            use_residual=config.get('use_residual', False),
            use_layer_norm=config.get('use_layer_norm', False),
            use_input_projection=config.get('use_input_projection', False),
            dropout=config.get('dropout', 0.0),
            use_rwf=config.get('use_rwf', False),
            rwf_scale_mean=config.get('rwf_scale_mean', 0.0),
            rwf_scale_std=config.get('rwf_scale_std', 0.1),
            sine_omega_0=config.get('sine_omega_0', 1.0),  # é è¨­å€¼æ”¹ç‚º 1.0
            fourier_normalize_input=config.get('fourier_normalize_input', False),  # ğŸ”§ æ–°åƒæ•¸
            input_scale_factors=input_scale_factors  # ğŸ”§ æ–°åƒæ•¸
        )
    
    elif model_type == 'multiscale':
        return MultiScalePINNNet(
            in_dim=config.get('in_dim', 3),
            out_dim=config.get('out_dim', 4),
            width=config.get('width', 128),
            depth=config.get('depth', 4),
            num_scales=config.get('num_scales', 3),
            sigma_min=config.get('sigma_min', 1.0),
            sigma_max=config.get('sigma_max', 10.0),
            fourier_m=config.get('fourier_m', 16),
            activation=config.get('activation', 'tanh')
        )
    
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {model_type}")


def create_standard_pinn(**kwargs) -> PINNNet:
    """å»ºç«‹æ¨™æº– PINN æ¨¡å‹ (è¼•é‡ç´šé…ç½®)"""
    defaults = {
        'width': 128,
        'depth': 4,
        'fourier_m': 32,
        'activation': 'tanh',
        'use_residual': False,
        'use_layer_norm': False,
        'dropout': 0.0,
        'use_rwf': False
    }
    defaults.update(kwargs)
    return PINNNet(**defaults)


def create_enhanced_pinn(**kwargs) -> PINNNet:
    """å»ºç«‹å¢å¼· PINN æ¨¡å‹ (é«˜å®¹é‡é…ç½®)"""
    defaults = {
        'width': 256,
        'depth': 8,
        'fourier_m': 64,
        'fourier_multiscale': True,
        'activation': 'swish',
        'use_residual': True,
        'use_layer_norm': True,
        'use_input_projection': True,
        'dropout': 0.1,
        'use_rwf': False,
        'rwf_scale_std': 0.1
    }
    defaults.update(kwargs)
    return PINNNet(**defaults)


def multiscale_pinn(in_dim: int = 3, out_dim: int = 4, **kwargs) -> MultiScalePINNNet:
    """å¿«é€Ÿå»ºç«‹å¤šå°ºåº¦ PINN æ¨¡å‹"""
    return MultiScalePINNNet(in_dim=in_dim, out_dim=out_dim, **kwargs)


def init_siren_weights(model: PINNNet) -> None:
    """
    å°ä½¿ç”¨ Sine æ¿€æ´»çš„æ¨¡å‹æ‡‰ç”¨ SIREN æ¬Šé‡åˆå§‹åŒ–
    
    åƒè€ƒ: Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"
    
    åˆå§‹åŒ–è¦å‰‡ï¼š
    - ç¬¬ä¸€å±¤ï¼šU(-1/n_in, +1/n_in)
    - éš±è—å±¤ï¼šU(-sqrt(6/n_in)/omega_0, +sqrt(6/n_in)/omega_0)
    - è¼¸å‡ºå±¤ï¼šä¿æŒåŸå§‹åˆå§‹åŒ–ï¼ˆå°æ¬Šé‡ï¼‰
    
    æ”¯æ´ï¼š
    - æ¨™æº– nn.Linear å±¤
    - RWFLinear å±¤ï¼ˆé€é apply_siren_init() æ–¹æ³•ï¼‰
    
    Args:
        model: PINNNet æ¨¡å‹å¯¦ä¾‹ï¼ˆå¿…é ˆä½¿ç”¨ sine æ¿€æ´»ï¼‰
    """
    import numpy as np
    
    # æª¢æŸ¥ç¬¬ä¸€å±¤
    if len(model.hidden_layers) > 0:
        first_layer = model.hidden_layers[0]  # type: ignore
        if isinstance(first_layer, DenseLayer) and isinstance(first_layer.activation, SineActivation):
            # ç²å– omega_0 åƒæ•¸
            omega_0 = first_layer.activation.omega_0
            
            # ç¬¬ä¸€å±¤ç‰¹æ®Šåˆå§‹åŒ–
            first_linear = first_layer.linear
            
            if isinstance(first_linear, RWFLinear):
                # RWF è·¯å¾‘ï¼šä½¿ç”¨å°ˆç”¨åˆå§‹åŒ–æ–¹æ³•
                first_linear.apply_siren_init(omega_0, is_first=True)
                print(f"âœ… SIREN åˆå§‹åŒ–å®Œæˆï¼ˆRWF æ¨¡å¼ï¼‰ï¼šç¬¬ä¸€å±¤ omega_0={omega_0:.2f}")
                
                # åˆå§‹åŒ–å¾ŒçºŒ RWF å±¤
                for i, layer in enumerate(model.hidden_layers[1:], start=2):
                    if isinstance(layer, DenseLayer) and isinstance(layer.linear, RWFLinear):
                        layer.linear.apply_siren_init(omega_0, is_first=False)
                
            elif isinstance(first_linear, nn.Linear):
                # æ¨™æº– nn.Linear è·¯å¾‘
                n_in = first_linear.weight.shape[1]  # type: ignore
                bound = float(1.0 / n_in)  # type: ignore
                with torch.no_grad():
                    nn.init.uniform_(first_linear.weight, -bound, bound)  # type: ignore
                    nn.init.zeros_(first_linear.bias)  # type: ignore
                
                # å¾ŒçºŒå±¤å·²ç¶“åœ¨ DenseLayer.__init__ ä¸­è™•ç†
                print(f"âœ… SIREN åˆå§‹åŒ–å®Œæˆï¼ˆæ¨™æº–æ¨¡å¼ï¼‰ï¼šç¬¬ä¸€å±¤ bound=Â±{bound:.6f}")
            else:
                print(f"âš ï¸  æœªçŸ¥çš„ç·šæ€§å±¤é¡å‹: {type(first_linear)}")
        else:
            print("âš ï¸  æ¨¡å‹æœªä½¿ç”¨ Sine æ¿€æ´»ï¼Œè·³é SIREN åˆå§‹åŒ–")


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    print("=== æ¨™æº– PINNNet æ¸¬è©¦ ===")
    
    # å»ºç«‹æ¨™æº–æ¨¡å‹
    model_std = create_standard_pinn(in_dim=3, out_dim=4)
    print(f"æ¨™æº–æ¨¡å‹: {model_std}")
    print(f"åƒæ•¸ç¸½æ•¸: {model_std.get_num_params():,}")
    
    # å»ºç«‹å¢å¼·æ¨¡å‹
    model_enh = create_enhanced_pinn(in_dim=2, out_dim=3)
    print(f"\nå¢å¼·æ¨¡å‹: {model_enh}")
    print(f"åƒæ•¸ç¸½æ•¸: {model_enh.get_num_params():,}")
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    x = torch.randn(100, 3)
    with torch.no_grad():
        y_std = model_std(x)
    print(f"\næ¨™æº–æ¨¡å‹è¼¸å‡ºå½¢ç‹€: {y_std.shape}")
    
    x2 = torch.randn(100, 2)
    with torch.no_grad():
        y_enh = model_enh(x2)
    print(f"å¢å¼·æ¨¡å‹è¼¸å‡ºå½¢ç‹€: {y_enh.shape}")
    
    # æ¸¬è©¦æ¢¯åº¦è¨ˆç®—
    x.requires_grad_(True)
    y = model_std(x)
    u = y[:, 0]
    du_dx = torch.autograd.grad(u.sum(), x, create_graph=True)[0][:, 1]
    print(f"\næ¢¯åº¦è¨ˆç®—æˆåŠŸ: âˆ‚u/âˆ‚x å½¢ç‹€ = {du_dx.shape}")
    
    # æ¸¬è©¦å¤šå°ºåº¦æ¨¡å‹
    print("\n=== MultiScalePINNNet æ¸¬è©¦ ===")
    ms_model = multiscale_pinn(in_dim=3, out_dim=4, num_scales=2)
    print(f"å¤šå°ºåº¦æ¨¡å‹åƒæ•¸: {ms_model.get_num_params():,}")
    
    with torch.no_grad():
        y_ms = ms_model(x)
    print(f"å¤šå°ºåº¦è¼¸å‡ºå½¢ç‹€: {y_ms.shape}")
    
    print("\nâœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
