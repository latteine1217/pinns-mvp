"""
QR-pivot æ„Ÿæ¸¬é»é¸æ“‡ç®—æ³•

å¯¦ç¾åŸºæ–¼ QR åˆ†è§£çš„æœ€é©æ„Ÿæ¸¬é»é¸æ“‡ï¼Œé€™æ˜¯ç¨€ç–æ„Ÿæ¸¬èˆ‡é‡å»ºå•é¡Œçš„ç¶“å…¸æ–¹æ³•ã€‚
ç‰¹åˆ¥é©ç”¨æ–¼ PINNs é€†å•é¡Œä¸­çš„å°‘é‡è§€æ¸¬é»æœ€é©åŒ–é…ç½®ã€‚

æ ¸å¿ƒç®—æ³•ï¼š
1. QR-pivot: åŸºæ–¼ QR åˆ†è§£é¸ä¸»å…ƒçš„è²ªå¿ƒæœ€é©åŒ–
2. POD-based: çµåˆ POD æ¨¡æ…‹çš„æ„Ÿæ¸¬é»é…ç½®
3. Greedy: è²ªå¿ƒæœ€é©åŒ–ç­–ç•¥
4. Multi-objective: å¤šç›®æ¨™æœ€é©åŒ– (ç²¾åº¦ vs. ç©©å¥æ€§ vs. K)

åƒè€ƒæ–‡ç»ï¼š
- Sensor Selection via Convex Optimization (IEEE 2009)
- Sparsity-promoting optimal control for a class of distributed systems (SIAM 2012)
- Sparse sensor placement optimization for classification (SIAM 2016)
"""

import numpy as np
import torch
import torch.nn as nn
from typing import List, Dict, Tuple, Optional, Union, Callable, Any
from scipy.linalg import qr, svd
from scipy.optimize import differential_evolution, minimize
import logging
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


class BaseSensorSelector(ABC):
    """æ„Ÿæ¸¬é»é¸æ“‡å™¨åŸºé¡"""
    
    @abstractmethod
    def select_sensors(self, 
                      data_matrix: np.ndarray,
                      n_sensors: int,
                      **kwargs) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        é¸æ“‡æ„Ÿæ¸¬é»
        
        Args:
            data_matrix: è³‡æ–™çŸ©é™£ [n_samples, n_features] 
            n_sensors: æ„Ÿæ¸¬é»æ•¸é‡ K
            
        Returns:
            (selected_indices, metrics)
        """
        pass


class QRPivotSelector(BaseSensorSelector):
    """
    QR-pivot æ„Ÿæ¸¬é»é¸æ“‡å™¨
    
    ä½¿ç”¨ QR åˆ†è§£çš„é¸ä¸»å…ƒç­–ç•¥é¸æ“‡æœ€å…·ä»£è¡¨æ€§çš„æ„Ÿæ¸¬é»ã€‚
    é€™æ˜¯ç¶“å…¸çš„è²ªå¿ƒç®—æ³•ï¼Œè¨ˆç®—é«˜æ•ˆä¸”ç†è«–ä¿è­‰è‰¯å¥½ã€‚
    """
    
    def __init__(self, 
                 mode: str = 'column',
                 pivoting: bool = True,
                 regularization: float = 1e-12):
        """
        Args:
            mode: é¸æ“‡æ¨¡å¼ ('column' é¸åˆ—, 'row' é¸è¡Œ)
            pivoting: æ˜¯å¦ä½¿ç”¨é¸ä¸»å…ƒ
            regularization: æ­£å‰‡åŒ–é …é¿å…æ•¸å€¼ä¸ç©©å®š
        """
        self.mode = mode
        self.pivoting = pivoting
        self.regularization = regularization
    
    def select_sensors(self, 
                      data_matrix: np.ndarray,
                      n_sensors: int,
                      return_qr: bool = False) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        ä½¿ç”¨ QR-pivot é¸æ“‡æ„Ÿæ¸¬é»
        
        Args:
            data_matrix: è³‡æ–™çŸ©é™£ [n_samples, n_locations] (å¿«ç…§æ³•)
                        æˆ– [n_locations, n_modes] (POD æ¨¡æ…‹)
            n_sensors: æ„Ÿæ¸¬é»æ•¸é‡ K
            return_qr: æ˜¯å¦è¿”å› QR åˆ†è§£çµæœ
            
        Returns:
            (selected_indices, metrics)
        """
        # ç¢ºä¿æ•¸æ“šç‚º numpy æ•¸çµ„
        if isinstance(data_matrix, torch.Tensor):
            data_matrix = data_matrix.detach().cpu().numpy()
        
        X = data_matrix.copy()
        n_locations, n_features = X.shape
        
        # é™åˆ¶æ„Ÿæ¸¬é»æ•¸é‡
        n_sensors = min(n_sensors, n_locations, n_features)
        
        # æ·»åŠ æ­£å‰‡åŒ–é¿å…å¥‡ç•°çŸ©é™£
        if X.shape[1] > X.shape[0]:
            # å¯¬çŸ©é™£ï¼šæ·»åŠ è¡Œæ­£å‰‡åŒ–
            reg_rows = np.random.randn(max(1, X.shape[1] - X.shape[0]), X.shape[1]) * self.regularization
            X = np.vstack([X, reg_rows])
        
        try:
            if self.pivoting:
                # ä½¿ç”¨é¸ä¸»å…ƒ QR åˆ†è§£
                if self.mode == 'column':
                    # å° X^T åš QR åˆ†è§£é¸æ“‡åˆ— (å°æ‡‰åŸçŸ©é™£çš„è¡Œ)
                    Q, R, piv = qr(X.T, mode='economic', pivoting=True)
                    selected_indices = piv[:n_sensors]
                else:
                    # å° X åš QR åˆ†è§£é¸æ“‡è¡Œ
                    Q, R, piv = qr(X, mode='economic', pivoting=True)
                    selected_indices = piv[:n_sensors]
            else:
                # æ¨™æº– QR åˆ†è§£
                Q, R = qr(X.T if self.mode == 'column' else X, mode='economic')
                # ä½¿ç”¨å°è§’å…ƒç´ å¤§å°é¸æ“‡
                diag_importance = np.abs(np.diag(R))
                selected_indices = np.argsort(diag_importance)[-n_sensors:][::-1]
        
        except np.linalg.LinAlgError as e:
            logger.warning(f"QR åˆ†è§£å¤±æ•—ï¼Œä½¿ç”¨ SVD å›é€€: {e}")
            # å›é€€åˆ° SVD æ–¹æ³•
            U, s, Vt = svd(X, full_matrices=False)
            # ä½¿ç”¨å¥‡ç•°å€¼æ¬Šé‡é¸æ“‡
            importance = np.sum(np.abs(Vt.T) * s, axis=1)
            selected_indices = np.argsort(importance)[-n_sensors:][::-1]
        
        # ç¢ºä¿ç´¢å¼•åœ¨æœ‰æ•ˆç¯„åœå…§
        selected_indices = selected_indices[selected_indices < n_locations]
        selected_indices = selected_indices[:n_sensors]
        
        # è¨ˆç®—å“è³ªæŒ‡æ¨™
        metrics = self._compute_metrics(X, selected_indices)
        
        result = (selected_indices, metrics)
        if return_qr:
            result = (*result, Q, R)
        
        return result
    
    def _compute_metrics(self, 
                        data_matrix: np.ndarray, 
                        selected_indices: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—æ„Ÿæ¸¬é»é…ç½®çš„å“è³ªæŒ‡æ¨™"""
        
        selected_data = data_matrix[selected_indices, :]
        
        # æ¢ä»¶æ•¸
        try:
            cond_number = np.linalg.cond(selected_data @ selected_data.T + self.regularization * np.eye(len(selected_indices)))
        except:
            cond_number = np.inf
        
        # è¡Œåˆ—å¼ (é«”ç©)
        try:
            det_value = np.linalg.det(selected_data @ selected_data.T + self.regularization * np.eye(len(selected_indices)))
            log_det = np.log(max(det_value, 1e-16))
        except:
            log_det = -np.inf
        
        # è¦†è“‹ç‡ (å­ç©ºé–“è§’åº¦)
        try:
            U_full, s_full, _ = svd(data_matrix, full_matrices=False)
            U_selected, s_selected, _ = svd(selected_data, full_matrices=False)
            
            # è¨ˆç®—ä¸»å­ç©ºé–“ä¹‹é–“çš„è§’åº¦
            if len(s_selected) > 0 and len(s_full) > 0:
                n_compare = min(len(s_selected), len(s_full), 5)  # æ¯”è¼ƒå‰5å€‹æ¨¡æ…‹
                subspace_angle = np.trace(U_full[:, :n_compare].T @ U_selected[:, :n_compare]) / n_compare
                coverage = abs(subspace_angle)
            else:
                coverage = 0.0
        except:
            coverage = 0.0
        
        # å¥‡ç•°å€¼æ¯”ä¾‹ (èƒ½é‡ä¿ç•™)
        try:
            energy_ratio = np.sum(s_selected**2) / (np.sum(s_full**2) + 1e-16)
        except:
            energy_ratio = 0.0
        
        return {
            'condition_number': float(cond_number),
            'log_determinant': float(log_det),
            'subspace_coverage': float(coverage),
            'energy_ratio': float(energy_ratio),
            'n_sensors': len(selected_indices)
        }


class PODBasedSelector(BaseSensorSelector):
    """
    åŸºæ–¼ POD çš„æ„Ÿæ¸¬é»é¸æ“‡å™¨
    
    å…ˆé€²è¡Œ POD åˆ†è§£ï¼Œç„¶å¾Œåœ¨ POD æ¨¡æ…‹ç©ºé–“ä¸­é€²è¡Œæ„Ÿæ¸¬é»é¸æ“‡ã€‚
    é©ç”¨æ–¼å…·æœ‰æ˜ç¢ºä½ç¶­çµæ§‹çš„æµå ´è³‡æ–™ã€‚
    """
    
    def __init__(self,
                 n_modes: Optional[int] = None,
                 energy_threshold: float = 0.99,
                 mode_weighting: str = 'energy'):
        """
        Args:
            n_modes: POD æ¨¡æ…‹æ•¸é‡ (None ç‚ºè‡ªå‹•é¸æ“‡)
            energy_threshold: èƒ½é‡ä¿ç•™é–¾å€¼
            mode_weighting: æ¨¡æ…‹æ¬Šé‡ç­–ç•¥ ('energy', 'uniform', 'decay')
        """
        self.n_modes = n_modes
        self.energy_threshold = energy_threshold
        self.mode_weighting = mode_weighting
        
    def select_sensors(self, 
                      data_matrix: np.ndarray,
                      n_sensors: int) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        åŸºæ–¼ POD çš„æ„Ÿæ¸¬é»é¸æ“‡
        
        Args:
            data_matrix: å¿«ç…§çŸ©é™£ [n_locations, n_snapshots]
            n_sensors: æ„Ÿæ¸¬é»æ•¸é‡
            
        Returns:
            (selected_indices, metrics)
        """
        # ç¢ºä¿æ•¸æ“šç‚º numpy æ•¸çµ„
        if isinstance(data_matrix, torch.Tensor):
            data_matrix = data_matrix.detach().cpu().numpy()
        
        # POD åˆ†è§£
        U, s, Vt = svd(data_matrix, full_matrices=False)
        
        # ç¢ºå®š POD æ¨¡æ…‹æ•¸é‡
        if self.n_modes is None:
            cumulative_energy = np.cumsum(s**2) / np.sum(s**2)
            n_modes = np.argmax(cumulative_energy >= self.energy_threshold) + 1
            n_modes = min(n_modes, len(s))
        else:
            n_modes = min(self.n_modes, len(s))
        
        # æå– POD æ¨¡æ…‹
        pod_modes = U[:, :n_modes]  # [n_locations, n_modes]
        
        # æ ¹æ“šæ¨¡æ…‹æ¬Šé‡ç­–ç•¥èª¿æ•´
        if self.mode_weighting == 'energy':
            # ä½¿ç”¨å¥‡ç•°å€¼ä½œç‚ºæ¬Šé‡
            weights = s[:n_modes]
            weighted_modes = pod_modes * weights[np.newaxis, :]
        elif self.mode_weighting == 'uniform':
            # çµ±ä¸€æ¬Šé‡
            weighted_modes = pod_modes
        elif self.mode_weighting == 'decay':
            # æŒ‡æ•¸è¡°æ¸›æ¬Šé‡
            weights = np.exp(-np.arange(n_modes) / max(1, n_modes / 3))
            weighted_modes = pod_modes * weights[np.newaxis, :]
        else:
            weighted_modes = pod_modes
        
        # åœ¨ POD æ¨¡æ…‹ç©ºé–“ä¸­ä½¿ç”¨ QR-pivot é¸æ“‡
        qr_selector = QRPivotSelector(mode='row', pivoting=True)
        selected_indices, qr_metrics = qr_selector.select_sensors(weighted_modes, n_sensors)
        
        # è¨ˆç®— POD ç›¸é—œæŒ‡æ¨™
        pod_metrics = {
            'n_pod_modes': n_modes,
            'pod_energy_ratio': float(np.sum(s[:n_modes]**2) / np.sum(s**2)),
            'effective_rank': float(np.sum(s**2)**2 / np.sum(s**4)),  # æœ‰æ•ˆç§©
        }
        
        # åˆä½µæŒ‡æ¨™
        metrics = {**qr_metrics, **pod_metrics}
        
        return selected_indices, metrics


class GreedySelector(BaseSensorSelector):
    """
    è²ªå¿ƒæ„Ÿæ¸¬é»é¸æ“‡å™¨
    
    ä½¿ç”¨è²ªå¿ƒç®—æ³•é€æ­¥é¸æ“‡æœ€å¤§åŒ–æŸå€‹ç›®æ¨™å‡½æ•¸çš„æ„Ÿæ¸¬é»ã€‚
    æ”¯æ´å¤šç¨®ç›®æ¨™å‡½æ•¸ï¼šè³‡è¨Šå¢ç›Šã€æ¢ä»¶æ•¸æœ€é©åŒ–ã€èƒ½é‡æœ€å¤§åŒ–ç­‰ã€‚
    """
    
    def __init__(self,
                 objective: str = 'info_gain',
                 regularization: float = 1e-8):
        """
        Args:
            objective: ç›®æ¨™å‡½æ•¸ ('info_gain', 'condition', 'energy', 'determinant')
            regularization: æ­£å‰‡åŒ–åƒæ•¸
        """
        self.objective = objective
        self.regularization = regularization
        
    def select_sensors(self, 
                      data_matrix: np.ndarray,
                      n_sensors: int) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        è²ªå¿ƒæ„Ÿæ¸¬é»é¸æ“‡
        
        Args:
            data_matrix: è³‡æ–™çŸ©é™£ [n_locations, n_features]
            n_sensors: æ„Ÿæ¸¬é»æ•¸é‡
            
        Returns:
            (selected_indices, metrics)
        """
        if isinstance(data_matrix, torch.Tensor):
            data_matrix = data_matrix.detach().cpu().numpy()
        
        n_locations, n_features = data_matrix.shape
        n_sensors = min(n_sensors, n_locations)
        
        selected_indices = []
        remaining_indices = list(range(n_locations))
        objective_values = []
        
        for step in range(n_sensors):
            best_idx = None
            best_objective = -np.inf
            
            for candidate_idx in remaining_indices:
                # æš«æ™‚æ·»åŠ å€™é¸é»
                test_indices = selected_indices + [candidate_idx]
                test_data = data_matrix[test_indices, :]
                
                # è¨ˆç®—ç›®æ¨™å‡½æ•¸å€¼
                objective_val = self._compute_objective(test_data)
                
                if objective_val > best_objective:
                    best_objective = objective_val
                    best_idx = candidate_idx
            
            # æ·»åŠ æœ€ä½³å€™é¸é»
            if best_idx is not None:
                selected_indices.append(best_idx)
                remaining_indices.remove(best_idx)
                objective_values.append(best_objective)
            else:
                logger.warning(f"ç„¡æ³•åœ¨ç¬¬ {step+1} æ­¥æ‰¾åˆ°æœ‰æ•ˆçš„æ„Ÿæ¸¬é»")
                break
        
        selected_indices = np.array(selected_indices)
        
        # è¨ˆç®—æœ€çµ‚æŒ‡æ¨™
        final_data = data_matrix[selected_indices, :]
        metrics = {
            'final_objective': float(best_objective),
            'objective_progression': objective_values,
            'greedy_efficiency': float(len(selected_indices) / n_sensors),
        }
        
        # æ·»åŠ åŸºæœ¬æŒ‡æ¨™
        qr_selector = QRPivotSelector()
        basic_metrics = qr_selector._compute_metrics(data_matrix, selected_indices)
        metrics.update(basic_metrics)
        
        return selected_indices, metrics
    
    def _compute_objective(self, data_subset: np.ndarray) -> float:
        """è¨ˆç®—ç›®æ¨™å‡½æ•¸å€¼"""
        
        if data_subset.shape[0] == 0:
            return -np.inf
        
        try:
            gram_matrix = data_subset @ data_subset.T + self.regularization * np.eye(data_subset.shape[0])
            
            if self.objective == 'info_gain':
                # è³‡è¨Šå¢ç›Š = log det(Gram)
                sign, logdet = np.linalg.slogdet(gram_matrix)
                return logdet if sign > 0 else -np.inf
                
            elif self.objective == 'condition':
                # æ¢ä»¶æ•¸çš„å€’æ•¸ (è¶Šå¤§è¶Šå¥½)
                cond = np.linalg.cond(gram_matrix)
                return -np.log(cond + 1e-16)
                
            elif self.objective == 'energy':
                # èƒ½é‡ = trace(Gram)
                return np.trace(gram_matrix)
                
            elif self.objective == 'determinant':
                # è¡Œåˆ—å¼
                det = np.linalg.det(gram_matrix)
                return det if det > 0 else -np.inf
                
            else:
                raise ValueError(f"æœªçŸ¥çš„ç›®æ¨™å‡½æ•¸: {self.objective}")
                
        except np.linalg.LinAlgError:
            return -np.inf


class MultiObjectiveSelector(BaseSensorSelector):
    """
    å¤šç›®æ¨™æ„Ÿæ¸¬é»é¸æ“‡å™¨
    
    åŒæ™‚æœ€é©åŒ–å¤šå€‹ç›®æ¨™ï¼šç²¾åº¦ã€ç©©å¥æ€§ã€æ„Ÿæ¸¬é»æ•¸é‡ç­‰ã€‚
    ä½¿ç”¨é€²åŒ–ç®—æ³•æˆ–æ¢¯åº¦ç‚ºåŸºç¤çš„å¤šç›®æ¨™æœ€é©åŒ–ã€‚
    """
    
    def __init__(self,
                 objectives: List[str] = ['accuracy', 'robustness', 'efficiency'],
                 weights: Optional[List[float]] = None,
                 method: str = 'weighted_sum',
                 max_iterations: int = 100):
        """
        Args:
            objectives: ç›®æ¨™å‡½æ•¸åˆ—è¡¨
            weights: ç›®æ¨™æ¬Šé‡ (None ç‚ºç­‰æ¬Šé‡)
            method: å¤šç›®æ¨™æ–¹æ³• ('weighted_sum', 'pareto', 'lexicographic')
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•¸
        """
        self.objectives = objectives
        self.weights = weights or [1.0/len(objectives)] * len(objectives)
        self.method = method
        self.max_iterations = max_iterations
        
    def select_sensors(self, 
                      data_matrix: np.ndarray,
                      n_sensors: int,
                      noise_level: float = 0.01) -> Tuple[np.ndarray, Dict[str, float]]:
        """
        å¤šç›®æ¨™æ„Ÿæ¸¬é»é¸æ“‡
        
        Args:
            data_matrix: è³‡æ–™çŸ©é™£
            n_sensors: æ„Ÿæ¸¬é»æ•¸é‡
            noise_level: é›œè¨Šæ°´æº– (ç”¨æ–¼ç©©å¥æ€§è©•ä¼°)
            
        Returns:
            (selected_indices, metrics)
        """
        if isinstance(data_matrix, torch.Tensor):
            data_matrix = data_matrix.detach().cpu().numpy()
        
        n_locations = data_matrix.shape[0]
        
        if self.method == 'weighted_sum':
            return self._weighted_sum_optimization(data_matrix, n_sensors, noise_level)
        elif self.method == 'pareto':
            return self._pareto_optimization(data_matrix, n_sensors, noise_level)
        else:
            # å›é€€åˆ° QR-pivot
            logger.warning(f"æœªå¯¦ç¾çš„å¤šç›®æ¨™æ–¹æ³• {self.method}ï¼Œä½¿ç”¨ QR-pivot")
            qr_selector = QRPivotSelector()
            return qr_selector.select_sensors(data_matrix, n_sensors)
    
    def _weighted_sum_optimization(self, 
                                 data_matrix: np.ndarray, 
                                 n_sensors: int, 
                                 noise_level: float) -> Tuple[np.ndarray, Dict[str, float]]:
        """åŠ æ¬Šå’Œå¤šç›®æ¨™æœ€é©åŒ–"""
        
        n_locations = data_matrix.shape[0]
        
        def objective_function(binary_selection):
            """ç›®æ¨™å‡½æ•¸ï¼šäºŒé€²åˆ¶é¸æ“‡å‘é‡ -> æ¨™é‡ç›®æ¨™å€¼"""
            indices = np.where(binary_selection > 0.5)[0]
            if len(indices) == 0:
                return 1e10  # æ‡²ç½°ç©ºé¸æ“‡
            
            # èª¿æ•´é¸æ“‡çš„æ„Ÿæ¸¬é»æ•¸é‡
            if len(indices) > n_sensors:
                # å¦‚æœé¸æ“‡å¤ªå¤šï¼Œä¿ç•™æœ€é‡è¦çš„
                importance = np.sum(np.abs(data_matrix[indices, :]), axis=1)
                top_indices = np.argsort(importance)[-n_sensors:]
                indices = indices[top_indices]
            
            objectives_values = self._compute_multi_objectives(data_matrix, indices, noise_level)
            
            # åŠ æ¬Šçµ„åˆ
            weighted_objective = sum(w * obj for w, obj in zip(self.weights, objectives_values))
            
            # æ‡²ç½°é …ï¼šæ„Ÿæ¸¬é»æ•¸é‡åå·®
            count_penalty = abs(len(indices) - n_sensors) * 0.1
            
            return -weighted_objective + count_penalty  # è² è™Ÿå› ç‚ºè¦æœ€å¤§åŒ–
        
        # ä½¿ç”¨å·®åˆ†é€²åŒ–ç®—æ³•
        bounds = [(0, 1)] * n_locations
        
        result = differential_evolution(
            objective_function,
            bounds,
            maxiter=self.max_iterations,
            popsize=min(15, max(10, n_locations // 10)),
            seed=42,
            atol=1e-6,
            tol=1e-6
        )
        
        # æå–é¸æ“‡çš„æ„Ÿæ¸¬é»
        binary_solution = result.x
        selected_indices = np.where(binary_solution > 0.5)[0]
        
        # å¦‚æœæ•¸é‡ä¸å°ï¼Œä½¿ç”¨è²ªå¿ƒèª¿æ•´
        if len(selected_indices) != n_sensors:
            if len(selected_indices) > n_sensors:
                # ç§»é™¤é‡è¦æ€§è¼ƒä½çš„é»
                importance = np.sum(np.abs(data_matrix[selected_indices, :]), axis=1)
                top_k = np.argsort(importance)[-n_sensors:]
                selected_indices = selected_indices[top_k]
            else:
                # æ·»åŠ é‡è¦æ€§è¼ƒé«˜çš„é»
                remaining = np.setdiff1d(np.arange(n_locations), selected_indices)
                importance = np.sum(np.abs(data_matrix[remaining, :]), axis=1)
                n_add = n_sensors - len(selected_indices)
                top_add = np.argsort(importance)[-n_add:]
                selected_indices = np.concatenate([selected_indices, remaining[top_add]])
        
        # è¨ˆç®—æœ€çµ‚æŒ‡æ¨™
        final_objectives = self._compute_multi_objectives(data_matrix, selected_indices, noise_level)
        
        metrics = {
            'multi_objective_score': float(-result.fun),
            'optimization_success': bool(result.success),
            'n_iterations': int(result.nit),
        }
        
        # æ·»åŠ å„å€‹ç›®æ¨™çš„å€¼
        for i, obj_name in enumerate(self.objectives):
            metrics[f'objective_{obj_name}'] = float(final_objectives[i])
        
        return selected_indices, metrics
    
    def _compute_multi_objectives(self, 
                                data_matrix: np.ndarray, 
                                indices: np.ndarray, 
                                noise_level: float) -> List[float]:
        """è¨ˆç®—å¤šå€‹ç›®æ¨™å‡½æ•¸å€¼"""
        
        if len(indices) == 0:
            return [0.0] * len(self.objectives)
        
        selected_data = data_matrix[indices, :]
        objectives_values = []
        
        for obj_name in self.objectives:
            if obj_name == 'accuracy':
                # ç²¾åº¦ï¼šæ¢ä»¶æ•¸çš„å€’æ•¸
                try:
                    gram = selected_data @ selected_data.T + 1e-12 * np.eye(len(indices))
                    cond = np.linalg.cond(gram)
                    accuracy = 1.0 / (1.0 + np.log(cond + 1e-16))
                except:
                    accuracy = 0.0
                objectives_values.append(accuracy)
                
            elif obj_name == 'robustness':
                # ç©©å¥æ€§ï¼šå°é›œè¨Šçš„æ•æ„Ÿåº¦
                try:
                    # æ·»åŠ é›œè¨Šä¸¦è¨ˆç®—é‡å»ºèª¤å·®
                    noisy_data = selected_data + noise_level * np.random.randn(*selected_data.shape)
                    reconstruction_error = np.linalg.norm(noisy_data - selected_data, 'fro')
                    robustness = 1.0 / (1.0 + reconstruction_error)
                except:
                    robustness = 0.0
                objectives_values.append(robustness)
                
            elif obj_name == 'efficiency':
                # æ•ˆç‡ï¼šå–®ä½æ„Ÿæ¸¬é»çš„è³‡è¨Šé‡
                try:
                    info_content = np.linalg.slogdet(selected_data @ selected_data.T + 1e-12 * np.eye(len(indices)))[1]
                    efficiency = info_content / max(1, len(indices))
                except:
                    efficiency = 0.0
                objectives_values.append(efficiency)
                
            elif obj_name == 'coverage':
                # è¦†è“‹ç‡ï¼šç©ºé–“åˆ†ä½ˆçš„å‡å‹»æ€§
                if len(indices) > 1:
                    # è¨ˆç®—æ„Ÿæ¸¬é»ä¹‹é–“çš„æœ€å°è·é›¢
                    min_dist = np.min([np.linalg.norm(data_matrix[i] - data_matrix[j]) 
                                     for i in indices for j in indices if i != j])
                    coverage = min_dist / (np.linalg.norm(data_matrix.max(axis=0) - data_matrix.min(axis=0)) + 1e-16)
                else:
                    coverage = 0.0
                objectives_values.append(coverage)
                
            else:
                objectives_values.append(0.0)
        
        return objectives_values
    
    def _pareto_optimization(self, 
                           data_matrix: np.ndarray, 
                           n_sensors: int, 
                           noise_level: float) -> Tuple[np.ndarray, Dict[str, float]]:
        """Pareto å‰æ²¿å¤šç›®æ¨™æœ€é©åŒ– (ç°¡åŒ–ç‰ˆ)"""
        
        # ç°¡åŒ–å¯¦ç¾ï¼šç”Ÿæˆå¤šå€‹å€™é¸è§£ï¼Œé¸æ“‡ Pareto æœ€é©
        n_candidates = min(50, data_matrix.shape[0])
        candidates = []
        
        # ä½¿ç”¨ä¸åŒç­–ç•¥ç”Ÿæˆå€™é¸è§£
        selectors = [
            QRPivotSelector(mode='column'),
            PODBasedSelector(n_modes=min(10, data_matrix.shape[1] // 2)),
            GreedySelector(objective='info_gain'),
            GreedySelector(objective='condition')
        ]
        
        for selector in selectors:
            try:
                indices, _ = selector.select_sensors(data_matrix, n_sensors)
                objectives = self._compute_multi_objectives(data_matrix, indices, noise_level)
                candidates.append((indices, objectives))
            except:
                continue
        
        # æ·»åŠ éš¨æ©Ÿå€™é¸
        for _ in range(n_candidates - len(candidates)):
            random_indices = np.random.choice(data_matrix.shape[0], n_sensors, replace=False)
            objectives = self._compute_multi_objectives(data_matrix, random_indices, noise_level)
            candidates.append((random_indices, objectives))
        
        # æ‰¾åˆ° Pareto å‰æ²¿
        pareto_candidates = self._find_pareto_front(candidates)
        
        if pareto_candidates:
            # å¾ Pareto å‰æ²¿ä¸­é¸æ“‡åŠ æ¬Šæœ€ä½³è§£
            best_score = -np.inf
            best_solution = None
            
            for indices, objectives in pareto_candidates:
                weighted_score = sum(w * obj for w, obj in zip(self.weights, objectives))
                if weighted_score > best_score:
                    best_score = weighted_score
                    best_solution = (indices, objectives)
            
            selected_indices, final_objectives = best_solution
        else:
            # å›é€€åˆ°ç¬¬ä¸€å€‹å€™é¸
            selected_indices, final_objectives = candidates[0]
        
        metrics = {
            'pareto_front_size': len(pareto_candidates),
            'n_candidates_evaluated': len(candidates),
            'pareto_score': float(best_score),
        }
        
        for i, obj_name in enumerate(self.objectives):
            metrics[f'objective_{obj_name}'] = float(final_objectives[i])
        
        return selected_indices, metrics
    
    def _find_pareto_front(self, candidates: List[Tuple]) -> List[Tuple]:
        """æ‰¾åˆ° Pareto å‰æ²¿"""
        pareto_front = []
        
        for candidate in candidates:
            is_dominated = False
            
            for other in candidates:
                if candidate == other:
                    continue
                
                # æª¢æŸ¥æ˜¯å¦è¢«æ”¯é…ï¼ˆæ‰€æœ‰ç›®æ¨™éƒ½ä¸å„ªæ–¼å…¶ä»–è§£ï¼‰
                candidate_objectives = candidate[1]
                other_objectives = other[1]
                
                if all(c <= o for c, o in zip(candidate_objectives, other_objectives)) and \
                   any(c < o for c, o in zip(candidate_objectives, other_objectives)):
                    is_dominated = True
                    break
            
            if not is_dominated:
                pareto_front.append(candidate)
        
        return pareto_front


class SensorOptimizer:
    """
    æ„Ÿæ¸¬é»æœ€é©åŒ–å™¨
    
    æä¾›é«˜å±¤ç´šçš„æ„Ÿæ¸¬é»é¸æ“‡æ¥å£ï¼Œæ•´åˆå¤šç¨®ç®—æ³•ä¸¦æ”¯æ´è‡ªå‹•è¶…åƒæ•¸èª¿å„ªã€‚
    """
    
    def __init__(self,
                 strategy: str = 'auto',
                 config: Optional[Dict] = None):
        """
        Args:
            strategy: é¸æ“‡ç­–ç•¥ ('qr_pivot', 'pod_based', 'greedy', 'multi_objective', 'auto')
            config: ç­–ç•¥é…ç½®å­—å…¸
        """
        self.strategy = strategy
        self.config = config or {}
        
    def optimize_sensor_placement(self,
                                 data_matrix: np.ndarray,
                                 n_sensors: int,
                                 validation_data: Optional[np.ndarray] = None) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        æœ€é©åŒ–æ„Ÿæ¸¬é»é…ç½®
        
        Args:
            data_matrix: è¨“ç·´è³‡æ–™çŸ©é™£
            n_sensors: æ„Ÿæ¸¬é»æ•¸é‡
            validation_data: é©—è­‰è³‡æ–™ (ç”¨æ–¼è©•ä¼°)
            
        Returns:
            (optimal_indices, comprehensive_metrics)
        """
        if self.strategy == 'auto':
            return self._auto_strategy_selection(data_matrix, n_sensors, validation_data)
        else:
            selector = self._create_selector(self.strategy)
            selected_indices, metrics = selector.select_sensors(data_matrix, n_sensors)
            
            # å¦‚æœæœ‰é©—è­‰è³‡æ–™ï¼Œè¨ˆç®—é©—è­‰æŒ‡æ¨™
            if validation_data is not None:
                validation_metrics = self._evaluate_on_validation(
                    data_matrix, validation_data, selected_indices)
                metrics.update(validation_metrics)
            
            return selected_indices, metrics
    
    def _create_selector(self, strategy: str) -> BaseSensorSelector:
        """å‰µå»ºç‰¹å®šç­–ç•¥çš„é¸æ“‡å™¨"""
        
        if strategy == 'qr_pivot':
            return QRPivotSelector(**self.config.get('qr_pivot', {}))
        elif strategy == 'pod_based':
            return PODBasedSelector(**self.config.get('pod_based', {}))
        elif strategy == 'greedy':
            return GreedySelector(**self.config.get('greedy', {}))
        elif strategy == 'multi_objective':
            return MultiObjectiveSelector(**self.config.get('multi_objective', {}))
        else:
            raise ValueError(f"æœªçŸ¥çš„æ„Ÿæ¸¬é»é¸æ“‡ç­–ç•¥: {strategy}")
    
    def _auto_strategy_selection(self,
                               data_matrix: np.ndarray,
                               n_sensors: int,
                               validation_data: Optional[np.ndarray]) -> Tuple[np.ndarray, Dict[str, Any]]:
        """è‡ªå‹•ç­–ç•¥é¸æ“‡"""
        
        # åˆ†æè³‡æ–™ç‰¹æ€§
        n_locations, n_features = data_matrix.shape
        data_rank = np.linalg.matrix_rank(data_matrix)
        aspect_ratio = n_features / n_locations
        
        # æ ¹æ“šè³‡æ–™ç‰¹æ€§é¸æ“‡ç­–ç•¥
        if data_rank < min(n_locations, n_features) * 0.8:
            # ä½ç§©è³‡æ–™ï¼šä½¿ç”¨ POD
            strategy = 'pod_based'
            logger.info("æª¢æ¸¬åˆ°ä½ç§©çµæ§‹ï¼Œä½¿ç”¨ POD-based ç­–ç•¥")
        elif aspect_ratio > 2.0:
            # å¯¬çŸ©é™£ï¼šä½¿ç”¨ QR-pivot
            strategy = 'qr_pivot'
            logger.info("æª¢æ¸¬åˆ°å¯¬çŸ©é™£çµæ§‹ï¼Œä½¿ç”¨ QR-pivot ç­–ç•¥")
        elif n_sensors / n_locations < 0.1:
            # æ¥µç¨€ç–æ„Ÿæ¸¬ï¼šä½¿ç”¨å¤šç›®æ¨™æœ€é©åŒ–
            strategy = 'multi_objective'
            logger.info("æª¢æ¸¬åˆ°æ¥µç¨€ç–æ„Ÿæ¸¬éœ€æ±‚ï¼Œä½¿ç”¨å¤šç›®æ¨™æœ€é©åŒ–")
        else:
            # é è¨­ï¼šè²ªå¿ƒç®—æ³•
            strategy = 'greedy'
            logger.info("ä½¿ç”¨é è¨­è²ªå¿ƒç­–ç•¥")
        
        # åŸ·è¡Œé¸æ“‡
        selector = self._create_selector(strategy)
        selected_indices, metrics = selector.select_sensors(data_matrix, n_sensors)
        
        # æ·»åŠ è‡ªå‹•é¸æ“‡ä¿¡æ¯
        metrics['auto_selected_strategy'] = strategy
        metrics['data_analysis'] = {
            'rank': int(data_rank),
            'aspect_ratio': float(aspect_ratio),
            'sparsity_ratio': float(n_sensors / n_locations)
        }
        
        # é©—è­‰è©•ä¼°
        if validation_data is not None:
            validation_metrics = self._evaluate_on_validation(
                data_matrix, validation_data, selected_indices)
            metrics.update(validation_metrics)
        
        return selected_indices, metrics
    
    def _evaluate_on_validation(self,
                              train_data: np.ndarray,
                              validation_data: np.ndarray,
                              selected_indices: np.ndarray) -> Dict[str, float]:
        """åœ¨é©—è­‰è³‡æ–™ä¸Šè©•ä¼°æ„Ÿæ¸¬é»é…ç½®"""
        
        try:
            # ä½¿ç”¨é¸æ“‡çš„æ„Ÿæ¸¬é»é€²è¡Œé‡å»º
            sensor_data_train = train_data[selected_indices, :]
            sensor_data_val = validation_data[selected_indices, :]
            
            # è¨ˆç®—é‡å»ºèª¤å·®ï¼ˆç°¡å–®ç·šæ€§é‡å»ºï¼‰
            if sensor_data_train.shape[0] >= sensor_data_train.shape[1]:
                # è¶…å®šç³»çµ±
                reconstruction_matrix = np.linalg.pinv(sensor_data_train)
                coefficients = reconstruction_matrix @ validation_data
                reconstructed = sensor_data_train @ coefficients
            else:
                # æ¬ å®šç³»çµ±
                regularization = 1e-6
                gram = sensor_data_train @ sensor_data_train.T + regularization * np.eye(sensor_data_train.shape[0])
                reconstruction_matrix = sensor_data_train.T @ np.linalg.pinv(gram)
                coefficients = reconstruction_matrix @ sensor_data_val
                reconstructed = train_data @ coefficients
            
            # è¨ˆç®—èª¤å·®æŒ‡æ¨™
            mse = np.mean((validation_data - reconstructed)**2)
            relative_error = np.linalg.norm(validation_data - reconstructed, 'fro') / \
                           (np.linalg.norm(validation_data, 'fro') + 1e-16)
            
            return {
                'validation_mse': float(mse),
                'validation_relative_error': float(relative_error),
                'reconstruction_rank': int(np.linalg.matrix_rank(reconstruction_matrix))
            }
            
        except Exception as e:
            logger.warning(f"é©—è­‰è©•ä¼°å¤±æ•—: {e}")
            return {
                'validation_mse': np.inf,
                'validation_relative_error': np.inf,
                'reconstruction_rank': 0
            }


def evaluate_sensor_placement(data_matrix: np.ndarray,
                            selected_indices: np.ndarray,
                            test_data: Optional[np.ndarray] = None,
                            noise_levels: List[float] = [0.01, 0.05, 0.1]) -> Dict[str, Any]:
    """
    è©•ä¼°æ„Ÿæ¸¬é»é…ç½®çš„å“è³ª
    
    Args:
        data_matrix: åŸå§‹è³‡æ–™çŸ©é™£
        selected_indices: é¸æ“‡çš„æ„Ÿæ¸¬é»ç´¢å¼•
        test_data: æ¸¬è©¦è³‡æ–™ (å¯é¸)
        noise_levels: é›œè¨Šæ°´æº–åˆ—è¡¨
        
    Returns:
        ç¶œåˆè©•ä¼°æŒ‡æ¨™å­—å…¸
    """
    metrics = {}
    
    # åŸºæœ¬æŒ‡æ¨™
    qr_selector = QRPivotSelector()
    basic_metrics = qr_selector._compute_metrics(data_matrix, selected_indices)
    metrics.update(basic_metrics)
    
    # é›œè¨Šç©©å¥æ€§æ¸¬è©¦
    if test_data is not None:
        robustness_metrics = {}
        
        for noise_level in noise_levels:
            try:
                # æ·»åŠ é›œè¨Š
                noisy_test = test_data + noise_level * np.random.randn(*test_data.shape)
                
                # é‡å»ºæ¸¬è©¦
                sensor_train = data_matrix[selected_indices, :]
                sensor_test = noisy_test[selected_indices, :]
                
                # ç°¡å–®ç·šæ€§é‡å»º
                reconstruction_matrix = np.linalg.pinv(sensor_train)
                reconstructed = sensor_train @ (reconstruction_matrix @ test_data)
                
                # è¨ˆç®—èª¤å·®
                reconstruction_error = np.linalg.norm(reconstructed - test_data, 'fro') / \
                                     (np.linalg.norm(test_data, 'fro') + 1e-16)
                
                robustness_metrics[f'noise_{noise_level}_error'] = float(reconstruction_error)
                
            except Exception as e:
                robustness_metrics[f'noise_{noise_level}_error'] = np.inf
        
        metrics['robustness'] = robustness_metrics
    
    # å¹¾ä½•åˆ†ä½ˆåˆ†æ
    if len(selected_indices) > 1:
        coordinates = data_matrix[selected_indices, :2] if data_matrix.shape[1] >= 2 else data_matrix[selected_indices, :]
        
        # è¨ˆç®—æœ€å°è·é›¢
        min_distance = np.inf
        max_distance = 0.0
        
        for i in range(len(selected_indices)):
            for j in range(i+1, len(selected_indices)):
                dist = np.linalg.norm(coordinates[i] - coordinates[j])
                min_distance = min(min_distance, dist)
                max_distance = max(max_distance, dist)
        
        metrics['geometry'] = {
            'min_sensor_distance': float(min_distance),
            'max_sensor_distance': float(max_distance),
            'distance_ratio': float(max_distance / (min_distance + 1e-16))
        }
    
    return metrics


def create_sensor_selector(strategy: str = 'qr_pivot', 
                         **kwargs) -> BaseSensorSelector:
    """
    å‰µå»ºæ„Ÿæ¸¬é»é¸æ“‡å™¨çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        strategy: é¸æ“‡ç­–ç•¥
        **kwargs: ç­–ç•¥ç‰¹å®šåƒæ•¸
        
    Returns:
        æ„Ÿæ¸¬é»é¸æ“‡å™¨å¯¦ä¾‹
    """
    if strategy == 'qr_pivot':
        return QRPivotSelector(**kwargs)
    elif strategy == 'pod_based':
        return PODBasedSelector(**kwargs)
    elif strategy == 'greedy':
        return GreedySelector(**kwargs)
    elif strategy == 'multi_objective':
        return MultiObjectiveSelector(**kwargs)
    else:
        raise ValueError(f"æœªçŸ¥çš„æ„Ÿæ¸¬é»é¸æ“‡ç­–ç•¥: {strategy}")


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    print("ğŸ§ª æ¸¬è©¦æ„Ÿæ¸¬é»é¸æ“‡æ¨¡çµ„...")
    
    # å‰µå»ºæ¸¬è©¦è³‡æ–™
    np.random.seed(42)
    n_locations = 100
    n_snapshots = 50
    
    # æ¨¡æ“¬ä½ç¶­æµå ´è³‡æ–™
    t = np.linspace(0, 2*np.pi, n_snapshots)
    x = np.linspace(0, 1, n_locations)
    
    # å‰µå»ºå«æœ‰å¹¾å€‹ä¸»è¦æ¨¡æ…‹çš„è³‡æ–™
    data_matrix = np.zeros((n_locations, n_snapshots))
    for i in range(3):  # 3å€‹ä¸»è¦æ¨¡æ…‹
        mode = np.sin((i+1) * np.pi * x[:, np.newaxis])
        coeff = np.cos((i+1) * t) * np.exp(-0.1 * i)
        data_matrix += mode @ coeff[np.newaxis, :]
    
    # æ·»åŠ é›œè¨Š
    data_matrix += 0.01 * np.random.randn(n_locations, n_snapshots)
    
    n_sensors = 8
    
    # æ¸¬è©¦ä¸åŒçš„é¸æ“‡ç­–ç•¥
    strategies = {
        'QR-Pivot': QRPivotSelector(),
        'POD-based': PODBasedSelector(n_modes=5),
        'Greedy': GreedySelector(objective='info_gain'),
        'Multi-objective': MultiObjectiveSelector(objectives=['accuracy', 'robustness'])
    }
    
    results = {}
    
    for name, selector in strategies.items():
        print(f"\næ¸¬è©¦ {name} ç­–ç•¥...")
        try:
            indices, metrics = selector.select_sensors(data_matrix, n_sensors)
            results[name] = {
                'indices': indices,
                'condition_number': metrics.get('condition_number', np.inf),
                'energy_ratio': metrics.get('energy_ratio', 0.0),
                'n_selected': len(indices)
            }
            print(f"  é¸æ“‡æ„Ÿæ¸¬é»: {len(indices)} å€‹")
            print(f"  æ¢ä»¶æ•¸: {metrics.get('condition_number', 'N/A'):.2f}")
            print(f"  èƒ½é‡æ¯”ä¾‹: {metrics.get('energy_ratio', 0.0):.3f}")
        except Exception as e:
            print(f"  âŒ å¤±æ•—: {e}")
            results[name] = {'error': str(e)}
    
    # æ¸¬è©¦è‡ªå‹•ç­–ç•¥é¸æ“‡
    print(f"\næ¸¬è©¦è‡ªå‹•ç­–ç•¥é¸æ“‡...")
    optimizer = SensorOptimizer(strategy='auto')
    auto_indices, auto_metrics = optimizer.optimize_sensor_placement(data_matrix, n_sensors)
    print(f"  è‡ªå‹•é¸æ“‡ç­–ç•¥: {auto_metrics.get('auto_selected_strategy', 'unknown')}")
    print(f"  é¸æ“‡æ„Ÿæ¸¬é»: {len(auto_indices)} å€‹")
    
    # è©•ä¼°æ‰€æœ‰ç­–ç•¥
    print(f"\nç¶œåˆè©•ä¼°...")
    for name, result in results.items():
        if 'error' not in result:
            eval_metrics = evaluate_sensor_placement(data_matrix, result['indices'])
            print(f"  {name}: æ¢ä»¶æ•¸={eval_metrics.get('condition_number', 'N/A'):.2f}, "
                  f"è¦†è“‹ç‡={eval_metrics.get('subspace_coverage', 0.0):.3f}")
    
    print("âœ… æ„Ÿæ¸¬é»é¸æ“‡æ¨¡çµ„æ¸¬è©¦å®Œæˆï¼")