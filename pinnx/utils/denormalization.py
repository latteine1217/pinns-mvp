"""
è¼¸å‡ºåæ¨™æº–åŒ–å·¥å…·æ¨¡çµ„

æä¾›çµ±ä¸€çš„æ¨¡å‹è¼¸å‡ºåæ¨™æº–åŒ–åŠŸèƒ½ï¼Œç¢ºä¿è©•ä¼°æ™‚é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼åœ¨ç›¸åŒé‡ç¶±ä¸‹æ¯”è¼ƒã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- æ”¯æŒå¤šç¨®æ¨™æº–åŒ–é¡å‹ï¼šfriction_velocity, channel_flow, manual_scaling
- å¾é…ç½®æ–‡ä»¶é‡å»ºç¸®æ”¾åƒæ•¸
- è™•ç† ManualScalingWrapper çš„åç¸®æ”¾é‚è¼¯
- è©³ç´°çš„æ—¥èªŒè¼¸å‡ºèˆ‡ç¯„åœé©—è­‰

ä½¿ç”¨æ–¹å¼ï¼š
    from pinnx.utils.denormalization import denormalize_output
    
    predictions = model(coords)  # å·²æ¨™æº–åŒ–çš„é æ¸¬
    predictions_physical = denormalize_output(predictions, config)
"""

from __future__ import annotations

import numpy as np
import torch
from typing import Dict, Optional, Tuple, Union
import logging
from pathlib import Path

logger = logging.getLogger(__name__)


# ===================================================================
# å·¥å…·å‡½æ•¸ï¼šå¾ Checkpoint è¼‰å…¥ Normalization Metadata
# ===================================================================

def _load_normalization_metadata(checkpoint_path: str) -> Optional[Dict]:
    """
    å¾ checkpoint è¼‰å…¥ normalization metadata
    
    Args:
        checkpoint_path: checkpoint æª”æ¡ˆè·¯å¾‘
        
    Returns:
        normalization metadata å­—å…¸ï¼Œè‹¥ä¸å­˜åœ¨å‰‡è¿”å› None
        æ ¼å¼: {'type': str, 'scales': dict, 'params': dict}
    """
    try:
        path = Path(checkpoint_path)
        if not path.exists():
            logger.warning(f"âš ï¸  Checkpoint ä¸å­˜åœ¨: {checkpoint_path}")
            return None
        
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        if 'normalization' in checkpoint:
            metadata = checkpoint['normalization']
            logger.info(f"âœ… å¾ checkpoint è®€å– normalization metadata: type={metadata.get('type')}")
            return metadata
        else:
            logger.warning("âš ï¸  Checkpoint ä¸­æœªæ‰¾åˆ° 'normalization' metadata")
            return None
            
    except Exception as e:
        logger.error(f"âŒ è¼‰å…¥ checkpoint å¤±æ•—: {e}")
        return None


def denormalize_output(
    predictions: Union[np.ndarray, torch.Tensor],
    config: Dict,
    output_norm_type: Optional[str] = None,
    verbose: bool = True,
    true_ranges: Optional[Dict[str, Tuple[float, float]]] = None,
    checkpoint_path: Optional[str] = None
) -> np.ndarray:
    """
    åæ¨™æº–åŒ–æ¨¡å‹è¼¸å‡ºåˆ°ç‰©ç†é‡ç¶±
    
    æ”¯æŒçš„æ¨™æº–åŒ–é¡å‹ï¼š
    1. "training_data_norm": è¨“ç·´è³‡æ–™æ¨™æº–åŒ–ï¼ˆTASK-008 æ–°å¢ï¼‰
       - å„ªå…ˆå¾ checkpoint è®€å– normalization metadata
       - è‹¥ç„¡ checkpoint å‰‡å¾é…ç½®è®€å– normalization.params
    
    2. "friction_velocity": åŸºæ–¼æ‘©æ“¦é€Ÿåº¦å°ºåº¦
       - u, v, w â†’ u, v, w * u_Ï„
       - p â†’ p * Ïu_Ï„Â²
    
    3. "manual_scaling": ManualScalingWrapper çš„åç¸®æ”¾
       - å¾ [-1, 1] åç¸®æ”¾åˆ°é…ç½®ä¸­çš„ output_ranges
    
    4. "post_scaling": å¾Œè™•ç†ç¸®æ”¾ï¼ˆè‡ªå‹•ç¯„åœæ˜ å°„ï¼‰
       - å¾æ¨¡å‹è¼¸å‡ºç¯„åœç·šæ€§æ˜ å°„åˆ°çœŸå¯¦æ•¸æ“šç¯„åœ
       - é©ç”¨æ–¼è¨“ç·´æ™‚æœªä½¿ç”¨ç¸®æ”¾çš„æƒ…æ³
    
    5. "none" / "identity": ä¸è™•ç†
    
    Args:
        predictions: æ¨¡å‹é æ¸¬ [N, out_dim]ï¼Œout_dim âˆˆ {3, 4, 5}
                    - 3D: (u, v, w) æˆ– (u, v, p)
                    - 4D: (u, v, w, p)
                    - 5D: (u, v, w, p, S) - å«æºé …
        config: è¨“ç·´é…ç½®å­—å…¸ï¼ˆéœ€åŒ…å« physics å’Œ model.scaling æ®µï¼‰
        output_norm_type: æ¨™æº–åŒ–é¡å‹ï¼ˆè‹¥ç‚º None å‰‡å¾ config è®€å–ï¼‰
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        true_ranges: çœŸå¯¦æ•¸æ“šç¯„åœ (ç”¨æ–¼ post_scaling)
                    ä¾‹å¦‚: {'u': (0.0, 17.37), 'v': (-1.31, 1.33), ...}
        checkpoint_path: checkpoint æª”æ¡ˆè·¯å¾‘ï¼ˆç”¨æ–¼è¼‰å…¥ normalization metadataï¼‰
        
    Returns:
        åæ¨™æº–åŒ–å¾Œçš„é æ¸¬ [N, out_dim] (numpy array)
        
    Raises:
        ValueError: é…ç½®ç¼ºå¤±æˆ–ä¸æ”¯æŒçš„æ¨™æº–åŒ–é¡å‹
        
    Examples:
        >>> # Training data normalization (TASK-008)
        >>> pred_normalized = np.array([[1.0, 0.1, 0.5, 0.01]])  # (u, v, w, p)
        >>> pred_physical = denormalize_output(
        ...     pred_normalized, config, 
        ...     checkpoint_path='checkpoints/model.pth'
        ... )
        
        >>> # Friction velocity æ¨™æº–åŒ–
        >>> config = {
        ...     'model': {'scaling': {'output_norm': 'friction_velocity'}},
        ...     'physics': {'rho': 1.0, 'channel_flow': {'u_tau': 1.0}}
        ... }
        >>> pred_physical = denormalize_output(pred_normalized, config)
        
        >>> # å¾Œè™•ç†ç¸®æ”¾
        >>> true_ranges = {'u': (0, 17.4), 'v': (-1.3, 1.3), 'w': (-22, 21), 'p': (-226, 2.4)}
        >>> pred_scaled = denormalize_output(pred_normalized, config, 'post_scaling', 
        ...                                   true_ranges=true_ranges)
    """
    # è½‰æ›ç‚º numpy array (å¦‚æœæ˜¯ torch.Tensor)
    if isinstance(predictions, torch.Tensor):
        predictions = predictions.detach().cpu().numpy()
    
    predictions = np.array(predictions, dtype=np.float32)
    
    # ç¢ºå®šæ¨™æº–åŒ–é¡å‹
    if output_norm_type is None:
        if 'model' in config and 'scaling' in config['model']:
            output_norm_type = config['model']['scaling'].get('output_norm', 'none')
        else:
            output_norm_type = 'none'
    
    if verbose:
        logger.info(f"ğŸ”„ åæ¨™æº–åŒ–é¡å‹: {output_norm_type}")
        logger.info(f"ğŸ“Š è¼¸å…¥é æ¸¬ç¯„åœ: min={predictions.min():.4f}, max={predictions.max():.4f}")
    
    # ä¸éœ€è¦åæ¨™æº–åŒ–çš„æƒ…æ³
    if output_norm_type in ('none', 'identity', None):
        if verbose:
            logger.info("âœ… ç„¡éœ€åæ¨™æº–åŒ– (type=none)")
        return predictions
    
    # ===================================================================
    # é¡å‹ 0: Training Data Normalization (TASK-008 æ–°å¢)
    # ===================================================================
    if output_norm_type == 'training_data_norm':
        return _denormalize_training_data(predictions, config, checkpoint_path, verbose)
    
    # ===================================================================
    # é¡å‹ 1: Friction Velocity æ¨™æº–åŒ–
    # ===================================================================
    if output_norm_type == 'friction_velocity':
        return _denormalize_friction_velocity(predictions, config, verbose)
    
    # ===================================================================
    # é¡å‹ 2: Manual Scaling (å¾çµ±è¨ˆç¯„åœåç¸®æ”¾)
    # ===================================================================
    elif output_norm_type == 'manual_scaling':
        return _denormalize_manual_scaling(predictions, config, verbose)
    
    # ===================================================================
    # é¡å‹ 3: Post Scaling (å¾Œè™•ç†è‡ªå‹•ç¯„åœæ˜ å°„)
    # ===================================================================
    elif output_norm_type == 'post_scaling':
        if true_ranges is None:
            raise ValueError("post_scaling æ¨¡å¼éœ€è¦æä¾› true_ranges åƒæ•¸")
        return _denormalize_post_scaling(predictions, true_ranges, verbose)
    
    # ===================================================================
    # é¡å‹ 4: å­—å…¸æ ¼å¼ (ç›´æ¥æŒ‡å®šç¯„åœ)
    # ===================================================================
    elif isinstance(output_norm_type, dict):
        return _denormalize_from_dict(predictions, output_norm_type, verbose)
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ output_norm é¡å‹: {output_norm_type}")




def _denormalize_training_data(
    predictions: np.ndarray,
    config: Dict,
    checkpoint_path: Optional[str],
    verbose: bool
) -> np.ndarray:
    """
    Training Data Normalization åæ¨™æº–åŒ–ï¼ˆZ-score: x * std + meanï¼‰
    
    å°æ‡‰è¨“ç·´æ™‚çš„ Z-score æ¨™æº–åŒ–ï¼š
    - è¨“ç·´æ™‚ï¼šnormalized = (x - mean) / std
    - è©•ä¼°æ™‚ï¼šphysical = normalized * std + mean
    
    å„ªå…ˆç´šï¼š
    1. å¾ checkpoint è¼‰å…¥ normalization metadataï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
    2. å¾ config['normalization']['params'] è®€å–
    3. ä½¿ç”¨ç¡¬ç·¨ç¢¼é è¨­å€¼ï¼ˆJHTDB Channel Re_tau=1000 çµ±è¨ˆï¼‰
    
    Args:
        predictions: æ¨¡å‹é æ¸¬ï¼ˆæ¨™æº–åŒ–ç©ºé–“ï¼‰ [N, out_dim]
        config: é…ç½®å­—å…¸
        checkpoint_path: checkpoint æª”æ¡ˆè·¯å¾‘ï¼ˆè‹¥æä¾›å‰‡å„ªå…ˆè¼‰å…¥ï¼‰
        verbose: æ˜¯å¦è¼¸å‡ºæ—¥èªŒ
        
    Returns:
        ç‰©ç†ç©ºé–“çš„é æ¸¬ [N, out_dim]
    """
    # å„ªå…ˆç´š 1: å¾ checkpoint è¼‰å…¥
    means = None
    stds = None
    
    if checkpoint_path is not None:
        metadata = _load_normalization_metadata(checkpoint_path)
        if metadata is not None:
            means = metadata.get('means', None)
            stds = metadata.get('scales', None)  # scales æ˜¯ stds
            if means and stds and verbose:
                logger.info(f"ğŸ“¦ ä½¿ç”¨ checkpoint çš„ Z-score ä¿‚æ•¸:")
                logger.info(f"   means={means}")
                logger.info(f"   stds={stds}")
    
    # å„ªå…ˆç´š 2: å¾é…ç½®è®€å–
    if means is None or stds is None:
        if 'normalization' in config:
            norm_cfg = config['normalization']
            if 'params' in norm_cfg:
                params = norm_cfg['params']
                means = {
                    'u': params.get('u_mean'),
                    'v': params.get('v_mean'),
                    'w': params.get('w_mean'),
                    'p': params.get('p_mean')
                }
                stds = {
                    'u': params.get('u_std'),
                    'v': params.get('v_std'),
                    'w': params.get('w_std'),
                    'p': params.get('p_std')
                }
                
                # å‘å¾Œå…¼å®¹ï¼šå¦‚æœé…ç½®ä½¿ç”¨èˆŠæ ¼å¼ (*_scale)ï¼Œç™¼å‡ºè­¦å‘Š
                if any(k.endswith('_scale') for k in params.keys()):
                    logger.warning("âš ï¸  æª¢æ¸¬åˆ°èˆŠæ ¼å¼æ¨™æº–åŒ–ä¿‚æ•¸ (*_scale)ï¼Œå»ºè­°æ›´æ–°ç‚ºæ–°æ ¼å¼ (*_mean, *_std)")
                    # å˜—è©¦ä½¿ç”¨èˆŠæ ¼å¼
                    if means['u'] is None:
                        stds = {
                            'u': params.get('u_scale'),
                            'v': params.get('v_scale'),
                            'w': params.get('w_scale'),
                            'p': params.get('p_scale')
                        }
                        means = {'u': 0.0, 'v': 0.0, 'w': 0.0, 'p': 0.0}  # å‡è¨­èˆŠæ ¼å¼å‡å€¼ç‚º 0
                
                if verbose:
                    logger.info(f"ğŸ“‹ ä½¿ç”¨é…ç½®çš„ Z-score ä¿‚æ•¸:")
                    logger.info(f"   means={means}")
                    logger.info(f"   stds={stds}")
    
    # å„ªå…ˆç´š 3: ç¡¬ç·¨ç¢¼é è¨­å€¼ï¼ˆJHTDB Channel Re_tau=1000 æ­£ç¢ºçµ±è¨ˆé‡ï¼‰
    # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•çµ±è¨ˆé‡ç‚º None æˆ–ç©ºå­—å…¸
    needs_default = False
    if means is None or stds is None:
        needs_default = True
    elif isinstance(means, dict) and isinstance(stds, dict):
        # æª¢æŸ¥å­—å…¸æ˜¯å¦ç‚ºç©ºæˆ–ä»»ä½•å€¼ç‚º None
        if not means or not stds:
            needs_default = True
        elif any(means.get(k) is None for k in ['u', 'v', 'w', 'p']):
            needs_default = True
        elif any(stds.get(k) is None for k in ['u', 'v', 'w', 'p']):
            needs_default = True
    
    if needs_default:
        means = {
            'u': 9.921185,
            'v': -0.000085,
            'w': -0.002202,
            'p': -40.374241
        }
        stds = {
            'u': 4.593879,
            'v': 0.329614,
            'w': 3.865396,
            'p': 28.619722
        }
        if verbose:
            logger.warning("âš ï¸  æœªæ‰¾åˆ°æ¨™æº–åŒ–ä¿‚æ•¸ï¼Œä½¿ç”¨ç¡¬ç·¨ç¢¼é è¨­å€¼ï¼ˆJHTDB Re_tau=1000 æ­£ç¢ºçµ±è¨ˆï¼‰")
    
    # ç¢ºä¿ means å’Œ stds æ˜¯å­—å…¸é¡å‹
    if not isinstance(means, dict) or not isinstance(stds, dict):
        logger.error(f"âŒ æ¨™æº–åŒ–ä¿‚æ•¸æ ¼å¼éŒ¯èª¤: means={type(means)}, stds={type(stds)}")
        # å›é€€åˆ°é è¨­å€¼
        means = {
            'u': 9.921185,
            'v': -0.000085,
            'w': -0.002202,
            'p': -40.374241
        }
        stds = {
            'u': 4.593879,
            'v': 0.329614,
            'w': 3.865396,
            'p': 28.619722
        }
    
    u_mean, v_mean, w_mean, p_mean = means['u'], means['v'], means['w'], means['p']
    u_std, v_std, w_std, p_std = stds['u'], stds['v'], stds['w'], stds['p']
    
    if verbose:
        logger.info("ğŸ”§ åŸ·è¡Œ Z-score åæ¨™æº–åŒ– (x * std + mean)")
        logger.info(f"ğŸ“ u: mean={u_mean:.4f}, std={u_std:.4f}")
        logger.info(f"ğŸ“ v: mean={v_mean:.6f}, std={v_std:.4f}")
        logger.info(f"ğŸ“ w: mean={w_mean:.6f}, std={w_std:.4f}")
        logger.info(f"ğŸ“ p: mean={p_mean:.4f}, std={p_std:.4f}")
    
    result = predictions.copy()
    out_dim = predictions.shape[-1]
    
    if out_dim == 3:
        # (u, v, p) - 2D é€šé“æµ
        result[:, 0] = result[:, 0] * u_std + u_mean
        result[:, 1] = result[:, 1] * v_std + v_mean
        result[:, 2] = result[:, 2] * p_std + p_mean
        
    elif out_dim == 4:
        # (u, v, w, p) - 3D é€šé“æµ
        result[:, 0] = result[:, 0] * u_std + u_mean
        result[:, 1] = result[:, 1] * v_std + v_mean
        result[:, 2] = result[:, 2] * w_std + w_mean
        result[:, 3] = result[:, 3] * p_std + p_mean
        
    elif out_dim == 5:
        # (u, v, w, p, S) - å«æºé …
        result[:, 0] = result[:, 0] * u_std + u_mean
        result[:, 1] = result[:, 1] * v_std + v_mean
        result[:, 2] = result[:, 2] * w_std + w_mean
        result[:, 3] = result[:, 3] * p_std + p_mean
        # æºé … S ä¸æ¨™æº–åŒ–
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è¼¸å‡ºç¶­åº¦: {out_dim} (é æœŸ 3, 4, æˆ– 5)")
    
    if verbose:
        logger.info(f"ğŸ“Š åæ¨™æº–åŒ–å¾Œç¯„åœ: min={result.min():.4f}, max={result.max():.4f}")
        for i, name in enumerate(['u', 'v', 'w', 'p', 'S'][:out_dim]):
            logger.info(f"  {name}: [{result[:, i].min():.2f}, {result[:, i].max():.2f}]")
        logger.info("âœ… Z-score åæ¨™æº–åŒ–å®Œæˆ")
    
    return result


def _denormalize_friction_velocity(
    predictions: np.ndarray,
    config: Dict,
    verbose: bool
) -> np.ndarray:
    """
    Friction velocity åæ¨™æº–åŒ–ï¼šu,v,w * u_Ï„; p * Ïu_Ï„Â²
    
    æ³¨æ„ï¼šå¯¦éš›ä¸Šè¨“ç·´æ™‚å¯èƒ½ä½¿ç”¨ ManualScalingWrapperï¼Œæ­¤å‡½æ•¸åƒ…è™•ç†
    é…ç½®ä¸­æ˜ç¢ºè²æ˜ä½¿ç”¨ friction_velocity æ¨™æº–åŒ–çš„æƒ…æ³ã€‚
    """
    # æå–ç‰©ç†åƒæ•¸
    if 'physics' not in config:
        raise ValueError("é…ç½®ç¼ºå°‘ 'physics' æ®µè½")
    
    physics = config['physics']
    
    # æ‘©æ“¦é€Ÿåº¦ u_Ï„
    if 'channel_flow' in physics and 'u_tau' in physics['channel_flow']:
        u_tau = physics['channel_flow']['u_tau']
    else:
        u_tau = 1.0  # é»˜èªå€¼
        if verbose:
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ° u_tauï¼Œä½¿ç”¨é»˜èªå€¼ 1.0")
    
    # å¯†åº¦ Ï
    rho = physics.get('rho', 1.0)
    
    # è¨ˆç®—ç¸®æ”¾å› å­
    velocity_scale = u_tau
    pressure_scale = rho * u_tau ** 2
    
    if verbose:
        logger.info(f"ğŸ“ ç‰©ç†åƒæ•¸: u_Ï„={u_tau}, Ï={rho}")
        logger.info(f"ğŸ“ ç¸®æ”¾å› å­: vel_scale={velocity_scale}, p_scale={pressure_scale}")
    
    # æ‡‰ç”¨åç¸®æ”¾
    result = predictions.copy()
    out_dim = predictions.shape[-1]
    
    if out_dim == 3:
        # (u, v, p) æˆ– (u, v, w)
        # å‡è¨­å‰å…©å€‹æ˜¯é€Ÿåº¦ï¼Œæœ€å¾Œä¸€å€‹æ˜¯å£“åŠ›æˆ– w
        result[:, 0:2] *= velocity_scale  # u, v
        # ç¬¬ä¸‰å€‹è®Šé‡éœ€è¦æ ¹æ“šå¯¦éš›æƒ…æ³åˆ¤æ–·
        # é€™è£¡å‡è¨­æ˜¯ wï¼ˆ3D æƒ…æ³ï¼‰
        result[:, 2] *= velocity_scale  # w
        
    elif out_dim == 4:
        # (u, v, w, p)
        result[:, 0:3] *= velocity_scale  # u, v, w
        result[:, 3] *= pressure_scale    # p
        
    elif out_dim == 5:
        # (u, v, w, p, S)
        result[:, 0:3] *= velocity_scale  # u, v, w
        result[:, 3] *= pressure_scale    # p
        # æºé … S é€šå¸¸å·²ç¶“åœ¨ç‰©ç†é‡ç¶±ï¼Œä¸ç¸®æ”¾
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è¼¸å‡ºç¶­åº¦: {out_dim} (é æœŸ 3, 4, æˆ– 5)")
    
    if verbose:
        logger.info(f"ğŸ“Š è¼¸å‡ºé æ¸¬ç¯„åœ: min={result.min():.4f}, max={result.max():.4f}")
        logger.info("âœ… Friction velocity åæ¨™æº–åŒ–å®Œæˆ")
    
    return result


def _denormalize_manual_scaling(
    predictions: np.ndarray,
    config: Dict,
    verbose: bool
) -> np.ndarray:
    """
    ManualScalingWrapper åæ¨™æº–åŒ–ï¼šå¾ [-1, 1] åç¸®æ”¾åˆ°ç‰©ç†ç¯„åœ
    
    åç¸®æ”¾å…¬å¼ï¼ˆèˆ‡ ManualScalingWrapper.forward ç¬¬ 67 è¡Œå°æ‡‰ï¼‰ï¼š
        y_physical = (y_scaled + 1) / 2 * (max - min) + min
    
    æ³¨æ„ï¼šManualScalingWrapper åœ¨è¨“ç·´æ™‚å·²ç¶“åŸ·è¡Œäº†åç¸®æ”¾ï¼ˆforward è¿”å›ç‰©ç†å€¼ï¼‰ï¼Œ
    æ‰€ä»¥è©•ä¼°æ™‚å¦‚æœä½¿ç”¨ç›¸åŒçš„ wrapperï¼Œè¼¸å‡ºå·²ç¶“æ˜¯ç‰©ç†é‡ã€‚
    
    æ­¤å‡½æ•¸ç”¨æ–¼ï¼šç›´æ¥å¾åŸºç¤æ¨¡å‹ï¼ˆæœªåŒ…è£ï¼‰ç²å–é æ¸¬æ™‚çš„åç¸®æ”¾ã€‚
    """
    if verbose:
        logger.warning("âš ï¸  manual_scaling æ¨¡å¼ï¼šéœ€è¦å¾é…ç½®é‡å»ºè¼¸å‡ºç¯„åœ")
    
    # å¾é…ç½®æå–è¼¸å‡ºç¯„åœ
    output_ranges = _extract_output_ranges(config, verbose)
    
    # å‡è¨­ç¶²è·¯è¼¸å‡ºåœ¨ [-1, 1]
    result = predictions.copy()
    
    # åç¸®æ”¾å…¬å¼: y = (y_scaled + 1) / 2 * (max - min) + min
    for i, (var_name, (min_val, max_val)) in enumerate(output_ranges.items()):
        if i >= result.shape[-1]:
            break
        result[:, i] = (result[:, i] + 1) / 2 * (max_val - min_val) + min_val
    
    if verbose:
        logger.info(f"ğŸ“Š è¼¸å‡ºé æ¸¬ç¯„åœ: min={result.min():.4f}, max={result.max():.4f}")
        logger.info("âœ… Manual scaling åæ¨™æº–åŒ–å®Œæˆ")
    
    return result


def _denormalize_post_scaling(
    predictions: np.ndarray,
    true_ranges: Dict[str, Tuple[float, float]],
    verbose: bool
) -> np.ndarray:
    """
    å¾Œè™•ç†ç¸®æ”¾ï¼šè‡ªå‹•å°‡æ¨¡å‹è¼¸å‡ºç¯„åœæ˜ å°„åˆ°çœŸå¯¦æ•¸æ“šç¯„åœ
    
    é©ç”¨å ´æ™¯ï¼š
    - è¨“ç·´æ™‚æœªä½¿ç”¨ä»»ä½•è¼¸å‡ºç¸®æ”¾ï¼ˆVS-PINN è·³é ManualScalingWrapperï¼‰
    - æ¨¡å‹è¼¸å‡ºåœ¨è¿‘ä¼¼ç‰©ç†é‡ç¶±ä½†æ•¸å€¼ç¯„åœéŒ¯èª¤
    - éœ€è¦å¾Œè™•ç†æ ¡æ­£åˆ°çœŸå¯¦ç‰©ç†å°ºåº¦
    
    ç¸®æ”¾å…¬å¼ï¼ˆç·šæ€§æ˜ å°„ï¼‰ï¼š
        y_scaled = (y_pred - pred_min) / (pred_max - pred_min) * (true_max - true_min) + true_min
    
    Args:
        predictions: æ¨¡å‹åŸå§‹é æ¸¬ [N, out_dim]
        true_ranges: çœŸå¯¦æ•¸æ“šç¯„åœï¼Œä¾‹å¦‚:
                    {'u': (0.0, 17.37), 'v': (-1.31, 1.33), 'w': (-22, 21), 'p': (-226, 2.4)}
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
        
    Returns:
        ç¸®æ”¾å¾Œçš„é æ¸¬ [N, out_dim]
        
    Notes:
        - è‡ªå‹•å¾ predictions è¨ˆç®—ç•¶å‰ç¯„åœï¼ˆpred_min, pred_maxï¼‰
        - åŸ·è¡Œç·šæ€§æ˜ å°„åˆ° (true_min, true_max)
        - å‡è¨­æ¨¡å‹å·²å­¸åˆ°æµå ´ç›¸å°çµæ§‹ï¼Œåƒ…çµ•å°å°ºåº¦éŒ¯èª¤
    """
    if verbose:
        logger.info("ğŸ”§ åŸ·è¡Œå¾Œè™•ç†ç¸®æ”¾ (post_scaling)")
    
    result = predictions.copy()
    var_names = list(true_ranges.keys())
    
    # å°æ¯å€‹è®Šé‡åˆ†åˆ¥ç¸®æ”¾
    for i, var_name in enumerate(var_names):
        if i >= result.shape[-1]:
            break
        
        # ç•¶å‰è®Šé‡çš„é æ¸¬ç¯„åœ
        pred_min = result[:, i].min()
        pred_max = result[:, i].max()
        
        # çœŸå¯¦æ•¸æ“šç¯„åœ
        true_min, true_max = true_ranges[var_name]
        
        # ç·šæ€§æ˜ å°„å…¬å¼
        if pred_max - pred_min < 1e-8:
            # é¿å…é™¤ä»¥é›¶ï¼ˆé æ¸¬ç‚ºå¸¸æ•¸ï¼‰
            if verbose:
                logger.warning(f"âš ï¸  {var_name}: é æ¸¬ç‚ºå¸¸æ•¸ ({pred_min:.4f})ï¼Œè·³éç¸®æ”¾")
            continue
        
        result[:, i] = (result[:, i] - pred_min) / (pred_max - pred_min) * (true_max - true_min) + true_min
        
        if verbose:
            scaled_min = result[:, i].min()
            scaled_max = result[:, i].max()
            logger.info(f"  {var_name}: [{pred_min:.4f}, {pred_max:.4f}] â†’ [{scaled_min:.4f}, {scaled_max:.4f}] "
                       f"(çœŸå¯¦: [{true_min:.4f}, {true_max:.4f}])")
    
    if verbose:
        logger.info("âœ… å¾Œè™•ç†ç¸®æ”¾å®Œæˆ")
    
    return result


def _denormalize_from_dict(
    predictions: np.ndarray,
    output_ranges: Dict[str, Tuple[float, float]],
    verbose: bool
) -> np.ndarray:
    """
    å¾å­—å…¸æ ¼å¼çš„è¼¸å‡ºç¯„åœåæ¨™æº–åŒ–
    
    Args:
        output_ranges: ä¾‹å¦‚ {'u': (0, 20), 'v': (-1, 1), 'w': (-5, 5), 'p': (-100, 10)}
    """
    result = predictions.copy()
    
    # åç¸®æ”¾å…¬å¼: y = (y_scaled + 1) / 2 * (max - min) + min
    for i, (var_name, (min_val, max_val)) in enumerate(output_ranges.items()):
        if i >= result.shape[-1]:
            break
        result[:, i] = (result[:, i] + 1) / 2 * (max_val - min_val) + min_val
        
        if verbose:
            logger.info(f"  {var_name}: [{min_val}, {max_val}] -> "
                       f"[{result[:, i].min():.4f}, {result[:, i].max():.4f}]")
    
    if verbose:
        logger.info("âœ… å­—å…¸æ ¼å¼åæ¨™æº–åŒ–å®Œæˆ")
    
    return result


def _extract_output_ranges(
    config: Dict,
    verbose: bool
) -> Dict[str, Tuple[float, float]]:
    """
    å¾é…ç½®æå–è¼¸å‡ºç¯„åœï¼ˆç”¨æ–¼ ManualScalingWrapperï¼‰
    
    å„ªå…ˆç´šï¼š
    1. config['model']['scaling']['output_norm'] (dict æ ¼å¼)
    2. å¾çµ±è¨ˆä¿¡æ¯æ¨å°ï¼ˆå¦‚æœ‰ï¼‰
    3. ç¡¬ç·¨ç¢¼é»˜èªå€¼
    """
    output_ranges = {}
    
    # æª¢æŸ¥ scaling é…ç½®
    if 'model' in config and 'scaling' in config['model']:
        scaling_cfg = config['model']['scaling']
        output_norm_raw = scaling_cfg.get('output_norm')
        
        if isinstance(output_norm_raw, dict):
            # å­—å…¸æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
            output_ranges = {
                'u': tuple(output_norm_raw.get('u', [0.0, 20.0])),
                'v': tuple(output_norm_raw.get('v', [-1.0, 1.0])),
                'w': tuple(output_norm_raw.get('w', [-5.0, 5.0])),
                'p': tuple(output_norm_raw.get('p', [-100.0, 10.0]))
            }
            if verbose:
                logger.info("âœ… å¾é…ç½®æ–‡ä»¶è®€å–è¼¸å‡ºç¯„åœ (dict æ ¼å¼)")
            return output_ranges
    
    # ç¡¬ç·¨ç¢¼é»˜èªå€¼ï¼ˆåŸºæ–¼ JHTDB Channel Re1000ï¼‰
    output_ranges = {
        'u': (0.0, 20.0),
        'v': (-1.0, 1.0),
        'w': (-5.0, 5.0),
        'p': (-100.0, 10.0)
    }
    
    if verbose:
        logger.warning("âš ï¸  ä½¿ç”¨ç¡¬ç·¨ç¢¼é»˜èªè¼¸å‡ºç¯„åœï¼ˆå¯èƒ½ä¸æº–ç¢ºï¼‰")
    
    return output_ranges


def verify_denormalization(
    predictions: np.ndarray,
    true_values: np.ndarray,
    var_names: Optional[list] = None,
    tolerance: float = 0.3
) -> bool:
    """
    é©—è­‰åæ¨™æº–åŒ–æ˜¯å¦æ­£ç¢ºï¼ˆé€šéç¯„åœåŒ¹é…ï¼‰
    
    Args:
        predictions: åæ¨™æº–åŒ–å¾Œçš„é æ¸¬
        true_values: çœŸå¯¦å€¼
        var_names: è®Šé‡åç¨±åˆ—è¡¨
        tolerance: ç¯„åœå®¹å·®ï¼ˆé æ¸¬ç¯„åœæ‡‰åœ¨çœŸå¯¦ç¯„åœçš„ Â±tolerance å…§ï¼‰
        
    Returns:
        é©—è­‰æ˜¯å¦é€šé
    """
    if var_names is None:
        var_names = ['u', 'v', 'w', 'p', 'S'][:predictions.shape[-1]]
    
    all_pass = True
    
    for i, var in enumerate(var_names):
        if i >= predictions.shape[-1]:
            break
        
        pred_min, pred_max = predictions[:, i].min(), predictions[:, i].max()
        true_min, true_max = true_values[:, i].min(), true_values[:, i].max()
        
        true_range = true_max - true_min
        
        # æª¢æŸ¥é æ¸¬ç¯„åœæ˜¯å¦åœ¨çœŸå¯¦ç¯„åœçš„å®¹å·®å…§
        min_ok = abs(pred_min - true_min) <= tolerance * true_range
        max_ok = abs(pred_max - true_max) <= tolerance * true_range
        
        status = "âœ…" if (min_ok and max_ok) else "âŒ"
        logger.info(f"{status} {var}: é æ¸¬[{pred_min:.2f}, {pred_max:.2f}] vs "
                   f"çœŸå¯¦[{true_min:.2f}, {true_max:.2f}]")
        
        if not (min_ok and max_ok):
            all_pass = False
    
    return all_pass


# ===================================================================
# ä¾¿æ·å‡½æ•¸
# ===================================================================

def create_denormalizer_from_config(
    config: Dict, 
    checkpoint_path: Optional[str] = None,
    verbose: bool = True
):
    """
    å¾é…ç½®å‰µå»ºåæ¨™æº–åŒ–å‡½æ•¸ï¼ˆé–‰åŒ…ï¼‰
    
    Args:
        config: é…ç½®å­—å…¸
        checkpoint_path: checkpoint è·¯å¾‘ï¼ˆå„ªå…ˆè¼‰å…¥ï¼‰
        verbose: æ˜¯å¦è¼¸å‡ºè©³ç´°æ—¥èªŒ
    
    Returns:
        denorm_fn: æ¥å— predictions ä¸¦è¿”å›åæ¨™æº–åŒ–çµæœçš„å‡½æ•¸
    """
    output_norm_type = None
    if 'model' in config and 'scaling' in config['model']:
        output_norm_type = config['model']['scaling'].get('output_norm', 'none')
    
    def denorm_fn(predictions):
        return denormalize_output(
            predictions, config, output_norm_type, verbose, 
            checkpoint_path=checkpoint_path
        )
    
    return denorm_fn


if __name__ == "__main__":
    # æ¸¬è©¦ä»£ç¢¼
    import sys
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    
    print("=" * 60)
    print("ğŸ“‹ åæ¨™æº–åŒ–å·¥å…·æ¸¬è©¦")
    print("=" * 60)
    
    # æ¸¬è©¦ 1: Friction velocity
    print("\n--- æ¸¬è©¦ 1: Friction Velocity åæ¨™æº–åŒ– ---")
    config_fv = {
        'model': {'scaling': {'output_norm': 'friction_velocity'}},
        'physics': {
            'rho': 1.0,
            'channel_flow': {'u_tau': 0.05}  # å¯¦éš› JHTDB çš„ u_tau
        }
    }
    
    pred_normalized = np.array([
        [1.0, 0.1, 0.5, 0.01],  # (u, v, w, p) æ¨™æº–åŒ–å¾Œ
        [1.2, -0.05, 0.3, -0.005]
    ])
    
    pred_physical = denormalize_output(pred_normalized, config_fv, verbose=True)
    print(f"\næ¨™æº–åŒ–é æ¸¬:\n{pred_normalized}")
    print(f"\nç‰©ç†é‡é æ¸¬:\n{pred_physical}")
    
    # é æœŸ: u,v,w * 0.05, p * (1.0 * 0.05^2) = p * 0.0025
    expected = pred_normalized.copy()
    expected[:, 0:3] *= 0.05  # u, v, w
    expected[:, 3] *= 0.0025  # p
    
    assert np.allclose(pred_physical, expected), "Friction velocity åæ¨™æº–åŒ–éŒ¯èª¤ï¼"
    print("âœ… æ¸¬è©¦ 1 é€šé")
    
    # æ¸¬è©¦ 2: None (ç„¡éœ€åæ¨™æº–åŒ–)
    print("\n--- æ¸¬è©¦ 2: None é¡å‹ (ç„¡éœ€åæ¨™æº–åŒ–) ---")
    config_none = {'model': {'scaling': {'output_norm': 'none'}}}
    
    pred_none = denormalize_output(pred_normalized, config_none, verbose=True)
    assert np.array_equal(pred_none, pred_normalized), "None é¡å‹æ‡‰è¿”å›åŸå§‹å€¼ï¼"
    print("âœ… æ¸¬è©¦ 2 é€šé")
    
    # æ¸¬è©¦ 3: ç¯„åœé©—è­‰
    print("\n--- æ¸¬è©¦ 3: ç¯„åœé©—è­‰ ---")
    true_values = np.array([
        [1.0, 0.1, 0.5, 0.02],
        [1.5, -0.03, 0.4, -0.003]
    ])
    
    is_valid = verify_denormalization(
        pred_physical, true_values, 
        var_names=['u', 'v', 'w', 'p'],
        tolerance=0.5  # å¯¬å®¹åº¦
    )
    
    if is_valid:
        print("âœ… æ¸¬è©¦ 3 é€šé")
    else:
        print("âš ï¸  æ¸¬è©¦ 3 éƒ¨åˆ†å¤±æ•—ï¼ˆä½†å¯èƒ½æ˜¯åˆç†çš„æ¨¡å‹èª¤å·®ï¼‰")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
    print("=" * 60)
