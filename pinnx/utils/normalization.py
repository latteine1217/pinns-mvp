"""
çµ±ä¸€æ¨™æº–åŒ–ç³»çµ±æ¨¡çµ„ (Phase 1 Refactor - 2025-10-17)

æä¾›çµ±ä¸€çš„è¼¸å…¥/è¼¸å‡ºæ¨™æº–åŒ–ç®¡ç†ï¼Œæ¶ˆé™¤å†—é¤˜å®šç¾©ï¼Œç¢ºä¿è¨“ç·´èˆ‡è©•ä¼°ä¸€è‡´æ€§ã€‚

æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ï¼š
1. å–®ä¸€çœŸç›¸ä¾†æº (Single Source of Truth)
2. æ˜ç¢ºçš„å„ªå…ˆç´š (checkpoint > config > computed)
3. Fail-fast éŒ¯èª¤è™•ç† (ç¼ºå¤±å¿…è¦åƒæ•¸æ™‚ç›´æ¥å¤±æ•—)
4. VS-PINN ç‰©ç†ç¸®æ”¾ç¨ç«‹ (ä¿ç•™åœ¨ scaling.py/NonDimensionalizer)

ä¸»è¦é¡åˆ¥ï¼š
- UnifiedNormalizer: çµ±ä¸€æ¨™æº–åŒ–å™¨ï¼ˆè¼¸å…¥ + è¼¸å‡ºï¼‰
- InputTransform: åæ¨™æ¨™æº–åŒ– (å…§éƒ¨çµ„ä»¶)
- OutputTransform: è®Šé‡æ¨™æº–åŒ– (å…§éƒ¨çµ„ä»¶)
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict, Union, Any
import logging

import torch
import numpy as np

logger = logging.getLogger(__name__)


# ===================================================================
# é…ç½®æ•¸æ“šé¡
# ===================================================================

@dataclass
class InputNormConfig:
    """è¼¸å…¥æ¨™æº–åŒ–é…ç½®"""
    norm_type: str = "none"  # none, standard, minmax, channel_flow
    feature_range: Tuple[float, float] = (-1.0, 1.0)
    bounds: Optional[torch.Tensor] = None  # shape [dim, 2]


@dataclass
class OutputNormConfig:
    """è¼¸å‡ºæ¨™æº–åŒ–é…ç½®"""
    norm_type: str = "none"  # none, training_data_norm, friction_velocity, manual
    variable_order: Optional[List[str]] = None  # è®Šé‡é †åºï¼ˆå–®ä¸€ä¾†æºï¼‰
    means: Optional[Dict[str, float]] = None
    stds: Optional[Dict[str, float]] = None
    params: Optional[Dict[str, Any]] = None  # é¡å¤–åƒæ•¸


# ===================================================================
# è¼¸å…¥æ¨™æº–åŒ– (åæ¨™)
# ===================================================================

class InputTransform:
    """
    åæ¨™æ¨™æº–åŒ–å™¨ï¼ˆå…§éƒ¨çµ„ä»¶ï¼‰
    
    æ”¯æ´é¡å‹ï¼š
    - none/identity: ä¸è™•ç†
    - standard: Z-score (x - mean) / std
    - minmax: ç·šæ€§æ˜ å°„åˆ° feature_range
    - channel_flow: ä½¿ç”¨é å®šç¾© bounds æ˜ å°„åˆ° feature_range
    """
    
    def __init__(self, config: InputNormConfig):
        self.norm_type = (config.norm_type or "none").lower()
        self.feature_range = config.feature_range
        self.bounds = config.bounds
        
        self.mean: Optional[torch.Tensor] = None
        self.std: Optional[torch.Tensor] = None
        self.data_min: Optional[torch.Tensor] = None
        self.data_range: Optional[torch.Tensor] = None
    
    def fit(self, samples: torch.Tensor) -> "InputTransform":
        """å¾æ¨£æœ¬æ•¸æ“šæ“¬åˆçµ±è¨ˆé‡"""
        if self.norm_type == "standard":
            mean = torch.mean(samples, dim=0, keepdim=True)
            std = torch.std(samples, dim=0, keepdim=True)
            std = torch.where(std < 1e-8, torch.ones_like(std), std)
            self.mean = mean
            self.std = std
            
        elif self.norm_type == "minmax":
            data_min = torch.min(samples, dim=0, keepdim=True)[0]
            data_max = torch.max(samples, dim=0, keepdim=True)[0]
            data_range = data_max - data_min
            data_range = torch.where(data_range < 1e-8, torch.ones_like(data_range), data_range)
            self.data_min = data_min
            self.data_range = data_range
            
        elif self.norm_type in ("channel_flow", "vs_pinn"):
            # ä½¿ç”¨é å®šç¾© boundsï¼Œç„¡éœ€æ“¬åˆ
            pass
        else:
            # none/identity
            pass
        
        return self
    
    def transform(self, tensor: torch.Tensor) -> torch.Tensor:
        """æ¨™æº–åŒ–è½‰æ›"""
        if self.norm_type in ("none", "identity", "vs_pinn"):
            return tensor
        
        if self.norm_type == "standard":
            if self.mean is None or self.std is None:
                raise RuntimeError("Standard normalizer å°šæœªæ“¬åˆ")
            return (tensor - self.mean) / self.std
        
        if self.norm_type == "minmax":
            if self.data_min is None or self.data_range is None:
                raise RuntimeError("MinMax normalizer å°šæœªæ“¬åˆ")
            norm = (tensor - self.data_min) / self.data_range
            lo, hi = self.feature_range
            return norm * (hi - lo) + lo
        
        if self.norm_type == "channel_flow":
            if self.bounds is None:
                raise RuntimeError("Channel-flow bounds æœªæä¾›")
            mins = self.bounds[:, 0].unsqueeze(0)
            maxs = self.bounds[:, 1].unsqueeze(0)
            denom = torch.where((maxs - mins) < 1e-8, torch.ones_like(maxs - mins), maxs - mins)
            norm = (tensor - mins) / denom
            lo, hi = self.feature_range
            return norm * (hi - lo) + lo
        
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨™æº–åŒ–é¡å‹: {self.norm_type}")
    
    def inverse_transform(self, tensor: torch.Tensor) -> torch.Tensor:
        """é€†æ¨™æº–åŒ–"""
        if self.norm_type in ("none", "identity", "vs_pinn"):
            return tensor
        
        if self.norm_type == "standard":
            if self.mean is None or self.std is None:
                raise RuntimeError("Standard normalizer å°šæœªæ“¬åˆ")
            return tensor * self.std + self.mean
        
        if self.norm_type == "minmax":
            if self.data_min is None or self.data_range is None:
                raise RuntimeError("MinMax normalizer å°šæœªæ“¬åˆ")
            lo, hi = self.feature_range
            norm = (tensor - lo) / (hi - lo + 1e-12)
            return norm * self.data_range + self.data_min
        
        if self.norm_type == "channel_flow":
            if self.bounds is None:
                raise RuntimeError("Channel-flow bounds æœªæä¾›")
            lo, hi = self.feature_range
            norm = (tensor - lo) / (hi - lo + 1e-12)
            mins = self.bounds[:, 0].unsqueeze(0)
            maxs = self.bounds[:, 1].unsqueeze(0)
            return norm * (maxs - mins) + mins
        
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨™æº–åŒ–é¡å‹: {self.norm_type}")
    
    def to(self, device: torch.device) -> "InputTransform":
        """ç§»å‹•åˆ°æŒ‡å®šè¨­å‚™"""
        if self.mean is not None:
            self.mean = self.mean.to(device)
        if self.std is not None:
            self.std = self.std.to(device)
        if self.data_min is not None:
            self.data_min = self.data_min.to(device)
        if self.data_range is not None:
            self.data_range = self.data_range.to(device)
        if self.bounds is not None:
            self.bounds = self.bounds.to(device)
        return self
    
    def get_metadata(self) -> Dict[str, Any]:
        """ç²å–å…ƒæ•¸æ“šï¼ˆç”¨æ–¼ checkpointï¼‰"""
        if self.mean is not None:
            device = self.mean.device
        elif self.data_min is not None:
            device = self.data_min.device
        elif self.bounds is not None:
            device = self.bounds.device
        else:
            device = torch.device('cpu')
        
        feature_range_tensor = torch.tensor(
            self.feature_range, dtype=torch.float32, device=device
        )
        
        metadata: Dict[str, Any] = {
            'norm_type': self.norm_type,
            'feature_range': feature_range_tensor
        }
        
        if self.mean is not None:
            metadata['mean'] = self.mean.clone().detach()
        if self.std is not None:
            metadata['std'] = self.std.clone().detach()
        if self.data_min is not None:
            metadata['data_min'] = self.data_min.clone().detach()
        if self.data_range is not None:
            metadata['data_range'] = self.data_range.clone().detach()
        if self.bounds is not None:
            metadata['bounds'] = self.bounds.clone().detach()
        
        return metadata


# ===================================================================
# è¼¸å‡ºæ¨™æº–åŒ– (è®Šé‡)
# ===================================================================

class OutputTransform:
    """
    è®Šé‡æ¨™æº–åŒ–å™¨ï¼ˆå…§éƒ¨çµ„ä»¶ï¼‰
    
    æ”¯æ´é¡å‹ï¼š
    - none: ä¸è™•ç†
    - training_data_norm: Z-score æ¨™æº–åŒ–ï¼ˆå¾è¨“ç·´è³‡æ–™è¨ˆç®—ï¼‰
    - friction_velocity: åŸºæ–¼æ‘©æ“¦é€Ÿåº¦ç¸®æ”¾
    - manual: æ‰‹å‹•æŒ‡å®š means/stds
    """
    
    SUPPORTED_TYPES = ['none', 'training_data_norm', 'friction_velocity', 'manual']
    DEFAULT_VAR_ORDER = ['u', 'v', 'w', 'p', 'S']
    
    def __init__(self, config: OutputNormConfig):
        if config.norm_type not in self.SUPPORTED_TYPES:
            raise ValueError(
                f"ä¸æ”¯æ´çš„æ¨™æº–åŒ–é¡å‹: {config.norm_type}ã€‚æ”¯æ´: {self.SUPPORTED_TYPES}"
            )
        
        self.norm_type = config.norm_type
        self.variable_order = config.variable_order or self.DEFAULT_VAR_ORDER.copy()
        self.means = config.means or {}
        self.stds = config.stds or {}
        self.params = config.params or {}
        
        logger.info(f"âœ… OutputTransform åˆå§‹åŒ–: type={self.norm_type}, variables={self.variable_order}")
    
    @classmethod
    def from_data(
        cls,
        data: Dict[str, Union[np.ndarray, torch.Tensor]],
        norm_type: str = 'training_data_norm',
        variable_order: Optional[List[str]] = None
    ) -> 'OutputTransform':
        """
        å¾è³‡æ–™è‡ªå‹•è¨ˆç®—æ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆåƒ… training_data_norm æ¨¡å¼ï¼‰
        
        Args:
            data: è³‡æ–™å­—å…¸ {'u': array, 'v': array, ...}
            norm_type: æ¨™æº–åŒ–é¡å‹
            variable_order: è®Šé‡é †åºï¼ˆè‹¥ç‚º None å‰‡å¾ data.keys() æ¨æ–·ï¼‰
        """
        if norm_type != 'training_data_norm':
            raise ValueError(f"from_data åƒ…æ”¯æ´ training_data_normï¼Œç•¶å‰ç‚º: {norm_type}")
        
        means = {}
        stds = {}
        valid_vars = []  # ğŸ›¡ï¸ è¿½è¹¤æœ‰æ•ˆè®Šé‡ï¼ˆæ’é™¤ç©ºå¼µé‡ï¼‰
        
        for var_name, values in data.items():
            if isinstance(values, torch.Tensor):
                values = values.detach().cpu().numpy()
            
            # âš ï¸ è·³éç©ºå¼µé‡æˆ–é›¶é•·åº¦é™£åˆ—ï¼ˆé˜²æ­¢ NaNï¼‰
            if values.size == 0:
                logger.info(f"â­ï¸  {var_name} ç‚ºç©ºå¼µé‡ï¼Œè·³éæ¨™æº–åŒ–çµ±è¨ˆé‡è¨ˆç®—")
                continue
            
            mean = float(np.mean(values))
            std = float(np.std(values))
            
            # ğŸ›¡ï¸ é˜²ç¦¦æ€§æª¢æŸ¥ï¼šæ‹’çµ• NaN æˆ– Inf
            if not np.isfinite(mean) or not np.isfinite(std):
                logger.warning(f"âš ï¸  {var_name} çš„çµ±è¨ˆé‡åŒ…å« NaN/Inf (mean={mean}, std={std})ï¼Œè·³é")
                continue
            
            if abs(std) < 1e-10:
                logger.warning(f"âš ï¸  {var_name} çš„æ¨™æº–å·®æ¥è¿‘é›¶ï¼Œè¨­ç‚º 1.0")
                std = 1.0
            
            means[var_name] = mean
            stds[var_name] = std
            valid_vars.append(var_name)
        
        if variable_order is None:
            variable_order = valid_vars  # ğŸ”§ ä½¿ç”¨æœ‰æ•ˆè®Šé‡åˆ—è¡¨è€Œéæ‰€æœ‰éµ
        
        params = {'source': 'auto_computed_from_data'}
        
        config = OutputNormConfig(
            norm_type=norm_type,
            variable_order=variable_order,
            means=means,
            stds=stds,
            params=params
        )
        
        logger.info(f"âœ… å¾è³‡æ–™è¨ˆç®— Z-score ä¿‚æ•¸: means={means}, stds={stds}")
        return cls(config)
    
    @classmethod
    def from_metadata(cls, metadata: Dict) -> 'OutputTransform':
        """
        å¾ checkpoint metadata å¿«é€Ÿé‡å»º OutputTransform
        
        é€™æ˜¯ä¸€å€‹ä¾¿åˆ©æ–¹æ³•ï¼Œç°¡åŒ–å¾ä¿å­˜çš„ checkpoint ä¸­æ¢å¾©æ¨™æº–åŒ–å™¨çš„æµç¨‹ã€‚
        
        Args:
            metadata: checkpoint çš„ 'normalization' å­—æ®µï¼Œæ‡‰åŒ…å«ï¼š
                - norm_type: æ¨™æº–åŒ–é¡å‹ (str)
                - variable_order: è®Šé‡é †åºåˆ—è¡¨ (List[str])
                - means: å‡å€¼å­—å…¸ (Dict[str, float])
                - stds: æ¨™æº–å·®å­—å…¸ (Dict[str, float])
                - params: å…¶ä»–åƒæ•¸ (Dict, å¯é¸)
        
        Returns:
            OutputTransform å¯¦ä¾‹
        
        Example:
            >>> checkpoint = torch.load('model.pth')
            >>> normalizer = OutputTransform.from_metadata(checkpoint['normalization'])
            >>> predictions = model(x)
            >>> denormalized = normalizer.denormalize_batch(predictions, var_order=['u', 'v', 'p'])
        
        Raises:
            KeyError: è‹¥ metadata ç¼ºå°‘å¿…è¦æ¬„ä½
            ValueError: è‹¥ metadata æ ¼å¼ä¸æ­£ç¢º
        """
        # é©—è­‰å¿…è¦æ¬„ä½
        required_fields = ['norm_type', 'means', 'stds']
        missing_fields = [f for f in required_fields if f not in metadata]
        if missing_fields:
            raise KeyError(
                f"metadata ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}. "
                f"éœ€è¦: {required_fields}, å¾—åˆ°: {list(metadata.keys())}"
            )
        
        # å»ºç«‹ OutputNormConfig
        config = OutputNormConfig(
            norm_type=metadata['norm_type'],
            variable_order=metadata.get('variable_order', cls.DEFAULT_VAR_ORDER.copy()),
            means=metadata['means'],
            stds=metadata['stds'],
            params=metadata.get('params', {})
        )
        
        logger.info(
            f"âœ… å¾ metadata é‡å»º OutputTransform: "
            f"type={config.norm_type}, variables={config.variable_order}"
        )
        return cls(config)
    
    @classmethod
    def from_config(
        cls,
        config: Dict,
        training_data: Optional[Dict[str, torch.Tensor]] = None
    ) -> 'OutputTransform':
        """
        å¾é…ç½®å‰µå»º OutputTransformï¼ˆå‘å¾Œå…¼å®¹ Trainerï¼‰
        
        Args:
            config: å®Œæ•´é…ç½®å­—å…¸
            training_data: è¨“ç·´è³‡æ–™ï¼ˆç”¨æ–¼ training_data_norm æ¨¡å¼ï¼‰
        
        Returns:
            OutputTransform å¯¦ä¾‹
        """
        norm_config = config.get('normalization', {})
        norm_type = norm_config.get('type', 'none')
        params = norm_config.get('params', {})
        variable_order = norm_config.get('variable_order', cls.DEFAULT_VAR_ORDER.copy())
        
        # æ ¹æ“šé¡å‹æå–æ¨™æº–åŒ–ä¿‚æ•¸
        if norm_type == 'training_data_norm':
            means, stds = cls._extract_training_data_scales(params, training_data, config)
        elif norm_type == 'friction_velocity':
            means, stds = cls._extract_friction_velocity_scales(params, config)
        elif norm_type == 'manual':
            means = {k.replace('_mean', ''): v for k, v in params.items() if k.endswith('_mean')}
            stds = {k.replace('_std', ''): v for k, v in params.items() if k.endswith('_std')}
        else:  # 'none'
            means = {}
            stds = {}
        
        output_config = OutputNormConfig(
            norm_type=norm_type,
            variable_order=variable_order,
            means=means,
            stds=stds,
            params=params
        )
        
        return cls(output_config)
    
    @staticmethod
    def _extract_training_data_scales(
        params: Dict,
        training_data: Optional[Dict[str, torch.Tensor]],
        config: Dict
    ) -> Tuple[Dict[str, float], Dict[str, float]]:
        """å¾è¨“ç·´è³‡æ–™æˆ–é…ç½®ä¸­æå–æ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆå…§éƒ¨è¼”åŠ©æ–¹æ³•ï¼‰"""
        # å„ªå…ˆç´š 1: å¾é…ç½®ä¸­æ˜ç¢ºæä¾›
        if all(k in params for k in ['u_mean', 'u_std', 'v_mean', 'v_std', 'w_mean', 'w_std', 'p_mean', 'p_std']):
            means = {
                'u': params['u_mean'],
                'v': params['v_mean'],
                'w': params['w_mean'],
                'p': params['p_mean']
            }
            stds = {
                'u': params['u_std'],
                'v': params['v_std'],
                'w': params['w_std'],
                'p': params['p_std']
            }
            logger.info("ğŸ“ ä½¿ç”¨é…ç½®ä¸­çš„æ¨™æº–åŒ–ä¿‚æ•¸")
            return means, stds
        
        # å„ªå…ˆç´š 2: å¾è¨“ç·´è³‡æ–™è¨ˆç®—
        if training_data is not None:
            means = {}
            stds = {}
            for var_name in ['u', 'v', 'w', 'p']:
                # æ”¯æ´å…©ç¨®éµåæ ¼å¼ï¼š'u' æˆ– 'u_sensors'
                key = var_name if var_name in training_data else f'{var_name}_sensors'
                
                if key in training_data:
                    values = training_data[key]
                    if isinstance(values, torch.Tensor):
                        values = values.detach().cpu().numpy()
                    
                    # âš ï¸ è·³éç©ºå¼µé‡æˆ–é›¶é•·åº¦é™£åˆ—ï¼ˆé˜²æ­¢ NaNï¼‰
                    if values.size == 0:
                        logger.info(f"â­ï¸  {var_name} ç‚ºç©ºå¼µé‡ï¼Œè·³éæ¨™æº–åŒ–çµ±è¨ˆé‡è¨ˆç®—")
                        continue
                    
                    mean = float(np.mean(values))
                    std = float(np.std(values))
                    
                    # ğŸ›¡ï¸ é˜²ç¦¦æ€§æª¢æŸ¥ï¼šæ‹’çµ• NaN æˆ– Inf
                    if not np.isfinite(mean) or not np.isfinite(std):
                        logger.warning(f"âš ï¸  {var_name} çš„çµ±è¨ˆé‡åŒ…å« NaN/Inf (mean={mean}, std={std})ï¼Œè·³é")
                        continue
                    
                    if abs(std) < 1e-10:
                        logger.warning(f"âš ï¸  {var_name} çš„æ¨™æº–å·®æ¥è¿‘é›¶ï¼Œè¨­ç‚º 1.0")
                        std = 1.0
                    
                    means[var_name] = mean
                    stds[var_name] = std
            
            if means:
                logger.info(f"ğŸ“ å¾è¨“ç·´è³‡æ–™è¨ˆç®—æ¨™æº–åŒ–ä¿‚æ•¸: {list(means.keys())}")
                return means, stds
        
        # å„ªå…ˆç´š 3: å¤±æ•—
        raise ValueError(
            "training_data_norm æ¨¡å¼éœ€è¦æä¾›æ¨™æº–åŒ–ä¿‚æ•¸ï¼\n"
            "è«‹åœ¨é…ç½®ä¸­æä¾› normalization.params (u_mean, u_std, ...)\n"
            "æˆ–å‚³å…¥ training_data ä»¥è‡ªå‹•è¨ˆç®—ã€‚"
        )
    
    @staticmethod
    def _extract_friction_velocity_scales(
        params: Dict,
        config: Dict
    ) -> Tuple[Dict[str, float], Dict[str, float]]:
        """å¾ç‰©ç†åƒæ•¸è¨ˆç®— friction_velocity æ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆå…§éƒ¨è¼”åŠ©æ–¹æ³•ï¼‰"""
        u_tau = params.get('u_tau')
        if u_tau is None and 'physics' in config:
            physics = config['physics']
            if 'channel_flow' in physics:
                u_tau = physics['channel_flow'].get('u_tau', 1.0)
            else:
                u_tau = 1.0
        
        if u_tau is None:
            raise ValueError("friction_velocity æ¨¡å¼éœ€è¦æä¾› u_tau åƒæ•¸")
        
        rho = params.get('rho')
        if rho is None and 'physics' in config:
            rho = config['physics'].get('rho', 1.0)
        if rho is None:
            rho = 1.0
        
        velocity_scale = u_tau
        pressure_scale = rho * u_tau ** 2
        
        means = {'u': 0.0, 'v': 0.0, 'w': 0.0, 'p': 0.0}
        stds = {
            'u': velocity_scale,
            'v': velocity_scale,
            'w': velocity_scale,
            'p': pressure_scale
        }
        
        logger.info(f"ğŸ“ Friction velocity scales: u_Ï„={u_tau}, Ï={rho}")
        return means, stds
    
    def normalize(
        self,
        value: Union[np.ndarray, torch.Tensor, float],
        var_name: str
    ) -> Union[np.ndarray, torch.Tensor, float]:
        """æ¨™æº–åŒ–å–®å€‹è®Šé‡"""
        if self.norm_type == 'none':
            return value
        
        if var_name not in self.stds:
            logger.warning(f"âš ï¸  è®Šé‡ {var_name} ç„¡æ¨™æº–åŒ–ä¿‚æ•¸ï¼Œè·³éæ¨™æº–åŒ–")
            return value
        
        mean = self.means.get(var_name, 0.0)
        std = self.stds[var_name]
        
        return (value - mean) / std
    
    def denormalize(
        self,
        value: Union[np.ndarray, torch.Tensor, float],
        var_name: str
    ) -> Union[np.ndarray, torch.Tensor, float]:
        """åæ¨™æº–åŒ–å–®å€‹è®Šé‡"""
        if self.norm_type == 'none':
            return value
        
        if var_name not in self.stds:
            logger.warning(f"âš ï¸  è®Šé‡ {var_name} ç„¡æ¨™æº–åŒ–ä¿‚æ•¸ï¼Œè·³éåæ¨™æº–åŒ–")
            return value
        
        mean = self.means.get(var_name, 0.0)
        std = self.stds[var_name]
        
        return value * std + mean
    
    def normalize_batch(
        self,
        predictions: Union[np.ndarray, torch.Tensor],
        var_order: Optional[List[str]] = None
    ) -> Union[np.ndarray, torch.Tensor]:
        """æ‰¹æ¬¡æ¨™æº–åŒ–ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰"""
        if self.norm_type == 'none':
            return predictions
        
        if var_order is None:
            var_order = self.variable_order
        
        is_torch = isinstance(predictions, torch.Tensor)
        
        if is_torch:
            # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨ detach().clone() åƒ…åœ¨ä¸éœ€è¦æ¢¯åº¦æ™‚æ‰è¤‡è£½
            # è‹¥ predictions.requires_grad=Trueï¼Œå‰‡éœ€è¦ä¿æŒè¨ˆç®—åœ–é€£æ¥
            if predictions.requires_grad:
                # ä¿æŒæ¢¯åº¦è¿½è¹¤ï¼šç›´æ¥åœ¨åŸå¼µé‡ä¸Šæ“ä½œï¼ˆå‰µå»ºæ–°è¦–åœ–ï¼‰
                result = []
                for i, var_name in enumerate(var_order):
                    if i >= predictions.shape[-1]:
                        break
                    col = predictions[:, i:i+1]  # ä¿æŒç¶­åº¦ [batch, 1]
                    if var_name in self.stds:
                        mean = self.means.get(var_name, 0.0)
                        std = self.stds[var_name]
                        col = (col - mean) / std
                    result.append(col)
                return torch.cat(result, dim=1)
            else:
                # ä¸éœ€è¦æ¢¯åº¦ï¼šå¯ä»¥å®‰å…¨ clone
                result = predictions.clone()
                for i, var_name in enumerate(var_order):
                    if i >= result.shape[-1]:
                        break
                    if var_name in self.stds:
                        mean = self.means.get(var_name, 0.0)
                        std = self.stds[var_name]
                        result[:, i] = (result[:, i] - mean) / std
                return result
        else:
            result = predictions.copy()
            for i, var_name in enumerate(var_order):
                if i >= result.shape[-1]:
                    break
                if var_name in self.stds:
                    mean = self.means.get(var_name, 0.0)
                    std = self.stds[var_name]
                    result[:, i] = (result[:, i] - mean) / std
            return result
    
    def denormalize_batch(
        self,
        predictions: Union[np.ndarray, torch.Tensor],
        var_order: Optional[List[str]] = None
    ) -> Union[np.ndarray, torch.Tensor]:
        """æ‰¹æ¬¡åæ¨™æº–åŒ–ï¼ˆç”¨æ–¼è©•ä¼°ï¼‰
        
        ğŸ”§ ä¿®æ­£ï¼šä¿æŒæ¢¯åº¦è¿½è¹¤èƒ½åŠ›
        - è‹¥ predictions.requires_grad=Trueï¼Œå‰‡ä½¿ç”¨ torch.cat ä¿æŒè¨ˆç®—åœ–
        - å¦å‰‡ä½¿ç”¨ clone() æå‡æ•ˆç‡
        """
        if self.norm_type == 'none':
            return predictions
        
        if var_order is None:
            var_order = self.variable_order
        
        is_torch = isinstance(predictions, torch.Tensor)
        
        if is_torch:
            # ğŸ”§ ä¿®æ­£ï¼šæ ¹æ“šæ˜¯å¦éœ€è¦æ¢¯åº¦é¸æ“‡ä¸åŒç­–ç•¥
            if predictions.requires_grad:
                # ä¿æŒæ¢¯åº¦è¿½è¹¤ï¼šä½¿ç”¨ torch.cat æ§‹å»ºæ–°å¼µé‡
                result = []
                for i, var_name in enumerate(var_order):
                    if i >= predictions.shape[-1]:
                        break
                    col = predictions[:, i:i+1]  # ä¿æŒç¶­åº¦ [batch, 1]
                    if var_name in self.stds:
                        mean = self.means.get(var_name, 0.0)
                        std = self.stds[var_name]
                        col = col * std + mean
                    result.append(col)
                return torch.cat(result, dim=1)
            else:
                # ä¸éœ€è¦æ¢¯åº¦ï¼šä½¿ç”¨ clone() æå‡æ•ˆç‡
                result = predictions.clone()
                for i, var_name in enumerate(var_order):
                    if i >= result.shape[-1]:
                        break
                    if var_name in self.stds:
                        mean = self.means.get(var_name, 0.0)
                        std = self.stds[var_name]
                        result[:, i] = result[:, i] * std + mean
                return result
        else:
            result = predictions.copy()
            for i, var_name in enumerate(var_order):
                if i >= result.shape[-1]:
                    break
                if var_name in self.stds:
                    mean = self.means.get(var_name, 0.0)
                    std = self.stds[var_name]
                    result[:, i] = result[:, i] * std + mean
            return result
    
    def get_metadata(self) -> Dict[str, Any]:
        """ç²å–å…ƒæ•¸æ“šï¼ˆç”¨æ–¼ checkpointï¼‰"""
        return {
            'norm_type': self.norm_type,
            'variable_order': self.variable_order.copy(),
            'means': self.means.copy(),
            'stds': self.stds.copy(),
            'params': self.params.copy()
        }


# ===================================================================
# çµ±ä¸€æ¨™æº–åŒ–å™¨
# ===================================================================

class UnifiedNormalizer:
    """
    çµ±ä¸€æ¨™æº–åŒ–å™¨ï¼šç®¡ç†è¼¸å…¥ï¼ˆåæ¨™ï¼‰èˆ‡è¼¸å‡ºï¼ˆè®Šé‡ï¼‰çš„æ¨™æº–åŒ–
    
    è¨­è¨ˆåŸå‰‡ï¼š
    1. å–®ä¸€çœŸç›¸ä¾†æºï¼švariable_order åƒ…å®šç¾©ä¸€æ¬¡
    2. æ˜ç¢ºå„ªå…ˆç´šï¼šcheckpoint > config > computed
    3. Fail-fastï¼šç¼ºå¤±å¿…è¦åƒæ•¸æ™‚ç›´æ¥å¤±æ•—ï¼ˆé™¤é training_data_norm æ¨¡å¼å…è¨±è¨ˆç®—ï¼‰
    
    ä½¿ç”¨ç¯„ä¾‹ï¼š
        >>> # å¾é…ç½®å‰µå»º
        >>> normalizer = UnifiedNormalizer.from_config(config, training_data)
        >>> 
        >>> # è¨“ç·´æ™‚æ¨™æº–åŒ–
        >>> coords_norm = normalizer.transform_input(coords)
        >>> outputs_norm = normalizer.transform_output(outputs)
        >>> 
        >>> # è©•ä¼°æ™‚åæ¨™æº–åŒ–
        >>> predictions_phys = normalizer.inverse_transform_output(predictions_norm)
        >>> 
        >>> # ä¿å­˜åˆ° checkpoint
        >>> metadata = normalizer.get_metadata()
        >>> torch.save({'model': ..., 'normalization': metadata}, 'ckpt.pth')
    """
    
    def __init__(
        self,
        input_transform: InputTransform,
        output_transform: OutputTransform
    ):
        self.input_transform = input_transform
        self.output_transform = output_transform
        
        logger.info(f"âœ… UnifiedNormalizer åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è¼¸å…¥: {self.input_transform.norm_type}")
        logger.info(f"   è¼¸å‡º: {self.output_transform.norm_type}")
        logger.info(f"   è®Šé‡é †åº: {self.output_transform.variable_order}")
    
    @classmethod
    def from_config(
        cls,
        config: Dict,
        training_data: Optional[Dict[str, torch.Tensor]] = None,
        device: torch.device = torch.device('cpu')
    ) -> 'UnifiedNormalizer':
        """
        å¾é…ç½®å‰µå»ºæ¨™æº–åŒ–å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ˆéœ€åŒ…å« 'normalization' æ®µè½ï¼‰
            training_data: è¨“ç·´è³‡æ–™ï¼ˆç”¨æ–¼è¨ˆç®—çµ±è¨ˆé‡ï¼Œåƒ… training_data_norm æ¨¡å¼ï¼‰
            device: è¨­å‚™
        
        Returns:
            UnifiedNormalizer å¯¦ä¾‹
        """
        # === è¼¸å…¥æ¨™æº–åŒ–é…ç½® ===
        scaling_cfg = config.get('model', {}).get('scaling', {})
        input_norm_type = scaling_cfg.get('input_norm', 'none')
        feature_range = tuple(scaling_cfg.get('input_norm_range', [-1.0, 1.0]))
        
        # ç²å– bounds (channel_flow æ¨¡å¼éœ€è¦)
        bounds_tensor = None
        if input_norm_type == 'channel_flow':
            domain = config.get('physics', {}).get('domain', {})
            bounds: List[Tuple[float, float]] = []
            for axis in ['x', 'y', 'z']:
                rng = domain.get(f'{axis}_range')
                if rng is not None:
                    bounds.append((float(rng[0]), float(rng[1])))
            if bounds:
                bounds_tensor = torch.tensor(bounds, dtype=torch.float32, device=device)
        
        input_config = InputNormConfig(
            norm_type=input_norm_type,
            feature_range=(float(feature_range[0]), float(feature_range[1])),
            bounds=bounds_tensor
        )
        input_transform = InputTransform(input_config)
        
        # å¾è¨“ç·´è³‡æ–™æ“¬åˆè¼¸å…¥çµ±è¨ˆé‡
        if training_data is not None:
            coord_tensors = cls._collect_coordinate_tensors(training_data)
            if coord_tensors:
                samples = torch.cat(coord_tensors, dim=0)
                if input_transform.bounds is not None and input_transform.bounds.shape[0] > samples.shape[1]:
                    input_transform.bounds = input_transform.bounds[:samples.shape[1], :]
                input_transform.fit(samples)
        
        input_transform.to(device)
        
        # === è¼¸å‡ºæ¨™æº–åŒ–é…ç½® ===
        if 'normalization' not in config:
            logger.warning("âš ï¸  é…ç½®ä¸­æœªæ‰¾åˆ° 'normalization' æ®µè½ï¼Œä½¿ç”¨é»˜èª (type='none')")
            output_config = OutputNormConfig(norm_type='none')
            output_transform = OutputTransform(output_config)
        else:
            norm_cfg = config['normalization']
            norm_type = norm_cfg.get('type', 'none')
            params = norm_cfg.get('params', {})
            
            # è®Šé‡é †åºï¼šå„ªå…ˆå¾é…ç½®è®€å–ï¼Œå¦å‰‡å¾è³‡æ–™æ¨æ–·
            variable_order = norm_cfg.get('variable_order')
            if variable_order is None and training_data is not None:
                # å¾è¨“ç·´è³‡æ–™çš„ keys æ¨æ–·ï¼ˆæŒ‰é è¨­é †åºæ’åºï¼‰
                # ğŸ›¡ï¸ éæ¿¾æ‰ç©ºå¼µé‡ï¼ˆåªä¿ç•™æœ‰æ•ˆè³‡æ–™çš„è®Šé‡ï¼‰
                data_vars = []
                for k in training_data.keys():
                    if k in OutputTransform.DEFAULT_VAR_ORDER:
                        val = training_data[k]
                        # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºå¼µé‡
                        if isinstance(val, torch.Tensor) and val.numel() == 0:
                            continue
                        elif isinstance(val, np.ndarray) and val.size == 0:
                            continue
                        data_vars.append(k)
                
                if data_vars:
                    variable_order = sorted(data_vars, key=lambda x: OutputTransform.DEFAULT_VAR_ORDER.index(x))
                    logger.info(f"ğŸ“‹ å¾è³‡æ–™æ¨æ–·è®Šé‡é †åºï¼ˆå·²éæ¿¾ç©ºå¼µé‡ï¼‰: {variable_order}")
            
            # æ ¹æ“šé¡å‹æå–æ¨™æº–åŒ–ä¿‚æ•¸
            if norm_type == 'training_data_norm':
                means, stds = OutputTransform._extract_training_data_scales(params, training_data, config)
            elif norm_type == 'friction_velocity':
                means, stds = OutputTransform._extract_friction_velocity_scales(params, config)
            elif norm_type == 'manual':
                means = {k.replace('_mean', ''): v for k, v in params.items() if k.endswith('_mean')}
                stds = {k.replace('_std', ''): v for k, v in params.items() if k.endswith('_std')}
            else:  # 'none'
                means = {}
                stds = {}
            
            output_config = OutputNormConfig(
                norm_type=norm_type,
                variable_order=variable_order,
                means=means,
                stds=stds,
                params=params
            )
            output_transform = OutputTransform(output_config)
        
        return cls(input_transform, output_transform)
    
    @classmethod
    def from_metadata(cls, metadata: Dict) -> 'UnifiedNormalizer':
        """
        å¾ checkpoint metadata æ¢å¾©æ¨™æº–åŒ–å™¨
        
        Args:
            metadata: {'input': dict, 'output': dict}
        """
        input_meta = metadata.get('input', {})
        output_meta = metadata.get('output', {})
        
        # é‡å»º InputTransform
        input_config = InputNormConfig(
            norm_type=input_meta.get('norm_type', 'none'),
            feature_range=tuple(input_meta.get('feature_range', (-1.0, 1.0))),
            bounds=input_meta.get('bounds')
        )
        input_transform = InputTransform(input_config)
        
        # æ¢å¾©çµ±è¨ˆé‡
        if 'mean' in input_meta:
            input_transform.mean = input_meta['mean']
        if 'std' in input_meta:
            input_transform.std = input_meta['std']
        if 'data_min' in input_meta:
            input_transform.data_min = input_meta['data_min']
        if 'data_range' in input_meta:
            input_transform.data_range = input_meta['data_range']
        
        # é‡å»º OutputTransform
        output_config = OutputNormConfig(
            norm_type=output_meta.get('norm_type', 'none'),
            variable_order=output_meta.get('variable_order', OutputTransform.DEFAULT_VAR_ORDER.copy()),
            means=output_meta.get('means', {}),
            stds=output_meta.get('stds', {}),
            params=output_meta.get('params', {})
        )
        output_transform = OutputTransform(output_config)
        
        logger.info(f"ğŸ”„ å¾ checkpoint æ¢å¾© UnifiedNormalizer")
        return cls(input_transform, output_transform)
    
    # === è¼¸å…¥æ¨™æº–åŒ–æ¥å£ ===
    
    def transform_input(self, coords: torch.Tensor) -> torch.Tensor:
        """æ¨™æº–åŒ–è¼¸å…¥åæ¨™"""
        return self.input_transform.transform(coords)
    
    def inverse_transform_input(self, coords: torch.Tensor) -> torch.Tensor:
        """åæ¨™æº–åŒ–è¼¸å…¥åæ¨™"""
        return self.input_transform.inverse_transform(coords)
    
    # === è¼¸å‡ºæ¨™æº–åŒ–æ¥å£ ===
    
    def transform_output(
        self,
        predictions: Union[np.ndarray, torch.Tensor],
        var_order: Optional[List[str]] = None
    ) -> Union[np.ndarray, torch.Tensor]:
        """æ¨™æº–åŒ–è¼¸å‡ºè®Šé‡ï¼ˆæ‰¹æ¬¡ï¼‰"""
        return self.output_transform.normalize_batch(predictions, var_order)
    
    def inverse_transform_output(
        self,
        predictions: Union[np.ndarray, torch.Tensor],
        var_order: Optional[List[str]] = None
    ) -> Union[np.ndarray, torch.Tensor]:
        """åæ¨™æº–åŒ–è¼¸å‡ºè®Šé‡ï¼ˆæ‰¹æ¬¡ï¼‰"""
        return self.output_transform.denormalize_batch(predictions, var_order)
    
    # === å…ƒæ•¸æ“šç®¡ç† ===
    
    def get_metadata(self) -> Dict[str, Any]:
        """ç²å–å®Œæ•´å…ƒæ•¸æ“šï¼ˆç”¨æ–¼ checkpointï¼‰"""
        return {
            'input': self.input_transform.get_metadata(),
            'output': self.output_transform.get_metadata()
        }
    
    @property
    def variable_order(self) -> List[str]:
        """ç²å–è®Šé‡é †åºï¼ˆå–®ä¸€ä¾†æºï¼‰"""
        return self.output_transform.variable_order
    
    def to(self, device: torch.device) -> 'UnifiedNormalizer':
        """ç§»å‹•åˆ°æŒ‡å®šè¨­å‚™"""
        self.input_transform.to(device)
        return self
    
    # === å…§éƒ¨å·¥å…·å‡½æ•¸ ===
    
    @staticmethod
    def _collect_coordinate_tensors(training_data: Dict) -> List[torch.Tensor]:
        """å¾è¨“ç·´è³‡æ–™æ”¶é›†åæ¨™å¼µé‡"""
        coord_tensors = []
        for key in ['coords', 'boundary_coords', 'pde_coords', 'sensor_coords']:
            if key in training_data:
                val = training_data[key]
                if isinstance(val, torch.Tensor):
                    coord_tensors.append(val)
                elif isinstance(val, np.ndarray):
                    coord_tensors.append(torch.from_numpy(val).float())
        return coord_tensors


# ===================================================================
# å‘å¾Œå…¼å®¹ï¼šä¿ç•™èˆŠæ¥å£
# ===================================================================

# ç‚ºäº†ä¸ç ´å£ç¾æœ‰ä»£ç¢¼ï¼Œæä¾›åˆ¥å
InputNormalizer = InputTransform
DataNormalizer = OutputTransform

# é…ç½®å…¼å®¹
NormalizationConfig = InputNormConfig


def create_normalizer_from_checkpoint(checkpoint_path: str) -> OutputTransform:
    """
    å‘å¾Œå…¼å®¹ï¼šå¾ checkpoint å‰µå»ºèˆŠç‰ˆ DataNormalizer
    
    âš ï¸ å·²æ£„ç”¨ï¼Œè«‹ä½¿ç”¨ UnifiedNormalizer.from_metadata()
    """
    import torch
    
    if not torch.cuda.is_available():
        ckpt = torch.load(checkpoint_path, map_location='cpu')
    else:
        ckpt = torch.load(checkpoint_path)
    
    if 'normalization' not in ckpt:
        logger.warning("âš ï¸  Checkpoint ä¸­æœªæ‰¾åˆ° 'normalization' metadataï¼Œä½¿ç”¨é»˜èª (type='none')")
        return OutputTransform(OutputNormConfig(norm_type='none'))
    
    # å˜—è©¦æ–°æ ¼å¼ï¼ˆUnifiedNormalizerï¼‰
    norm_meta = ckpt['normalization']
    if 'output' in norm_meta:
        output_meta = norm_meta['output']
    else:
        # èˆŠæ ¼å¼ï¼ˆDataNormalizerï¼‰
        output_meta = norm_meta
    
    config = OutputNormConfig(
        norm_type=output_meta.get('type', 'none'),
        variable_order=output_meta.get('variable_order', OutputTransform.DEFAULT_VAR_ORDER.copy()),
        means=output_meta.get('means', {}),
        stds=output_meta.get('stds', {}),
        params=output_meta.get('params', {})
    )
    
    return OutputTransform(config)
