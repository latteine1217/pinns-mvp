"""
çµ±ä¸€æ¨™æº–åŒ–ç³»çµ±æ¨¡çµ„

æä¾›å…©ç¨®æ¨™æº–åŒ–å™¨ï¼š
1. InputNormalizer: ç©ºé–“åæ¨™æ¨™æº–åŒ–ï¼ˆä¿æŒåŸæœ‰å¯¦ä½œï¼‰
2. DataNormalizer: è¼¸å‡ºè®Šé‡æ¨™æº–åŒ–ï¼ˆæ–°å¢ï¼Œç”¨æ–¼ u, v, w, pï¼‰
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Tuple, Dict, Union, Any
import logging

import torch
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class NormalizationConfig:
    norm_type: str = "none"
    feature_range: Tuple[float, float] = (-1.0, 1.0)
    bounds: Optional[torch.Tensor] = None  # shape [dim, 2]


class InputNormalizer:
    """
    Lightweight normalizer for spatial coordinates.

    Supported types:
        - none / identity
        - standard  : (x - mean) / std
        - minmax    : map to feature_range using observed min/max
        - channel_flow : map using provided domain bounds to feature_range
    """

    def __init__(self, config: NormalizationConfig):
        self.norm_type = (config.norm_type or "none").lower()
        self.feature_range = config.feature_range
        self.bounds = config.bounds

        self.mean: Optional[torch.Tensor] = None
        self.std: Optional[torch.Tensor] = None
        self.data_min: Optional[torch.Tensor] = None
        self.data_range: Optional[torch.Tensor] = None

    def fit(self, samples: torch.Tensor) -> "InputNormalizer":
        """
        Fit statistics from samples.

        Args:
            samples: [N, D] tensor on any device
        """
        if self.norm_type == "standard":
            mean = torch.mean(samples, dim=0, keepdim=True)
            std = torch.std(samples, dim=0, keepdim=True)
            std = torch.where(std < 1e-8, torch.ones_like(std), std)
            self.mean = mean
            self.std = std
        elif self.norm_type == "minmax":
            data_min = torch.min(samples, dim=0, keepdim=True)[0]
            data_max = torch.max(samples, dim=0, keepdim=True)[0]
            data_range = data_max - data_min
            data_range = torch.where(data_range < 1e-8, torch.ones_like(data_range), data_range)
            self.data_min = data_min
            self.data_range = data_range
        elif self.norm_type in ("channel_flow", "vs_pinn"):
            # channel_flow uses predefined bounds; nothing to fit.
            pass
        else:
            # identity / none
            pass
        return self

    def transform(self, tensor: torch.Tensor) -> torch.Tensor:
        if self.norm_type in ("none", "identity", "vs_pinn"):
            return tensor
        if self.norm_type == "standard":
            if self.mean is None or self.std is None:
                raise RuntimeError("Standard normalizer is not fitted.")
            return (tensor - self.mean) / self.std
        if self.norm_type == "minmax":
            if self.data_min is None or self.data_range is None:
                raise RuntimeError("MinMax normalizer is not fitted.")
            norm = (tensor - self.data_min) / self.data_range
            lo, hi = self.feature_range
            return norm * (hi - lo) + lo
        if self.norm_type == "channel_flow":
            if self.bounds is None:
                raise RuntimeError("Channel-flow bounds not provided.")
            mins = self.bounds[:, 0].unsqueeze(0)
            maxs = self.bounds[:, 1].unsqueeze(0)
            denom = torch.where((maxs - mins) < 1e-8, torch.ones_like(maxs - mins), maxs - mins)
            norm = (tensor - mins) / denom
            lo, hi = self.feature_range
            return norm * (hi - lo) + lo
        raise ValueError(f"Unsupported normalization type: {self.norm_type}")

    def inverse_transform(self, tensor: torch.Tensor) -> torch.Tensor:
        if self.norm_type in ("none", "identity", "vs_pinn"):
            return tensor
        if self.norm_type == "standard":
            if self.mean is None or self.std is None:
                raise RuntimeError("Standard normalizer is not fitted.")
            return tensor * self.std + self.mean
        if self.norm_type == "minmax":
            if self.data_min is None or self.data_range is None:
                raise RuntimeError("MinMax normalizer is not fitted.")
            lo, hi = self.feature_range
            norm = (tensor - lo) / (hi - lo + 1e-12)
            return norm * self.data_range + self.data_min
        if self.norm_type == "channel_flow":
            if self.bounds is None:
                raise RuntimeError("Channel-flow bounds not provided.")
            lo, hi = self.feature_range
            norm = (tensor - lo) / (hi - lo + 1e-12)
            mins = self.bounds[:, 0].unsqueeze(0)
            maxs = self.bounds[:, 1].unsqueeze(0)
            return norm * (maxs - mins) + mins
        raise ValueError(f"Unsupported normalization type: {self.norm_type}")

    def to(self, device: torch.device) -> "InputNormalizer":
        if self.mean is not None:
            self.mean = self.mean.to(device)
        if self.std is not None:
            self.std = self.std.to(device)
        if self.data_min is not None:
            self.data_min = self.data_min.to(device)
        if self.data_range is not None:
            self.data_range = self.data_range.to(device)
        if self.bounds is not None:
            self.bounds = self.bounds.to(device)
        return self


# ===================================================================
# è¼¸å‡ºè®Šé‡æ¨™æº–åŒ–å™¨ï¼ˆæ–°å¢æ–¼ TASK-008ï¼‰
# ===================================================================

class DataNormalizer:
    """
    è¼¸å‡ºè®Šé‡æ¨™æº–åŒ–ç®¡ç†å™¨ï¼ˆç”¨æ–¼ u, v, w, p ç­‰ç‰©ç†é‡ï¼‰
    
    è§£æ±º Phase 6B/6C çš„æ ¸å¿ƒå•é¡Œï¼šè¨“ç·´èˆ‡è©•ä¼°ä½¿ç”¨ä¸€è‡´çš„æ¨™æº–åŒ–ç­–ç•¥ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    - æ”¯æŒå¤šç¨®æ¨™æº–åŒ–é¡å‹ï¼štraining_data_norm, friction_velocity, manual, none
    - å¾é…ç½®æˆ–è³‡æ–™è‡ªå‹•è¨ˆç®—æ¨™æº–åŒ–ä¿‚æ•¸
    - æ¨™æº–åŒ– metadata å¯ä¿å­˜è‡³ checkpoint
    - èˆ‡ denormalization.py å®Œå…¨å…¼å®¹
    
    ä½¿ç”¨ç¯„ä¾‹ï¼š
        >>> from pinnx.utils.normalization import DataNormalizer
        >>> 
        >>> # å¾é…ç½®å‰µå»º
        >>> config = {
        ...     'normalization': {
        ...         'type': 'training_data_norm',
        ...         'params': {'u_scale': 9.84, 'v_scale': 0.19, ...}
        ...     }
        ... }
        >>> normalizer = DataNormalizer.from_config(config)
        >>> 
        >>> # è¨“ç·´æ™‚æ¨™æº–åŒ–
        >>> u_true_norm = normalizer.normalize(u_true, 'u')
        >>> 
        >>> # ä¿å­˜åˆ° checkpoint
        >>> metadata = normalizer.get_metadata()
        >>> torch.save({'model': ..., 'normalization': metadata}, 'ckpt.pth')
    
    Attributes:
        norm_type: æ¨™æº–åŒ–é¡å‹
        scales: æ¨™æº–åŒ–ä¿‚æ•¸å­—å…¸ {'u': float, 'v': float, ...}
        params: åŸå§‹åƒæ•¸ï¼ˆç”¨æ–¼é‡å»ºï¼‰
    """
    
    SUPPORTED_TYPES = ['training_data_norm', 'friction_velocity', 'manual', 'none']
    DEFAULT_VAR_ORDER = ['u', 'v', 'w', 'p', 'S']  # é è¨­è®Šé‡é †åº
    
    def __init__(
        self, 
        norm_type: str = 'none',
        scales: Optional[Dict[str, float]] = None,
        means: Optional[Dict[str, float]] = None,
        params: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–æ¨™æº–åŒ–å™¨
        
        Args:
            norm_type: æ¨™æº–åŒ–é¡å‹
            scales: æ¨™æº–åŒ–ä¿‚æ•¸å­—å…¸ï¼ˆæ¨™æº–å·® stdï¼‰
            means: å‡å€¼å­—å…¸ï¼ˆç”¨æ–¼ Z-score æ¨™æº–åŒ–ï¼‰
            params: é¡å¤–åƒæ•¸ï¼ˆç”¨æ–¼é‡å»ºæˆ–ç‰©ç†åƒæ•¸ï¼‰
        """
        if norm_type not in self.SUPPORTED_TYPES:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ¨™æº–åŒ–é¡å‹: {norm_type}ã€‚æ”¯æŒ: {self.SUPPORTED_TYPES}"
            )
        
        self.norm_type = norm_type
        self.scales = scales if scales is not None else {}
        self.means = means if means is not None else {}
        self.params = params if params is not None else {}
        
        logger.info(f"âœ… DataNormalizer åˆå§‹åŒ–: type={norm_type}, means={self.means}, stds={self.scales}")
    
    @classmethod
    def from_config(cls, config: Dict) -> 'DataNormalizer':
        """
        å¾é…ç½®å­—å…¸å‰µå»ºæ¨™æº–åŒ–å™¨
        
        é…ç½®æ ¼å¼ï¼ˆæ–°ç‰ˆ Z-scoreï¼‰ï¼š
            normalization:
              type: 'training_data_norm'
              params:
                u_mean: 9.921185
                u_std: 4.593879
                v_mean: -0.000085
                v_std: 0.329614
                w_mean: -0.002202
                w_std: 3.865396
                p_mean: -40.374241
                p_std: 28.619722
        
        Args:
            config: é…ç½®å­—å…¸ï¼ˆéœ€åŒ…å« 'normalization' æ®µè½ï¼‰
        
        Returns:
            DataNormalizer å¯¦ä¾‹
        """
        if 'normalization' not in config:
            logger.warning("âš ï¸  é…ç½®ä¸­æœªæ‰¾åˆ° 'normalization' æ®µè½ï¼Œä½¿ç”¨é»˜èª (type='none')")
            return cls(norm_type='none')
        
        norm_cfg = config['normalization']
        norm_type = norm_cfg.get('type', 'none')
        params = norm_cfg.get('params', {})
        
        # æ ¹æ“šé¡å‹æå–æ¨™æº–åŒ–ä¿‚æ•¸
        if norm_type == 'training_data_norm':
            means, scales = cls._extract_training_data_scales(params)
        elif norm_type == 'friction_velocity':
            means, scales = cls._extract_friction_velocity_scales(params, config)
        elif norm_type == 'manual':
            # æ‰‹å‹•æ¨¡å¼ï¼šå‡è¨­ç”¨æˆ¶æä¾› *_mean å’Œ *_std
            means = {k.replace('_mean', ''): v for k, v in params.items() if k.endswith('_mean')}
            scales = {k.replace('_std', ''): v for k, v in params.items() if k.endswith('_std')}
        else:  # 'none'
            means = {}
            scales = {}
        
        return cls(norm_type=norm_type, scales=scales, means=means, params=params)
    
    @classmethod
    def from_metadata(cls, metadata: Dict) -> 'DataNormalizer':
        """
        å¾ checkpoint metadata æ¢å¾©æ¨™æº–åŒ–å™¨
        
        Args:
            metadata: {'type': str, 'means': dict, 'scales': dict, 'params': dict}
        
        Returns:
            DataNormalizer å¯¦ä¾‹
        """
        norm_type = metadata.get('type', 'none')
        scales = metadata.get('scales', {})
        means = metadata.get('means', {})
        params = metadata.get('params', {})
        
        logger.info(f"ğŸ”„ å¾ checkpoint æ¢å¾© DataNormalizer: type={norm_type}")
        return cls(norm_type=norm_type, scales=scales, means=means, params=params)
    
    @classmethod
    def from_data(
        cls, 
        data: Dict[str, Union[np.ndarray, torch.Tensor]], 
        norm_type: str = 'training_data_norm'
    ) -> 'DataNormalizer':
        """
        å¾è³‡æ–™è‡ªå‹•è¨ˆç®—æ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆZ-score: mean + stdï¼‰
        
        Args:
            data: è³‡æ–™å­—å…¸ {'u': array, 'v': array, ...}
            norm_type: æ¨™æº–åŒ–é¡å‹
        
        Returns:
            DataNormalizer å¯¦ä¾‹
        """
        means = {}
        stds = {}
        
        for var_name, values in data.items():
            if isinstance(values, torch.Tensor):
                values = values.detach().cpu().numpy()
            
            mean = float(np.mean(values))
            std = float(np.std(values))
            
            # é¿å…é™¤ä»¥é›¶
            if abs(std) < 1e-10:
                logger.warning(f"âš ï¸  {var_name} çš„æ¨™æº–å·®æ¥è¿‘é›¶ï¼Œè¨­ç‚º 1.0")
                std = 1.0
            
            means[var_name] = mean
            stds[var_name] = std
        
        params = {'source': 'auto_computed_zscore'}
        
        logger.info(f"âœ… å¾è³‡æ–™è¨ˆç®— Z-score ä¿‚æ•¸: means={means}, stds={stds}")
        return cls(norm_type=norm_type, scales=stds, means=means, params=params)
    
    def normalize(
        self, 
        value: Union[np.ndarray, torch.Tensor, float], 
        var_name: str
    ) -> Union[np.ndarray, torch.Tensor, float]:
        """
        æ¨™æº–åŒ–å–®å€‹è®Šé‡ï¼ˆZ-score: (x - mean) / stdï¼‰
        
        Args:
            value: åŸå§‹å€¼ï¼ˆç‰©ç†ç©ºé–“ï¼‰
            var_name: è®Šé‡åç¨± ('u', 'v', 'w', 'p', ...)
        
        Returns:
            æ¨™æº–åŒ–å¾Œçš„å€¼
        """
        if self.norm_type == 'none':
            return value
        
        if var_name not in self.scales:
            logger.warning(f"âš ï¸  è®Šé‡ {var_name} ç„¡æ¨™æº–åŒ–ä¿‚æ•¸ï¼Œè·³éæ¨™æº–åŒ–")
            return value
        
        mean = self.means.get(var_name, 0.0)
        std = self.scales[var_name]
        
        return (value - mean) / std
    
    def denormalize(
        self, 
        value: Union[np.ndarray, torch.Tensor, float], 
        var_name: str
    ) -> Union[np.ndarray, torch.Tensor, float]:
        """
        åæ¨™æº–åŒ–å–®å€‹è®Šé‡ï¼ˆé€† Z-score: x * std + meanï¼‰
        
        Args:
            value: æ¨™æº–åŒ–ç©ºé–“çš„å€¼
            var_name: è®Šé‡åç¨±
        
        Returns:
            ç‰©ç†ç©ºé–“çš„å€¼
        """
        if self.norm_type == 'none':
            return value
        
        if var_name not in self.scales:
            logger.warning(f"âš ï¸  è®Šé‡ {var_name} ç„¡æ¨™æº–åŒ–ä¿‚æ•¸ï¼Œè·³éåæ¨™æº–åŒ–")
            return value
        
        mean = self.means.get(var_name, 0.0)
        std = self.scales[var_name]
        
        return value * std + mean
    
    def normalize_batch(
        self, 
        predictions: Union[np.ndarray, torch.Tensor],
        var_order: Optional[list] = None
    ) -> Union[np.ndarray, torch.Tensor]:
        """
        æ‰¹æ¬¡æ¨™æº–åŒ–ï¼ˆç”¨æ–¼è¨“ç·´æ™‚æ¨™æº–åŒ–æ•´å€‹è¼¸å‡ºå¼µé‡ï¼ŒZ-scoreï¼‰
        
        Args:
            predictions: é æ¸¬å¼µé‡ [N, out_dim]
            var_order: è®Šé‡é †åºåˆ—è¡¨ï¼ˆé»˜èª ['u', 'v', 'w', 'p', 'S']ï¼‰
        
        Returns:
            æ¨™æº–åŒ–å¾Œçš„å¼µé‡ [N, out_dim]
        """
        if self.norm_type == 'none':
            return predictions
        
        if var_order is None:
            var_order = self.DEFAULT_VAR_ORDER
        
        is_torch = isinstance(predictions, torch.Tensor)
        result = predictions.clone() if is_torch else predictions.copy()  # type: ignore
        
        for i, var_name in enumerate(var_order):
            if i >= result.shape[-1]:
                break
            if var_name in self.scales:
                mean = self.means.get(var_name, 0.0)
                std = self.scales[var_name]
                if is_torch:
                    result[:, i] = (result[:, i] - mean) / std  # type: ignore
                else:
                    result[:, i] = (result[:, i] - mean) / std  # type: ignore
        
        return result
    
    def denormalize_batch(
        self, 
        predictions: Union[np.ndarray, torch.Tensor],
        var_order: Optional[list] = None
    ) -> Union[np.ndarray, torch.Tensor]:
        """
        æ‰¹æ¬¡åæ¨™æº–åŒ–ï¼ˆç”¨æ–¼è©•ä¼°æ™‚æ¢å¾©ç‰©ç†é‡ç¶±ï¼Œé€† Z-scoreï¼‰
        
        Args:
            predictions: æ¨™æº–åŒ–ç©ºé–“çš„é æ¸¬ [N, out_dim]
            var_order: è®Šé‡é †åºåˆ—è¡¨
        
        Returns:
            ç‰©ç†ç©ºé–“çš„é æ¸¬ [N, out_dim]
        """
        if self.norm_type == 'none':
            return predictions
        
        if var_order is None:
            var_order = self.DEFAULT_VAR_ORDER
        
        is_torch = isinstance(predictions, torch.Tensor)
        result = predictions.clone() if is_torch else predictions.copy()  # type: ignore
        
        for i, var_name in enumerate(var_order):
            if i >= result.shape[-1]:
                break
            if var_name in self.scales:
                mean = self.means.get(var_name, 0.0)
                std = self.scales[var_name]
                if is_torch:
                    result[:, i] = result[:, i] * std + mean  # type: ignore
                else:
                    result[:, i] = result[:, i] * std + mean  # type: ignore
        
        return result
    
    def get_metadata(self) -> Dict:
        """
        ç²å–å¯ä¿å­˜çš„ metadataï¼ˆç”¨æ–¼ checkpointï¼‰
        
        Returns:
            åŒ…å« type, means, scales (stds), params çš„å­—å…¸
        """
        return {
            'type': self.norm_type,
            'means': self.means.copy(),
            'scales': self.scales.copy(),
            'params': self.params.copy()
        }
    
    def __repr__(self) -> str:
        return f"DataNormalizer(type={self.norm_type}, means={self.means}, stds={self.scales})"
    
    # ===================================================================
    # å…§éƒ¨å·¥å…·å‡½æ•¸
    # ===================================================================
    
    @staticmethod
    def _extract_training_data_scales(params: Dict) -> Tuple[Dict[str, float], Dict[str, float]]:
        """
        æå– training_data_norm æ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆZ-score: mean + stdï¼‰
        
        Returns:
            (means, stds) å…ƒçµ„
        """
        # æ­£ç¢ºçš„ JHTDB Channel Re_Ï„=1000 çµ±è¨ˆé‡ (cutout3d_128x128x32.npz)
        means = {
            'u': params.get('u_mean', 9.921185),
            'v': params.get('v_mean', -0.000085),
            'w': params.get('w_mean', -0.002202),
            'p': params.get('p_mean', -40.374241)
        }
        
        stds = {
            'u': params.get('u_std', 4.593879),
            'v': params.get('v_std', 0.329614),
            'w': params.get('w_std', 3.865396),
            'p': params.get('p_std', 28.619722)
        }
        
        if 'u_mean' not in params or 'u_std' not in params:
            logger.warning(
                "âš ï¸  é…ç½®ä¸­æœªæä¾›å®Œæ•´çµ±è¨ˆé‡ï¼Œä½¿ç”¨ JHTDB Channel Re1000 é»˜èªå€¼"
            )
        
        return means, stds
    
    @staticmethod
    def _extract_friction_velocity_scales(params: Dict, config: Dict) -> Tuple[Dict[str, float], Dict[str, float]]:
        """
        å¾ç‰©ç†åƒæ•¸è¨ˆç®— friction_velocity æ¨™æº–åŒ–ä¿‚æ•¸
        
        Returns:
            (means, stds) å…ƒçµ„
        """
        u_tau = params.get('u_tau')
        if u_tau is None and 'physics' in config:
            physics = config['physics']
            if 'channel_flow' in physics:
                u_tau = physics['channel_flow'].get('u_tau', 1.0)
            else:
                u_tau = 1.0
        
        if u_tau is None:
            u_tau = 1.0
            logger.warning("âš ï¸  æœªæ‰¾åˆ° u_tauï¼Œä½¿ç”¨é»˜èªå€¼ 1.0")
        
        rho = params.get('rho')
        if rho is None and 'physics' in config:
            rho = config['physics'].get('rho', 1.0)
        if rho is None:
            rho = 1.0
        
        velocity_scale = u_tau
        pressure_scale = rho * u_tau ** 2
        
        # Friction velocity æ–¹æ³•å‡è¨­å‡å€¼ç‚º 0ï¼ˆå·²å»é™¤å¹³å‡æµï¼‰
        means = {
            'u': 0.0,
            'v': 0.0,
            'w': 0.0,
            'p': 0.0
        }
        
        stds = {
            'u': velocity_scale,
            'v': velocity_scale,
            'w': velocity_scale,
            'p': pressure_scale
        }
        
        logger.info(f"ğŸ“ Friction velocity scales: u_Ï„={u_tau}, Ï={rho}")
        
        return means, stds


# ===================================================================
# ä¾¿æ·å‡½æ•¸
# ===================================================================

def create_normalizer_from_checkpoint(checkpoint_path: str) -> DataNormalizer:
    """
    å¾ checkpoint å‰µå»ºæ¨™æº–åŒ–å™¨
    
    Args:
        checkpoint_path: checkpoint æª”æ¡ˆè·¯å¾‘
    
    Returns:
        DataNormalizer å¯¦ä¾‹
    """
    import torch
    
    if not torch.cuda.is_available():
        ckpt = torch.load(checkpoint_path, map_location='cpu')
    else:
        ckpt = torch.load(checkpoint_path)
    
    if 'normalization' not in ckpt:
        logger.warning(
            "âš ï¸  Checkpoint ä¸­æœªæ‰¾åˆ° 'normalization' metadataï¼Œä½¿ç”¨é»˜èª (type='none')"
        )
        return DataNormalizer(norm_type='none')
    
    return DataNormalizer.from_metadata(ckpt['normalization'])

