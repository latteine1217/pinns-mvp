"""
ç´„ç¿°éœæ™®é‡‘æ–¯æ¹æµè³‡æ–™åº« (JHTDB) å®¢æˆ¶ç«¯

æä¾›é«˜æ•ˆã€å¯é‡ç¾çš„ JHTDB è³‡æ–™å­˜å–æ¥å£ï¼Œæ”¯æ´ï¼š
- Cutout æ‰¹é‡è³‡æ–™ä¸‹è¼‰
- æ•£é»æ’å€¼å–æ¨£
- è‡ªå‹•å¿«å–èˆ‡ç‰ˆæœ¬ç®¡ç†
- å¤šç¨®æ¹æµè³‡æ–™é›† (é€šé“æµã€HITã€é‚Šç•Œå±¤ç­‰)
- è³‡æ–™é©—è­‰èˆ‡å“è³ªæª¢æŸ¥

åƒè€ƒæ–‡ç»ï¼š
- JHTDB Official Documentation: https://turbulence.pha.jhu.edu/
- pyJHTDB: Python interface for JHTDB
- SciServer: Cloud-based data access platform

æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ï¼š
1. å¯é‡ç¾æ€§ï¼šå›ºå®šç¨®å­ã€ç‰ˆæœ¬è¨˜éŒ„ã€è³‡æ–™æ ¡é©—
2. æ•ˆç‡ï¼šæ™ºèƒ½å¿«å–ã€æ‰¹é‡ä¸‹è¼‰ã€å¢é‡æ›´æ–°
3. ç©©å¥æ€§ï¼šéŒ¯èª¤è™•ç†ã€é‡è©¦æ©Ÿåˆ¶ã€å‚™ç”¨ç«¯é»
4. æ¨™æº–åŒ–ï¼šçµ±ä¸€è³‡æ–™æ ¼å¼ã€åº§æ¨™ç³»çµ±ã€å‘½åè¦ç¯„
"""

import numpy as np
import h5py
import os
import json
import hashlib
import time
import logging
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from pathlib import Path
import urllib.request
import urllib.error
import urllib.parse
import xml.etree.ElementTree as ET
import struct
import base64
from abc import ABC, abstractmethod

try:
    import pyJHTDB
    PYJHTDB_AVAILABLE = True
except ImportError:
    PYJHTDB_AVAILABLE = False
    pyJHTDB = None

logger = logging.getLogger(__name__)


class JHTDBConfig:
    """JHTDB é€£æ¥é…ç½®"""
    
    # è³‡æ–™é›†é…ç½®
    DATASETS = {
        'channel': {
            'name': 'channel',
            'description': 'é€šé“æµ (Re_tau=1000)',
            'domain': {'x': [0, 8*np.pi], 'y': [-1, 1], 'z': [0, 3*np.pi]},
            'resolution': {'x': 2048, 'y': 512, 'z': 1536},
            'time_range': [0.0, 26.0],
            'dt': 0.0065,
            'variables': ['u', 'v', 'w', 'p']
        },
        'isotropic1024coarse': {
            'name': 'isotropic1024coarse',
            'description': 'å„å‘åŒæ€§æ¹æµ (1024^3, ç²—æ™‚é–“è§£æåº¦)',
            'domain': {'x': [0, 2*np.pi], 'y': [0, 2*np.pi], 'z': [0, 2*np.pi]},
            'resolution': {'x': 1024, 'y': 1024, 'z': 1024},
            'time_range': [0.0, 10.0],
            'dt': 0.04,
            'variables': ['u', 'v', 'w', 'p']
        },
        'transition_bl': {
            'name': 'transition_bl',
            'description': 'é‚Šç•Œå±¤è½‰æ©',
            'domain': {'x': [0, 4000], 'y': [0, 120], 'z': [0, 300]},
            'resolution': {'x': 4000, 'y': 120, 'z': 300},
            'time_range': [0.0, 100.0],
            'dt': 0.5,
            'variables': ['u', 'v', 'w', 'p']
        }
    }
    
    # é è¨­é€£æ¥åƒæ•¸
    # âš ï¸ å®‰å…¨æ€§ï¼šå¾ç’°å¢ƒè®Šæ•¸è®€å– auth tokenï¼Œé¿å…ç¡¬ç·¨ç¢¼
    DEFAULT_AUTH_TOKEN = None  # å¾ç’°å¢ƒè®Šæ•¸ JHTDB_AUTH_TOKEN è¼‰å…¥
    DEFAULT_CACHE_DIR = "data/jhtdb"
    DEFAULT_TIMEOUT = 300  # 5 åˆ†é˜
    MAX_RETRY = 3
    
    # è³‡æ–™é©—è­‰é–¾å€¼
    VALIDATION_THRESHOLDS = {
        'velocity_magnitude_max': 100.0,  # m/s
        'pressure_range': [-1000.0, 1000.0],  # Pa
        'nan_fraction_max': 0.01,  # æœ€å¤§ NaN æ¯”ä¾‹
        'inf_fraction_max': 0.001  # æœ€å¤§ Inf æ¯”ä¾‹
    }


class JHTDBError(Exception):
    """JHTDB ç‰¹å®šéŒ¯èª¤"""
    pass


class DataValidator:
    """è³‡æ–™é©—è­‰å™¨"""
    
    @staticmethod
    def validate_field(data: np.ndarray, 
                      field_name: str, 
                      thresholds: Dict[str, float]) -> Dict[str, Any]:
        """
        é©—è­‰æµå ´è³‡æ–™çš„ç‰©ç†åˆç†æ€§
        
        Args:
            data: è³‡æ–™é™£åˆ—
            field_name: æ¬„ä½åç¨± ('u', 'v', 'w', 'p')
            thresholds: é©—è­‰é–¾å€¼
            
        Returns:
            é©—è­‰å ±å‘Šå­—å…¸
        """
        report = {
            'field': field_name,
            'shape': data.shape,
            'dtype': str(data.dtype),
            'valid': True,
            'warnings': [],
            'errors': []
        }
        
        # åŸºæœ¬çµ±è¨ˆ
        if data.size > 0:
            report['stats'] = {
                'mean': float(np.mean(data)),
                'std': float(np.std(data)),
                'min': float(np.min(data)),
                'max': float(np.max(data)),
                'nan_count': int(np.sum(np.isnan(data))),
                'inf_count': int(np.sum(np.isinf(data)))
            }
            
            # NaN/Inf æª¢æŸ¥
            nan_fraction = report['stats']['nan_count'] / data.size
            inf_fraction = report['stats']['inf_count'] / data.size
            
            if nan_fraction > thresholds.get('nan_fraction_max', 0.01):
                report['errors'].append(f"éå¤š NaN å€¼: {nan_fraction:.3f}")
                report['valid'] = False
            
            if inf_fraction > thresholds.get('inf_fraction_max', 0.001):
                report['errors'].append(f"éå¤š Inf å€¼: {inf_fraction:.3f}")
                report['valid'] = False
            
            # ç‰©ç†ç¯„åœæª¢æŸ¥
            if field_name in ['u', 'v', 'w']:
                max_velocity = max(abs(report['stats']['min']), abs(report['stats']['max']))
                if max_velocity > thresholds.get('velocity_magnitude_max', 100.0):
                    report['warnings'].append(f"é€Ÿåº¦éå¤§: {max_velocity:.2f}")
            
            elif field_name == 'p':
                p_min, p_max = report['stats']['min'], report['stats']['max']
                threshold_range = thresholds.get('pressure_range', [-1000.0, 1000.0])
                if p_min < threshold_range[0] or p_max > threshold_range[1]:
                    report['warnings'].append(f"å£“åŠ›è¶…å‡ºåˆç†ç¯„åœ: [{p_min:.2f}, {p_max:.2f}]")
        
        else:
            report['errors'].append("ç©ºè³‡æ–™é™£åˆ—")
            report['valid'] = False
        
        return report


class CacheManager:
    """æ™ºèƒ½å¿«å–ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict:
        """è¼‰å…¥å¿«å–å…ƒè³‡æ–™"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r') as f:
                    return json.load(f)
            except:
                logger.warning("å¿«å–å…ƒè³‡æ–™æå£ï¼Œé‡æ–°åˆå§‹åŒ–")
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜å¿«å–å…ƒè³‡æ–™"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)
    
    def _compute_key(self, dataset: str, query_params: Dict) -> str:
        """è¨ˆç®—æŸ¥è©¢çš„å¿«å–éµ"""
        # å»ºç«‹æ¨™æº–åŒ–çš„æŸ¥è©¢å­—ä¸²
        sorted_params = json.dumps(query_params, sort_keys=True)
        key_string = f"{dataset}:{sorted_params}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get_cache_path(self, dataset: str, query_params: Dict) -> Path:
        """ç²å–å¿«å–æª”æ¡ˆè·¯å¾‘"""
        cache_key = self._compute_key(dataset, query_params)
        return self.cache_dir / f"{dataset}_{cache_key}.h5"
    
    def is_cached(self, dataset: str, query_params: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦å·²å¿«å–"""
        cache_path = self.get_cache_path(dataset, query_params)
        cache_key = self._compute_key(dataset, query_params)
        
        if not cache_path.exists():
            return False
        
        # æª¢æŸ¥å…ƒè³‡æ–™
        if cache_key in self.metadata:
            metadata = self.metadata[cache_key]
            # æª¢æŸ¥æª”æ¡ˆå®Œæ•´æ€§
            if cache_path.stat().st_size != metadata.get('file_size', -1):
                logger.warning(f"å¿«å–æª”æ¡ˆå¤§å°ä¸ç¬¦ï¼Œåˆªé™¤: {cache_path}")
                cache_path.unlink()
                del self.metadata[cache_key]
                self._save_metadata()
                return False
            return True
        
        return False
    
    def save_to_cache(self, dataset: str, query_params: Dict, data: Dict[str, np.ndarray], 
                     metadata: Dict = None):
        """ä¿å­˜è³‡æ–™åˆ°å¿«å–"""
        cache_path = self.get_cache_path(dataset, query_params)
        cache_key = self._compute_key(dataset, query_params)
        
        try:
            with h5py.File(cache_path, 'w') as f:
                # ä¿å­˜è³‡æ–™
                for var_name, var_data in data.items():
                    f.create_dataset(var_name, data=var_data, compression='gzip', compression_opts=6)
                
                # ä¿å­˜æŸ¥è©¢åƒæ•¸å’Œå…ƒè³‡æ–™
                f.attrs['query_params'] = json.dumps(query_params)
                f.attrs['dataset'] = dataset
                f.attrs['timestamp'] = time.time()
                
                if metadata:
                    for key, value in metadata.items():
                        # å°‡è¤‡é›œçš„ Python ç‰©ä»¶åºåˆ—åŒ–ç‚º JSON å­—ä¸²
                        if isinstance(value, (dict, list, tuple)):
                            f.attrs[key] = json.dumps(value)
                        elif isinstance(value, np.ndarray):
                            # numpy é™£åˆ—è½‰ç‚ºåˆ—è¡¨å†åºåˆ—åŒ–
                            f.attrs[key] = json.dumps(value.tolist())
                        else:
                            # ç°¡å–®é¡å‹ç›´æ¥å„²å­˜
                            try:
                                f.attrs[key] = value
                            except (TypeError, ValueError):
                                # å¦‚æœç„¡æ³•ç›´æ¥å„²å­˜ï¼Œè½‰ç‚ºå­—ä¸²
                                f.attrs[key] = str(value)
            
            # æ›´æ–°å…ƒè³‡æ–™
            self.metadata[cache_key] = {
                'dataset': dataset,
                'query_params': query_params,
                'file_path': str(cache_path),
                'file_size': cache_path.stat().st_size,
                'timestamp': time.time(),
                'variables': list(data.keys())
            }
            self._save_metadata()
            
            logger.info(f"è³‡æ–™å·²å¿«å–: {cache_path}")
            
        except Exception as e:
            logger.error(f"å¿«å–ä¿å­˜å¤±æ•—: {e}")
            if cache_path.exists():
                cache_path.unlink()
    
    def load_from_cache(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """å¾å¿«å–è¼‰å…¥è³‡æ–™"""
        cache_path = self.get_cache_path(dataset, query_params)
        
        try:
            data = {}
            with h5py.File(cache_path, 'r') as f:
                for var_name in f.keys():
                    data[var_name] = f[var_name][:]
                
                # é©—è­‰æŸ¥è©¢åƒæ•¸
                cached_params = json.loads(f.attrs['query_params'])
                if cached_params != query_params:
                    logger.warning("å¿«å–æŸ¥è©¢åƒæ•¸ä¸ç¬¦ï¼Œå¯èƒ½æ˜¯é›œæ¹Šè¡çª")
                    return {}
            
            logger.info(f"å¾å¿«å–è¼‰å…¥è³‡æ–™: {cache_path}")
            return data
            
        except Exception as e:
            logger.error(f"å¿«å–è¼‰å…¥å¤±æ•—: {e}")
            return {}
    
    def clear_cache(self, older_than_days: int = 30):
        """æ¸…ç†èˆŠå¿«å–"""
        current_time = time.time()
        cutoff_time = current_time - (older_than_days * 24 * 3600)
        
        removed_count = 0
        for cache_key, metadata in list(self.metadata.items()):
            if metadata.get('timestamp', 0) < cutoff_time:
                cache_path = Path(metadata['file_path'])
                if cache_path.exists():
                    cache_path.unlink()
                    removed_count += 1
                del self.metadata[cache_key]
        
        if removed_count > 0:
            self._save_metadata()
            logger.info(f"æ¸…ç†äº† {removed_count} å€‹èˆŠå¿«å–æª”æ¡ˆ")


class BaseJHTDBClient(ABC):
    """JHTDB å®¢æˆ¶ç«¯åŸºé¡"""
    
    def __init__(self, 
                 auth_token: Optional[str] = None,
                 cache_dir: str = None,
                 timeout: int = None):
        # å„ªå…ˆé †åºï¼šå‚³å…¥åƒæ•¸ > ç’°å¢ƒè®Šæ•¸ > DEFAULT_AUTH_TOKEN
        import os
        self.auth_token = (
            auth_token or 
            os.getenv('JHTDB_AUTH_TOKEN') or 
            JHTDBConfig.DEFAULT_AUTH_TOKEN
        )
        
        # ä¸å¼·åˆ¶è¦æ±‚ tokenï¼ˆå…è¨± Mock å®¢æˆ¶ç«¯é‹è¡Œï¼‰
        # å¯¦éš›çš„ token é©—è­‰ç”±å­é¡è² è²¬
        
        self.timeout = timeout or JHTDBConfig.DEFAULT_TIMEOUT
        self.cache_manager = CacheManager(cache_dir or JHTDBConfig.DEFAULT_CACHE_DIR)
        self.validator = DataValidator()
    
    @abstractmethod
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """å¯¦éš›çš„è³‡æ–™ç²å–å¯¦ç¾ï¼ˆç”±å­é¡å¯¦ç¾ï¼‰"""
        pass
    
    def fetch_data(self, 
                   dataset: str,
                   query_params: Dict,
                   use_cache: bool = True,
                   validate: bool = True) -> Dict[str, Any]:
        """
        ç²å– JHTDB è³‡æ–™çš„ä¸»è¦æ¥å£
        
        Args:
            dataset: è³‡æ–™é›†åç¨±
            query_params: æŸ¥è©¢åƒæ•¸
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
            validate: æ˜¯å¦é©—è­‰è³‡æ–™
            
        Returns:
            åŒ…å«è³‡æ–™å’Œå…ƒè³‡æ–™çš„å­—å…¸
        """
        # æª¢æŸ¥å¿«å–
        if use_cache and self.cache_manager.is_cached(dataset, query_params):
            logger.info("ä½¿ç”¨å¿«å–è³‡æ–™")
            data = self.cache_manager.load_from_cache(dataset, query_params)
            if data:  # å¿«å–è¼‰å…¥æˆåŠŸ
                result = {'data': data, 'from_cache': True}
                if validate:
                    result['validation'] = self._validate_data(data)
                return result
        
        # å¾ JHTDB ç²å–æ–°è³‡æ–™
        logger.info(f"å¾ JHTDB ç²å–è³‡æ–™: {dataset}")
        data = self._fetch_raw_data(dataset, query_params)
        
        # é©—è­‰è³‡æ–™
        validation_report = None
        if validate:
            validation_report = self._validate_data(data)
            if not all(report['valid'] for report in validation_report.values()):
                logger.warning("è³‡æ–™é©—è­‰ç™¼ç¾å•é¡Œ")
        
        # ä¿å­˜åˆ°å¿«å–
        if use_cache and data:
            metadata = {'validation_report': validation_report} if validation_report else {}
            self.cache_manager.save_to_cache(dataset, query_params, data, metadata)
        
        return {
            'data': data,
            'from_cache': False,
            'validation': validation_report
        }
    
    def _validate_data(self, data: Dict[str, np.ndarray]) -> Dict[str, Dict]:
        """é©—è­‰ç²å–çš„è³‡æ–™"""
        validation_results = {}
        
        for var_name, var_data in data.items():
            validation_results[var_name] = self.validator.validate_field(
                var_data, var_name, JHTDBConfig.VALIDATION_THRESHOLDS)
        
        return validation_results


class PyJHTDBClient(BaseJHTDBClient):
    """ä½¿ç”¨ pyJHTDB çš„å®¢æˆ¶ç«¯å¯¦ç¾"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        if not PYJHTDB_AVAILABLE:
            raise JHTDBError("pyJHTDB æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install pyJHTDB")
        
        # åˆå§‹åŒ– pyJHTDB
        if self.auth_token:
            pyJHTDB.dbinfo.auth_token = self.auth_token
    
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """ä½¿ç”¨ pyJHTDB ç²å–è³‡æ–™"""
        
        query_type = query_params.get('type', 'cutout')
        variables = query_params.get('variables', ['u', 'v', 'w', 'p'])
        
        try:
            if query_type == 'cutout':
                return self._fetch_cutout(dataset, query_params, variables)
            elif query_type == 'points':
                return self._fetch_points(dataset, query_params, variables)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹: {query_type}")
                
        except Exception as e:
            logger.error(f"pyJHTDB è³‡æ–™ç²å–å¤±æ•—: {e}")
            raise JHTDBError(f"è³‡æ–™ç²å–å¤±æ•—: {e}")
    
    def _fetch_cutout(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç²å– cutout è³‡æ–™"""
        
        start_coords = params['start']  # [x, y, z]
        end_coords = params['end']      # [x, y, z]
        timestep = params.get('timestep', 0)
        
        data = {}
        
        for var in variables:
            logger.info(f"ç²å–è®Šæ•¸ {var} çš„ cutout è³‡æ–™...")
            
            # pyJHTDB cutout èª¿ç”¨
            if var in ['u', 'v', 'w']:
                # é€Ÿåº¦å ´
                cutout_data = pyJHTDB.getvelocity(
                    start=start_coords,
                    end=end_coords,
                    step=[1, 1, 1],  # ç©ºé–“æ­¥é•·
                    dataset=dataset,
                    time=timestep
                )
                
                # pyJHTDB è¿”å› [u, v, w] é™£åˆ—
                if var == 'u':
                    data[var] = cutout_data[:, :, :, 0]
                elif var == 'v':
                    data[var] = cutout_data[:, :, :, 1]
                elif var == 'w':
                    data[var] = cutout_data[:, :, :, 2]
            
            elif var == 'p':
                # å£“åŠ›å ´
                data[var] = pyJHTDB.getpressure(
                    start=start_coords,
                    end=end_coords,
                    step=[1, 1, 1],
                    dataset=dataset,
                    time=timestep
                )
        
        return data
    
    def _fetch_points(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç²å–æ•£é»è³‡æ–™"""
        
        points = params['points']  # [[x1,y1,z1], [x2,y2,z2], ...]
        timestep = params.get('timestep', 0)
        
        data = {}
        
        for var in variables:
            logger.info(f"ç²å–è®Šæ•¸ {var} çš„æ•£é»è³‡æ–™...")
            
            if var in ['u', 'v', 'w']:
                # é€Ÿåº¦å ´æ’å€¼
                velocity_data = pyJHTDB.getvelocity(
                    points=points,
                    dataset=dataset,
                    time=timestep
                )
                
                if var == 'u':
                    data[var] = velocity_data[:, 0]
                elif var == 'v':
                    data[var] = velocity_data[:, 1]
                elif var == 'w':
                    data[var] = velocity_data[:, 2]
            
            elif var == 'p':
                # å£“åŠ›å ´æ’å€¼
                data[var] = pyJHTDB.getpressure(
                    points=points,
                    dataset=dataset,
                    time=timestep
                )
        
        return data


class MockJHTDBClient(BaseJHTDBClient):
    """æ¨¡æ“¬ JHTDB å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼æ¸¬è©¦å’Œé›¢ç·šé–‹ç™¼ï¼‰"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.seed = 42  # å›ºå®šéš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
    
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ“¬çš„æ¹æµè³‡æ–™"""
        
        np.random.seed(self.seed)
        
        query_type = query_params.get('type', 'cutout')
        variables = query_params.get('variables', ['u', 'v', 'w', 'p'])
        
        if query_type == 'cutout':
            return self._generate_cutout_data(dataset, query_params, variables)
        elif query_type == 'points':
            return self._generate_points_data(dataset, query_params, variables)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹: {query_type}")
    
    def _generate_cutout_data(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ“¬ cutout è³‡æ–™"""
        
        start = np.array(params['start'])
        end = np.array(params['end'])
        resolution = params.get('resolution', [64, 64, 64])
        
        # ç”Ÿæˆåº§æ¨™ç¶²æ ¼
        x = np.linspace(start[0], end[0], resolution[0])
        y = np.linspace(start[1], end[1], resolution[1])
        z = np.linspace(start[2], end[2], resolution[2])
        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
        
        data = {}
        
        # ç²å–è³‡æ–™é›†é…ç½®
        dataset_config = JHTDBConfig.DATASETS.get(dataset, {})
        domain = dataset_config.get('domain', {'x': [0, 2*np.pi], 'y': [0, 2*np.pi], 'z': [0, 2*np.pi]})
        
        # é‡å° Channel Flow ç”Ÿæˆå…·æœ‰æ­£ç¢ºç‰©ç†ç‰¹å¾µçš„æ•¸æ“š
        if dataset == 'channel':
            return self._generate_channel_flow_data(X, Y, Z, variables, domain)
        else:
            # å°æ–¼å…¶ä»–æ•¸æ“šé›†ï¼Œä½¿ç”¨ç­‰å‘æ€§æ¹æµæ¨¡æ“¬
            return self._generate_isotropic_data(X, Y, Z, variables, domain)
    
    def _generate_channel_flow_data(self, X: np.ndarray, Y: np.ndarray, Z: np.ndarray, 
                                   variables: List[str], domain: Dict) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆå…·æœ‰ Channel Flow ç‰¹å¾µçš„æ¨¡æ“¬æ•¸æ“š"""
        
        # æ¨™æº–åŒ–åº§æ¨™ï¼šy æ–¹å‘ç‚ºå£é¢æ³•å‘ [-1, 1]
        x_norm = 2 * np.pi * (X - domain['x'][0]) / (domain['x'][1] - domain['x'][0])
        y_norm = (Y - domain['y'][0]) / (domain['y'][1] - domain['y'][0]) * 2 - 1  # [-1, 1]
        z_norm = 2 * np.pi * (Z - domain['z'][0]) / (domain['z'][1] - domain['z'][0])
        
        data = {}
        
        for var in variables:
            if var == 'u':
                # æµå‘é€Ÿåº¦ï¼šæ‹‹ç‰©å‹å¹³å‡åˆ†å¸ƒ + æ¹æµæ“¾å‹•
                u_mean = 15.0 * (1 - y_norm**2)  # æ‹‹ç‰©å‹åˆ†å¸ƒï¼Œæœ€å¤§å€¼ 15 m/s
                
                # æ¹æµæ“¾å‹•ï¼šè¿‘å£é¢å°ï¼Œä¸­å¿ƒå¤§
                turbulent_intensity = 0.5 * (1 - abs(y_norm)**3)
                u_fluctuation = (
                    turbulent_intensity * 3.0 * np.sin(2*x_norm) * np.cos(3*z_norm) +
                    turbulent_intensity * 1.5 * np.sin(4*x_norm) * np.cos(6*z_norm) +
                    turbulent_intensity * 0.3 * np.random.randn(*X.shape)
                )
                
                data[var] = u_mean + u_fluctuation
                
            elif var == 'v':
                # å£é¢æ³•å‘é€Ÿåº¦ï¼šå¾ˆå°ï¼Œä¸»è¦æ˜¯æ¹æµæ“¾å‹•
                # é‚Šç•Œæ¢ä»¶ï¼šå£é¢è™• v=0
                wall_factor = 1 - y_norm**2  # å£é¢è™•ç‚º 0
                
                data[var] = wall_factor * (
                    0.8 * np.cos(x_norm) * np.sin(2*z_norm) +
                    0.4 * np.cos(3*x_norm) * np.sin(4*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'w':
                # å±•å‘é€Ÿåº¦ï¼šå¢å¼·æ¹æµæ“¾å‹•ä»¥é”åˆ°åˆç†çš„å±•å‘æ¯”ä¾‹ (ç›®æ¨™ w/u ~ 0.4-0.6)
                wall_factor = 1 - y_norm**2  # å£é¢è™•ç‚º 0
                turbulent_intensity = 0.8 * (1 - abs(y_norm)**2)
                
                data[var] = wall_factor * (
                    8.0 * np.sin(x_norm) * np.cos(z_norm) +
                    4.0 * np.sin(3*x_norm) * np.cos(2*z_norm) +
                    turbulent_intensity * 4.0 * np.random.randn(*X.shape)
                )
                
            elif var == 'p':
                # å£“åŠ›å ´ï¼šå—å¹³å‡é€Ÿåº¦æ¢¯åº¦å’Œæ¹æµå½±éŸ¿
                u_val = data.get('u', np.zeros_like(X))
                v_val = data.get('v', np.zeros_like(X))
                w_val = data.get('w', np.zeros_like(X))
                
                # åŸºæ–¼é€£çºŒæ€§æ–¹ç¨‹å’Œå‹•é‡æ–¹ç¨‹çš„å£“åŠ›ä¼°è¨ˆ
                p_mean = -2.0 * y_norm  # ç·šæ€§å£“åŠ›æ¢¯åº¦ï¼ˆé©…å‹•æµå‹•ï¼‰
                p_fluctuation = (
                    -0.3 * (u_val**2 + v_val**2 + w_val**2) +  # å‹•å£“é …
                    5.0 * np.cos(x_norm + z_norm) * (1 - y_norm**2) +  # æ¹æµå£“åŠ›
                    0.1 * np.random.randn(*X.shape)
                )
                
                data[var] = p_mean + p_fluctuation
        
        return data
    
    def _generate_isotropic_data(self, X: np.ndarray, Y: np.ndarray, Z: np.ndarray, 
                                variables: List[str], domain: Dict) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆç­‰å‘æ€§æ¹æµæ•¸æ“šï¼ˆåŸæœ‰é‚è¼¯ï¼‰"""
        
        # æ¨™æº–åŒ–åº§æ¨™
        x_norm = 2 * np.pi * (X - domain['x'][0]) / (domain['x'][1] - domain['x'][0])
        y_norm = 2 * np.pi * (Y - domain['y'][0]) / (domain['y'][1] - domain['y'][0])
        z_norm = 2 * np.pi * (Z - domain['z'][0]) / (domain['z'][1] - domain['z'][0])
        
        data = {}
        
        for var in variables:
            if var == 'u':
                # ä¸»æµæ–¹å‘é€Ÿåº¦ï¼šåŒ…å«å¤šå°ºåº¦æ¸¦çµæ§‹
                data[var] = (
                    5.0 * np.sin(x_norm) * np.cos(y_norm) * np.sin(z_norm) +
                    2.0 * np.sin(2*x_norm) * np.cos(2*y_norm) * np.sin(2*z_norm) +
                    0.5 * np.sin(4*x_norm) * np.cos(4*y_norm) * np.sin(4*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'v':
                # æ©«å‘é€Ÿåº¦
                data[var] = (
                    3.0 * np.cos(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.5 * np.cos(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'w':
                # å±•å‘é€Ÿåº¦
                data[var] = (
                    2.0 * np.sin(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.0 * np.sin(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'p':
                # å£“åŠ›å ´ï¼šéœ€è¦æ»¿è¶³é€£çºŒæ€§æ–¹ç¨‹çš„ç´„æŸ
                data[var] = (
                    -0.5 * (data.get('u', 0)**2 + data.get('v', 0)**2 + data.get('w', 0)**2) +
                    10.0 * np.cos(x_norm + y_norm + z_norm) +
                    0.05 * np.random.randn(*X.shape)
                )
        
        return data
    
    def _generate_points_data(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ“¬æ•£é»è³‡æ–™"""
        
        points = np.array(params['points'])  # [N, 3]
        n_points = points.shape[0]
        
        data = {}
        
        # ç²å–è³‡æ–™é›†é…ç½®
        dataset_config = JHTDBConfig.DATASETS.get(dataset, {})
        domain = dataset_config.get('domain', {'x': [0, 2*np.pi], 'y': [0, 2*np.pi], 'z': [0, 2*np.pi]})
        
        # é‡å° Channel Flow ç”Ÿæˆå…·æœ‰æ­£ç¢ºç‰©ç†ç‰¹å¾µçš„æ•¸æ“š
        if dataset == 'channel':
            return self._generate_channel_flow_points(points, variables, domain)
        else:
            # å°æ–¼å…¶ä»–æ•¸æ“šé›†ï¼Œä½¿ç”¨ç­‰å‘æ€§æ¹æµæ¨¡æ“¬
            return self._generate_isotropic_points(points, variables, domain)
    
    def _generate_channel_flow_points(self, points: np.ndarray, variables: List[str], domain: Dict) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆå…·æœ‰ Channel Flow ç‰¹å¾µçš„æ•£é»æ•¸æ“š"""
        
        # æ¨™æº–åŒ–åº§æ¨™ï¼šy æ–¹å‘ç‚ºå£é¢æ³•å‘ [-1, 1]
        x_norm = 2 * np.pi * (points[:, 0] - domain['x'][0]) / (domain['x'][1] - domain['x'][0])
        y_norm = (points[:, 1] - domain['y'][0]) / (domain['y'][1] - domain['y'][0]) * 2 - 1  # [-1, 1]
        z_norm = 2 * np.pi * (points[:, 2] - domain['z'][0]) / (domain['z'][1] - domain['z'][0])
        
        n_points = len(points)
        data = {}
        
        for var in variables:
            if var == 'u':
                # æµå‘é€Ÿåº¦ï¼šæ‹‹ç‰©å‹å¹³å‡åˆ†å¸ƒ + æ¹æµæ“¾å‹•
                u_mean = 15.0 * (1 - y_norm**2)  # æ‹‹ç‰©å‹åˆ†å¸ƒï¼Œæœ€å¤§å€¼ 15 m/s
                
                # æ¹æµæ“¾å‹•ï¼šè¿‘å£é¢å°ï¼Œä¸­å¿ƒå¤§
                turbulent_intensity = 0.5 * (1 - np.abs(y_norm)**3)
                u_fluctuation = (
                    turbulent_intensity * 3.0 * np.sin(2*x_norm) * np.cos(3*z_norm) +
                    turbulent_intensity * 1.5 * np.sin(4*x_norm) * np.cos(6*z_norm) +
                    turbulent_intensity * 0.3 * np.random.randn(n_points)
                )
                
                data[var] = u_mean + u_fluctuation
                
            elif var == 'v':
                # å£é¢æ³•å‘é€Ÿåº¦ï¼šå¾ˆå°ï¼Œä¸»è¦æ˜¯æ¹æµæ“¾å‹•
                wall_factor = 1 - y_norm**2  # å£é¢è™•ç‚º 0
                
                data[var] = wall_factor * (
                    0.8 * np.cos(x_norm) * np.sin(2*z_norm) +
                    0.4 * np.cos(3*x_norm) * np.sin(4*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
                
            elif var == 'w':
                # å±•å‘é€Ÿåº¦ï¼šä¸­ç­‰å¼·åº¦çš„æ¹æµæ“¾å‹•
                wall_factor = 1 - y_norm**2  # å£é¢è™•ç‚º 0
                
                data[var] = wall_factor * (
                    2.0 * np.sin(x_norm) * np.cos(z_norm) +
                    1.0 * np.sin(3*x_norm) * np.cos(2*z_norm) +
                    0.2 * np.random.randn(n_points)
                )
                
            elif var == 'p':
                # å£“åŠ›å ´ï¼šå—å¹³å‡é€Ÿåº¦æ¢¯åº¦å’Œæ¹æµå½±éŸ¿
                u_val = data.get('u', np.zeros(n_points))
                v_val = data.get('v', np.zeros(n_points))
                w_val = data.get('w', np.zeros(n_points))
                
                # åŸºæ–¼é€£çºŒæ€§æ–¹ç¨‹å’Œå‹•é‡æ–¹ç¨‹çš„å£“åŠ›ä¼°è¨ˆ
                p_mean = -2.0 * y_norm  # ç·šæ€§å£“åŠ›æ¢¯åº¦ï¼ˆé©…å‹•æµå‹•ï¼‰
                p_fluctuation = (
                    -0.3 * (u_val**2 + v_val**2 + w_val**2) +  # å‹•å£“é …
                    5.0 * np.cos(x_norm + z_norm) * (1 - y_norm**2) +  # æ¹æµå£“åŠ›
                    0.1 * np.random.randn(n_points)
                )
                
                data[var] = p_mean + p_fluctuation
        
        return data
    
    def _generate_isotropic_points(self, points: np.ndarray, variables: List[str], domain: Dict) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆç­‰å‘æ€§æ¹æµæ•£é»æ•¸æ“šï¼ˆåŸæœ‰é‚è¼¯ï¼‰"""
        
        # æ¨™æº–åŒ–åº§æ¨™
        x_norm = 2 * np.pi * (points[:, 0] - domain['x'][0]) / (domain['x'][1] - domain['x'][0])
        y_norm = 2 * np.pi * (points[:, 1] - domain['y'][0]) / (domain['y'][1] - domain['y'][0])
        z_norm = 2 * np.pi * (points[:, 2] - domain['z'][0]) / (domain['z'][1] - domain['z'][0])
        
        n_points = len(points)
        data = {}
        
        for var in variables:
            if var == 'u':
                data[var] = (
                    5.0 * np.sin(x_norm) * np.cos(y_norm) * np.sin(z_norm) +
                    2.0 * np.sin(2*x_norm) * np.cos(2*y_norm) * np.sin(2*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
            elif var == 'v':
                data[var] = (
                    3.0 * np.cos(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.5 * np.cos(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
            elif var == 'w':
                data[var] = (
                    2.0 * np.sin(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.0 * np.sin(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
            elif var == 'p':
                u_val = data.get('u', np.zeros(n_points))
                v_val = data.get('v', np.zeros(n_points))
                w_val = data.get('w', np.zeros(n_points))
                data[var] = (
                    -0.5 * (u_val**2 + v_val**2 + w_val**2) +
                    10.0 * np.cos(x_norm + y_norm + z_norm) +
                    0.05 * np.random.randn(n_points)
                )
        
        return data


class HTTPJHTDBClient(BaseJHTDBClient):
    """ä½¿ç”¨ HTTP Web Services API çš„ JHTDB å®¢æˆ¶ç«¯å¯¦ç¾"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # JHTDB Web Services é…ç½®
        self.base_url = "https://turbulence.pha.jhu.edu/service/turbulence.asmx"
        self.test_token = "edu.jhu.pha.turbulence.testing-201406"  # æ¸¬è©¦ç”¨ token (fallback)
        
        # å„ªå…ˆç´šï¼šå‚³å…¥åƒæ•¸ > DEFAULT_AUTH_TOKENï¼ˆæ­£å¼ tokenï¼‰ > test_token
        # çˆ¶é¡å·²ç¶“è™•ç†äº†ï¼šself.auth_token = auth_token or DEFAULT_AUTH_TOKEN
        # é€™è£¡åªéœ€ç¢ºä¿æœ‰ token å³å¯
        if not self.auth_token:
            logger.warning("æœªæä¾› JHTDB èªè­‰ä»¤ç‰Œï¼Œå°‡ä½¿ç”¨æ¸¬è©¦ä»¤ç‰Œ")
            self.auth_token = self.test_token
        
        # Mock fallback å®¢æˆ¶ç«¯ (ç•¶tokenå¤±æ•ˆæ™‚ä½¿ç”¨)
        self.mock_client = None
        self.token_verified = False
        self.use_mock_fallback = False
            
        logger.info(f"HTTPJHTDBClient å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨ token: {self.auth_token[:20]}...")
        logger.info("ğŸ“¡ åŸºæ–¼æœ€æ–°è¨ºæ–·çµæœï¼šä½¿ç”¨ GetAnyCutoutWeb API + 1-based ç´¢å¼•")
    
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """ä½¿ç”¨ HTTP Web Services API ç²å–è³‡æ–™"""
        
        query_type = query_params.get('type', 'cutout')
        variables = query_params.get('variables', ['u', 'v', 'w', 'p'])
        
        try:
            # é¦–æ¬¡å˜—è©¦é©—è­‰ tokenï¼ˆå¦‚æœå°šæœªé©—è­‰ï¼‰
            if not self.token_verified and not self.use_mock_fallback:
                test_success = self._verify_token(dataset)
                if not test_success:
                    logger.warning("Token é©—è­‰å¤±æ•—ï¼Œå•Ÿç”¨ Mock fallback æ©Ÿåˆ¶")
                    self.use_mock_fallback = True
                    self._initialize_mock_client()
                
            # å¦‚æœå•Ÿç”¨äº† Mock fallbackï¼Œä½¿ç”¨ Mock å®¢æˆ¶ç«¯
            if self.use_mock_fallback:
                return self.mock_client._fetch_raw_data(dataset, query_params)
            
            # å¦å‰‡ä½¿ç”¨ HTTP API
            if query_type == 'cutout':
                return self._fetch_cutout_http(dataset, query_params, variables)
            elif query_type == 'points':
                return self._fetch_points_http(dataset, query_params, variables)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹: {query_type}")
                
        except JHTDBError as e:
            # å¦‚æœæ˜¯ token ç›¸é—œéŒ¯èª¤ï¼Œå˜—è©¦å•Ÿç”¨ Mock fallback
            if "Invalid identification token" in str(e):
                logger.warning("Token èªè­‰å¤±æ•—ï¼Œåˆ‡æ›åˆ° Mock fallback æ©Ÿåˆ¶")
                self.use_mock_fallback = True
                self._initialize_mock_client()
                return self.mock_client._fetch_raw_data(dataset, query_params)
            else:
                raise e
        except Exception as e:
            logger.error(f"HTTP API è³‡æ–™ç²å–å¤±æ•—: {e}")
            raise JHTDBError(f"è³‡æ–™ç²å–å¤±æ•—: {e}")
    
    def _verify_token(self, dataset: str) -> bool:
        """é©—è­‰ token æ˜¯å¦æœ‰æ•ˆ"""
        try:
            logger.info("ğŸ”‘ é©—è­‰ JHTDB token...")
            
            # ä½¿ç”¨æœ€å°çš„è«‹æ±‚ä¾†æ¸¬è©¦ token
            test_params = {
                'type': 'points',
                'points': [[1.0, 1.0, 1.0]],
                'timestep': 1,
                'variables': ['u']
            }
            
            # å˜—è©¦é€²è¡Œç°¡å–®çš„ GetVelocity è«‹æ±‚
            self._call_get_velocity(dataset, [[1.0, 1.0, 1.0]], 1)
            
            logger.info("âœ… Token é©—è­‰æˆåŠŸ")
            self.token_verified = True
            return True
            
        except Exception as e:
            if "Invalid identification token" in str(e):
                logger.warning("âŒ Token ç„¡æ•ˆ")
                return False
            else:
                logger.warning(f"âš ï¸ Token é©—è­‰éç¨‹å‡ºéŒ¯ï¼Œä½†ä¸ç¢ºå®šæ˜¯å¦ token å•é¡Œ: {e}")
                return False
    
    def _initialize_mock_client(self):
        """åˆå§‹åŒ– Mock fallback å®¢æˆ¶ç«¯"""
        if self.mock_client is None:
            logger.info("ğŸ­ åˆå§‹åŒ– Mock fallback å®¢æˆ¶ç«¯")
            self.mock_client = MockJHTDBClient(
                auth_token=None,
                cache_dir=self.cache_manager.cache_dir,
                timeout=self.timeout
            )
    
    def _fetch_cutout_http(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ä½¿ç”¨ HTTP API ç²å– cutout è³‡æ–™"""
        
        start_coords = params['start']  # [x, y, z] 
        end_coords = params['end']      # [x, y, z]
        timestep = params.get('timestep', 0)
        
        data = {}
        
        for var in variables:
            logger.info(f"ç²å–è®Šæ•¸ {var} çš„ cutout è³‡æ–™...")
            
            if var in ['u', 'v', 'w']:
                # ä½¿ç”¨ GetAnyCutoutWeb ç²å–é€Ÿåº¦å ´ï¼ˆä¸€æ¬¡æ€§ç²å–æ‰€æœ‰åˆ†é‡ï¼‰
                velocity_data = self._call_get_any_cutout_web(
                    dataset, "velocity", start_coords, end_coords, timestep
                )
                
                # è§£æé€Ÿåº¦åˆ†é‡
                if var == 'u':
                    data[var] = velocity_data[:, :, :, 0]
                elif var == 'v':
                    data[var] = velocity_data[:, :, :, 1]
                elif var == 'w':
                    data[var] = velocity_data[:, :, :, 2]
            
            elif var == 'p':
                # ä½¿ç”¨ GetAnyCutoutWeb ç²å–å£“åŠ›å ´
                data[var] = self._call_get_any_cutout_web(
                    dataset, "pressure", start_coords, end_coords, timestep
                )
        
        return data
    
    def _fetch_points_http(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ä½¿ç”¨ HTTP API ç²å–æ•£é»è³‡æ–™"""
        
        points = params['points']  # [[x1,y1,z1], [x2,y2,z2], ...]
        timestep = params.get('timestep', 0)
        
        data = {}
        
        for var in variables:
            logger.info(f"ç²å–è®Šæ•¸ {var} çš„æ•£é»è³‡æ–™...")
            
            if var in ['u', 'v', 'w']:
                # ä½¿ç”¨ GetVelocity ç²å–é€Ÿåº¦å ´æ’å€¼
                velocity_data = self._call_get_velocity(
                    dataset, points, timestep
                )
                
                if var == 'u':
                    data[var] = velocity_data[:, 0]
                elif var == 'v':
                    data[var] = velocity_data[:, 1]
                elif var == 'w':
                    data[var] = velocity_data[:, 2]
            
            elif var == 'p':
                # ä½¿ç”¨ GetPressure ç²å–å£“åŠ›å ´æ’å€¼
                data[var] = self._call_get_pressure(
                    dataset, points, timestep
                )
        
        return data
    
    def _call_get_any_cutout_web(self, dataset: str, field: str, 
                                 start: List[float], end: List[float], 
                                 timestep: int) -> np.ndarray:
        """èª¿ç”¨ GetAnyCutoutWeb APIï¼ˆæ›¿ä»£å·²æ£„ç”¨çš„ GetRawVelocityï¼‰"""
        
        # å°‡ç‰©ç†åº§æ¨™è½‰æ›ç‚º 1-based ç¶²æ ¼ç´¢å¼•
        # æ³¨æ„ï¼šJHTDB å¾ 2023å¹´9æœˆ16æ—¥èµ·ä½¿ç”¨ 1-based ç´¢å¼•
        start_int = [max(1, int(s) + 1) for s in start]
        end_int = [max(1, int(e) + 1) for e in end]
        
        # æ§‹å»º SOAP è«‹æ±‚ - ä½¿ç”¨æ­£ç¢ºçš„ GetAnyCutoutWeb API æ ¼å¼
        soap_request = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
               xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
               xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <GetAnyCutoutWeb xmlns="http://turbulence.pha.jhu.edu/">
      <authToken>{self.auth_token}</authToken>
      <dataset>{dataset}</dataset>
      <field>{field}</field>
      <T>{timestep}</T>
      <x_start>{start_int[0]}</x_start>
      <y_start>{start_int[1]}</y_start>
      <z_start>{start_int[2]}</z_start>
      <x_end>{end_int[0]}</x_end>
      <y_end>{end_int[1]}</y_end>
      <z_end>{end_int[2]}</z_end>
      <x_step>1</x_step>
      <y_step>1</y_step>
      <z_step>1</z_step>
      <filter_width>1</filter_width>
      <addr></addr>
    </GetAnyCutoutWeb>
  </soap:Body>
</soap:Envelope>"""
        
        response_data = self._send_soap_request(soap_request, "GetAnyCutoutWeb")
        
        # è¨ˆç®—å¯¦éš›çš„ç¶²æ ¼å°ºå¯¸
        width = [end_int[0] - start_int[0] + 1, 
                 end_int[1] - start_int[1] + 1, 
                 end_int[2] - start_int[2] + 1]
        
        if field == "velocity":
            return self._parse_velocity_response(response_data, width)
        elif field == "pressure":
            return self._parse_pressure_response(response_data, width)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å ´é¡å‹: {field}")
    
    def _call_get_raw_velocity(self, dataset: str, start: List[float], 
                              width: List[int], timestep: int) -> np.ndarray:
        """èª¿ç”¨ GetRawVelocity API"""
        
        # å°‡åº§æ¨™è½‰æ›ç‚ºæ•´æ•¸ç¶²æ ¼ç´¢å¼• (JHTDB ä½¿ç”¨ç¶²æ ¼ç´¢å¼•ï¼Œä¸æ˜¯ç‰©ç†åº§æ¨™)
        start_int = [int(s) for s in start]
        
        # æ§‹å»º SOAP è«‹æ±‚ - ä½¿ç”¨æ­£ç¢ºçš„ JHTDB API æ ¼å¼
        soap_request = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
               xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
               xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <GetRawVelocity xmlns="http://turbulence.pha.jhu.edu/">
      <authToken>{self.auth_token}</authToken>
      <dataset>{dataset}</dataset>
      <T>{timestep}</T>
      <X>{start_int[0]}</X>
      <Y>{start_int[1]}</Y>
      <Z>{start_int[2]}</Z>
      <Xwidth>{width[0]}</Xwidth>
      <Ywidth>{width[1]}</Ywidth>
      <Zwidth>{width[2]}</Zwidth>
    </GetRawVelocity>
  </soap:Body>
</soap:Envelope>"""
        
        response_data = self._send_soap_request(soap_request, "GetRawVelocity")
        return self._parse_velocity_response(response_data, width)
    
    def _call_get_velocity(self, dataset: str, points: List[List[float]], 
                          timestep: int) -> np.ndarray:
        """èª¿ç”¨ GetVelocity APIï¼ˆæ•£é»æ’å€¼ï¼‰"""
        
        # æ§‹å»ºé»çš„ XML æ ¼å¼ (ä¸æ˜¯äºŒé€²åˆ¶ç·¨ç¢¼)
        points_xml = ""
        for point in points:
            points_xml += f"""
        <Point3>
          <x>{point[0]}</x>
          <y>{point[1]}</y>
          <z>{point[2]}</z>
        </Point3>"""
        
        # æ§‹å»º SOAP è«‹æ±‚ - ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸æ ¼å¼
        soap_request = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
               xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
               xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <GetVelocity xmlns="http://turbulence.pha.jhu.edu/">
      <authToken>{self.auth_token}</authToken>
      <dataset>{dataset}</dataset>
      <time>{float(timestep)}</time>
      <spatialInterpolation>Lag4</spatialInterpolation>
      <temporalInterpolation>None</temporalInterpolation>
      <points>{points_xml}
      </points>
    </GetVelocity>
  </soap:Body>
</soap:Envelope>"""
        
        response_data = self._send_soap_request(soap_request, "GetVelocity")
        return self._parse_velocity_points_response(response_data, len(points))
    
    def _call_get_raw_pressure(self, dataset: str, start: List[float], 
                              width: List[int], timestep: int) -> np.ndarray:
        """èª¿ç”¨ GetRawPressure API"""
        
        # å°‡åº§æ¨™è½‰æ›ç‚ºæ•´æ•¸ç¶²æ ¼ç´¢å¼• (JHTDB ä½¿ç”¨ç¶²æ ¼ç´¢å¼•ï¼Œä¸æ˜¯ç‰©ç†åº§æ¨™)
        start_int = [int(s) for s in start]
        
        # æ§‹å»º SOAP è«‹æ±‚ - ä½¿ç”¨æ­£ç¢ºçš„ JHTDB API æ ¼å¼
        soap_request = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
               xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
               xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <GetRawPressure xmlns="http://turbulence.pha.jhu.edu/">
      <authToken>{self.auth_token}</authToken>
      <dataset>{dataset}</dataset>
      <T>{timestep}</T>
      <X>{start_int[0]}</X>
      <Y>{start_int[1]}</Y>
      <Z>{start_int[2]}</Z>
      <Xwidth>{width[0]}</Xwidth>
      <Ywidth>{width[1]}</Ywidth>
      <Zwidth>{width[2]}</Zwidth>
    </GetRawPressure>
  </soap:Body>
</soap:Envelope>"""
        
        response_data = self._send_soap_request(soap_request, "GetRawPressure")
        return self._parse_pressure_response(response_data, width)
    
    def _call_get_pressure(self, dataset: str, points: List[List[float]], 
                          timestep: int) -> np.ndarray:
        """èª¿ç”¨ GetPressure APIï¼ˆæ•£é»æ’å€¼ï¼‰"""
        
        points_binary = self._encode_points(points)
        
        soap_request = f"""<?xml version="1.0" encoding="utf-8"?>
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
               xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
               xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <GetPressure xmlns="http://turbulence.pha.jhu.edu/">
      <authToken>{self.auth_token}</authToken>
      <dataset>{dataset}</dataset>
      <time>{timestep}</time>
      <spatialInterpolation>6</spatialInterpolation>
      <temporalInterpolation>0</temporalInterpolation>
       <points>{points_binary}</points>
     </GetPressure>
   </soap:Body>
</soap:Envelope>"""
        
        response_data = self._send_soap_request(soap_request, "GetPressure")
        return self._parse_pressure_points_response(response_data, len(points))
    
    def _send_soap_request(self, soap_request: str, api_method: str) -> bytes:
        """ç™¼é€ SOAP è«‹æ±‚åˆ° JHTDB æœå‹™å™¨"""
        
        # ASMX Web Service ä½¿ç”¨çµ±ä¸€çš„ç«¯é» URL
        url = self.base_url
        
        headers = {
            'Content-Type': 'text/xml; charset=utf-8',
            'SOAPAction': f'http://turbulence.pha.jhu.edu/{api_method}',
            'User-Agent': 'Python JHTDB Client'
        }
        
        # ç·¨ç¢¼è«‹æ±‚
        request_data = soap_request.encode('utf-8')
        
        # å‰µå»ºè«‹æ±‚
        req = urllib.request.Request(
            url, 
            data=request_data, 
            headers=headers
        )
        
        # ç™¼é€è«‹æ±‚ä¸¦è™•ç†é‡è©¦
        for attempt in range(JHTDBConfig.MAX_RETRY):
            try:
                logger.debug(f"ç™¼é€ SOAP è«‹æ±‚ï¼ˆå˜—è©¦ {attempt + 1}/{JHTDBConfig.MAX_RETRY}ï¼‰")
                
                with urllib.request.urlopen(req, timeout=self.timeout) as response:
                    if response.status == 200:
                        response_data = response.read()
                        logger.debug(f"è«‹æ±‚æˆåŠŸï¼ŒéŸ¿æ‡‰å¤§å°: {len(response_data)} bytes")
                        return self._extract_binary_data(response_data)
                    else:
                        raise JHTDBError(f"HTTP éŒ¯èª¤: {response.status}")
                        
            except urllib.error.URLError as e:
                logger.warning(f"è«‹æ±‚å¤±æ•—ï¼ˆå˜—è©¦ {attempt + 1}ï¼‰: {e}")
                if attempt == JHTDBConfig.MAX_RETRY - 1:
                    raise JHTDBError(f"æ‰€æœ‰é‡è©¦å‡å¤±æ•—: {e}")
                time.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
            
            except Exception as e:
                logger.error(f"æœªé æœŸçš„éŒ¯èª¤: {e}")
                if attempt == JHTDBConfig.MAX_RETRY - 1:
                    raise JHTDBError(f"è«‹æ±‚è™•ç†å¤±æ•—: {e}")
                time.sleep(2 ** attempt)
        
        # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œæ‹‹å‡ºéŒ¯èª¤
        raise JHTDBError("æ‰€æœ‰é€£æ¥å˜—è©¦å‡å¤±æ•—")
    
    def _extract_binary_data(self, response_data: bytes) -> bytes:
        """å¾ SOAP éŸ¿æ‡‰ä¸­æå– Base64 ç·¨ç¢¼çš„äºŒé€²åˆ¶æ•¸æ“š"""
        
        try:
            # è§£æ XML éŸ¿æ‡‰
            response_str = response_data.decode('utf-8')
            root = ET.fromstring(response_str)
            
            # å°‹æ‰¾åŒ…å« Base64 æ•¸æ“šçš„å…ƒç´ 
            # JHTDB é€šå¸¸åœ¨ <soap:Body> çš„çµæœå…ƒç´ ä¸­è¿”å› Base64 æ•¸æ“š
            namespaces = {
                'soap': 'http://schemas.xmlsoap.org/soap/envelope/',
                'jhtdb': 'http://turbulence.pha.jhu.edu/'
            }
            
            # å°‹æ‰¾çµæœå…ƒç´ ï¼ˆå¯èƒ½æ˜¯ä¸åŒçš„åç¨±ï¼‰
            result_elements = root.findall('.//soap:Body/*/*', namespaces)
            
            if not result_elements:
                # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦æ²’æœ‰å‘½åç©ºé–“çš„æŸ¥æ‰¾
                result_elements = root.findall('.//Body/*/*')
            
            if result_elements:
                base64_data = result_elements[0].text
                if base64_data:
                    return base64.b64decode(base64_data)
                else:
                    logger.error("Base64 æ•¸æ“šç‚ºç©º")
                    raise JHTDBError("éŸ¿æ‡‰ä¸­çš„æ•¸æ“šç‚ºç©º")
            
            # å¦‚æœä»ç„¶æ²’æœ‰æ‰¾åˆ°ï¼Œè¨˜éŒ„éŸ¿æ‡‰å…§å®¹ç”¨æ–¼èª¿è©¦
            logger.error(f"ç„¡æ³•å¾ SOAP éŸ¿æ‡‰ä¸­æå–æ•¸æ“š")
            logger.debug(f"éŸ¿æ‡‰å…§å®¹ï¼ˆå‰1000å­—ç¬¦ï¼‰: {response_str[:1000]}")
            raise JHTDBError("ç„¡æ³•è§£æ SOAP éŸ¿æ‡‰")
            
        except ET.ParseError as e:
            logger.error(f"XML è§£æå¤±æ•—: {e}")
            raise JHTDBError(f"éŸ¿æ‡‰æ ¼å¼éŒ¯èª¤: {e}")
        except Exception as e:
            logger.error(f"æ•¸æ“šæå–å¤±æ•—: {e}")
            raise JHTDBError(f"éŸ¿æ‡‰è™•ç†å¤±æ•—: {e}")
    
    def _encode_points(self, points: List[List[float]]) -> str:
        """å°‡é»åº§æ¨™ç·¨ç¢¼ç‚º Base64 äºŒé€²åˆ¶æ ¼å¼"""
        
        # JHTDB æœŸæœ›çš„æ ¼å¼æ˜¯ï¼šfloat32 é™£åˆ—ï¼Œæ¯å€‹é» 3 å€‹åº§æ¨™ (x, y, z)
        points_array = np.array(points, dtype=np.float32)
        
        # è½‰æ›ç‚ºäºŒé€²åˆ¶
        binary_data = points_array.tobytes()
        
        # Base64 ç·¨ç¢¼
        return base64.b64encode(binary_data).decode('ascii')
    
    def _parse_velocity_response(self, binary_data: bytes, width: List[int]) -> np.ndarray:
        """è§£æé€Ÿåº¦å ´ cutout éŸ¿æ‡‰"""
        
        # JHTDB velocity æ•¸æ“šæ ¼å¼: float32, [width[0], width[1], width[2], 3]
        expected_size = width[0] * width[1] * width[2] * 3 * 4  # 4 bytes per float32
        
        if len(binary_data) != expected_size:
            logger.warning(f"æ•¸æ“šå¤§å°ä¸ç¬¦ï¼šæœŸæœ› {expected_size}, å¯¦éš› {len(binary_data)}")
        
        # è§£æç‚º float32 é™£åˆ—
        data_array = np.frombuffer(binary_data, dtype=np.float32)
        
        # é‡å¡‘ç‚º [width[0], width[1], width[2], 3] å½¢ç‹€
        return data_array.reshape(width[0], width[1], width[2], 3)
    
    def _parse_pressure_response(self, binary_data: bytes, width: List[int]) -> np.ndarray:
        """è§£æå£“åŠ›å ´ cutout éŸ¿æ‡‰"""
        
        # JHTDB pressure æ•¸æ“šæ ¼å¼: float32, [width[0], width[1], width[2]]
        expected_size = width[0] * width[1] * width[2] * 4  # 4 bytes per float32
        
        if len(binary_data) != expected_size:
            logger.warning(f"æ•¸æ“šå¤§å°ä¸ç¬¦ï¼šæœŸæœ› {expected_size}, å¯¦éš› {len(binary_data)}")
        
        # è§£æç‚º float32 é™£åˆ—
        data_array = np.frombuffer(binary_data, dtype=np.float32)
        
        # é‡å¡‘ç‚º [width[0], width[1], width[2]] å½¢ç‹€
        return data_array.reshape(width[0], width[1], width[2])
    
    def _parse_velocity_points_response(self, response_data: bytes, n_points: int) -> np.ndarray:
        """è§£æ GetVelocity çš„ XML éŸ¿æ‡‰ï¼ŒåŒ…å« Vector3 æ•¸çµ„"""
        
        try:
            # è§£æ XML éŸ¿æ‡‰
            response_str = response_data.decode('utf-8')
            root = ET.fromstring(response_str)
            
            # å°‹æ‰¾ GetVelocityResult å…ƒç´ 
            namespaces = {
                'soap': 'http://schemas.xmlsoap.org/soap/envelope/',
                'jhtdb': 'http://turbulence.pha.jhu.edu/'
            }
            
            # æŸ¥æ‰¾çµæœå…ƒç´ 
            result_elem = root.find('.//jhtdb:GetVelocityResult', namespaces)
            if result_elem is None:
                # å˜—è©¦ä¸ä½¿ç”¨å‘½åç©ºé–“
                result_elem = root.find('.//GetVelocityResult')
            
            if result_elem is None:
                logger.error("ç„¡æ³•æ‰¾åˆ° GetVelocityResult å…ƒç´ ")
                logger.debug(f"éŸ¿æ‡‰å…§å®¹: {response_str[:1000]}")
                raise JHTDBError("éŸ¿æ‡‰æ ¼å¼éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°çµæœå…ƒç´ ")
            
            # è§£æ Vector3 å…ƒç´ 
            vectors = []
            vector_elems = result_elem.findall('.//Vector3') or result_elem.findall('.//jhtdb:Vector3', namespaces)
            
            for vector_elem in vector_elems:
                x = float(vector_elem.find('x').text or vector_elem.find('jhtdb:x', namespaces).text)
                y = float(vector_elem.find('y').text or vector_elem.find('jhtdb:y', namespaces).text)
                z = float(vector_elem.find('z').text or vector_elem.find('jhtdb:z', namespaces).text)
                vectors.append([x, y, z])
            
            if len(vectors) != n_points:
                logger.warning(f"è¿”å›çš„é»æ•¸ä¸ç¬¦ï¼šæœŸæœ› {n_points}, å¯¦éš› {len(vectors)}")
            
            return np.array(vectors, dtype=np.float32)
            
        except Exception as e:
            logger.error(f"è§£æ Vector3 éŸ¿æ‡‰å¤±æ•—: {e}")
            # å›é€€åˆ°äºŒé€²åˆ¶è§£æï¼ˆé©ç”¨æ–¼èˆŠç‰ˆæœ¬æˆ–ä¸åŒæ ¼å¼ï¼‰
            try:
                binary_data = self._extract_binary_data(response_data)
                expected_size = n_points * 3 * 4  # 4 bytes per float32
                
                if len(binary_data) != expected_size:
                    logger.warning(f"æ•¸æ“šå¤§å°ä¸ç¬¦ï¼šæœŸæœ› {expected_size}, å¯¦éš› {len(binary_data)}")
                
                # è§£æç‚º float32 é™£åˆ—
                data_array = np.frombuffer(binary_data, dtype=np.float32)
                
                # é‡å¡‘ç‚º [n_points, 3] å½¢ç‹€
                return data_array.reshape(n_points, 3)
            except Exception as e2:
                logger.error(f"äºŒé€²åˆ¶å›é€€è§£æä¹Ÿå¤±æ•—: {e2}")
                raise JHTDBError(f"ç„¡æ³•è§£æéŸ¿æ‡‰æ•¸æ“š: {e}")
    
    def _parse_pressure_points_response(self, binary_data: bytes, n_points: int) -> np.ndarray:
        """è§£æå£“åŠ›å ´æ•£é»éŸ¿æ‡‰"""
        
        # JHTDB pressure points æ•¸æ“šæ ¼å¼: float32, [n_points]
        expected_size = n_points * 4  # 4 bytes per float32
        
        if len(binary_data) != expected_size:
            logger.warning(f"æ•¸æ“šå¤§å°ä¸ç¬¦ï¼šæœŸæœ› {expected_size}, å¯¦éš› {len(binary_data)}")
        
        # è§£æç‚º float32 é™£åˆ—
        data_array = np.frombuffer(binary_data, dtype=np.float32)
        
        # è¿”å›ä¸€ç¶­é™£åˆ—
        return data_array


class JHTDBManager:
    """JHTDB ç®¡ç†å™¨ï¼šæä¾›é«˜å±¤ç´šçš„è³‡æ–™å­˜å–æ¥å£"""
    
    def __init__(self, 
                 use_mock: bool = False,
                 use_http: bool = True,
                 auth_token: Optional[str] = None,
                 cache_dir: Optional[str] = None,
                 **kwargs):
        """
        Args:
            use_mock: æ˜¯å¦å¼·åˆ¶ä½¿ç”¨æ¨¡æ“¬å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼é›¢ç·šé–‹ç™¼ï¼‰
            use_http: æ˜¯å¦å„ªå…ˆä½¿ç”¨ HTTP å®¢æˆ¶ç«¯ï¼ˆé è¨­ï¼‰
            auth_token: JHTDB èªè­‰ä»¤ç‰Œ
            cache_dir: å¿«å–ç›®éŒ„
        """
        
        # å®¢æˆ¶ç«¯é¸æ“‡é‚è¼¯ï¼š
        # 1. å¦‚æœ use_mock=Trueï¼Œå¼·åˆ¶ä½¿ç”¨ MockJHTDBClient
        # 2. å¦‚æœ use_http=Trueï¼ˆé è¨­ï¼‰ï¼Œå„ªå…ˆä½¿ç”¨ HTTPJHTDBClient
        # 3. å¦‚æœ pyJHTDB å¯ç”¨ä¸” use_http=Falseï¼Œä½¿ç”¨ PyJHTDBClient
        # 4. æœ€å¾Œé€€å›åˆ° MockJHTDBClient
        
        if use_mock:
            logger.info("ä½¿ç”¨è€…æŒ‡å®šæ¨¡æ“¬å®¢æˆ¶ç«¯")
            self.client = MockJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
            self.client_type = "mock"
            
        elif use_http:
            logger.info("ä½¿ç”¨ HTTP Web Services å®¢æˆ¶ç«¯")
            try:
                self.client = HTTPJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                self.client_type = "http"
            except Exception as e:
                logger.warning(f"HTTP å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
                logger.info("é€€å›åˆ°æ¨¡æ“¬å®¢æˆ¶ç«¯")
                self.client = MockJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                self.client_type = "mock"
                
        elif PYJHTDB_AVAILABLE:
            logger.info("ä½¿ç”¨ pyJHTDB å®¢æˆ¶ç«¯")
            try:
                self.client = PyJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                self.client_type = "pyjhtdb"
            except Exception as e:
                logger.warning(f"pyJHTDB å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
                logger.info("é€€å›åˆ° HTTP å®¢æˆ¶ç«¯")
                try:
                    self.client = HTTPJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                    self.client_type = "http"
                except Exception as e2:
                    logger.warning(f"HTTP å®¢æˆ¶ç«¯ä¹Ÿå¤±æ•—: {e2}")
                    logger.info("æœ€çµ‚é€€å›åˆ°æ¨¡æ“¬å®¢æˆ¶ç«¯")
                    self.client = MockJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                    self.client_type = "mock"
                    
        else:
            logger.warning("pyJHTDB ä¸å¯ç”¨ï¼Œå˜—è©¦ HTTP å®¢æˆ¶ç«¯")
            try:
                self.client = HTTPJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                self.client_type = "http"
            except Exception as e:
                logger.warning(f"HTTP å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
                logger.info("é€€å›åˆ°æ¨¡æ“¬å®¢æˆ¶ç«¯")
                self.client = MockJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
                self.client_type = "mock"
        
        logger.info(f"JHTDB å®¢æˆ¶ç«¯é¡å‹: {self.client_type}")
        self.datasets = JHTDBConfig.DATASETS
    
    def fetch_cutout(self,
                    dataset: str,
                    start: List[float],
                    end: List[float],
                    timestep: int = 0,
                    variables: List[str] = None,
                    resolution: List[int] = None,
                    **kwargs) -> Dict[str, Any]:
        """
        ç²å– cutout è³‡æ–™
        
        Args:
            dataset: è³‡æ–™é›†åç¨±
            start: èµ·å§‹åº§æ¨™ [x, y, z]
            end: çµæŸåº§æ¨™ [x, y, z]
            timestep: æ™‚é–“æ­¥
            variables: è®Šæ•¸åˆ—è¡¨
            resolution: è§£æåº¦ [nx, ny, nz] (åƒ…ç”¨æ–¼æ¨¡æ“¬è³‡æ–™)
            
        Returns:
            åŒ…å«è³‡æ–™å’Œå…ƒè³‡æ–™çš„å­—å…¸
        """
        
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥è³‡æ–™é›†: {dataset}")
        
        variables = variables or ['u', 'v', 'w', 'p']
        
        query_params = {
            'type': 'cutout',
            'start': start,
            'end': end,
            'timestep': timestep,
            'variables': variables
        }
        
        if resolution:
            query_params['resolution'] = resolution
        
        return self.client.fetch_data(dataset, query_params, **kwargs)
    
    def fetch_points(self,
                    dataset: str,
                    points: List[List[float]],
                    timestep: int = 0,
                    variables: List[str] = None,
                    **kwargs) -> Dict[str, Any]:
        """
        ç²å–æ•£é»è³‡æ–™
        
        Args:
            dataset: è³‡æ–™é›†åç¨±
            points: åº§æ¨™é»åˆ—è¡¨ [[x1,y1,z1], [x2,y2,z2], ...]
            timestep: æ™‚é–“æ­¥
            variables: è®Šæ•¸åˆ—è¡¨
            
        Returns:
            åŒ…å«è³‡æ–™å’Œå…ƒè³‡æ–™çš„å­—å…¸
        """
        
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥è³‡æ–™é›†: {dataset}")
        
        variables = variables or ['u', 'v', 'w', 'p']
        
        query_params = {
            'type': 'points',
            'points': points,
            'timestep': timestep,
            'variables': variables
        }
        
        return self.client.fetch_data(dataset, query_params, **kwargs)
    
    def get_dataset_info(self, dataset: str) -> Dict:
        """ç²å–è³‡æ–™é›†è³‡è¨Š"""
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥è³‡æ–™é›†: {dataset}")
        return self.datasets[dataset].copy()
    
    def list_datasets(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨è³‡æ–™é›†"""
        return list(self.datasets.keys())
    
    def clear_cache(self, older_than_days: int = 30):
        """æ¸…ç†å¿«å–"""
        self.client.cache_manager.clear_cache(older_than_days)


# ä¾¿æ·å‡½æ•¸
def create_jhtdb_manager(use_mock: bool = False, **kwargs) -> JHTDBManager:
    """å‰µå»º JHTDB ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•¸"""
    return JHTDBManager(use_mock=use_mock, **kwargs)


def fetch_sample_data(dataset: str = 'isotropic1024coarse',
                     n_points: int = 100,
                     use_mock: bool = True) -> Dict[str, Any]:
    """
    ç²å–æ¨£æœ¬è³‡æ–™çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        dataset: è³‡æ–™é›†åç¨±
        n_points: æ¨£æœ¬é»æ•¸
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ“¬è³‡æ–™
        
    Returns:
        æ¨£æœ¬è³‡æ–™å­—å…¸
    """
    
    manager = create_jhtdb_manager(use_mock=use_mock)
    dataset_info = manager.get_dataset_info(dataset)
    
    # åœ¨è³‡æ–™é›†åŸŸå…§ç”Ÿæˆéš¨æ©Ÿé»
    domain = dataset_info['domain']
    np.random.seed(42)  # å›ºå®šç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
    
    points = []
    for _ in range(n_points):
        x = np.random.uniform(domain['x'][0], domain['x'][1])
        y = np.random.uniform(domain['y'][0], domain['y'][1])
        z = np.random.uniform(domain['z'][0], domain['z'][1])
        points.append([x, y, z])
    
    return manager.fetch_points(dataset, points, timestep=0)


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    print("ğŸŒŠ æ¸¬è©¦ JHTDB å®¢æˆ¶ç«¯...")
    
    # æ¸¬è©¦æ¨¡æ“¬å®¢æˆ¶ç«¯
    print("\n=== æ¸¬è©¦æ¨¡æ“¬å®¢æˆ¶ç«¯ ===")
    
    manager = create_jhtdb_manager(use_mock=True)
    
    # åˆ—å‡ºè³‡æ–™é›†
    print(f"å¯ç”¨è³‡æ–™é›†: {manager.list_datasets()}")
    
    # ç²å–è³‡æ–™é›†è³‡è¨Š
    dataset = 'isotropic1024coarse'
    info = manager.get_dataset_info(dataset)
    print(f"\nè³‡æ–™é›† {dataset} è³‡è¨Š:")
    print(f"  æè¿°: {info['description']}")
    print(f"  åŸŸç¯„åœ: {info['domain']}")
    print(f"  è§£æåº¦: {info['resolution']}")
    
    # æ¸¬è©¦ cutout è³‡æ–™
    print(f"\næ¸¬è©¦ cutout è³‡æ–™...")
    cutout_result = manager.fetch_cutout(
        dataset=dataset,
        start=[0.0, 0.0, 0.0],
        end=[1.0, 1.0, 1.0],
        resolution=[32, 32, 32],
        variables=['u', 'v', 'p']
    )
    
    print(f"Cutout è³‡æ–™ç²å–æˆåŠŸ: {not cutout_result['from_cache']}")
    data = cutout_result['data']
    for var, arr in data.items():
        print(f"  {var}: {arr.shape}, ç¯„åœ=[{arr.min():.3f}, {arr.max():.3f}]")
    
    # æ¸¬è©¦æ•£é»è³‡æ–™
    print(f"\næ¸¬è©¦æ•£é»è³‡æ–™...")
    points = [[0.5, 0.5, 0.5], [1.0, 1.0, 1.0], [1.5, 1.5, 1.5]]
    points_result = manager.fetch_points(
        dataset=dataset,
        points=points,
        variables=['u', 'v', 'w', 'p']
    )
    
    print(f"æ•£é»è³‡æ–™ç²å–æˆåŠŸ: {not points_result['from_cache']}")
    data = points_result['data']
    for var, arr in data.items():
        print(f"  {var}: {arr.shape}, å€¼={arr}")
    
    # æ¸¬è©¦å¿«å–åŠŸèƒ½
    print(f"\næ¸¬è©¦å¿«å–åŠŸèƒ½...")
    cutout_result_cached = manager.fetch_cutout(
        dataset=dataset,
        start=[0.0, 0.0, 0.0],
        end=[1.0, 1.0, 1.0],
        resolution=[32, 32, 32],
        variables=['u', 'v', 'p']
    )
    print(f"ä½¿ç”¨å¿«å–: {cutout_result_cached['from_cache']}")
    
    # æ¸¬è©¦é©—è­‰åŠŸèƒ½
    if 'validation' in cutout_result:
        print(f"\nè³‡æ–™é©—è­‰çµæœ:")
        for var, report in cutout_result['validation'].items():
            status = "âœ…" if report['valid'] else "âŒ"
            print(f"  {var}: {status} (è­¦å‘Š: {len(report['warnings'])}, éŒ¯èª¤: {len(report['errors'])})")
    
    # æ¸¬è©¦ä¾¿æ·å‡½æ•¸
    print(f"\næ¸¬è©¦ä¾¿æ·å‡½æ•¸...")
    sample_data = fetch_sample_data(dataset='channel', n_points=10, use_mock=True)
    print(f"æ¨£æœ¬è³‡æ–™ç²å–æˆåŠŸ: {len(sample_data['data'])} å€‹è®Šæ•¸")
    
    print("\nâœ… JHTDB å®¢æˆ¶ç«¯æ¸¬è©¦å®Œæˆï¼")