"""
ç´„ç¿°éœæ™®é‡‘æ–¯æ¹æµè³‡æ–™åº« (JHTDB) å®¢æˆ¶ç«¯

æä¾›é«˜æ•ˆã€å¯é‡ç¾çš„ JHTDB è³‡æ–™å­˜å–æ¥å£ï¼Œæ”¯æ´ï¼š
- Cutout æ‰¹é‡è³‡æ–™ä¸‹è¼‰
- æ•£é»æ’å€¼å–æ¨£
- è‡ªå‹•å¿«å–èˆ‡ç‰ˆæœ¬ç®¡ç†
- å¤šç¨®æ¹æµè³‡æ–™é›† (é€šé“æµã€HITã€é‚Šç•Œå±¤ç­‰)
- è³‡æ–™é©—è­‰èˆ‡å“è³ªæª¢æŸ¥

åƒè€ƒæ–‡ç»ï¼š
- JHTDB Official Documentation: https://turbulence.pha.jhu.edu/
- pyJHTDB: Python interface for JHTDB
- SciServer: Cloud-based data access platform

æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ï¼š
1. å¯é‡ç¾æ€§ï¼šå›ºå®šç¨®å­ã€ç‰ˆæœ¬è¨˜éŒ„ã€è³‡æ–™æ ¡é©—
2. æ•ˆç‡ï¼šæ™ºèƒ½å¿«å–ã€æ‰¹é‡ä¸‹è¼‰ã€å¢é‡æ›´æ–°
3. ç©©å¥æ€§ï¼šéŒ¯èª¤è™•ç†ã€é‡è©¦æ©Ÿåˆ¶ã€å‚™ç”¨ç«¯é»
4. æ¨™æº–åŒ–ï¼šçµ±ä¸€è³‡æ–™æ ¼å¼ã€åº§æ¨™ç³»çµ±ã€å‘½åè¦ç¯„
"""

import numpy as np
import h5py
import os
import json
import hashlib
import time
import logging
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from pathlib import Path
import urllib.request
import urllib.error
from abc import ABC, abstractmethod

try:
    import pyJHTDB
    PYJHTDB_AVAILABLE = True
except ImportError:
    PYJHTDB_AVAILABLE = False
    pyJHTDB = None

logger = logging.getLogger(__name__)


class JHTDBConfig:
    """JHTDB é€£æ¥é…ç½®"""
    
    # è³‡æ–™é›†é…ç½®
    DATASETS = {
        'channel': {
            'name': 'channel',
            'description': 'é€šé“æµ (Re_tau=1000)',
            'domain': {'x': [0, 8*np.pi], 'y': [-1, 1], 'z': [0, 3*np.pi]},
            'resolution': {'x': 2048, 'y': 512, 'z': 1536},
            'time_range': [0.0, 26.0],
            'dt': 0.0065,
            'variables': ['u', 'v', 'w', 'p']
        },
        'isotropic1024coarse': {
            'name': 'isotropic1024coarse',
            'description': 'å„å‘åŒæ€§æ¹æµ (1024^3, ç²—æ™‚é–“è§£æåº¦)',
            'domain': {'x': [0, 2*np.pi], 'y': [0, 2*np.pi], 'z': [0, 2*np.pi]},
            'resolution': {'x': 1024, 'y': 1024, 'z': 1024},
            'time_range': [0.0, 10.0],
            'dt': 0.04,
            'variables': ['u', 'v', 'w', 'p']
        },
        'transition_bl': {
            'name': 'transition_bl',
            'description': 'é‚Šç•Œå±¤è½‰æ©',
            'domain': {'x': [0, 4000], 'y': [0, 120], 'z': [0, 300]},
            'resolution': {'x': 4000, 'y': 120, 'z': 300},
            'time_range': [0.0, 100.0],
            'dt': 0.5,
            'variables': ['u', 'v', 'w', 'p']
        }
    }
    
    # é è¨­é€£æ¥åƒæ•¸
    DEFAULT_AUTH_TOKEN = None  # éœ€è¦åœ¨ JHTDB è¨»å†Šç²å–
    DEFAULT_CACHE_DIR = "data/jhtdb"
    DEFAULT_TIMEOUT = 300  # 5 åˆ†é˜
    MAX_RETRY = 3
    
    # è³‡æ–™é©—è­‰é–¾å€¼
    VALIDATION_THRESHOLDS = {
        'velocity_magnitude_max': 100.0,  # m/s
        'pressure_range': [-1000.0, 1000.0],  # Pa
        'nan_fraction_max': 0.01,  # æœ€å¤§ NaN æ¯”ä¾‹
        'inf_fraction_max': 0.001  # æœ€å¤§ Inf æ¯”ä¾‹
    }


class JHTDBError(Exception):
    """JHTDB ç‰¹å®šéŒ¯èª¤"""
    pass


class DataValidator:
    """è³‡æ–™é©—è­‰å™¨"""
    
    @staticmethod
    def validate_field(data: np.ndarray, 
                      field_name: str, 
                      thresholds: Dict[str, float]) -> Dict[str, Any]:
        """
        é©—è­‰æµå ´è³‡æ–™çš„ç‰©ç†åˆç†æ€§
        
        Args:
            data: è³‡æ–™é™£åˆ—
            field_name: æ¬„ä½åç¨± ('u', 'v', 'w', 'p')
            thresholds: é©—è­‰é–¾å€¼
            
        Returns:
            é©—è­‰å ±å‘Šå­—å…¸
        """
        report = {
            'field': field_name,
            'shape': data.shape,
            'dtype': str(data.dtype),
            'valid': True,
            'warnings': [],
            'errors': []
        }
        
        # åŸºæœ¬çµ±è¨ˆ
        if data.size > 0:
            report['stats'] = {
                'mean': float(np.mean(data)),
                'std': float(np.std(data)),
                'min': float(np.min(data)),
                'max': float(np.max(data)),
                'nan_count': int(np.sum(np.isnan(data))),
                'inf_count': int(np.sum(np.isinf(data)))
            }
            
            # NaN/Inf æª¢æŸ¥
            nan_fraction = report['stats']['nan_count'] / data.size
            inf_fraction = report['stats']['inf_count'] / data.size
            
            if nan_fraction > thresholds.get('nan_fraction_max', 0.01):
                report['errors'].append(f"éå¤š NaN å€¼: {nan_fraction:.3f}")
                report['valid'] = False
            
            if inf_fraction > thresholds.get('inf_fraction_max', 0.001):
                report['errors'].append(f"éå¤š Inf å€¼: {inf_fraction:.3f}")
                report['valid'] = False
            
            # ç‰©ç†ç¯„åœæª¢æŸ¥
            if field_name in ['u', 'v', 'w']:
                max_velocity = max(abs(report['stats']['min']), abs(report['stats']['max']))
                if max_velocity > thresholds.get('velocity_magnitude_max', 100.0):
                    report['warnings'].append(f"é€Ÿåº¦éå¤§: {max_velocity:.2f}")
            
            elif field_name == 'p':
                p_min, p_max = report['stats']['min'], report['stats']['max']
                threshold_range = thresholds.get('pressure_range', [-1000.0, 1000.0])
                if p_min < threshold_range[0] or p_max > threshold_range[1]:
                    report['warnings'].append(f"å£“åŠ›è¶…å‡ºåˆç†ç¯„åœ: [{p_min:.2f}, {p_max:.2f}]")
        
        else:
            report['errors'].append("ç©ºè³‡æ–™é™£åˆ—")
            report['valid'] = False
        
        return report


class CacheManager:
    """æ™ºèƒ½å¿«å–ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict:
        """è¼‰å…¥å¿«å–å…ƒè³‡æ–™"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r') as f:
                    return json.load(f)
            except:
                logger.warning("å¿«å–å…ƒè³‡æ–™æå£ï¼Œé‡æ–°åˆå§‹åŒ–")
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜å¿«å–å…ƒè³‡æ–™"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)
    
    def _compute_key(self, dataset: str, query_params: Dict) -> str:
        """è¨ˆç®—æŸ¥è©¢çš„å¿«å–éµ"""
        # å»ºç«‹æ¨™æº–åŒ–çš„æŸ¥è©¢å­—ä¸²
        sorted_params = json.dumps(query_params, sort_keys=True)
        key_string = f"{dataset}:{sorted_params}"
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get_cache_path(self, dataset: str, query_params: Dict) -> Path:
        """ç²å–å¿«å–æª”æ¡ˆè·¯å¾‘"""
        cache_key = self._compute_key(dataset, query_params)
        return self.cache_dir / f"{dataset}_{cache_key}.h5"
    
    def is_cached(self, dataset: str, query_params: Dict) -> bool:
        """æª¢æŸ¥æ˜¯å¦å·²å¿«å–"""
        cache_path = self.get_cache_path(dataset, query_params)
        cache_key = self._compute_key(dataset, query_params)
        
        if not cache_path.exists():
            return False
        
        # æª¢æŸ¥å…ƒè³‡æ–™
        if cache_key in self.metadata:
            metadata = self.metadata[cache_key]
            # æª¢æŸ¥æª”æ¡ˆå®Œæ•´æ€§
            if cache_path.stat().st_size != metadata.get('file_size', -1):
                logger.warning(f"å¿«å–æª”æ¡ˆå¤§å°ä¸ç¬¦ï¼Œåˆªé™¤: {cache_path}")
                cache_path.unlink()
                del self.metadata[cache_key]
                self._save_metadata()
                return False
            return True
        
        return False
    
    def save_to_cache(self, dataset: str, query_params: Dict, data: Dict[str, np.ndarray], 
                     metadata: Dict = None):
        """ä¿å­˜è³‡æ–™åˆ°å¿«å–"""
        cache_path = self.get_cache_path(dataset, query_params)
        cache_key = self._compute_key(dataset, query_params)
        
        try:
            with h5py.File(cache_path, 'w') as f:
                # ä¿å­˜è³‡æ–™
                for var_name, var_data in data.items():
                    f.create_dataset(var_name, data=var_data, compression='gzip', compression_opts=6)
                
                # ä¿å­˜æŸ¥è©¢åƒæ•¸å’Œå…ƒè³‡æ–™
                f.attrs['query_params'] = json.dumps(query_params)
                f.attrs['dataset'] = dataset
                f.attrs['timestamp'] = time.time()
                
                if metadata:
                    for key, value in metadata.items():
                        f.attrs[key] = value
            
            # æ›´æ–°å…ƒè³‡æ–™
            self.metadata[cache_key] = {
                'dataset': dataset,
                'query_params': query_params,
                'file_path': str(cache_path),
                'file_size': cache_path.stat().st_size,
                'timestamp': time.time(),
                'variables': list(data.keys())
            }
            self._save_metadata()
            
            logger.info(f"è³‡æ–™å·²å¿«å–: {cache_path}")
            
        except Exception as e:
            logger.error(f"å¿«å–ä¿å­˜å¤±æ•—: {e}")
            if cache_path.exists():
                cache_path.unlink()
    
    def load_from_cache(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """å¾å¿«å–è¼‰å…¥è³‡æ–™"""
        cache_path = self.get_cache_path(dataset, query_params)
        
        try:
            data = {}
            with h5py.File(cache_path, 'r') as f:
                for var_name in f.keys():
                    data[var_name] = f[var_name][:]
                
                # é©—è­‰æŸ¥è©¢åƒæ•¸
                cached_params = json.loads(f.attrs['query_params'])
                if cached_params != query_params:
                    logger.warning("å¿«å–æŸ¥è©¢åƒæ•¸ä¸ç¬¦ï¼Œå¯èƒ½æ˜¯é›œæ¹Šè¡çª")
                    return {}
            
            logger.info(f"å¾å¿«å–è¼‰å…¥è³‡æ–™: {cache_path}")
            return data
            
        except Exception as e:
            logger.error(f"å¿«å–è¼‰å…¥å¤±æ•—: {e}")
            return {}
    
    def clear_cache(self, older_than_days: int = 30):
        """æ¸…ç†èˆŠå¿«å–"""
        current_time = time.time()
        cutoff_time = current_time - (older_than_days * 24 * 3600)
        
        removed_count = 0
        for cache_key, metadata in list(self.metadata.items()):
            if metadata.get('timestamp', 0) < cutoff_time:
                cache_path = Path(metadata['file_path'])
                if cache_path.exists():
                    cache_path.unlink()
                    removed_count += 1
                del self.metadata[cache_key]
        
        if removed_count > 0:
            self._save_metadata()
            logger.info(f"æ¸…ç†äº† {removed_count} å€‹èˆŠå¿«å–æª”æ¡ˆ")


class BaseJHTDBClient(ABC):
    """JHTDB å®¢æˆ¶ç«¯åŸºé¡"""
    
    def __init__(self, 
                 auth_token: Optional[str] = None,
                 cache_dir: str = None,
                 timeout: int = None):
        self.auth_token = auth_token or JHTDBConfig.DEFAULT_AUTH_TOKEN
        self.timeout = timeout or JHTDBConfig.DEFAULT_TIMEOUT
        self.cache_manager = CacheManager(cache_dir or JHTDBConfig.DEFAULT_CACHE_DIR)
        self.validator = DataValidator()
    
    @abstractmethod
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """å¯¦éš›çš„è³‡æ–™ç²å–å¯¦ç¾ï¼ˆç”±å­é¡å¯¦ç¾ï¼‰"""
        pass
    
    def fetch_data(self, 
                   dataset: str,
                   query_params: Dict,
                   use_cache: bool = True,
                   validate: bool = True) -> Dict[str, Any]:
        """
        ç²å– JHTDB è³‡æ–™çš„ä¸»è¦æ¥å£
        
        Args:
            dataset: è³‡æ–™é›†åç¨±
            query_params: æŸ¥è©¢åƒæ•¸
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
            validate: æ˜¯å¦é©—è­‰è³‡æ–™
            
        Returns:
            åŒ…å«è³‡æ–™å’Œå…ƒè³‡æ–™çš„å­—å…¸
        """
        # æª¢æŸ¥å¿«å–
        if use_cache and self.cache_manager.is_cached(dataset, query_params):
            logger.info("ä½¿ç”¨å¿«å–è³‡æ–™")
            data = self.cache_manager.load_from_cache(dataset, query_params)
            if data:  # å¿«å–è¼‰å…¥æˆåŠŸ
                result = {'data': data, 'from_cache': True}
                if validate:
                    result['validation'] = self._validate_data(data)
                return result
        
        # å¾ JHTDB ç²å–æ–°è³‡æ–™
        logger.info(f"å¾ JHTDB ç²å–è³‡æ–™: {dataset}")
        data = self._fetch_raw_data(dataset, query_params)
        
        # é©—è­‰è³‡æ–™
        validation_report = None
        if validate:
            validation_report = self._validate_data(data)
            if not all(report['valid'] for report in validation_report.values()):
                logger.warning("è³‡æ–™é©—è­‰ç™¼ç¾å•é¡Œ")
        
        # ä¿å­˜åˆ°å¿«å–
        if use_cache and data:
            metadata = {'validation_report': validation_report} if validation_report else {}
            self.cache_manager.save_to_cache(dataset, query_params, data, metadata)
        
        return {
            'data': data,
            'from_cache': False,
            'validation': validation_report
        }
    
    def _validate_data(self, data: Dict[str, np.ndarray]) -> Dict[str, Dict]:
        """é©—è­‰ç²å–çš„è³‡æ–™"""
        validation_results = {}
        
        for var_name, var_data in data.items():
            validation_results[var_name] = self.validator.validate_field(
                var_data, var_name, JHTDBConfig.VALIDATION_THRESHOLDS)
        
        return validation_results


class PyJHTDBClient(BaseJHTDBClient):
    """ä½¿ç”¨ pyJHTDB çš„å®¢æˆ¶ç«¯å¯¦ç¾"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        if not PYJHTDB_AVAILABLE:
            raise JHTDBError("pyJHTDB æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install pyJHTDB")
        
        # åˆå§‹åŒ– pyJHTDB
        if self.auth_token:
            pyJHTDB.dbinfo.auth_token = self.auth_token
    
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """ä½¿ç”¨ pyJHTDB ç²å–è³‡æ–™"""
        
        query_type = query_params.get('type', 'cutout')
        variables = query_params.get('variables', ['u', 'v', 'w', 'p'])
        
        try:
            if query_type == 'cutout':
                return self._fetch_cutout(dataset, query_params, variables)
            elif query_type == 'points':
                return self._fetch_points(dataset, query_params, variables)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹: {query_type}")
                
        except Exception as e:
            logger.error(f"pyJHTDB è³‡æ–™ç²å–å¤±æ•—: {e}")
            raise JHTDBError(f"è³‡æ–™ç²å–å¤±æ•—: {e}")
    
    def _fetch_cutout(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç²å– cutout è³‡æ–™"""
        
        start_coords = params['start']  # [x, y, z]
        end_coords = params['end']      # [x, y, z]
        timestep = params.get('timestep', 0)
        
        data = {}
        
        for var in variables:
            logger.info(f"ç²å–è®Šæ•¸ {var} çš„ cutout è³‡æ–™...")
            
            # pyJHTDB cutout èª¿ç”¨
            if var in ['u', 'v', 'w']:
                # é€Ÿåº¦å ´
                cutout_data = pyJHTDB.getvelocity(
                    start=start_coords,
                    end=end_coords,
                    step=[1, 1, 1],  # ç©ºé–“æ­¥é•·
                    dataset=dataset,
                    time=timestep
                )
                
                # pyJHTDB è¿”å› [u, v, w] é™£åˆ—
                if var == 'u':
                    data[var] = cutout_data[:, :, :, 0]
                elif var == 'v':
                    data[var] = cutout_data[:, :, :, 1]
                elif var == 'w':
                    data[var] = cutout_data[:, :, :, 2]
            
            elif var == 'p':
                # å£“åŠ›å ´
                data[var] = pyJHTDB.getpressure(
                    start=start_coords,
                    end=end_coords,
                    step=[1, 1, 1],
                    dataset=dataset,
                    time=timestep
                )
        
        return data
    
    def _fetch_points(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç²å–æ•£é»è³‡æ–™"""
        
        points = params['points']  # [[x1,y1,z1], [x2,y2,z2], ...]
        timestep = params.get('timestep', 0)
        
        data = {}
        
        for var in variables:
            logger.info(f"ç²å–è®Šæ•¸ {var} çš„æ•£é»è³‡æ–™...")
            
            if var in ['u', 'v', 'w']:
                # é€Ÿåº¦å ´æ’å€¼
                velocity_data = pyJHTDB.getvelocity(
                    points=points,
                    dataset=dataset,
                    time=timestep
                )
                
                if var == 'u':
                    data[var] = velocity_data[:, 0]
                elif var == 'v':
                    data[var] = velocity_data[:, 1]
                elif var == 'w':
                    data[var] = velocity_data[:, 2]
            
            elif var == 'p':
                # å£“åŠ›å ´æ’å€¼
                data[var] = pyJHTDB.getpressure(
                    points=points,
                    dataset=dataset,
                    time=timestep
                )
        
        return data


class MockJHTDBClient(BaseJHTDBClient):
    """æ¨¡æ“¬ JHTDB å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼æ¸¬è©¦å’Œé›¢ç·šé–‹ç™¼ï¼‰"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.seed = 42  # å›ºå®šéš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
    
    def _fetch_raw_data(self, dataset: str, query_params: Dict) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ“¬çš„æ¹æµè³‡æ–™"""
        
        np.random.seed(self.seed)
        
        query_type = query_params.get('type', 'cutout')
        variables = query_params.get('variables', ['u', 'v', 'w', 'p'])
        
        if query_type == 'cutout':
            return self._generate_cutout_data(dataset, query_params, variables)
        elif query_type == 'points':
            return self._generate_points_data(dataset, query_params, variables)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹: {query_type}")
    
    def _generate_cutout_data(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ“¬ cutout è³‡æ–™"""
        
        start = np.array(params['start'])
        end = np.array(params['end'])
        resolution = params.get('resolution', [64, 64, 64])
        
        # ç”Ÿæˆåº§æ¨™ç¶²æ ¼
        x = np.linspace(start[0], end[0], resolution[0])
        y = np.linspace(start[1], end[1], resolution[1])
        z = np.linspace(start[2], end[2], resolution[2])
        X, Y, Z = np.meshgrid(x, y, z, indexing='ij')
        
        data = {}
        
        # ç²å–è³‡æ–™é›†é…ç½®
        dataset_config = JHTDBConfig.DATASETS.get(dataset, {})
        domain = dataset_config.get('domain', {'x': [0, 2*np.pi], 'y': [0, 2*np.pi], 'z': [0, 2*np.pi]})
        
        # æ¨™æº–åŒ–åº§æ¨™
        x_norm = 2 * np.pi * (X - domain['x'][0]) / (domain['x'][1] - domain['x'][0])
        y_norm = 2 * np.pi * (Y - domain['y'][0]) / (domain['y'][1] - domain['y'][0])
        z_norm = 2 * np.pi * (Z - domain['z'][0]) / (domain['z'][1] - domain['z'][0])
        
        for var in variables:
            if var == 'u':
                # ä¸»æµæ–¹å‘é€Ÿåº¦ï¼šåŒ…å«å¤šå°ºåº¦æ¸¦çµæ§‹
                data[var] = (
                    5.0 * np.sin(x_norm) * np.cos(y_norm) * np.sin(z_norm) +
                    2.0 * np.sin(2*x_norm) * np.cos(2*y_norm) * np.sin(2*z_norm) +
                    0.5 * np.sin(4*x_norm) * np.cos(4*y_norm) * np.sin(4*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'v':
                # æ©«å‘é€Ÿåº¦
                data[var] = (
                    3.0 * np.cos(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.5 * np.cos(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'w':
                # å±•å‘é€Ÿåº¦
                data[var] = (
                    2.0 * np.sin(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.0 * np.sin(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(*X.shape)
                )
                
            elif var == 'p':
                # å£“åŠ›å ´ï¼šéœ€è¦æ»¿è¶³é€£çºŒæ€§æ–¹ç¨‹çš„ç´„æŸ
                data[var] = (
                    -0.5 * (data.get('u', 0)**2 + data.get('v', 0)**2 + data.get('w', 0)**2) +
                    10.0 * np.cos(x_norm + y_norm + z_norm) +
                    0.05 * np.random.randn(*X.shape)
                )
        
        return data
    
    def _generate_points_data(self, dataset: str, params: Dict, variables: List[str]) -> Dict[str, np.ndarray]:
        """ç”Ÿæˆæ¨¡æ“¬æ•£é»è³‡æ–™"""
        
        points = np.array(params['points'])  # [N, 3]
        n_points = points.shape[0]
        
        data = {}
        
        # ç²å–è³‡æ–™é›†é…ç½®
        dataset_config = JHTDBConfig.DATASETS.get(dataset, {})
        domain = dataset_config.get('domain', {'x': [0, 2*np.pi], 'y': [0, 2*np.pi], 'z': [0, 2*np.pi]})
        
        # æ¨™æº–åŒ–åº§æ¨™
        x_norm = 2 * np.pi * (points[:, 0] - domain['x'][0]) / (domain['x'][1] - domain['x'][0])
        y_norm = 2 * np.pi * (points[:, 1] - domain['y'][0]) / (domain['y'][1] - domain['y'][0])
        z_norm = 2 * np.pi * (points[:, 2] - domain['z'][0]) / (domain['z'][1] - domain['z'][0])
        
        for var in variables:
            if var == 'u':
                data[var] = (
                    5.0 * np.sin(x_norm) * np.cos(y_norm) * np.sin(z_norm) +
                    2.0 * np.sin(2*x_norm) * np.cos(2*y_norm) * np.sin(2*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
            elif var == 'v':
                data[var] = (
                    3.0 * np.cos(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.5 * np.cos(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
            elif var == 'w':
                data[var] = (
                    2.0 * np.sin(x_norm) * np.sin(y_norm) * np.cos(z_norm) +
                    1.0 * np.sin(2*x_norm) * np.sin(2*y_norm) * np.cos(2*z_norm) +
                    0.1 * np.random.randn(n_points)
                )
            elif var == 'p':
                u_val = data.get('u', np.zeros(n_points))
                v_val = data.get('v', np.zeros(n_points))
                w_val = data.get('w', np.zeros(n_points))
                data[var] = (
                    -0.5 * (u_val**2 + v_val**2 + w_val**2) +
                    10.0 * np.cos(x_norm + y_norm + z_norm) +
                    0.05 * np.random.randn(n_points)
                )
        
        return data


class JHTDBManager:
    """JHTDB ç®¡ç†å™¨ï¼šæä¾›é«˜å±¤ç´šçš„è³‡æ–™å­˜å–æ¥å£"""
    
    def __init__(self, 
                 use_mock: bool = False,
                 auth_token: Optional[str] = None,
                 cache_dir: str = None,
                 **kwargs):
        """
        Args:
            use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ“¬å®¢æˆ¶ç«¯ï¼ˆç”¨æ–¼é›¢ç·šé–‹ç™¼ï¼‰
            auth_token: JHTDB èªè­‰ä»¤ç‰Œ
            cache_dir: å¿«å–ç›®éŒ„
        """
        
        if use_mock or not PYJHTDB_AVAILABLE:
            if not use_mock:
                logger.warning("pyJHTDB ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ“¬å®¢æˆ¶ç«¯")
            self.client = MockJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
        else:
            self.client = PyJHTDBClient(auth_token=auth_token, cache_dir=cache_dir, **kwargs)
        
        self.datasets = JHTDBConfig.DATASETS
    
    def fetch_cutout(self,
                    dataset: str,
                    start: List[float],
                    end: List[float],
                    timestep: int = 0,
                    variables: List[str] = None,
                    resolution: List[int] = None,
                    **kwargs) -> Dict[str, Any]:
        """
        ç²å– cutout è³‡æ–™
        
        Args:
            dataset: è³‡æ–™é›†åç¨±
            start: èµ·å§‹åº§æ¨™ [x, y, z]
            end: çµæŸåº§æ¨™ [x, y, z]
            timestep: æ™‚é–“æ­¥
            variables: è®Šæ•¸åˆ—è¡¨
            resolution: è§£æåº¦ [nx, ny, nz] (åƒ…ç”¨æ–¼æ¨¡æ“¬è³‡æ–™)
            
        Returns:
            åŒ…å«è³‡æ–™å’Œå…ƒè³‡æ–™çš„å­—å…¸
        """
        
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥è³‡æ–™é›†: {dataset}")
        
        variables = variables or ['u', 'v', 'w', 'p']
        
        query_params = {
            'type': 'cutout',
            'start': start,
            'end': end,
            'timestep': timestep,
            'variables': variables
        }
        
        if resolution:
            query_params['resolution'] = resolution
        
        return self.client.fetch_data(dataset, query_params, **kwargs)
    
    def fetch_points(self,
                    dataset: str,
                    points: List[List[float]],
                    timestep: int = 0,
                    variables: List[str] = None,
                    **kwargs) -> Dict[str, Any]:
        """
        ç²å–æ•£é»è³‡æ–™
        
        Args:
            dataset: è³‡æ–™é›†åç¨±
            points: åº§æ¨™é»åˆ—è¡¨ [[x1,y1,z1], [x2,y2,z2], ...]
            timestep: æ™‚é–“æ­¥
            variables: è®Šæ•¸åˆ—è¡¨
            
        Returns:
            åŒ…å«è³‡æ–™å’Œå…ƒè³‡æ–™çš„å­—å…¸
        """
        
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥è³‡æ–™é›†: {dataset}")
        
        variables = variables or ['u', 'v', 'w', 'p']
        
        query_params = {
            'type': 'points',
            'points': points,
            'timestep': timestep,
            'variables': variables
        }
        
        return self.client.fetch_data(dataset, query_params, **kwargs)
    
    def get_dataset_info(self, dataset: str) -> Dict:
        """ç²å–è³‡æ–™é›†è³‡è¨Š"""
        if dataset not in self.datasets:
            raise ValueError(f"æœªçŸ¥è³‡æ–™é›†: {dataset}")
        return self.datasets[dataset].copy()
    
    def list_datasets(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨è³‡æ–™é›†"""
        return list(self.datasets.keys())
    
    def clear_cache(self, older_than_days: int = 30):
        """æ¸…ç†å¿«å–"""
        self.client.cache_manager.clear_cache(older_than_days)


# ä¾¿æ·å‡½æ•¸
def create_jhtdb_manager(use_mock: bool = False, **kwargs) -> JHTDBManager:
    """å‰µå»º JHTDB ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•¸"""
    return JHTDBManager(use_mock=use_mock, **kwargs)


def fetch_sample_data(dataset: str = 'isotropic1024coarse',
                     n_points: int = 100,
                     use_mock: bool = True) -> Dict[str, Any]:
    """
    ç²å–æ¨£æœ¬è³‡æ–™çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        dataset: è³‡æ–™é›†åç¨±
        n_points: æ¨£æœ¬é»æ•¸
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ“¬è³‡æ–™
        
    Returns:
        æ¨£æœ¬è³‡æ–™å­—å…¸
    """
    
    manager = create_jhtdb_manager(use_mock=use_mock)
    dataset_info = manager.get_dataset_info(dataset)
    
    # åœ¨è³‡æ–™é›†åŸŸå…§ç”Ÿæˆéš¨æ©Ÿé»
    domain = dataset_info['domain']
    np.random.seed(42)  # å›ºå®šç¨®å­ç¢ºä¿å¯é‡ç¾æ€§
    
    points = []
    for _ in range(n_points):
        x = np.random.uniform(domain['x'][0], domain['x'][1])
        y = np.random.uniform(domain['y'][0], domain['y'][1])
        z = np.random.uniform(domain['z'][0], domain['z'][1])
        points.append([x, y, z])
    
    return manager.fetch_points(dataset, points, timestep=0)


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    print("ğŸŒŠ æ¸¬è©¦ JHTDB å®¢æˆ¶ç«¯...")
    
    # æ¸¬è©¦æ¨¡æ“¬å®¢æˆ¶ç«¯
    print("\n=== æ¸¬è©¦æ¨¡æ“¬å®¢æˆ¶ç«¯ ===")
    
    manager = create_jhtdb_manager(use_mock=True)
    
    # åˆ—å‡ºè³‡æ–™é›†
    print(f"å¯ç”¨è³‡æ–™é›†: {manager.list_datasets()}")
    
    # ç²å–è³‡æ–™é›†è³‡è¨Š
    dataset = 'isotropic1024coarse'
    info = manager.get_dataset_info(dataset)
    print(f"\nè³‡æ–™é›† {dataset} è³‡è¨Š:")
    print(f"  æè¿°: {info['description']}")
    print(f"  åŸŸç¯„åœ: {info['domain']}")
    print(f"  è§£æåº¦: {info['resolution']}")
    
    # æ¸¬è©¦ cutout è³‡æ–™
    print(f"\næ¸¬è©¦ cutout è³‡æ–™...")
    cutout_result = manager.fetch_cutout(
        dataset=dataset,
        start=[0.0, 0.0, 0.0],
        end=[1.0, 1.0, 1.0],
        resolution=[32, 32, 32],
        variables=['u', 'v', 'p']
    )
    
    print(f"Cutout è³‡æ–™ç²å–æˆåŠŸ: {not cutout_result['from_cache']}")
    data = cutout_result['data']
    for var, arr in data.items():
        print(f"  {var}: {arr.shape}, ç¯„åœ=[{arr.min():.3f}, {arr.max():.3f}]")
    
    # æ¸¬è©¦æ•£é»è³‡æ–™
    print(f"\næ¸¬è©¦æ•£é»è³‡æ–™...")
    points = [[0.5, 0.5, 0.5], [1.0, 1.0, 1.0], [1.5, 1.5, 1.5]]
    points_result = manager.fetch_points(
        dataset=dataset,
        points=points,
        variables=['u', 'v', 'w', 'p']
    )
    
    print(f"æ•£é»è³‡æ–™ç²å–æˆåŠŸ: {not points_result['from_cache']}")
    data = points_result['data']
    for var, arr in data.items():
        print(f"  {var}: {arr.shape}, å€¼={arr}")
    
    # æ¸¬è©¦å¿«å–åŠŸèƒ½
    print(f"\næ¸¬è©¦å¿«å–åŠŸèƒ½...")
    cutout_result_cached = manager.fetch_cutout(
        dataset=dataset,
        start=[0.0, 0.0, 0.0],
        end=[1.0, 1.0, 1.0],
        resolution=[32, 32, 32],
        variables=['u', 'v', 'p']
    )
    print(f"ä½¿ç”¨å¿«å–: {cutout_result_cached['from_cache']}")
    
    # æ¸¬è©¦é©—è­‰åŠŸèƒ½
    if 'validation' in cutout_result:
        print(f"\nè³‡æ–™é©—è­‰çµæœ:")
        for var, report in cutout_result['validation'].items():
            status = "âœ…" if report['valid'] else "âŒ"
            print(f"  {var}: {status} (è­¦å‘Š: {len(report['warnings'])}, éŒ¯èª¤: {len(report['errors'])})")
    
    # æ¸¬è©¦ä¾¿æ·å‡½æ•¸
    print(f"\næ¸¬è©¦ä¾¿æ·å‡½æ•¸...")
    sample_data = fetch_sample_data(dataset='channel', n_points=10, use_mock=True)
    print(f"æ¨£æœ¬è³‡æ–™ç²å–æˆåŠŸ: {len(sample_data['data'])} å€‹è®Šæ•¸")
    
    print("\nâœ… JHTDB å®¢æˆ¶ç«¯æ¸¬è©¦å®Œæˆï¼")