"""
Channel Flow Re1000 å°ˆç”¨è³‡æ–™è¼‰å…¥å™¨

æä¾›çµ±ä¸€ä»‹é¢è¼‰å…¥å’Œè™•ç† Channel Flow Re1000 ç›¸é—œè³‡æ–™ï¼š
- NPZ å¿«å–çš„æ„Ÿæ¸¬é»è³‡æ–™ (QR-pivot/Random)
- ä½ä¿çœŸ RANS å…ˆé©—è³‡æ–™
- JHTDB é…ç½®èˆ‡åŸŸåƒæ•¸
- PINNs è¨“ç·´æ‰€éœ€çš„è³‡æ–™æ ¼å¼æ¨™æº–åŒ–

ä¸»è¦åŠŸèƒ½ï¼š
1. æ„Ÿæ¸¬é»è³‡æ–™è¼‰å…¥èˆ‡é©—è­‰
2. ä½ä¿çœŸå…ˆé©—æ’å€¼åˆ° PINNs è¨“ç·´é»
3. VS-PINN å°ºåº¦åŒ–çµ±è¨ˆè³‡è¨Šæå–
4. èˆ‡ç¾æœ‰è¨“ç·´æµç¨‹å®Œå…¨ç›¸å®¹
5. å¿«å–ç®¡ç†èˆ‡è³‡æ–™å®Œæ•´æ€§æª¢æŸ¥

è¨­è¨ˆåŸå‰‡ï¼š
- èˆ‡ scripts/train.py ç„¡ç¸«æ•´åˆ
- åƒ…æ”¯æ´çœŸå¯¦ JHTDB è³‡æ–™ï¼Œç§»é™¤MockåŠŸèƒ½
- é«˜æ•ˆç‡çš„è¨˜æ†¶é«”ç®¡ç†
- å®Œæ•´çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
"""

import numpy as np
import yaml
from typing import Dict, List, Optional, Union, Tuple, Any
from pathlib import Path
import logging
from dataclasses import dataclass
import warnings

# å°å…¥ç¾æœ‰æ¨¡çµ„
from .lowfi_loader import LowFiData, LowFiLoader, SpatialInterpolator
from .jhtdb_client import JHTDBManager, JHTDBConfig
from .structures import (
    StructuredGrid,
    StructuredField,
    PointSamples,
    FlowDataBundle,
    DomainSpec,
)


logger = logging.getLogger(__name__)


@dataclass
class ChannelFlowData:
    """Channel Flow data bundle built on unified data structures."""

    samples: PointSamples
    domain: DomainSpec
    selection_info: Dict[str, Any]
    coordinate_info: Dict[str, Any]
    statistics: Optional[Dict[str, Dict[str, float]]] = None
    lowfi_prior: Optional[PointSamples] = None
    lowfi_metadata: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None

    @property
    def sensor_points(self) -> np.ndarray:
        return self.samples.coordinates

    @property
    def sensor_data(self) -> Dict[str, np.ndarray]:
        return self.samples.values

    @property
    def sensor_axes(self) -> Tuple[str, ...]:
        return self.samples.axes

    @property
    def domain_config(self) -> Dict[str, Any]:
        return self.domain.to_config()

    def get_domain_bounds(self) -> Dict[str, Tuple[float, float]]:
        return dict(self.domain.bounds)

    def get_physical_parameters(self) -> Dict[str, float]:
        return dict(self.domain.parameters)

    def has_lowfi_prior(self) -> bool:
        return self.lowfi_prior is not None

    def to_flow_bundle(self) -> FlowDataBundle:
        meta = {
            'selection_info': self.selection_info,
            'coordinate_info': self.coordinate_info,
        }
        if self.metadata:
            meta.update(self.metadata)
        return FlowDataBundle(
            samples=self.samples,
            domain=self.domain,
            statistics=self.statistics or {},
            lowfi_prior=self.lowfi_prior,
            metadata=meta
        )


class ChannelFlowLoader:
    """Channel Flow Re1000 å°ˆç”¨è¼‰å…¥å™¨"""
    
    def __init__(self, 
                 config_path: Optional[Union[str, Path]] = None,
                 cache_dir: Optional[Union[str, Path]] = None,
                 interpolation_method: str = 'linear'):
        """
        åˆå§‹åŒ–è¼‰å…¥å™¨
        
        Args:
            config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼Œé è¨­ configs/channel_flow_re1000.yml
            cache_dir: å¿«å–ç›®éŒ„ï¼Œé è¨­ data/jhtdb/channel_flow_re1000/
            interpolation_method: æ’å€¼æ–¹æ³• ('linear', 'rbf', 'idw')
        """
        # è¨­å®šè·¯å¾‘
        self.config_path = Path(config_path) if config_path else Path('configs/channel_flow_re1000.yml')
        self.cache_dir = Path(cache_dir) if cache_dir else Path('data/jhtdb/channel_flow_re1000/')
        
        # è¼‰å…¥é…ç½®
        self.config = self._load_config()
        
        # åˆå§‹åŒ–å·¥å…·
        self.lowfi_loader = LowFiLoader()
        self.interpolator = SpatialInterpolator(method=interpolation_method)
        
        # JHTDB ç®¡ç†å™¨ (å¦‚æœéœ€è¦)
        self.jhtdb_manager = None
        self._init_jhtdb_manager()
        
        logger.info(f"Channel Flow loader initialized with config: {self.config_path}")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"Configuration file not found: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        logger.info(f"Loaded configuration from {self.config_path}")
        return config
    
    def _init_jhtdb_manager(self):
        """åˆå§‹åŒ– JHTDB ç®¡ç†å™¨"""
        try:
            # å¾é…ç½®ä¸­æå– JHTDB åƒæ•¸
            jhtdb_config = self.config.get('jhtdb', {})
            if jhtdb_config.get('enabled', False):
                self.jhtdb_manager = JHTDBManager(
                    cache_root=self.cache_dir.parent,
                    auth_token=jhtdb_config.get('auth_token')
                )
                logger.info("JHTDB manager initialized")
            else:
                raise RuntimeError("JHTDB is disabled and no mock fallback available. Please enable JHTDB to proceed.")
        except Exception as e:
            logger.warning(f"Failed to initialize JHTDB manager: {e}")
            self.jhtdb_manager = None
    
    def load_sensor_data(self, 
                        strategy: str = 'qr_pivot',
                        K: int = 8,
                        noise_sigma: Optional[float] = None,
                        dropout_prob: Optional[float] = None,
                        sensor_file: Optional[str] = None) -> ChannelFlowData:
        """
        è¼‰å…¥æ„Ÿæ¸¬é»è³‡æ–™
        
        Args:
            strategy: é¸æ“‡ç­–ç•¥ ('qr_pivot', 'random', 'uniform')
            K: æ„Ÿæ¸¬é»æ•¸é‡
            noise_sigma: å™ªè²æ°´å¹³ (å¯é¸)
            dropout_prob: ä¸Ÿå¤±æ¦‚ç‡ (å¯é¸)
            sensor_file: è‡ªå®šç¾©æ„Ÿæ¸¬é»æ–‡ä»¶å (å¯é¸ï¼Œå„ªå…ˆæ–¼è‡ªå‹•æ§‹å»º)
            
        Returns:
            Channel Flow è³‡æ–™å®¹å™¨
        """
        # æ§‹å»ºå¿«å–æª”æ¡ˆåï¼ˆå…è¨±è‡ªå®šç¾©è¦†è“‹ï¼‰
        if sensor_file is not None:
            # å¦‚æœæä¾›çµ•å°è·¯å¾‘ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦å‰‡ç›¸å°æ–¼ cache_dir
            cache_path = Path(sensor_file)
            if not cache_path.is_absolute():
                cache_path = self.cache_dir / sensor_file
        else:
            # å¦å‰‡ä½¿ç”¨ cache_dir + è‡ªå‹•ç”Ÿæˆçš„æª”å
            cache_filename = f"sensors_K{K}_{strategy}.npz"
            cache_path = self.cache_dir / cache_filename
        
        if not cache_path.exists():
            raise FileNotFoundError(
                f"Sensor data not found: {cache_path}\n"
                f"Please run scripts/fetch_channel_flow.py first"
            )
        
        logger.info(f"Loading sensor data from {cache_path}")
        
        # è¼‰å…¥ NPZ è³‡æ–™
        data = np.load(cache_path, allow_pickle=True)
        
        # æå–æ„Ÿæ¸¬é»è³‡è¨Šï¼ˆæ”¯æ´å¤šç¨®éµåï¼‰
        if 'sensor_points' in data:
            sensor_points = data['sensor_points']
        elif 'coords' in data:
            sensor_points = data['coords']  # â­ æ–°æ ¼å¼ï¼šä½¿ç”¨ 'coords' éµ
        elif 'coords_2d' in data:
            # 2D åº§æ¨™éœ€è¦æ“´å±•åˆ° 3D (x, y) â†’ (x, y, z_slice)
            coords_2d = data['coords_2d']
            z_slice = self.config.get('normalization', {}).get('slice_config', {}).get('z_position', 4.71)
            sensor_points = np.column_stack([
                coords_2d[:, 0],  # x
                coords_2d[:, 1],  # y
                np.full(len(coords_2d), z_slice)  # z (constant)
            ])
        else:
            raise KeyError(f"Cannot find sensor coordinates in {cache_path}. Expected 'sensor_points', 'coords', or 'coords_2d'")
        
        # è™•ç† sensor_data (å¯èƒ½æ˜¯ç‰©ä»¶ã€åˆ†é›¢çš„é™£åˆ—ã€æˆ–ç›´æ¥çš„ 2D ndarray)
        if 'sensor_data' in data:
            sensor_data_raw = data['sensor_data']
            
            # æƒ…æ³ 1: 0 ç¶­ç‰©ä»¶ï¼ˆåŒ…å«å­—å…¸ï¼‰
            if sensor_data_raw.ndim == 0:
                sensor_data_raw = sensor_data_raw.item()
            
            # æƒ…æ³ 2: 2D ndarray (K, n_vars) - ç›´æ¥å„²å­˜é€Ÿåº¦åˆ†é‡
            elif sensor_data_raw.ndim == 2:
                # å¾ metadata æˆ–é è¨­è®Šæ•¸åç¨±æå–æ¬„ä½
                if 'metadata' in data:
                    metadata = data['metadata'].item() if data['metadata'].ndim == 0 else data['metadata']
                    variables = metadata.get('variables', ['u', 'v', 'w'])
                else:
                    # æ ¹æ“šæ¬„ä½æ•¸åˆ¤æ–·
                    n_vars = sensor_data_raw.shape[1]
                    if n_vars == 2:
                        variables = ['u', 'v']
                    elif n_vars == 3:
                        variables = ['u', 'v', 'w']
                    elif n_vars == 4:
                        variables = ['u', 'v', 'w', 'p']
                    else:
                        raise ValueError(f"Cannot infer variable names for {n_vars} columns")
                
                # å°‡ ndarray è½‰æ›ç‚ºå­—å…¸æ ¼å¼
                sensor_data_raw = {
                    var: sensor_data_raw[:, i]
                    for i, var in enumerate(variables)
                }
        else:
            # æƒ…æ³ 3: åˆ†é›¢çš„æ¬„ä½ï¼ˆsensor_u, sensor_v ç­‰ æˆ–ç›´æ¥ u, v, w, pï¼‰
            sensor_data_raw = {}
            
            # å„ªå…ˆæª¢æŸ¥ 'sensor_*' æ ¼å¼
            for field in ['u', 'v', 'w', 'p']:
                key_sensor = f'sensor_{field}'
                if key_sensor in data:
                    sensor_data_raw[field] = data[key_sensor]
            
            # â­ è‹¥ç„¡ 'sensor_*'ï¼Œå˜—è©¦ç›´æ¥éµåï¼ˆæ–°æ ¼å¼ï¼‰
            if not sensor_data_raw:
                for field in ['u', 'v', 'w', 'p']:
                    if field in data:
                        sensor_data_raw[field] = data[field]
            
            if not sensor_data_raw:
                raise KeyError(f"Cannot find velocity/pressure data in {cache_path}. Expected 'sensor_data', 'sensor_u/v/w/p', or 'u/v/w/p'")
        
        sensor_values = {
            field: np.asarray(values).reshape(-1)
            for field, values in sensor_data_raw.items()
        }
        
        # æå– sensor_indices (å¯èƒ½ä¸å­˜åœ¨ï¼Œç”Ÿæˆé è¨­ç´¢å¼•)
        if 'sensor_indices' in data:
            sensor_indices = np.asarray(data['sensor_indices'])
        else:
            # å¦‚æœæ²’æœ‰ indicesï¼Œç”Ÿæˆé€£çºŒç´¢å¼•
            sensor_indices = np.arange(len(sensor_points))
        
        # æå–é¸æ“‡è³‡è¨Š
        if 'selection_info' in data:
            selection_info = data['selection_info'].item() if data['selection_info'].ndim == 0 else data['selection_info']
        else:
            selection_info = {
                'strategy': str(data.get('strategy', strategy)),
                'K_requested': int(data.get('K_requested', K)),
                'K_actual': int(len(sensor_points)),
                'selection_timestamp': str(data.get('timestamp', 'unknown'))
            }
        
        # æ·»åŠ å™ªè² (å¦‚æœæŒ‡å®š)
        if noise_sigma is not None and noise_sigma > 0:
            sensor_values = self._add_noise(sensor_values, noise_sigma)
            selection_info['noise_sigma'] = noise_sigma
        
        # æ·»åŠ ä¸Ÿå¤± (å¦‚æœæŒ‡å®š)
        if dropout_prob is not None and dropout_prob > 0:
            sensor_values, valid_mask = self._add_dropout(sensor_values, dropout_prob)
            sensor_points = sensor_points[valid_mask]
            sensor_indices = sensor_indices[valid_mask]
            selection_info['dropout_prob'] = dropout_prob
            selection_info['K_after_dropout'] = int(len(sensor_points))
        
        # æå–åŸŸé…ç½®
        domain_config = self._extract_domain_config()
        coordinate_info = self._extract_coordinate_info(data)
        domain_spec = self._build_domain_spec(domain_config)
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Šï¼ˆç”¨æ–¼ VS-PINN èˆ‡è‡ªå‹•è¼¸å‡ºç¯„åœï¼‰
        statistics = self._compute_statistics(sensor_values, sensor_points)
        
        samples = PointSamples(
            coordinates=sensor_points,
            values=sensor_values,
            axes=('x', 'y', 'z') if sensor_points.shape[1] == 3 else ('x', 'y'),
            metadata={'sensor_indices': sensor_indices}
        )
        
        channel_data = ChannelFlowData(
            samples=samples,
            domain=domain_spec,
            selection_info=selection_info,
            coordinate_info=coordinate_info,
            statistics=statistics,
            metadata={
                'source': str(cache_path),
                'config_file': str(self.config_path),
                'loader_version': '2.0',
                'loaded_timestamp': str(np.datetime64('now')),
                'strategy': strategy,
                'requested_K': int(K),
                'actual_K': int(len(sensor_points))
            }
        )
        
        logger.info(f"Loaded {len(sensor_points)} sensor points using {strategy} strategy")
        logger.info(f"Computed statistics for fields: {list(statistics.keys())}")
        return channel_data
    
    
    def add_lowfi_prior(self, 
                       channel_data: ChannelFlowData,
                       prior_type: str = 'rans',
                       interpolate_to_sensors: bool = True) -> ChannelFlowData:
        """
        æ·»åŠ ä½ä¿çœŸå…ˆé©—è³‡æ–™
        
        Args:
            channel_data: ç¾æœ‰çš„ Channel Flow è³‡æ–™
            prior_type: å…ˆé©—é¡å‹ ('rans', 'mock', 'none')
            interpolate_to_sensors: æ˜¯å¦æ’å€¼åˆ°æ„Ÿæ¸¬é»
            
        Returns:
            æ·»åŠ å…ˆé©—å¾Œçš„è³‡æ–™å®¹å™¨
        """
        if prior_type == 'none':
            logger.info("No low-fidelity prior requested")
            return channel_data
        
        try:
            if prior_type == 'rans':
                # è¼‰å…¥çœŸå¯¦ RANS è³‡æ–™ (å¦‚æœå¯ç”¨)
                lowfi_data = self._load_rans_prior()
            elif prior_type == 'mock':
                # ä½¿ç”¨ç°¡åŒ–çš„ mock å…ˆé©— (åŸºæ–¼å±¤æµè§£æˆ–çµ±è¨ˆé‡)
                lowfi_data = self._create_mock_prior(channel_data)
            else:
                raise ValueError(f"Unknown prior type: {prior_type}. Supported: 'rans', 'mock', 'none'.")
            
            # æ’å€¼åˆ°æ„Ÿæ¸¬é» (å¦‚æœéœ€è¦)
            # å°æ–¼ mock priorï¼Œå·²ç¶“åœ¨æ„Ÿæ¸¬é»è¨ˆç®—ï¼Œä¸éœ€è¦æ’å€¼
            if interpolate_to_sensors and prior_type != 'mock':
                prior_fields = self.interpolator.interpolate_to_points(
                    lowfi_data,
                    channel_data.sensor_points
                )
            else:
                prior_fields = lowfi_data.fields

            prior_samples = PointSamples(
                coordinates=channel_data.sensor_points,
                values={k: np.asarray(v).reshape(-1) for k, v in prior_fields.items()},
                axes=channel_data.sensor_axes,
                metadata={'prior_type': prior_type}
            )

            channel_data.lowfi_prior = prior_samples
            channel_data.lowfi_metadata = lowfi_data.metadata
            
            logger.info(f"Added {prior_type} low-fidelity prior with {len(prior_fields)} fields")
            
        except Exception as e:
            logger.warning(f"Failed to load low-fidelity prior: {e}")
            # ä¿æŒ lowfi_prior ç‚º Noneï¼ˆä¸è¨­ç‚ºç©ºå­—å…¸ï¼‰
            channel_data.lowfi_prior = None
            channel_data.lowfi_metadata = {'type': 'none', 'error': str(e)}
            # ä¸è¦†è“‹ statistics - ä¿ç•™åŸæœ‰çš„çµ±è¨ˆè³‡è¨Š
        
        return channel_data
    
    def prepare_for_training(self, 
                           channel_data: ChannelFlowData,
                           target_fields: Optional[List[str]] = None) -> FlowDataBundle:
        """
        æº–å‚™ PINNs è¨“ç·´è³‡æ–™æ ¼å¼
        
        Args:
            channel_data: Channel Flow è³‡æ–™
            target_fields: ç›®æ¨™å ´åˆ—è¡¨ï¼Œé è¨­ ['u', 'v', 'w', 'p']ï¼ˆ3Dï¼‰æˆ– ['u', 'v', 'p']ï¼ˆ2Dï¼‰
            
        Returns:
            FlowDataBundle å°è£çš„è¨“ç·´è³‡æ–™
        """
        if target_fields is None:
            # ğŸ†• è‡ªå‹•æª¢æ¸¬å¯ç”¨æ¬„ä½ï¼ˆå„ªå…ˆä½¿ç”¨å®Œæ•´ 4 è®Šé‡ï¼‰
            available_fields = list(channel_data.sensor_data.keys())
            if 'w' in available_fields:
                target_fields = ['u', 'v', 'w', 'p']  # 3D æˆ–å« w çš„ 2D åˆ‡ç‰‡
            else:
                target_fields = ['u', 'v', 'p']  # èˆŠç‰ˆ 2Dï¼ˆå‘å¾Œå…¼å®¹ï¼‰
            logger.info(f"Auto-detected target_fields: {target_fields}")
        bundle = channel_data.to_flow_bundle()
        bundle.metadata['has_lowfi_prior'] = channel_data.has_lowfi_prior()
        bundle.metadata['target_fields'] = list(target_fields)
        logger.info(f"Prepared training bundle with fields: {target_fields}")
        return bundle
    
    def _add_noise(self, 
                  sensor_data: Dict[str, np.ndarray], 
                  noise_sigma: float) -> Dict[str, np.ndarray]:
        """æ·»åŠ é«˜æ–¯å™ªè²åˆ°æ„Ÿæ¸¬è³‡æ–™"""
        noisy_data = {}
        for field, values in sensor_data.items():
            noise = np.random.normal(0, noise_sigma * np.std(values), values.shape)
            noisy_data[field] = values + noise
        
        logger.debug(f"Added Gaussian noise with sigma={noise_sigma}")
        return noisy_data
    
    def _add_dropout(self, 
                    sensor_data: Dict[str, np.ndarray], 
                    dropout_prob: float) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
        """éš¨æ©Ÿä¸Ÿå¤±éƒ¨åˆ†æ„Ÿæ¸¬é»"""
        n_points = len(next(iter(sensor_data.values())))
        valid_mask = np.random.random(n_points) > dropout_prob
        
        dropped_data = {}
        for field, values in sensor_data.items():
            dropped_data[field] = values[valid_mask]
        
        logger.debug(f"Applied dropout with prob={dropout_prob}, kept {np.sum(valid_mask)}/{n_points} points")
        return dropped_data, valid_mask
    
    def _extract_domain_config(self) -> Dict[str, Any]:
        """å¾é…ç½®æª”æ¡ˆæå–åŸŸåƒæ•¸"""
        domain_config = {}
        
        # å¾ data.jhtdb_config æå– Channel Flow åƒæ•¸
        if 'data' in self.config and 'jhtdb_config' in self.config['data']:
            jhtdb_config = self.config['data']['jhtdb_config']
            
            # åŸŸç¯„åœ
            if 'domain' in jhtdb_config:
                domain = jhtdb_config['domain']
                domain_config.update({
                    'x_range': domain.get('x', [0.0, 25.13]),
                    'y_range': domain.get('y', [-1.0, 1.0]),
                    'z_range': domain.get('z', [0.0, 9.42])
                })
            
            # è§£æåº¦
            if 'resolution' in jhtdb_config:
                resolution = jhtdb_config['resolution']
                domain_config.update({
                    'nx': resolution.get('x', 2048),
                    'ny': resolution.get('y', 512),
                    'nz': resolution.get('z', 1536)
                })
            
            # æ™‚é–“åƒæ•¸
            domain_config.update({
                'time_range': jhtdb_config.get('time_range', [0.0, 26.0]),
                'dt': jhtdb_config.get('dt', 0.0065)
            })
        
        # å¾ physics æ®µè½æå–ç‰©ç†åƒæ•¸
        if 'physics' in self.config:
            physics_config = self.config['physics']
            domain_config.update({
                'Re_tau': physics_config.get('Re_tau', 1000),
                'nu': physics_config.get('nu', 1e-3),
                'u_tau': physics_config.get('u_tau', 1.0),
                'rho': physics_config.get('rho', 1.0)
            })
        
        # 2D åˆ‡ç‰‡é…ç½® (å¦‚æœå¯ç”¨)
        if 'data' in self.config and 'slice_config' in self.config['data']:
            slice_config = self.config['data']['slice_config']
            domain_config.update({
                'slice_plane': slice_config.get('plane', 'xy'),
                'slice_position': slice_config.get('z_position', 4.71),
                'steady_state': slice_config.get('steady_state', True)
            })
        
        return domain_config

    def _build_domain_spec(self, domain_config: Dict[str, Any]) -> DomainSpec:
        bounds: Dict[str, Tuple[float, float]] = {}
        for axis in ('x', 'y', 'z', 't'):
            key = f"{axis}_range"
            if key in domain_config:
                rng = domain_config[key]
                bounds[axis] = (float(rng[0]), float(rng[1]))

        resolution: Dict[str, int] = {}
        for axis_key, axis_name in (('nx', 'x'), ('ny', 'y'), ('nz', 'z')):
            if axis_key in domain_config:
                resolution[axis_name] = int(domain_config[axis_key])

        parameters = {
            key: float(domain_config[key])
            for key in ('Re_tau', 'nu', 'u_tau', 'rho', 'pressure_gradient')
            if key in domain_config
        }

        time_range = None
        if 'time_range' in domain_config:
            rng = domain_config['time_range']
            time_range = (float(rng[0]), float(rng[1]))

        return DomainSpec(
            bounds=bounds,
            parameters=parameters,
            resolution=resolution,
            time_range=time_range
        )

    def _extract_coordinate_info(self, data) -> Dict[str, Any]:
        """å¾ NPZ è³‡æ–™æå–åº§æ¨™è³‡è¨Š"""
        coord_info = {}
        
        # æå–åº§æ¨™é™£åˆ— (å¦‚æœå¯ç”¨)
        if 'x_coords' in data:
            coord_info['x_coords'] = data['x_coords']
        if 'y_coords' in data:
            coord_info['y_coords'] = data['y_coords']
        
        # æå–ç¶²æ ¼è³‡è¨Š
        for key in ['nx', 'ny', 'x_range', 'y_range']:
            if key in data:
                coord_info[key] = data[key]
        
        return coord_info
    
    def _compute_statistics(self, 
                          sensor_data: Dict[str, np.ndarray],
                          sensor_points: np.ndarray) -> Dict[str, Dict[str, float]]:
        """
        è¨ˆç®—å ´è³‡æ–™çš„çµ±è¨ˆè³‡è¨Šï¼ˆç”¨æ–¼ VS-PINN èˆ‡è‡ªå‹•è¼¸å‡ºç¯„åœï¼‰
        
        Args:
            sensor_data: æ„Ÿæ¸¬å™¨å ´è³‡æ–™å­—å…¸ {'u': array, 'v': array, 'p': array}
            sensor_points: æ„Ÿæ¸¬é»åº§æ¨™ (K, 2)
            
        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                'u': {'min': float, 'max': float, 'mean': float, 'std': float, 'range': (min, max)},
                'v': {...},
                'p': {...},
                'x': {'min': float, 'max': float, 'range': (min, max)},
                'y': {...}
            }
        """
        statistics = {}
        
        # è¨ˆç®—å ´è®Šé‡çš„çµ±è¨ˆè³‡è¨Š
        for field_name, field_values in sensor_data.items():
            field_values = np.asarray(field_values).flatten()
            
            # åŸºæœ¬çµ±è¨ˆé‡
            field_min = float(np.min(field_values))
            field_max = float(np.max(field_values))
            field_mean = float(np.mean(field_values))
            field_std = float(np.std(field_values))
            
            # æ·»åŠ å®‰å…¨é‚Šç•Œï¼ˆÂ±10% ç¯„åœï¼Œé¿å…é‚Šç•Œå€¼è¢«æˆªæ–·ï¼‰
            margin = 0.1 * (field_max - field_min)
            safe_min = field_min - margin
            safe_max = field_max + margin
            
            statistics[field_name] = {
                'min': field_min,
                'max': field_max,
                'mean': field_mean,
                'std': field_std,
                'range': (safe_min, safe_max),  # å¸¶å®‰å…¨é‚Šç•Œçš„ç¯„åœ
                'raw_range': (field_min, field_max)  # åŸå§‹ç¯„åœ
            }
        
        # è¨ˆç®—åº§æ¨™çš„çµ±è¨ˆè³‡è¨Šï¼ˆè‡ªå‹•æª¢æ¸¬ç¶­åº¦ï¼‰
        if sensor_points.size > 0:
            coord_names = ['x', 'y', 'z'][:sensor_points.shape[1]]  # æ ¹æ“šå¯¦éš›ç¶­åº¦
            for i, coord_name in enumerate(coord_names):
                coord_values = sensor_points[:, i]
                coord_min = float(np.min(coord_values))
                coord_max = float(np.max(coord_values))
                
                statistics[coord_name] = {
                    'min': coord_min,
                    'max': coord_max,
                    'range': (coord_min, coord_max)
                }
        
        logger.debug(f"Computed statistics: {statistics}")
        return statistics


    def _load_rans_prior(self) -> LowFiData:
        """è¼‰å…¥çœŸå¯¦ RANS å…ˆé©—è³‡æ–™"""
        # å°‹æ‰¾ RANS è³‡æ–™æª”æ¡ˆ
        rans_patterns = ['rans_data.npz', 'lowfi_prior.npz', 'rans_baseline.nc']
        
        for pattern in rans_patterns:
            rans_path = self.cache_dir / pattern
            if rans_path.exists():
                logger.info(f"Loading RANS prior from {rans_path}")
                return self.lowfi_loader.load(rans_path, data_type='rans')
        
        # ç„¡æ³•æ‰¾åˆ°RANSè³‡æ–™ï¼Œç›´æ¥æ‹‹å‡ºéŒ¯èª¤
        raise FileNotFoundError(
            f"No RANS prior data found in {self.cache_dir}. "
            f"Searched for: {rans_patterns}. "
            f"Mock fallback has been removed for this system."
        )
    
    def _create_mock_prior(self, channel_data: ChannelFlowData) -> LowFiData:
        """
        å‰µå»ºç°¡åŒ–çš„ mock å…ˆé©—è³‡æ–™ (åŸºæ–¼å±¤æµè§£æˆ–çµ±è¨ˆé‡ä¼°è¨ˆ)
        ç”¨æ–¼æ¸¬è©¦æˆ–ç¼ºå°‘çœŸå¯¦ RANS æ™‚çš„æš«æ™‚æ›¿ä»£æ–¹æ¡ˆ
        
        Args:
            channel_data: ç¾æœ‰çš„ Channel Flow è³‡æ–™ (ç”¨æ–¼æå–å¹¾ä½•è³‡è¨Š)
            
        Returns:
            Mock ä½ä¿çœŸè³‡æ–™å®¹å™¨
        """
        import numpy as np
        from pinnx.dataio.lowfi_loader import LowFiData
        
        logger.info("Creating mock low-fidelity prior based on laminar solution")
        
        # æå–å¹¾ä½•è³‡è¨Š
        y_range = channel_data.domain_config.get('y_range', [-1.0, 1.0])
        
        # å¾æ„Ÿæ¸¬é»åº§æ¨™å‰µå»ºå ´
        coords = channel_data.sensor_points  # (K, 2)
        x_coords = coords[:, 0]
        y_coords = coords[:, 1]
        
        # å±¤æµé€šé“æµè§£æè§£: u = U_max * (1 - (y/h)^2), v = 0, p ç·šæ€§åˆ†ä½ˆ
        h = (y_range[1] - y_range[0]) / 2.0  # åŠé«˜åº¦
        y_center = (y_range[1] + y_range[0]) / 2.0
        y_norm = (y_coords - y_center) / h  # æ¨™æº–åŒ–åˆ° [-1, 1]
        
        # åŸºæ–¼ Poiseuille æµçš„é€Ÿåº¦åˆ†ä½ˆ
        u_max = 1.5  # å¹³å‡é€Ÿåº¦çš„1.5å€ï¼ˆå±¤æµæ‹‹ç‰©ç·šå‹æœ€å¤§å€¼ï¼‰
        u_laminar = u_max * (1.0 - y_norm**2)
        v_laminar = np.zeros_like(u_laminar)
        
        # ç°¡åŒ–çš„å£“åŠ›å ´ï¼ˆç·šæ€§ä¸‹é™ï¼‰
        p_gradient = -1.0  # å¾é…ç½®è®€å–
        p_laminar = p_gradient * x_coords
        
        # å‰µå»º LowFiData å®¹å™¨ (coordinates éœ€è¦å­—å…¸æ ¼å¼)
        mock_coords = {
            'x': x_coords,
            'y': y_coords
        }
        
        mock_fields = {
            'u': u_laminar,
            'v': v_laminar,
            'p': p_laminar
        }
        
        mock_metadata = {
            'type': 'mock_laminar',
            'description': 'Analytical laminar channel flow solution',
            'u_max': u_max,
            'pressure_gradient': p_gradient
        }
        
        return LowFiData(
            coordinates=mock_coords,
            fields=mock_fields,
            metadata=mock_metadata
        )
    
    def get_available_datasets(self) -> List[str]:
        """ç²å–å¯ç”¨çš„è³‡æ–™é›†åˆ—è¡¨"""
        available = []
        
        if self.cache_dir.exists():
            for npz_file in self.cache_dir.glob("sensors_K*_*.npz"):
                parts = npz_file.stem.split('_')
                if len(parts) >= 3:
                    K = parts[1][1:]  # ç§»é™¤ 'K' å‰ç¶´
                    strategy = '_'.join(parts[2:])
                    available.append(f"K{K}_{strategy}")
        
        return sorted(available)
    
    def validate_data(self, channel_data: ChannelFlowData) -> Dict[str, bool]:
        """é©—è­‰è³‡æ–™å®Œæ•´æ€§å’Œç‰©ç†åˆç†æ€§"""
        checks = {}
        
        # åŸºæœ¬çµæ§‹æª¢æŸ¥
        checks['has_sensor_points'] = len(channel_data.sensor_points) > 0
        checks['has_sensor_data'] = len(channel_data.sensor_data) > 0
        checks['has_domain_config'] = len(channel_data.domain_config) > 0
        
        # è³‡æ–™ç¶­åº¦ä¸€è‡´æ€§
        if channel_data.sensor_points.size > 0:
            n_points = len(channel_data.sensor_points)
            for field, values in channel_data.sensor_data.items():
                checks[f'{field}_dimension_match'] = len(values) == n_points
        
        # ç‰©ç†åˆç†æ€§
        for field, values in channel_data.sensor_data.items():
            checks[f'{field}_finite'] = np.all(np.isfinite(values))
            if field in ['u', 'v']:
                # èª¿æ•´ Channel Flow Re1000 çš„åˆç†ç¯„åœ
                max_reasonable = 30.0 if field == 'u' else 5.0  # uå¯é”25+, vè¼ƒå°
                checks[f'{field}_reasonable'] = np.abs(values).max() < max_reasonable
        
        # åŸŸåƒæ•¸åˆç†æ€§
        domain = channel_data.domain_config
        if 'Re_tau' in domain:
            checks['Re_tau_reasonable'] = 100 <= domain['Re_tau'] <= 10000
        if 'nu' in domain:
            checks['nu_positive'] = domain['nu'] > 0
        
        # ä½ä¿çœŸå…ˆé©—æª¢æŸ¥ (å¦‚æœæœ‰)
        if channel_data.has_lowfi_prior() and channel_data.lowfi_prior:
            checks['lowfi_prior_available'] = True
            for field in ['u', 'v', 'p']:
                if field in channel_data.lowfi_prior.values:
                    values = channel_data.lowfi_prior.values[field]
                    checks[f'lowfi_{field}_finite'] = np.all(np.isfinite(values))
        
        # çµ±è¨ˆè³‡è¨Šæª¢æŸ¥
        if channel_data.statistics:
            checks['statistics_available'] = True
        
        return checks
    
    def load_full_field_data(self,
                           noise_sigma: Optional[float] = None) -> StructuredField:
        """
        è¼‰å…¥å®Œæ•´æµå ´æ•¸æ“šä¸¦è¿”å›çµ±ä¸€çš„ StructuredField ç‰©ä»¶ã€‚
        """
        cutout_file = self.cache_dir / "cutout_128x64_with_w.npz"
        if not cutout_file.exists():
            raise FileNotFoundError(
                f"Expected high-fidelity cutout at {cutout_file}. "
                "Regenerate 2D cutout data with scripts/fetch_channel_flow.py."
            )

        logger.info(f"Loading full field data from {cutout_file}")
        data = np.load(cutout_file, allow_pickle=True)

        fields = {
            'u': np.asarray(data['u']),
            'v': np.asarray(data['v']),
            'w': np.asarray(data['w']),
            'p': np.asarray(data['p'])
        }

        if noise_sigma is not None and noise_sigma > 0:
            noisy = self._add_noise({k: v.reshape(-1) for k, v in fields.items()}, noise_sigma)
            for key, arr in noisy.items():
                fields[key] = arr.reshape(fields[key].shape)

        if 'coordinates' not in data:
            raise KeyError("cutout_128x64_with_w.npz must include structured 'coordinates'")
        coordinates_obj = data['coordinates'].item()
        if not isinstance(coordinates_obj, dict):
            raise TypeError("coordinates metadata must be provided as a dictionary")
        if 'x' in coordinates_obj and 'y' in coordinates_obj:
            x_axis = np.asarray(coordinates_obj['x'])
            y_axis = np.asarray(coordinates_obj['y'])
        elif 'X' in coordinates_obj and 'Y' in coordinates_obj:
            X = np.asarray(coordinates_obj['X'])
            Y = np.asarray(coordinates_obj['Y'])
            x_axis = X[:, 0]
            y_axis = Y[0, :]
        else:
            raise KeyError(f"Unsupported coordinate keys: {list(coordinates_obj.keys())}")

        grid = StructuredGrid.from_axes({'x': x_axis, 'y': y_axis})
        stats_input = {k: v.reshape(-1) for k, v in fields.items()}
        statistics = self._compute_statistics(stats_input, grid.to_points(order=('x', 'y')))

        return StructuredField(
            grid=grid,
            fields=fields,
            metadata={
                'source': str(cutout_file),
                'config_file': str(self.config_path),
                'loader_version': '2.0',
                'noise_sigma': noise_sigma,
                'statistics': statistics
            }
        )


# ä¾¿åˆ©å‡½æ•¸
def load_channel_flow_data(strategy: str = 'qr_pivot',
                          K: int = 8,
                          config_path: Optional[Union[str, Path]] = None,
                          with_lowfi_prior: bool = True,
                          prior_type: str = 'rans') -> ChannelFlowData:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šè¼‰å…¥ Channel Flow è³‡æ–™
    
    Args:
        strategy: æ„Ÿæ¸¬é»é¸æ“‡ç­–ç•¥
        K: æ„Ÿæ¸¬é»æ•¸é‡  
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘
        with_lowfi_prior: æ˜¯å¦è¼‰å…¥ä½ä¿çœŸå…ˆé©—
        prior_type: å…ˆé©—é¡å‹
        
    Returns:
        Channel Flow è³‡æ–™å®¹å™¨
    """
    loader = ChannelFlowLoader(config_path=config_path)
    
    # è¼‰å…¥æ„Ÿæ¸¬é»è³‡æ–™
    channel_data = loader.load_sensor_data(strategy=strategy, K=K)
    
    # æ·»åŠ ä½ä¿çœŸå…ˆé©— (å¦‚æœéœ€è¦)
    if with_lowfi_prior:
        channel_data = loader.add_lowfi_prior(channel_data, prior_type=prior_type)
    
    return channel_data


def prepare_training_data(strategy: str = 'qr_pivot',
                         K: int = 8, 
                         config_path: Optional[Union[str, Path]] = None,
                         target_fields: Optional[List[str]] = None,
                         sensor_file: Optional[str] = None,
                         prior_type: str = 'none') -> FlowDataBundle:
    """
    ä¾¿åˆ©å‡½æ•¸ï¼šæº–å‚™ PINNs è¨“ç·´è³‡æ–™
    
    Args:
        strategy: æ„Ÿæ¸¬é»é¸æ“‡ç­–ç•¥
        K: æ„Ÿæ¸¬é»æ•¸é‡
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘  
        target_fields: ç›®æ¨™å ´åˆ—è¡¨
        sensor_file: è‡ªå®šç¾©æ„Ÿæ¸¬é»æ–‡ä»¶å (å¯é¸)
        prior_type: ä½ä¿çœŸå…ˆé©—é¡å‹ ('rans', 'mock', 'none')ï¼Œé è¨­ 'none'
        
    Returns:
        FlowDataBundle: æº–å‚™å¥½çš„è¨“ç·´è³‡æ–™å®¹å™¨
        
    Note:
        âš ï¸ prior_type='none' æ„å‘³è‘—åƒ…ä½¿ç”¨æ„Ÿæ¸¬é»æ•¸æ“šï¼Œä¸æ·»åŠ ä½ä¿çœŸå…ˆé©—
        é€™æ˜¯æ¨è–¦çš„é è¨­å€¼ï¼Œé¿å…è¦†è“‹çœŸå¯¦ JHTDB æ•¸æ“š
    """
    loader = ChannelFlowLoader(config_path=config_path)
    
    # è¼‰å…¥å®Œæ•´è³‡æ–™
    channel_data = loader.load_sensor_data(strategy=strategy, K=K, sensor_file=sensor_file)
    
    # åƒ…åœ¨æ˜ç¢ºè¦æ±‚æ™‚æ‰æ·»åŠ ä½ä¿çœŸå…ˆé©—
    if prior_type != 'none':
        channel_data = loader.add_lowfi_prior(channel_data, prior_type=prior_type)
    
    # æº–å‚™è¨“ç·´æ ¼å¼
    training_data = loader.prepare_for_training(channel_data, target_fields=target_fields)
    
    return training_data
