"""
æ¨¡å‹/ç‰©ç†/å„ªåŒ–å™¨å·¥å» æ¨¡çµ„

æœ¬æ¨¡çµ„æä¾›çµ±ä¸€çš„å·¥å» å‡½æ•¸ï¼Œç”¨æ–¼å‰µå»º PINN è¨“ç·´æ‰€éœ€çš„æ ¸å¿ƒçµ„ä»¶ï¼š
- è¨­å‚™é¸æ“‡ï¼ˆCUDA/MPS/CPUï¼‰
- æ¨¡å‹æ¶æ§‹ï¼ˆPINN/Enhanced/VS-PINNï¼‰
- ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆNS 2D/3Dã€VS-PINNï¼‰
- å„ªåŒ–å™¨èˆ‡å­¸ç¿’ç‡èª¿åº¦å™¨

æ‰€æœ‰å‡½æ•¸å‡æä¾›å®Œæ•´çš„éŒ¯èª¤è™•ç†ã€é¡å‹æç¤ºèˆ‡é…ç½®é©—è­‰ã€‚
"""

import logging
from typing import Any, Dict, Optional, Tuple, Union

import torch
import torch.nn as nn

from pinnx.models import PINNNet, create_pinn_model, init_siren_weights
from pinnx.physics.ns_2d import NSEquations2D
from pinnx.physics.vs_pinn_channel_flow import create_vs_pinn_channel_flow


# ============================================================================
# è¨­å‚™é¸æ“‡
# ============================================================================

def get_device(device_name: str) -> torch.device:
    """
    ç²å–é‹ç®—è¨­å‚™ï¼ˆæ”¯æ´è‡ªå‹•é¸æ“‡èˆ‡æ‰‹å‹•æŒ‡å®šï¼‰
    
    Args:
        device_name: è¨­å‚™åç¨±ï¼Œæ”¯æ´ï¼š
            - "auto": è‡ªå‹•é¸æ“‡æœ€ä½³å¯ç”¨è¨­å‚™ï¼ˆCUDA > MPS > CPUï¼‰
            - "cuda": NVIDIA GPUï¼ˆéœ€ CUDA å¯ç”¨ï¼‰
            - "mps": Apple Silicon GPUï¼ˆéœ€ MPS å¯ç”¨ï¼‰
            - "cpu": CPU é‹ç®—
    
    Returns:
        torch.device: PyTorch è¨­å‚™ç‰©ä»¶
    
    Raises:
        ValueError: è‹¥æŒ‡å®šè¨­å‚™åç¨±ç„¡æ•ˆ
    
    Examples:
        >>> device = get_device("auto")
        >>> device = get_device("cuda")  # è‹¥ CUDA ä¸å¯ç”¨æœƒå›é€€åˆ° CPU
    """
    valid_devices = ["auto", "cuda", "mps", "cpu"]
    if device_name not in valid_devices:
        raise ValueError(
            f"Invalid device name '{device_name}'. "
            f"Must be one of: {valid_devices}"
        )
    
    if device_name == "auto":
        # è‡ªå‹•é¸æ“‡æœ€ä½³å¯ç”¨è¨­å‚™
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logging.info(f"Auto-selected CUDA: {torch.cuda.get_device_name()}")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            logging.info("Auto-selected Apple Metal Performance Shaders")
        else:
            device = torch.device("cpu")
            logging.info("Auto-selected CPU (no GPU available)")
    
    elif device_name == "cuda":
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logging.info(f"Using CUDA: {torch.cuda.get_device_name()}")
        else:
            logging.warning("CUDA requested but not available, falling back to CPU")
            device = torch.device("cpu")
    
    elif device_name == "mps":
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            logging.info("Using Apple Metal Performance Shaders")
        else:
            logging.warning("MPS requested but not available, falling back to CPU")
            device = torch.device("cpu")
    
    else:  # cpu
        device = torch.device("cpu")
        logging.info("Using CPU")
    
    return device


# ============================================================================
# æ¨¡å‹å‰µå»º
# ============================================================================

def _create_axis_selective_model(
    model_cfg: Dict[str, Any],
    config: Dict[str, Any],
    device: torch.device
) -> nn.Module:
    """
    å‰µå»ºä½¿ç”¨è»¸å‘é¸æ“‡æ€§ Fourier Features çš„ PINN æ¨¡å‹
    
    Args:
        model_cfg: æ¨¡å‹é…ç½®ï¼ˆåŒ…å« fourier_features ç­‰ï¼‰
        config: å®Œæ•´é…ç½®ï¼ˆç”¨æ–¼æå–åŸŸé…ç½®ï¼‰
        device: è¨ˆç®—è¨­å‚™
    
    Returns:
        AxisSelectiveFourierMLP å¯¦ä¾‹
    
    Raises:
        ImportError: è‹¥è»¸å‘é¸æ“‡æ€§æ¨¡çµ„æœªæ‰¾åˆ°
        ValueError: è‹¥é…ç½®åƒæ•¸ç„¡æ•ˆ
    
    Notes:
        è‹¥å•Ÿç”¨ Fourier é€€ç«ï¼Œæ¨¡å‹å°‡ä½¿ç”¨**åˆå§‹éšæ®µé »ç‡**å‰µå»ºï¼Œ
        é¿å…è¨“ç·´é–‹å§‹æ™‚ç¶­åº¦ä¸åŒ¹é…ï¼ˆTASK-007 ä¿®å¾©ï¼‰
    """
    # å°å…¥è»¸å‘é¸æ“‡æ€§ Fourier æ¨¡çµ„
    try:
        from pinnx.models.axis_selective_fourier import AxisSelectiveFourierFeatures
    except ImportError as exc:
        raise ImportError(
            "Cannot create axis_selective_fourier_mlp: "
            "pinnx.models.axis_selective_fourier module not found"
        ) from exc
    
    # æå– Fourier Features é…ç½®
    fourier_cfg = model_cfg.get('fourier_features', {})
    if not fourier_cfg or fourier_cfg.get('type') != 'axis_selective':
        raise ValueError(
            "axis_selective_fourier_mlp requires 'fourier_features' config "
            "with 'type': 'axis_selective'"
        )
    
    axes_config = fourier_cfg.get('axes_config')
    if not axes_config:
        raise ValueError(
            "axis_selective Fourier requires 'axes_config' in fourier_features"
        )
    
    # ğŸ”§ TASK-007 Phase 2 ä¿®å¾©ï¼šé›™é…ç½®æ©Ÿåˆ¶
    # ç­–ç•¥ï¼š
    # 1. æ¨¡å‹å§‹çµ‚ä½¿ç”¨ full_axes_config åˆå§‹åŒ–ï¼ˆå›ºå®šæœ€å¤§ç¶­åº¦ï¼‰
    # 2. è‹¥å•Ÿç”¨é€€ç«ï¼Œå‚³é initial_axes_config ä½œç‚ºç•¶å‰é…ç½®
    # 3. forward() æ™‚æ ¹æ“šç•¶å‰é…ç½®æ‡‰ç”¨æ©ç¢¼ï¼ˆç½®é›¶æœªå•Ÿç”¨é »ç‡ï¼‰
    full_axes_config = axes_config  # ä¿å­˜å®Œæ•´é…ç½®
    current_axes_config = axes_config  # é è¨­ï¼šç•¶å‰é…ç½® = å®Œæ•´é…ç½®
    
    annealing_cfg = config.get('fourier_annealing', {})
    if annealing_cfg.get('enabled', False):
        try:
            from pinnx.train.fourier_annealing import (  # type: ignore
                create_channel_flow_annealing, 
                create_default_annealing
            )
        except ImportError as exc:
            raise ImportError(
                "Fourier annealing enabled but pinnx.train.fourier_annealing module not found"
            ) from exc
        
        strategy = annealing_cfg.get('strategy', 'channel_flow')
        initial_axes_config: Dict[str, list] = {}
        
        if strategy == 'channel_flow':
            # é€šé“æµå°ˆç”¨é…ç½®ï¼ˆæ¯è»¸ç¨ç«‹éšæ®µï¼‰
            per_axis_stages = create_channel_flow_annealing()
            
            # æå–åˆå§‹éšæ®µé »ç‡ï¼ˆend_ratio æœ€å°çš„éšæ®µï¼‰
            for axis, stages in per_axis_stages.items():
                if stages:
                    initial_stage = min(stages, key=lambda s: s.end_ratio)
                    initial_axes_config[axis] = initial_stage.frequencies
            
            logging.info("ğŸ”§ Fourier é€€ç«å•Ÿç”¨ï¼šä½¿ç”¨é€šé“æµåˆå§‹éšæ®µé »ç‡")
            logging.info(f"   åˆå§‹é »ç‡ï¼ˆç•¶å‰å•Ÿç”¨ï¼‰: {initial_axes_config}")
            logging.info(f"   å®Œæ•´é »ç‡ï¼ˆæ¨¡å‹å®¹é‡ï¼‰: {full_axes_config}")
            
        else:
            # é€šç”¨ç­–ç•¥ï¼ˆå…¨å±€é…ç½®ï¼‰
            global_stages = create_default_annealing(strategy)
            if global_stages:
                initial_stage = min(global_stages, key=lambda s: s.end_ratio)
                initial_freqs = initial_stage.frequencies
                
                # æ‡‰ç”¨æ–¼æ‰€æœ‰è»¸
                initial_axes_config = {axis: initial_freqs for axis in axes_config.keys()}
                
                logging.info(f"ğŸ”§ Fourier é€€ç«å•Ÿç”¨ï¼šä½¿ç”¨ '{strategy}' ç­–ç•¥åˆå§‹éšæ®µé »ç‡")
                logging.info(f"   åˆå§‹é »ç‡: {initial_freqs}")
        
        # è¨­å®šç•¶å‰é…ç½®ï¼ˆè¨“ç·´é–‹å§‹æ™‚çš„å•Ÿç”¨é »ç‡ï¼‰
        if initial_axes_config:
            current_axes_config = initial_axes_config
            logging.info("âœ… å°‡ä½¿ç”¨åˆå§‹éšæ®µé »ç‡ä½œç‚ºç•¶å‰é…ç½®ï¼ˆé€šéæ©ç¢¼æ§åˆ¶ï¼‰")
        else:
            logging.warning("âš ï¸  ç„¡æ³•æå–åˆå§‹éšæ®µé »ç‡ï¼Œä½¿ç”¨å®Œæ•´é…ç½®")
    else:
        logging.info("â„¹ï¸  Fourier é€€ç«æœªå•Ÿç”¨ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å®Œæ•´é »ç‡")
    
    # å¾ç‰©ç†åŸŸé…ç½®æå–åŸŸé•·åº¦
    domain_cfg = config.get('physics', {}).get('domain', {})
    domain_lengths = None
    if domain_cfg:
        axes_names = list(axes_config.keys())
        domain_lengths = {}
        if 'x_range' in domain_cfg and 'x' in axes_names:
            x_range = domain_cfg['x_range']
            domain_lengths['x'] = x_range[1] - x_range[0]
        if 'y_range' in domain_cfg and 'y' in axes_names:
            y_range = domain_cfg['y_range']
            domain_lengths['y'] = y_range[1] - y_range[0]
        if 'z_range' in domain_cfg and 'z' in axes_names:
            z_range = domain_cfg['z_range']
            domain_lengths['z'] = z_range[1] - z_range[0]
        
        logging.info(f"  åŸŸé•·åº¦: {domain_lengths}")
    
    # å‰µå»ºè»¸å‘é¸æ“‡æ€§ Fourier Features
    # ğŸ”§ TASK-007 Phase 2ï¼šé›™é…ç½®æ©Ÿåˆ¶
    # - full_axes_configï¼šå®Œæ•´é »ç‡ï¼ˆç”¨æ–¼æ§‹å»ºå›ºå®šå¤§å°çš„ B çŸ©é™£ï¼‰
    # - current_axes_configï¼šç•¶å‰å•Ÿç”¨é »ç‡ï¼ˆç”¨æ–¼æ©ç¢¼æ§åˆ¶ï¼‰
    trainable = fourier_cfg.get('trainable', False)
    fourier_features = AxisSelectiveFourierFeatures(
        axes_config=current_axes_config,     # ç•¶å‰å•Ÿç”¨é »ç‡
        domain_lengths=domain_lengths,
        trainable=trainable,
        full_axes_config=full_axes_config    # å®Œæ•´é »ç‡ï¼ˆå›ºå®šç¶­åº¦ï¼‰
    )
    
    # æ§‹å»ºå®Œæ•´ç¶²è·¯ï¼ˆFourier + MLPï¼‰
    # ä½¿ç”¨èˆ‡ PINNNet ç›¸åŒçš„ MLP æ¶æ§‹
    from pinnx.models.fourier_mlp import DenseLayer
    
    width = model_cfg['width']
    depth = model_cfg['depth']
    out_dim = model_cfg['out_dim']
    activation = model_cfg['activation']
    
    use_residual = model_cfg.get('use_residual', False)
    use_layer_norm = model_cfg.get('use_layer_norm', False)
    dropout = model_cfg.get('dropout', 0.0)
    use_rwf = model_cfg.get('use_rwf', False)
    rwf_scale_std = model_cfg.get('rwf_scale_std', 0.1)
    sine_omega_0 = model_cfg.get('sine_omega_0', 1.0)
    
    # å‰µå»ºä¸€å€‹åŒ…è£æ¨¡å‹
    class AxisSelectiveFourierMLP(nn.Module):
        """è»¸å‘é¸æ“‡æ€§ Fourier MLP æ¨¡å‹"""
        def __init__(self):
            super().__init__()
            self.fourier_features = fourier_features
            
            # è¨ˆç®— MLP è¼¸å…¥ç¶­åº¦ï¼ˆä¾†è‡ª Fourier Featuresï¼‰
            input_features = fourier_features.out_dim
            
            # éš±è—å±¤
            layers = []
            for i in range(depth):
                layer_in_dim = input_features if i == 0 else width
                
                layers.append(DenseLayer(
                    layer_in_dim, width,
                    activation=activation,
                    use_residual=use_residual and i > 0,
                    use_layer_norm=use_layer_norm,
                    dropout=dropout,
                    use_rwf=use_rwf,
                    rwf_scale_std=rwf_scale_std,
                    sine_omega_0=sine_omega_0
                ))
            
            self.hidden_layers = nn.ModuleList(layers)
            
            # è¼¸å‡ºå±¤
            self.output_layer = nn.Linear(width, out_dim)
            output_gain = 0.01 if use_residual else 0.1
            nn.init.xavier_normal_(self.output_layer.weight, gain=output_gain)
            nn.init.zeros_(self.output_layer.bias)
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            # Fourier ç‰¹å¾µç·¨ç¢¼
            h = self.fourier_features(x)
            
            # ğŸ¯ DEBUG: å°å‡º Fourier Features è¼¸å‡ºå½¢ç‹€
            if not hasattr(self, '_debug_printed'):
                logging.info(f"ğŸ” [DEBUG] Fourier Features è¼¸å‡ºå½¢ç‹€: {h.shape}")
                logging.info(f"ğŸ” [DEBUG] è¼¸å…¥å½¢ç‹€: {x.shape}")
                logging.info(f"ğŸ” [DEBUG] Fourier out_dim: {self.fourier_features.out_dim}")
                logging.info(f"ğŸ” [DEBUG] B çŸ©é™£å½¢ç‹€: {self.fourier_features.B.shape}")
                self._debug_printed = True
            
            # MLP å±¤
            for layer in self.hidden_layers:
                h = layer(h)
            
            # è¼¸å‡º
            return self.output_layer(h)
    
    model = AxisSelectiveFourierMLP().to(device)
    
    # æ—¥èªŒè¼¸å‡º
    num_params = sum(p.numel() for p in model.parameters())
    logging.info(f"  Fourier è¼¸å‡ºç¶­åº¦: {fourier_features.out_dim}")
    logging.info(f"  MLP çµæ§‹: {depth}Ã—{width}, æ¿€æ´»: {activation}")
    logging.info(f"  ç¸½åƒæ•¸é‡: {num_params:,}")
    
    return model


def create_model(
    config: Dict[str, Any],
    device: torch.device,
    statistics: Optional[Dict[str, Dict[str, float]]] = None
) -> nn.Module:
    """
    å»ºç«‹ PINN æ¨¡å‹ï¼ˆæ”¯æ´ Fourier featuresã€VS-PINN ç¸®æ”¾ã€æ‰‹å‹•æ¨™æº–åŒ–ã€è»¸å‘é¸æ“‡æ€§ Fourierï¼‰
    
    Args:
        config: å®Œæ•´é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - model: æ¨¡å‹é…ç½®ï¼ˆtype, in_dim, out_dim, width, depth, activationï¼‰
            - physics: ç‰©ç†é…ç½®ï¼ˆtype, domain, vs_pinnï¼‰
            - [å¯é¸] scaling: æ¨™æº–åŒ–é…ç½®ï¼ˆoutput_normï¼‰
        device: è¨ˆç®—è¨­å‚™
        statistics: [å¯é¸] è³‡æ–™çµ±è¨ˆè³‡è¨Šï¼ˆç”¨æ–¼è‡ªå‹•è¨­å®šè¼¸å‡ºç¯„åœï¼‰
    
    Returns:
        nn.Module: å·²åˆå§‹åŒ–ä¸¦ç§»è‡³ç›®æ¨™è¨­å‚™çš„ PINN æ¨¡å‹
    
    Raises:
        KeyError: è‹¥é…ç½®ç¼ºå°‘å¿…è¦æ¬„ä½
        ValueError: è‹¥é…ç½®åƒæ•¸ç„¡æ•ˆï¼ˆå¦‚è¼¸å…¥ç¯„åœä¸åŒ¹é…åŸŸé…ç½®ï¼‰
        ImportError: è‹¥ç¼ºå°‘å¿…è¦çš„æ¨¡çµ„ï¼ˆå¦‚ ManualScalingWrapperï¼‰
    
    Notes:
        - è‹¥ activation='sine'ï¼Œè‡ªå‹•æ‡‰ç”¨ SIREN æ¬Šé‡åˆå§‹åŒ–
        - VS-PINN æ¨¡å¼ä¸‹æœƒè·³é ManualScalingWrapperï¼ˆé¿å…é›™é‡æ¨™æº–åŒ–ï¼‰
        - æ”¯æ´ Fourier features æ¶ˆèå¯¦é©—ï¼ˆuse_fourier é–‹é—œï¼‰
        - æ”¯æ´è»¸å‘é¸æ“‡æ€§ Fourier Featuresï¼ˆtype='axis_selective_fourier_mlp'ï¼‰
    
    Examples:
        >>> config = {
        ...     'model': {'type': 'fourier_mlp', 'in_dim': 3, 'out_dim': 4,
        ...               'width': 200, 'depth': 8, 'activation': 'sine'},
        ...     'physics': {'type': 'vs_pinn_channel_flow', 'domain': {...}}
        ... }
        >>> model = create_model(config, device)
    """
    model_cfg = config.get('model')
    if not model_cfg:
        raise KeyError("Config missing required key: 'model'")
    
    # é©—è­‰å¿…è¦æ¬„ä½
    required_fields = ['in_dim', 'out_dim', 'width', 'depth', 'activation']
    missing = [f for f in required_fields if f not in model_cfg]
    if missing:
        raise KeyError(f"Model config missing required fields: {missing}")
    
    # === 1. Fourier Features é…ç½® ===
    use_fourier = model_cfg.get('use_fourier', True)  # é»˜èªå•Ÿç”¨
    
    # === 2. VS-PINN ç¸®æ”¾å› å­æå– ===
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_pinn = (physics_type == 'vs_pinn_channel_flow')
    
    input_scale_factors = None
    fourier_normalize_input = False
    
    if is_vs_pinn and use_fourier:
        # VS-PINN æ¨¡å¼ï¼šæå–ç¸®æ”¾å› å­ç”¨æ–¼ Fourier æ¨™æº–åŒ–ä¿®å¾©
        vs_pinn_cfg = config.get('physics', {}).get('vs_pinn', {})
        scaling_cfg = vs_pinn_cfg.get('scaling_factors', {})
        N_x = scaling_cfg.get('N_x', 2.0)
        N_y = scaling_cfg.get('N_y', 12.0)
        N_z = scaling_cfg.get('N_z', 2.0)
        input_scale_factors = torch.tensor([N_x, N_y, N_z], dtype=torch.float32)
        fourier_normalize_input = True
        logging.info(
            f"ğŸ”§ VS-PINN + Fourier ä¿®å¾©å•Ÿç”¨ï¼šç¸®æ”¾å› å­ N=[{N_x}, {N_y}, {N_z}]"
        )
    
    # === 3. å»ºç«‹åŸºç¤æ¨¡å‹ ===
    model_type = model_cfg.get('type', 'fourier_mlp')
    
    if model_type == 'axis_selective_fourier_mlp':
        # è»¸å‘é¸æ“‡æ€§ Fourier MLPï¼ˆç”¨æ–¼é€šé“æµç­‰éå‡å‹»å¹¾ä½•ï¼‰
        base_model = _create_axis_selective_model(
            model_cfg=model_cfg,
            config=config,
            device=device
        )
        logging.info("âœ… Created Axis-Selective Fourier MLP")
    
    elif model_type == 'fourier_vs_mlp':
        # Fourier-VS MLP çµ±ä¸€æ¶æ§‹
        base_model = create_pinn_model(model_cfg).to(device)
        logging.info(f"âœ… Created Fourier-VS MLP (use_fourier={use_fourier})")
    else:
        # åŸºç¤ PINN
        base_model = PINNNet(
            in_dim=model_cfg['in_dim'],
            out_dim=model_cfg['out_dim'],
            width=model_cfg['width'],
            depth=model_cfg['depth'],
            activation=model_cfg['activation'],
            use_fourier=use_fourier,
            fourier_m=model_cfg.get('fourier_m', 32),
            fourier_sigma=model_cfg.get('fourier_sigma', 1.0),
            use_rwf=model_cfg.get('use_rwf', False),
            rwf_scale_std=model_cfg.get('rwf_scale_std', 0.1),
            use_layer_norm=model_cfg.get('use_layer_norm', False),
            use_residual=model_cfg.get('use_residual', False),
            dropout=model_cfg.get('dropout', 0.0),
            sine_omega_0=model_cfg.get('sine_omega_0', 1.0),
            fourier_normalize_input=fourier_normalize_input,
            input_scale_factors=input_scale_factors
        ).to(device)
        logging.info(f"âœ… Created PINNNet (use_fourier={use_fourier}, use_rwf={model_cfg.get('use_rwf', False)})")
    
    # === 4. æ‡‰ç”¨æ‰‹å‹•æ¨™æº–åŒ–åŒ…è£å™¨ï¼ˆè‹¥é…ç½®å•Ÿç”¨ä¸”é VS-PINNï¼‰===
    scaling_cfg = model_cfg.get('scaling', {})
    scaling_enabled = bool(scaling_cfg) and not is_vs_pinn
    
    if scaling_enabled:
        # ä½¿ç”¨æ‰‹å‹•æ¨™æº–åŒ–åŒ…è£å™¨ï¼ˆé¿å… Fourier feature é£½å’Œï¼‰
        try:
            from pinnx.models.wrappers import ManualScalingWrapper
        except ImportError as exc:
            raise ImportError(
                "ManualScalingWrapper not found. "
                "Ensure pinnx.models.wrappers module exists."
            ) from exc
        
        # æå–è¼¸å…¥/è¼¸å‡ºç¯„åœ
        input_scales, output_scales = _extract_scaling_ranges(
            config, statistics, model_cfg
        )
        
        # é©—è­‰è¼¸å…¥ç¯„åœæ˜¯å¦åŒ¹é…åŸŸé…ç½®
        _validate_input_ranges(config, input_scales)
        
        # æ‡‰ç”¨åŒ…è£å™¨
        model = ManualScalingWrapper(
            base_model,
            input_ranges=input_scales,
            output_ranges=output_scales
        ).to(device)
        logging.info(
            f"âœ… Manual scaling wrapper applied:\n"
            f"   Inputs: {input_scales}\n"
            f"   Outputs: {output_scales}"
        )
    else:
        model = base_model
        if is_vs_pinn:
            logging.info("Using base model without scaling (VS-PINN handles scaling)")
        else:
            logging.info("Using base model without scaling")
    
    # === 5. SIREN æ¬Šé‡åˆå§‹åŒ–ï¼ˆè‹¥ä½¿ç”¨ Sine æ¿€æ´»å‡½æ•¸ï¼‰===
    if model_cfg['activation'] == 'sine':
        target_model = base_model
        if hasattr(model, 'model'):  # è‹¥æœ‰åŒ…è£å™¨
            target_model = model.model  # type: ignore
        
        if isinstance(target_model, PINNNet):
            init_siren_weights(target_model)
            logging.info("âœ… Applied SIREN weight initialization for Sine activation")
        else:
            logging.warning(
                f"âš ï¸  Cannot apply SIREN initialization: "
                f"base model type is {type(target_model)}"
            )
    
    # === 6. è¨“ç·´å‰é©—è­‰ï¼ˆè‹¥å•Ÿç”¨æ¨™æº–åŒ–ï¼‰===
    if scaling_enabled and hasattr(model, 'input_min'):
        _verify_model_scaling(model, config)
    
    num_params = sum(p.numel() for p in model.parameters())
    logging.info(f"Created model with {num_params:,} parameters")
    
    return model


def _extract_scaling_ranges(
    config: Dict[str, Any],
    statistics: Optional[Dict[str, Dict[str, float]]],
    model_cfg: Dict[str, Any]
) -> Tuple[Dict[str, Tuple[float, float]], Dict[str, Tuple[float, float]]]:
    """
    æå–è¼¸å…¥/è¼¸å‡ºæ¨™æº–åŒ–ç¯„åœï¼ˆå„ªå…ˆç´šï¼šé…ç½® > çµ±è¨ˆ > ç¡¬ç·¨ç¢¼ï¼‰
    
    Returns:
        (input_scales, output_scales): è¼¸å…¥èˆ‡è¼¸å‡ºçš„ç¯„åœå­—å…¸
    """
    domain_cfg = config.get('physics', {}).get('domain', {})
    scaling_cfg = model_cfg.get('scaling', {})
    
    # === è¼¸å…¥ç¯„åœ ===
    if domain_cfg and 'x_range' in domain_cfg:
        # å„ªå…ˆï¼šå¾é…ç½®æ–‡ä»¶è®€å–å®Œæ•´åŸŸç¯„åœ
        input_x_range = tuple(domain_cfg['x_range'])  # type: ignore
        input_y_range = tuple(domain_cfg['y_range'])  # type: ignore
        logging.info(
            f"âœ… Using domain ranges from config: "
            f"x={input_x_range}, y={input_y_range}"
        )
    elif statistics and 'x' in statistics and 'range' in statistics['x']:
        # å›é€€ï¼šä½¿ç”¨æ„Ÿæ¸¬é»çµ±è¨ˆï¼ˆå¯èƒ½å°è‡´æ³›åŒ–å•é¡Œï¼‰
        input_x_range = tuple(statistics['x']['range'])  # type: ignore
        input_y_range = tuple(statistics['y']['range'])  # type: ignore
        logging.warning(
            f"âš ï¸ Falling back to statistics-based ranges: "
            f"x={input_x_range}, y={input_y_range}"
        )
        logging.warning(
            "âš ï¸ This may cause generalization issues "
            "if sensors don't cover full domain!"
        )
    else:
        # æœ€çµ‚å›é€€ï¼šç¡¬ç·¨ç¢¼ï¼ˆJHTDB Channel Re1000ï¼‰
        input_x_range = (0.0, 25.13)
        input_y_range = (-1.0, 1.0)
        logging.warning(
            f"âš ï¸ Using hardcoded domain ranges: "
            f"x={input_x_range}, y={input_y_range}"
        )
    
    input_scales: Dict[str, Tuple[float, float]] = {
        'x': input_x_range,
        'y': input_y_range
    }
    
    # 3D è‡ªå‹•æª¢æ¸¬
    if domain_cfg and 'z_range' in domain_cfg:
        input_z_range = tuple(domain_cfg['z_range'])  # type: ignore
        input_scales['z'] = input_z_range
        logging.info(f"âœ… 3D mode: z={input_z_range}")
    elif statistics and 'z' in statistics and 'range' in statistics['z']:
        input_z_range = tuple(statistics['z']['range'])  # type: ignore
        input_scales['z'] = input_z_range
        logging.warning(f"âš ï¸ 3D mode using statistics: z={input_z_range}")
    
    # === è¼¸å‡ºç¯„åœ ===
    output_scales: Dict[str, Tuple[float, float]] | None = None
    
    # å„ªå…ˆï¼šå¾é…ç½®è®€å–
    if 'output_norm' in scaling_cfg:
        output_norm_raw = scaling_cfg['output_norm']
        
        if isinstance(output_norm_raw, dict):
            # å­—å…¸æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
            output_scales = {
                'u': tuple(output_norm_raw.get('u', [0.0, 20.0])),  # type: ignore
                'v': tuple(output_norm_raw.get('v', [-1.0, 1.0])),  # type: ignore
                'w': tuple(output_norm_raw.get('w', [-5.0, 5.0])),  # type: ignore
                'p': tuple(output_norm_raw.get('p', [-100.0, 10.0]))  # type: ignore
            }
            logging.info("âœ… Using output ranges from config file (dict format):")
            for key, val in output_scales.items():
                logging.info(f"   {key}: {val}")
        elif isinstance(output_norm_raw, str):
            # å­—ç¬¦ä¸²æ ¼å¼ï¼šå›é€€åˆ°çµ±è¨ˆ
            logging.info(
                f"âš ï¸ output_norm is string '{output_norm_raw}', "
                "falling back to statistics"
            )
    
    # å›é€€ï¼šä½¿ç”¨çµ±è¨ˆè³‡è¨Š
    if output_scales is None:
        if statistics:
            output_u_range = tuple(statistics.get('u', {}).get('range', (0.0, 20.0)))  # type: ignore
            output_v_range = tuple(statistics.get('v', {}).get('range', (-1.0, 1.0)))  # type: ignore
            output_w_range = tuple(statistics.get('w', {}).get('range', (-5.0, 5.0)))  # type: ignore
            output_p_range = tuple(statistics.get('p', {}).get('range', (-100.0, 10.0)))  # type: ignore
            
            output_scales = {
                'u': output_u_range,
                'v': output_v_range,
                'w': output_w_range,
                'p': output_p_range
            }
            logging.info("âœ… Using data-driven output ranges from statistics:")
            for key, val in output_scales.items():
                logging.info(f"   {key}: {val}")
        else:
            # æœ€çµ‚å›é€€ï¼šç¡¬ç·¨ç¢¼
            output_scales = {
                'u': (0.0, 20.0),
                'v': (-1.0, 1.0),
                'w': (-5.0, 5.0),
                'p': (-100.0, 10.0)
            }
            logging.warning(
                "âš ï¸ No statistics or config output_norm provided, "
                "using hardcoded ranges (may cause NaN)"
            )
    
    # è£œå……æºé …ç¯„åœï¼ˆè‹¥éœ€è¦ï¼‰
    expected_out_dim = model_cfg.get('out_dim', 3)
    if expected_out_dim == 5 and len(output_scales) == 4:
        output_scales['S'] = (-1.0, 1.0)
        logging.info("âœ… Added source term 'S' range: (-1.0, 1.0)")
    
    return input_scales, output_scales


def _validate_input_ranges(
    config: Dict[str, Any],
    input_scales: Dict[str, Tuple[float, float]]
) -> None:
    """
    é©—è­‰è¼¸å…¥ç¯„åœæ˜¯å¦èˆ‡åŸŸé…ç½®ä¸€è‡´
    
    Raises:
        ValueError: è‹¥ç¯„åœä¸åŒ¹é…ï¼ˆæœƒå°è‡´æ³›åŒ–å¤±æ•—ï¼‰
    """
    domain_cfg = config.get('physics', {}).get('domain', {})
    if not domain_cfg or 'x_range' not in domain_cfg:
        return  # ç„¡é…ç½®å¯æ¯”å°
    
    expected_x = tuple(domain_cfg['x_range'])
    expected_y = tuple(domain_cfg['y_range'])
    
    actual_x = input_scales.get('x')
    actual_y = input_scales.get('y')
    
    if actual_x is None or actual_y is None:
        raise ValueError("Input scales missing 'x' or 'y' range")
    
    # å®¹å·®æª¢æŸ¥ï¼ˆ1e-3ï¼‰
    x_match = (abs(actual_x[0] - expected_x[0]) < 1e-3 and
               abs(actual_x[1] - expected_x[1]) < 1e-3)
    y_match = (abs(actual_y[0] - expected_y[0]) < 1e-3 and
               abs(actual_y[1] - expected_y[1]) < 1e-3)
    
    if not (x_match and y_match):
        raise ValueError(
            f"Input scaling range configuration error:\n"
            f"  Expected x: {expected_x}, got: {actual_x}\n"
            f"  Expected y: {expected_y}, got: {actual_y}\n"
            f"  This will cause generalization failure outside sensor coverage!"
        )
    
    # 3D æª¢æŸ¥
    if 'z_range' in domain_cfg:
        expected_z = tuple(domain_cfg['z_range'])
        actual_z = input_scales.get('z')
        if actual_z is None:
            raise ValueError("3D domain configured but 'z' range missing in input_scales")
        
        z_match = (abs(actual_z[0] - expected_z[0]) < 1e-3 and
                   abs(actual_z[1] - expected_z[1]) < 1e-3)
        if not z_match:
            raise ValueError(
                f"Z-axis scaling range configuration error:\n"
                f"  Expected: {expected_z}, got: {actual_z}"
            )


def _verify_model_scaling(model: nn.Module, config: Dict[str, Any]) -> None:
    """
    è¨“ç·´å‰é©—è­‰ï¼šæª¢æŸ¥æ¨¡å‹ç¸®æ”¾åƒæ•¸æ˜¯å¦èˆ‡é…ç½®ä¸€è‡´
    
    Logs warnings if mismatches detected.
    """
    if not (hasattr(model, 'input_min') and hasattr(model, 'input_max')):
        return
    
    logging.info("=" * 60)
    logging.info("ğŸ“ Model Scaling Parameters Verification:")
    logging.info(f"   Input min:  {model.input_min.cpu().numpy()}")  # type: ignore
    logging.info(f"   Input max:  {model.input_max.cpu().numpy()}")  # type: ignore
    logging.info(f"   Output min: {model.output_min.cpu().numpy()}")  # type: ignore
    logging.info(f"   Output max: {model.output_max.cpu().numpy()}")  # type: ignore
    
    domain_cfg = config.get('physics', {}).get('domain', {})
    if domain_cfg and 'x_range' in domain_cfg:
        expected_x_range = domain_cfg['x_range']
        expected_y_range = domain_cfg['y_range']
        
        actual_x_min = model.input_min[0].item()  # type: ignore
        actual_x_max = model.input_max[0].item()  # type: ignore
        actual_y_min = model.input_min[1].item()  # type: ignore
        actual_y_max = model.input_max[1].item()  # type: ignore
        
        x_match = (abs(actual_x_min - expected_x_range[0]) < 1e-3 and
                   abs(actual_x_max - expected_x_range[1]) < 1e-3)
        y_match = (abs(actual_y_min - expected_y_range[0]) < 1e-3 and
                   abs(actual_y_max - expected_y_range[1]) < 1e-3)
        
        if x_match and y_match:
            logging.info(
                f"âœ… Input ranges match config: "
                f"x={expected_x_range}, y={expected_y_range}"
            )
        else:
            logging.error("=" * 60)
            logging.error("âŒ CRITICAL: Input range mismatch detected!")
            logging.error(
                f"   Expected x: {expected_x_range}, "
                f"got: [{actual_x_min:.4f}, {actual_x_max:.4f}]"
            )
            logging.error(
                f"   Expected y: {expected_y_range}, "
                f"got: [{actual_y_min:.4f}, {actual_y_max:.4f}]"
            )
            logging.error("=" * 60)
        
        # 3D æª¢æŸ¥
        if 'z_range' in domain_cfg and len(model.input_min) > 2:  # type: ignore
            expected_z_range = domain_cfg['z_range']
            actual_z_min = model.input_min[2].item()  # type: ignore
            actual_z_max = model.input_max[2].item()  # type: ignore
            z_match = (abs(actual_z_min - expected_z_range[0]) < 1e-3 and
                       abs(actual_z_max - expected_z_range[1]) < 1e-3)
            if z_match:
                logging.info(f"âœ… 3D z-range matches config: {expected_z_range}")
            else:
                logging.error(
                    f"âŒ Expected z: {expected_z_range}, "
                    f"got: [{actual_z_min:.4f}, {actual_z_max:.4f}]"
                )
    
    logging.info("=" * 60)


# ============================================================================
# ç‰©ç†æ–¹ç¨‹å‰µå»º
# ============================================================================

def create_physics(config: Dict[str, Any], device: torch.device):
    """
    å»ºç«‹ç‰©ç†æ–¹ç¨‹å¼æ¨¡çµ„ï¼ˆæ”¯æ´ VS-PINN èˆ‡æ¨™æº– NSï¼‰
    
    Args:
        config: å®Œæ•´é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - physics: ç‰©ç†é…ç½®ï¼ˆtype, nu, rho, domain, vs_pinn, channel_flowï¼‰
            - [å¯é¸] losses: æå¤±é…ç½®ï¼ˆç”¨æ–¼ warmup_epochs ç­‰ï¼‰
        device: è¨ˆç®—è¨­å‚™
    
    Returns:
        ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆå·²ç§»è‡³ç›®æ¨™è¨­å‚™ï¼‰
    
    Raises:
        KeyError: è‹¥é…ç½®ç¼ºå°‘å¿…è¦æ¬„ä½
        ValueError: è‹¥ç‰©ç†é¡å‹ä¸æ”¯æ´
    
    Supported Types:
        - 'vs_pinn_channel_flow': VS-PINN é€šé“æµæ±‚è§£å™¨ï¼ˆ3Dï¼‰
        - 'ns_2d': æ¨™æº– Navier-Stokes 2D æ±‚è§£å™¨
    
    Examples:
        >>> config = {
        ...     'physics': {
        ...         'type': 'vs_pinn_channel_flow',
        ...         'nu': 5e-5,
        ...         'domain': {'x_range': [0, 25.13], 'y_range': [-1, 1], ...},
        ...         'vs_pinn': {'scaling_factors': {'N_x': 2.0, 'N_y': 12.0, 'N_z': 2.0}}
        ...     }
        ... }
        >>> physics = create_physics(config, device)
    """
    physics_cfg = config.get('physics')
    if not physics_cfg:
        raise KeyError("Config missing required key: 'physics'")
    
    physics_type = physics_cfg.get('type', 'ns_2d')
    
    if physics_type == 'vs_pinn_channel_flow':
        # === VS-PINN é€šé“æµæ±‚è§£å™¨ ===
        vs_pinn_cfg = physics_cfg.get('vs_pinn', {})
        scaling_cfg = vs_pinn_cfg.get('scaling_factors', {})
        
        # ç‰©ç†åƒæ•¸ï¼ˆå…¼å®¹å¤šç¨®é…ç½®æ ¼å¼ï¼‰
        channel_flow_cfg = physics_cfg.get('channel_flow', {})
        
        # åŸŸé…ç½®
        domain_cfg = physics_cfg.get('domain', {})
        if not domain_cfg:
            raise ValueError(
                "VS-PINN requires 'domain' configuration with x/y/z_range"
            )
        
        domain_bounds = {
            'x': domain_cfg.get('x_range', [0.0, 25.13]),
            'y': domain_cfg.get('y_range', [-1.0, 1.0]),
            'z': domain_cfg.get('z_range', [0.0, 9.42]),
        }
        
        # æå–ç‰©ç†åƒæ•¸
        nu = physics_cfg.get('nu', channel_flow_cfg.get('u_tau', 5e-5))
        dP_dx = channel_flow_cfg.get('pressure_gradient',
                                      physics_cfg.get('dP_dx', 0.0025))
        rho = physics_cfg.get('rho', 1.0)
        
        # âœ… TASK-008: æå– RANS é…ç½®
        enable_rans = vs_pinn_cfg.get('enable_rans', False)
        rans_model = vs_pinn_cfg.get('rans_model', 'k_epsilon')
        
        # âš¡ æå–æ¢¯åº¦æª¢æŸ¥é»é…ç½®ï¼ˆå¾ model å€å¡Šè®€å–ï¼Œé»˜èª False ä»¥é¿å… autograd è¡çªï¼‰
        model_cfg = config.get('model', {})
        use_gradient_checkpointing = model_cfg.get('use_gradient_checkpointing', False)
        
        physics = create_vs_pinn_channel_flow(
            N_x=scaling_cfg.get('N_x', 2.0),
            N_y=scaling_cfg.get('N_y', 12.0),
            N_z=scaling_cfg.get('N_z', 2.0),
            nu=nu,
            dP_dx=dP_dx,
            rho=rho,
            domain_bounds=domain_bounds,
            loss_config=config.get('losses', {}),
            enable_rans=enable_rans,  # âœ… TASK-008: å‚³é RANS å•Ÿç”¨é–‹é—œ
            rans_model=rans_model,    # âœ… TASK-008: å‚³é RANS æ¨¡å‹é¡å‹
            use_gradient_checkpointing=use_gradient_checkpointing,  # âš¡ å‚³éæª¢æŸ¥é»é…ç½®
        )
        
        logging.info(
            f"âœ… ä½¿ç”¨ VS-PINN æ±‚è§£å™¨ ("
            f"N_x={scaling_cfg.get('N_x', 2.0)}, "
            f"N_y={scaling_cfg.get('N_y', 12.0)}, "
            f"N_z={scaling_cfg.get('N_z', 2.0)})"
        )
        logging.info(
            f"   åŸŸé‚Šç•Œ: x={domain_bounds['x']}, "
            f"y={domain_bounds['y']}, z={domain_bounds['z']}"
        )
    
    elif physics_type == 'ns_2d':
        # === æ¨™æº– NS 2D æ±‚è§£å™¨ ===
        physics = NSEquations2D(
            viscosity=physics_cfg.get('nu', 1e-3),
            density=physics_cfg.get('rho', 1.0)
        )
        logging.info("âœ… ä½¿ç”¨æ¨™æº– NS 2D æ±‚è§£å™¨")
    
    else:
        raise ValueError(
            f"Unsupported physics type: '{physics_type}'. "
            f"Supported types: 'vs_pinn_channel_flow', 'ns_2d'"
        )
    
    # ç§»è‡³ç›®æ¨™è¨­å‚™ï¼ˆåƒ…å° nn.Module å­é¡æœ‰æ•ˆï¼‰
    if isinstance(physics, nn.Module):
        physics = physics.to(device)
        logging.info(f"   Physics module moved to device: {device}")
    else:
        logging.info(f"   Physics object created (device handling in forward pass)")
    
    return physics


# ============================================================================
# å„ªåŒ–å™¨èˆ‡èª¿åº¦å™¨å‰µå»º
# ============================================================================

def create_optimizer(
    model: nn.Module,
    config: Dict[str, Any]
) -> Tuple[torch.optim.Optimizer, Optional[object]]:
    """
    å»ºç«‹å„ªåŒ–å™¨èˆ‡å­¸ç¿’ç‡èª¿åº¦å™¨
    
    Args:
        model: å¾…å„ªåŒ–çš„æ¨¡å‹
        config: å®Œæ•´é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - training.optimizer: å„ªåŒ–å™¨é…ç½®ï¼ˆtype, lr, weight_decayï¼‰
            - training.scheduler: [å¯é¸] èª¿åº¦å™¨é…ç½®ï¼ˆtype, warmup_epochs, min_lr, ...ï¼‰
            - training.epochs: [å¯é¸] è¨“ç·´ç¸½è¼ªæ•¸ï¼ˆç”¨æ–¼ Cosine èª¿åº¦å™¨ï¼‰
    
    Returns:
        (optimizer, scheduler): å„ªåŒ–å™¨èˆ‡èª¿åº¦å™¨ï¼ˆè‹¥æœªé…ç½®å‰‡ç‚º Noneï¼‰
    
    Raises:
        ImportError: è‹¥è«‹æ±‚çš„å„ªåŒ–å™¨ï¼ˆå¦‚ SOAPï¼‰æœªå®‰è£
        ValueError: è‹¥é…ç½®åƒæ•¸ç„¡æ•ˆ
    
    Supported Optimizers:
        - 'adam': torch.optim.Adamï¼ˆé»˜èªï¼‰
        - 'soap': SOAPï¼ˆéœ€å®‰è£ torch_optimizerï¼‰
    
    Supported Schedulers:
        - 'warmup_cosine': Warmup + CosineAnnealing
        - 'cosine_warm_restarts': CosineAnnealingWarmRestartsï¼ˆé€±æœŸæ€§é‡å•Ÿï¼‰
        - 'cosine': CosineAnnealingLR
        - 'exponential': ExponentialLR
        - 'none': ç„¡èª¿åº¦å™¨
    
    Examples:
        >>> config = {
        ...     'training': {
        ...         'optimizer': {'type': 'adam', 'lr': 1e-3, 'weight_decay': 0.0},
        ...         'scheduler': {'type': 'warmup_cosine', 'warmup_epochs': 10},
        ...         'epochs': 1000
        ...     }
        ... }
        >>> optimizer, scheduler = create_optimizer(model, config)
    """
    train_cfg = config.get('training', {})
    
    # === å„ªåŒ–å™¨é…ç½® ===
    optimizer_cfg = train_cfg.get('optimizer', {})
    lr = optimizer_cfg.get('lr', train_cfg.get('lr', 1e-3))
    weight_decay = optimizer_cfg.get('weight_decay',
                                      train_cfg.get('weight_decay', 0.0))
    optimizer_name = optimizer_cfg.get('type',
                                        train_cfg.get('optimizer', 'adam')).lower()
    
    if optimizer_name == 'soap':
        # ä½¿ç”¨å°ˆæ¡ˆå…§å»ºçš„ SOAP å„ªåŒ–å™¨ï¼ˆnikhilvyas/SOAPï¼‰
        try:
            from pinnx.optim import SOAP  # type: ignore
        except ImportError as exc:
            raise ImportError(
                "Requested optimizer 'SOAP' but pinnx.optim.SOAP not found. "
                "Ensure soap.py is in pinnx/optim/ directory"
            ) from exc
        
        # PirateNet è«–æ–‡åƒæ•¸
        betas = optimizer_cfg.get('betas', (0.9, 0.999))
        precondition_frequency = optimizer_cfg.get('precondition_frequency', 2)  # è«–æ–‡é è¨­: 2 steps
        shampoo_beta = optimizer_cfg.get('shampoo_beta', -1)
        
        optimizer = SOAP(
            model.parameters(),
            lr=lr,
            betas=betas,
            weight_decay=weight_decay,
            precondition_frequency=precondition_frequency,
            shampoo_beta=shampoo_beta
        )
        logging.info(
            f"âœ… Using SOAP optimizer (lr={lr}, betas={betas}, "
            f"precond_freq={precondition_frequency})"
        )
    
    elif optimizer_name == 'adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        logging.info("âœ… Using Adam optimizer")
    
    else:
        raise ValueError(
            f"Unsupported optimizer type: '{optimizer_name}'. "
            f"Supported types: 'adam', 'soap'"
        )
    
    # === å­¸ç¿’ç‡èª¿åº¦å™¨é…ç½® ===
    scheduler_cfg = train_cfg.get('scheduler', {})
    scheduler_type = scheduler_cfg.get('type',
                                        train_cfg.get('lr_scheduler', 'none'))
    max_epochs = train_cfg.get('epochs', train_cfg.get('max_epochs', 1000))
    
    scheduler: Optional[object] = None
    
    if scheduler_type == 'warmup_exponential_steps':
        # Steps-based Warmup + Exponential Decayï¼ˆç”¨æ–¼ PirateNetï¼‰
        from pinnx.train.schedulers import StepsBasedWarmupScheduler
        
        warmup_steps = scheduler_cfg.get('warmup_steps', 2000)
        total_steps = scheduler_cfg.get('total_steps', max_epochs * 100)  # å‡è¨­æ¯ epoch 100 steps
        decay_steps = scheduler_cfg.get('decay_steps', 2000)
        gamma = scheduler_cfg.get('decay_gamma', scheduler_cfg.get('gamma', 0.9))
        min_lr = scheduler_cfg.get('min_lr', 1e-6)
        
        scheduler = StepsBasedWarmupScheduler(
            optimizer,
            warmup_steps=warmup_steps,
            total_steps=total_steps,
            base_lr=lr,
            decay_steps=decay_steps,
            gamma=gamma,
            min_lr=min_lr
        )
        logging.info(
            f"âœ… Using StepsBasedWarmupScheduler "
            f"(warmup={warmup_steps} steps, decay_steps={decay_steps}, Î³={gamma})"
        )
    
    elif scheduler_type == 'warmup_cosine':
        # å‹•æ…‹å°å…¥ WarmupCosineSchedulerï¼ˆé¿å…å¾ªç’°ä¾è³´ï¼‰
        try:
            from pinnx.train.lr_scheduler import WarmupCosineScheduler  # type: ignore
        except ImportError:
            # å›é€€ï¼šä½¿ç”¨ train.py ä¸­å®šç¾©çš„ç‰ˆæœ¬
            logging.warning(
                "pinnx.train.lr_scheduler not found, "
                "falling back to inline WarmupCosineScheduler"
            )
            # TODO: å°‡ WarmupCosineScheduler æå–ç‚ºç¨ç«‹æ¨¡çµ„
            raise ValueError(
                "WarmupCosineScheduler requires pinnx.train.lr_scheduler module. "
                "Please create it or use alternative scheduler types."
            )
        
        warmup_epochs = scheduler_cfg.get('warmup_epochs', 10)
        min_lr = scheduler_cfg.get('min_lr', 1e-6)
        
        scheduler = WarmupCosineScheduler(
            optimizer,
            warmup_epochs=warmup_epochs,
            max_epochs=max_epochs,
            base_lr=lr,
            min_lr=min_lr
        )
        logging.info(
            f"âœ… Using WarmupCosineScheduler "
            f"(warmup={warmup_epochs}, max_epochs={max_epochs})"
        )
    
    elif scheduler_type == 'cosine_warm_restarts':
        T_0 = scheduler_cfg.get('T_0', 100)
        T_mult = scheduler_cfg.get('T_mult', 1)
        eta_min = scheduler_cfg.get('eta_min', 1e-6)
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=T_0,
            T_mult=T_mult,
            eta_min=eta_min
        )
        logging.info(
            f"âœ… Using CosineAnnealingWarmRestarts "
            f"(T_0={T_0}, T_mult={T_mult})"
        )
    
    elif scheduler_type == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=max_epochs
        )
        logging.info(f"âœ… Using CosineAnnealingLR (T_max={max_epochs})")
    
    elif scheduler_type == 'exponential':
        gamma = scheduler_cfg.get('gamma', 0.999)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
        logging.info(f"âœ… Using ExponentialLR (gamma={gamma})")
    
    elif scheduler_type in ['none', None]:
        logging.info("No learning rate scheduler configured")
    
    else:
        raise ValueError(
            f"Unsupported scheduler type: '{scheduler_type}'. "
            f"Supported types: 'warmup_cosine', 'cosine_warm_restarts', "
            f"'cosine', 'exponential', 'none'"
        )
    
    return optimizer, scheduler
