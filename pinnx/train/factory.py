"""
æ¨¡å‹/ç‰©ç†/å„ªåŒ–å™¨å·¥å» æ¨¡çµ„

æœ¬æ¨¡çµ„æä¾›çµ±ä¸€çš„å·¥å» å‡½æ•¸ï¼Œç”¨æ–¼å‰µå»º PINN è¨“ç·´æ‰€éœ€çš„æ ¸å¿ƒçµ„ä»¶ï¼š
- è¨­å‚™é¸æ“‡ï¼ˆCUDA/MPS/CPUï¼‰
- æ¨¡å‹æ¶æ§‹ï¼ˆPINN/Enhanced/VS-PINNï¼‰
- ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆNS 2D/3Dã€VS-PINNï¼‰
- å„ªåŒ–å™¨èˆ‡å­¸ç¿’ç‡èª¿åº¦å™¨

æ‰€æœ‰å‡½æ•¸å‡æä¾›å®Œæ•´çš„éŒ¯èª¤è™•ç†ã€é¡å‹æç¤ºèˆ‡é…ç½®é©—è­‰ã€‚
"""

import logging
from typing import Any, Dict, Optional, Tuple, Union

import torch
import torch.nn as nn

from pinnx.models.fourier_mlp import PINNNet, create_enhanced_pinn, init_siren_weights
from pinnx.physics.ns_2d import NSEquations2D
from pinnx.physics.vs_pinn_channel_flow import create_vs_pinn_channel_flow


# ============================================================================
# è¨­å‚™é¸æ“‡
# ============================================================================

def get_device(device_name: str) -> torch.device:
    """
    ç²å–é‹ç®—è¨­å‚™ï¼ˆæ”¯æ´è‡ªå‹•é¸æ“‡èˆ‡æ‰‹å‹•æŒ‡å®šï¼‰
    
    Args:
        device_name: è¨­å‚™åç¨±ï¼Œæ”¯æ´ï¼š
            - "auto": è‡ªå‹•é¸æ“‡æœ€ä½³å¯ç”¨è¨­å‚™ï¼ˆCUDA > MPS > CPUï¼‰
            - "cuda": NVIDIA GPUï¼ˆéœ€ CUDA å¯ç”¨ï¼‰
            - "mps": Apple Silicon GPUï¼ˆéœ€ MPS å¯ç”¨ï¼‰
            - "cpu": CPU é‹ç®—
    
    Returns:
        torch.device: PyTorch è¨­å‚™ç‰©ä»¶
    
    Raises:
        ValueError: è‹¥æŒ‡å®šè¨­å‚™åç¨±ç„¡æ•ˆ
    
    Examples:
        >>> device = get_device("auto")
        >>> device = get_device("cuda")  # è‹¥ CUDA ä¸å¯ç”¨æœƒå›é€€åˆ° CPU
    """
    valid_devices = ["auto", "cuda", "mps", "cpu"]
    if device_name not in valid_devices:
        raise ValueError(
            f"Invalid device name '{device_name}'. "
            f"Must be one of: {valid_devices}"
        )
    
    if device_name == "auto":
        # è‡ªå‹•é¸æ“‡æœ€ä½³å¯ç”¨è¨­å‚™
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logging.info(f"Auto-selected CUDA: {torch.cuda.get_device_name()}")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
            logging.info("Auto-selected Apple Metal Performance Shaders")
        else:
            device = torch.device("cpu")
            logging.info("Auto-selected CPU (no GPU available)")
    
    elif device_name == "cuda":
        if torch.cuda.is_available():
            device = torch.device("cuda")
            logging.info(f"Using CUDA: {torch.cuda.get_device_name()}")
        else:
            logging.warning("CUDA requested but not available, falling back to CPU")
            device = torch.device("cpu")
    
    elif device_name == "mps":
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            logging.info("Using Apple Metal Performance Shaders")
        else:
            logging.warning("MPS requested but not available, falling back to CPU")
            device = torch.device("cpu")
    
    else:  # cpu
        device = torch.device("cpu")
        logging.info("Using CPU")
    
    return device


# ============================================================================
# æ¨¡å‹å‰µå»º
# ============================================================================

def create_model(
    config: Dict[str, Any],
    device: torch.device,
    statistics: Optional[Dict[str, Dict[str, float]]] = None
) -> nn.Module:
    """
    å»ºç«‹ PINN æ¨¡å‹ï¼ˆæ”¯æ´ Fourier featuresã€VS-PINN ç¸®æ”¾ã€æ‰‹å‹•æ¨™æº–åŒ–ï¼‰
    
    Args:
        config: å®Œæ•´é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - model: æ¨¡å‹é…ç½®ï¼ˆtype, in_dim, out_dim, width, depth, activationï¼‰
            - physics: ç‰©ç†é…ç½®ï¼ˆtype, domain, vs_pinnï¼‰
            - [å¯é¸] scaling: æ¨™æº–åŒ–é…ç½®ï¼ˆoutput_normï¼‰
        device: è¨ˆç®—è¨­å‚™
        statistics: [å¯é¸] è³‡æ–™çµ±è¨ˆè³‡è¨Šï¼ˆç”¨æ–¼è‡ªå‹•è¨­å®šè¼¸å‡ºç¯„åœï¼‰
    
    Returns:
        nn.Module: å·²åˆå§‹åŒ–ä¸¦ç§»è‡³ç›®æ¨™è¨­å‚™çš„ PINN æ¨¡å‹
    
    Raises:
        KeyError: è‹¥é…ç½®ç¼ºå°‘å¿…è¦æ¬„ä½
        ValueError: è‹¥é…ç½®åƒæ•¸ç„¡æ•ˆï¼ˆå¦‚è¼¸å…¥ç¯„åœä¸åŒ¹é…åŸŸé…ç½®ï¼‰
        ImportError: è‹¥ç¼ºå°‘å¿…è¦çš„æ¨¡çµ„ï¼ˆå¦‚ ManualScalingWrapperï¼‰
    
    Notes:
        - è‹¥ activation='sine'ï¼Œè‡ªå‹•æ‡‰ç”¨ SIREN æ¬Šé‡åˆå§‹åŒ–
        - VS-PINN æ¨¡å¼ä¸‹æœƒè·³é ManualScalingWrapperï¼ˆé¿å…é›™é‡æ¨™æº–åŒ–ï¼‰
        - æ”¯æ´ Fourier features æ¶ˆèå¯¦é©—ï¼ˆuse_fourier é–‹é—œï¼‰
    
    Examples:
        >>> config = {
        ...     'model': {'type': 'fourier_mlp', 'in_dim': 3, 'out_dim': 4,
        ...               'width': 200, 'depth': 8, 'activation': 'sine'},
        ...     'physics': {'type': 'vs_pinn_channel_flow', 'domain': {...}}
        ... }
        >>> model = create_model(config, device)
    """
    model_cfg = config.get('model')
    if not model_cfg:
        raise KeyError("Config missing required key: 'model'")
    
    # é©—è­‰å¿…è¦æ¬„ä½
    required_fields = ['in_dim', 'out_dim', 'width', 'depth', 'activation']
    missing = [f for f in required_fields if f not in model_cfg]
    if missing:
        raise KeyError(f"Model config missing required fields: {missing}")
    
    # === 1. Fourier Features é…ç½® ===
    use_fourier = model_cfg.get('use_fourier', True)  # é»˜èªå•Ÿç”¨
    
    # === 2. VS-PINN ç¸®æ”¾å› å­æå– ===
    physics_type = config.get('physics', {}).get('type', '')
    is_vs_pinn = (physics_type == 'vs_pinn_channel_flow')
    
    input_scale_factors = None
    fourier_normalize_input = False
    
    if is_vs_pinn and use_fourier:
        # VS-PINN æ¨¡å¼ï¼šæå–ç¸®æ”¾å› å­ç”¨æ–¼ Fourier æ¨™æº–åŒ–ä¿®å¾©
        vs_pinn_cfg = config.get('physics', {}).get('vs_pinn', {})
        scaling_cfg = vs_pinn_cfg.get('scaling_factors', {})
        N_x = scaling_cfg.get('N_x', 2.0)
        N_y = scaling_cfg.get('N_y', 12.0)
        N_z = scaling_cfg.get('N_z', 2.0)
        input_scale_factors = torch.tensor([N_x, N_y, N_z], dtype=torch.float32)
        fourier_normalize_input = True
        logging.info(
            f"ğŸ”§ VS-PINN + Fourier ä¿®å¾©å•Ÿç”¨ï¼šç¸®æ”¾å› å­ N=[{N_x}, {N_y}, {N_z}]"
        )
    
    # === 3. å»ºç«‹åŸºç¤æ¨¡å‹ ===
    model_type = model_cfg.get('type', 'fourier_mlp')
    
    if model_type == 'enhanced_fourier_mlp':
        # å¢å¼·ç‰ˆ PINNï¼ˆæ”¯æ´ RWF ç­‰é€²éšç‰¹æ€§ï¼‰
        base_model = create_enhanced_pinn(
            in_dim=model_cfg['in_dim'],
            out_dim=model_cfg['out_dim'],
            width=model_cfg['width'],
            depth=model_cfg['depth'],
            activation=model_cfg['activation'],
            use_fourier=use_fourier,
            fourier_m=model_cfg.get('fourier_m', 32),
            fourier_sigma=model_cfg.get('fourier_sigma', 1.0),
            use_rwf=model_cfg.get('use_rwf', False),
            rwf_scale_std=model_cfg.get('rwf_scale_std', 0.1),
            fourier_normalize_input=fourier_normalize_input,
            input_scale_factors=input_scale_factors
        ).to(device)
        logging.info(f"Created Enhanced PINN (use_fourier={use_fourier})")
    else:
        # åŸºç¤ PINN
        base_model = PINNNet(
            in_dim=model_cfg['in_dim'],
            out_dim=model_cfg['out_dim'],
            width=model_cfg['width'],
            depth=model_cfg['depth'],
            activation=model_cfg['activation'],
            use_fourier=use_fourier,
            fourier_m=model_cfg.get('fourier_m', 32),
            fourier_sigma=model_cfg.get('fourier_sigma', 1.0),
            fourier_normalize_input=fourier_normalize_input,
            input_scale_factors=input_scale_factors
        ).to(device)
        logging.info(f"Created PINNNet (use_fourier={use_fourier})")
    
    # === 4. æ‡‰ç”¨æ‰‹å‹•æ¨™æº–åŒ–åŒ…è£å™¨ï¼ˆè‹¥é…ç½®å•Ÿç”¨ä¸”é VS-PINNï¼‰===
    scaling_cfg = model_cfg.get('scaling', {})
    scaling_enabled = bool(scaling_cfg) and not is_vs_pinn
    
    if scaling_enabled:
        # ä½¿ç”¨æ‰‹å‹•æ¨™æº–åŒ–åŒ…è£å™¨ï¼ˆé¿å… Fourier feature é£½å’Œï¼‰
        try:
            from pinnx.models.wrappers import ManualScalingWrapper
        except ImportError as exc:
            raise ImportError(
                "ManualScalingWrapper not found. "
                "Ensure pinnx.models.wrappers module exists."
            ) from exc
        
        # æå–è¼¸å…¥/è¼¸å‡ºç¯„åœ
        input_scales, output_scales = _extract_scaling_ranges(
            config, statistics, model_cfg
        )
        
        # é©—è­‰è¼¸å…¥ç¯„åœæ˜¯å¦åŒ¹é…åŸŸé…ç½®
        _validate_input_ranges(config, input_scales)
        
        # æ‡‰ç”¨åŒ…è£å™¨
        model = ManualScalingWrapper(
            base_model,
            input_ranges=input_scales,
            output_ranges=output_scales
        ).to(device)
        logging.info(
            f"âœ… Manual scaling wrapper applied:\n"
            f"   Inputs: {input_scales}\n"
            f"   Outputs: {output_scales}"
        )
    else:
        model = base_model
        if is_vs_pinn:
            logging.info("Using base model without scaling (VS-PINN handles scaling)")
        else:
            logging.info("Using base model without scaling")
    
    # === 5. SIREN æ¬Šé‡åˆå§‹åŒ–ï¼ˆè‹¥ä½¿ç”¨ Sine æ¿€æ´»å‡½æ•¸ï¼‰===
    if model_cfg['activation'] == 'sine':
        target_model = base_model
        if hasattr(model, 'model'):  # è‹¥æœ‰åŒ…è£å™¨
            target_model = model.model  # type: ignore
        
        if isinstance(target_model, PINNNet):
            init_siren_weights(target_model)
            logging.info("âœ… Applied SIREN weight initialization for Sine activation")
        else:
            logging.warning(
                f"âš ï¸  Cannot apply SIREN initialization: "
                f"base model type is {type(target_model)}"
            )
    
    # === 6. è¨“ç·´å‰é©—è­‰ï¼ˆè‹¥å•Ÿç”¨æ¨™æº–åŒ–ï¼‰===
    if scaling_enabled and hasattr(model, 'input_min'):
        _verify_model_scaling(model, config)
    
    num_params = sum(p.numel() for p in model.parameters())
    logging.info(f"Created model with {num_params:,} parameters")
    
    return model


def _extract_scaling_ranges(
    config: Dict[str, Any],
    statistics: Optional[Dict[str, Dict[str, float]]],
    model_cfg: Dict[str, Any]
) -> Tuple[Dict[str, Tuple[float, float]], Dict[str, Tuple[float, float]]]:
    """
    æå–è¼¸å…¥/è¼¸å‡ºæ¨™æº–åŒ–ç¯„åœï¼ˆå„ªå…ˆç´šï¼šé…ç½® > çµ±è¨ˆ > ç¡¬ç·¨ç¢¼ï¼‰
    
    Returns:
        (input_scales, output_scales): è¼¸å…¥èˆ‡è¼¸å‡ºçš„ç¯„åœå­—å…¸
    """
    domain_cfg = config.get('physics', {}).get('domain', {})
    scaling_cfg = model_cfg.get('scaling', {})
    
    # === è¼¸å…¥ç¯„åœ ===
    if domain_cfg and 'x_range' in domain_cfg:
        # å„ªå…ˆï¼šå¾é…ç½®æ–‡ä»¶è®€å–å®Œæ•´åŸŸç¯„åœ
        input_x_range = tuple(domain_cfg['x_range'])  # type: ignore
        input_y_range = tuple(domain_cfg['y_range'])  # type: ignore
        logging.info(
            f"âœ… Using domain ranges from config: "
            f"x={input_x_range}, y={input_y_range}"
        )
    elif statistics and 'x' in statistics and 'range' in statistics['x']:
        # å›é€€ï¼šä½¿ç”¨æ„Ÿæ¸¬é»çµ±è¨ˆï¼ˆå¯èƒ½å°è‡´æ³›åŒ–å•é¡Œï¼‰
        input_x_range = tuple(statistics['x']['range'])  # type: ignore
        input_y_range = tuple(statistics['y']['range'])  # type: ignore
        logging.warning(
            f"âš ï¸ Falling back to statistics-based ranges: "
            f"x={input_x_range}, y={input_y_range}"
        )
        logging.warning(
            "âš ï¸ This may cause generalization issues "
            "if sensors don't cover full domain!"
        )
    else:
        # æœ€çµ‚å›é€€ï¼šç¡¬ç·¨ç¢¼ï¼ˆJHTDB Channel Re1000ï¼‰
        input_x_range = (0.0, 25.13)
        input_y_range = (-1.0, 1.0)
        logging.warning(
            f"âš ï¸ Using hardcoded domain ranges: "
            f"x={input_x_range}, y={input_y_range}"
        )
    
    input_scales: Dict[str, Tuple[float, float]] = {
        'x': input_x_range,
        'y': input_y_range
    }
    
    # 3D è‡ªå‹•æª¢æ¸¬
    if domain_cfg and 'z_range' in domain_cfg:
        input_z_range = tuple(domain_cfg['z_range'])  # type: ignore
        input_scales['z'] = input_z_range
        logging.info(f"âœ… 3D mode: z={input_z_range}")
    elif statistics and 'z' in statistics and 'range' in statistics['z']:
        input_z_range = tuple(statistics['z']['range'])  # type: ignore
        input_scales['z'] = input_z_range
        logging.warning(f"âš ï¸ 3D mode using statistics: z={input_z_range}")
    
    # === è¼¸å‡ºç¯„åœ ===
    output_scales: Dict[str, Tuple[float, float]] | None = None
    
    # å„ªå…ˆï¼šå¾é…ç½®è®€å–
    if 'output_norm' in scaling_cfg:
        output_norm_raw = scaling_cfg['output_norm']
        
        if isinstance(output_norm_raw, dict):
            # å­—å…¸æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨
            output_scales = {
                'u': tuple(output_norm_raw.get('u', [0.0, 20.0])),  # type: ignore
                'v': tuple(output_norm_raw.get('v', [-1.0, 1.0])),  # type: ignore
                'w': tuple(output_norm_raw.get('w', [-5.0, 5.0])),  # type: ignore
                'p': tuple(output_norm_raw.get('p', [-100.0, 10.0]))  # type: ignore
            }
            logging.info("âœ… Using output ranges from config file (dict format):")
            for key, val in output_scales.items():
                logging.info(f"   {key}: {val}")
        elif isinstance(output_norm_raw, str):
            # å­—ç¬¦ä¸²æ ¼å¼ï¼šå›é€€åˆ°çµ±è¨ˆ
            logging.info(
                f"âš ï¸ output_norm is string '{output_norm_raw}', "
                "falling back to statistics"
            )
    
    # å›é€€ï¼šä½¿ç”¨çµ±è¨ˆè³‡è¨Š
    if output_scales is None:
        if statistics:
            output_u_range = tuple(statistics.get('u', {}).get('range', (0.0, 20.0)))  # type: ignore
            output_v_range = tuple(statistics.get('v', {}).get('range', (-1.0, 1.0)))  # type: ignore
            output_w_range = tuple(statistics.get('w', {}).get('range', (-5.0, 5.0)))  # type: ignore
            output_p_range = tuple(statistics.get('p', {}).get('range', (-100.0, 10.0)))  # type: ignore
            
            output_scales = {
                'u': output_u_range,
                'v': output_v_range,
                'w': output_w_range,
                'p': output_p_range
            }
            logging.info("âœ… Using data-driven output ranges from statistics:")
            for key, val in output_scales.items():
                logging.info(f"   {key}: {val}")
        else:
            # æœ€çµ‚å›é€€ï¼šç¡¬ç·¨ç¢¼
            output_scales = {
                'u': (0.0, 20.0),
                'v': (-1.0, 1.0),
                'w': (-5.0, 5.0),
                'p': (-100.0, 10.0)
            }
            logging.warning(
                "âš ï¸ No statistics or config output_norm provided, "
                "using hardcoded ranges (may cause NaN)"
            )
    
    # è£œå……æºé …ç¯„åœï¼ˆè‹¥éœ€è¦ï¼‰
    expected_out_dim = model_cfg.get('out_dim', 3)
    if expected_out_dim == 5 and len(output_scales) == 4:
        output_scales['S'] = (-1.0, 1.0)
        logging.info("âœ… Added source term 'S' range: (-1.0, 1.0)")
    
    return input_scales, output_scales


def _validate_input_ranges(
    config: Dict[str, Any],
    input_scales: Dict[str, Tuple[float, float]]
) -> None:
    """
    é©—è­‰è¼¸å…¥ç¯„åœæ˜¯å¦èˆ‡åŸŸé…ç½®ä¸€è‡´
    
    Raises:
        ValueError: è‹¥ç¯„åœä¸åŒ¹é…ï¼ˆæœƒå°è‡´æ³›åŒ–å¤±æ•—ï¼‰
    """
    domain_cfg = config.get('physics', {}).get('domain', {})
    if not domain_cfg or 'x_range' not in domain_cfg:
        return  # ç„¡é…ç½®å¯æ¯”å°
    
    expected_x = tuple(domain_cfg['x_range'])
    expected_y = tuple(domain_cfg['y_range'])
    
    actual_x = input_scales.get('x')
    actual_y = input_scales.get('y')
    
    if actual_x is None or actual_y is None:
        raise ValueError("Input scales missing 'x' or 'y' range")
    
    # å®¹å·®æª¢æŸ¥ï¼ˆ1e-3ï¼‰
    x_match = (abs(actual_x[0] - expected_x[0]) < 1e-3 and
               abs(actual_x[1] - expected_x[1]) < 1e-3)
    y_match = (abs(actual_y[0] - expected_y[0]) < 1e-3 and
               abs(actual_y[1] - expected_y[1]) < 1e-3)
    
    if not (x_match and y_match):
        raise ValueError(
            f"Input scaling range configuration error:\n"
            f"  Expected x: {expected_x}, got: {actual_x}\n"
            f"  Expected y: {expected_y}, got: {actual_y}\n"
            f"  This will cause generalization failure outside sensor coverage!"
        )
    
    # 3D æª¢æŸ¥
    if 'z_range' in domain_cfg:
        expected_z = tuple(domain_cfg['z_range'])
        actual_z = input_scales.get('z')
        if actual_z is None:
            raise ValueError("3D domain configured but 'z' range missing in input_scales")
        
        z_match = (abs(actual_z[0] - expected_z[0]) < 1e-3 and
                   abs(actual_z[1] - expected_z[1]) < 1e-3)
        if not z_match:
            raise ValueError(
                f"Z-axis scaling range configuration error:\n"
                f"  Expected: {expected_z}, got: {actual_z}"
            )


def _verify_model_scaling(model: nn.Module, config: Dict[str, Any]) -> None:
    """
    è¨“ç·´å‰é©—è­‰ï¼šæª¢æŸ¥æ¨¡å‹ç¸®æ”¾åƒæ•¸æ˜¯å¦èˆ‡é…ç½®ä¸€è‡´
    
    Logs warnings if mismatches detected.
    """
    if not (hasattr(model, 'input_min') and hasattr(model, 'input_max')):
        return
    
    logging.info("=" * 60)
    logging.info("ğŸ“ Model Scaling Parameters Verification:")
    logging.info(f"   Input min:  {model.input_min.cpu().numpy()}")  # type: ignore
    logging.info(f"   Input max:  {model.input_max.cpu().numpy()}")  # type: ignore
    logging.info(f"   Output min: {model.output_min.cpu().numpy()}")  # type: ignore
    logging.info(f"   Output max: {model.output_max.cpu().numpy()}")  # type: ignore
    
    domain_cfg = config.get('physics', {}).get('domain', {})
    if domain_cfg and 'x_range' in domain_cfg:
        expected_x_range = domain_cfg['x_range']
        expected_y_range = domain_cfg['y_range']
        
        actual_x_min = model.input_min[0].item()  # type: ignore
        actual_x_max = model.input_max[0].item()  # type: ignore
        actual_y_min = model.input_min[1].item()  # type: ignore
        actual_y_max = model.input_max[1].item()  # type: ignore
        
        x_match = (abs(actual_x_min - expected_x_range[0]) < 1e-3 and
                   abs(actual_x_max - expected_x_range[1]) < 1e-3)
        y_match = (abs(actual_y_min - expected_y_range[0]) < 1e-3 and
                   abs(actual_y_max - expected_y_range[1]) < 1e-3)
        
        if x_match and y_match:
            logging.info(
                f"âœ… Input ranges match config: "
                f"x={expected_x_range}, y={expected_y_range}"
            )
        else:
            logging.error("=" * 60)
            logging.error("âŒ CRITICAL: Input range mismatch detected!")
            logging.error(
                f"   Expected x: {expected_x_range}, "
                f"got: [{actual_x_min:.4f}, {actual_x_max:.4f}]"
            )
            logging.error(
                f"   Expected y: {expected_y_range}, "
                f"got: [{actual_y_min:.4f}, {actual_y_max:.4f}]"
            )
            logging.error("=" * 60)
        
        # 3D æª¢æŸ¥
        if 'z_range' in domain_cfg and len(model.input_min) > 2:  # type: ignore
            expected_z_range = domain_cfg['z_range']
            actual_z_min = model.input_min[2].item()  # type: ignore
            actual_z_max = model.input_max[2].item()  # type: ignore
            z_match = (abs(actual_z_min - expected_z_range[0]) < 1e-3 and
                       abs(actual_z_max - expected_z_range[1]) < 1e-3)
            if z_match:
                logging.info(f"âœ… 3D z-range matches config: {expected_z_range}")
            else:
                logging.error(
                    f"âŒ Expected z: {expected_z_range}, "
                    f"got: [{actual_z_min:.4f}, {actual_z_max:.4f}]"
                )
    
    logging.info("=" * 60)


# ============================================================================
# ç‰©ç†æ–¹ç¨‹å‰µå»º
# ============================================================================

def create_physics(config: Dict[str, Any], device: torch.device):
    """
    å»ºç«‹ç‰©ç†æ–¹ç¨‹å¼æ¨¡çµ„ï¼ˆæ”¯æ´ VS-PINN èˆ‡æ¨™æº– NSï¼‰
    
    Args:
        config: å®Œæ•´é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - physics: ç‰©ç†é…ç½®ï¼ˆtype, nu, rho, domain, vs_pinn, channel_flowï¼‰
            - [å¯é¸] losses: æå¤±é…ç½®ï¼ˆç”¨æ–¼ warmup_epochs ç­‰ï¼‰
        device: è¨ˆç®—è¨­å‚™
    
    Returns:
        ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆå·²ç§»è‡³ç›®æ¨™è¨­å‚™ï¼‰
    
    Raises:
        KeyError: è‹¥é…ç½®ç¼ºå°‘å¿…è¦æ¬„ä½
        ValueError: è‹¥ç‰©ç†é¡å‹ä¸æ”¯æ´
    
    Supported Types:
        - 'vs_pinn_channel_flow': VS-PINN é€šé“æµæ±‚è§£å™¨ï¼ˆ3Dï¼‰
        - 'ns_2d': æ¨™æº– Navier-Stokes 2D æ±‚è§£å™¨
    
    Examples:
        >>> config = {
        ...     'physics': {
        ...         'type': 'vs_pinn_channel_flow',
        ...         'nu': 5e-5,
        ...         'domain': {'x_range': [0, 25.13], 'y_range': [-1, 1], ...},
        ...         'vs_pinn': {'scaling_factors': {'N_x': 2.0, 'N_y': 12.0, 'N_z': 2.0}}
        ...     }
        ... }
        >>> physics = create_physics(config, device)
    """
    physics_cfg = config.get('physics')
    if not physics_cfg:
        raise KeyError("Config missing required key: 'physics'")
    
    physics_type = physics_cfg.get('type', 'ns_2d')
    
    if physics_type == 'vs_pinn_channel_flow':
        # === VS-PINN é€šé“æµæ±‚è§£å™¨ ===
        vs_pinn_cfg = physics_cfg.get('vs_pinn', {})
        scaling_cfg = vs_pinn_cfg.get('scaling_factors', {})
        
        # ç‰©ç†åƒæ•¸ï¼ˆå…¼å®¹å¤šç¨®é…ç½®æ ¼å¼ï¼‰
        channel_flow_cfg = physics_cfg.get('channel_flow', {})
        
        # åŸŸé…ç½®
        domain_cfg = physics_cfg.get('domain', {})
        if not domain_cfg:
            raise ValueError(
                "VS-PINN requires 'domain' configuration with x/y/z_range"
            )
        
        domain_bounds = {
            'x': domain_cfg.get('x_range', [0.0, 25.13]),
            'y': domain_cfg.get('y_range', [-1.0, 1.0]),
            'z': domain_cfg.get('z_range', [0.0, 9.42]),
        }
        
        # æå–ç‰©ç†åƒæ•¸
        nu = physics_cfg.get('nu', channel_flow_cfg.get('u_tau', 5e-5))
        dP_dx = channel_flow_cfg.get('pressure_gradient',
                                      physics_cfg.get('dP_dx', 0.0025))
        rho = physics_cfg.get('rho', 1.0)
        
        physics = create_vs_pinn_channel_flow(
            N_x=scaling_cfg.get('N_x', 2.0),
            N_y=scaling_cfg.get('N_y', 12.0),
            N_z=scaling_cfg.get('N_z', 2.0),
            nu=nu,
            dP_dx=dP_dx,
            rho=rho,
            domain_bounds=domain_bounds,
            loss_config=config.get('losses', {}),
        )
        
        logging.info(
            f"âœ… ä½¿ç”¨ VS-PINN æ±‚è§£å™¨ ("
            f"N_x={scaling_cfg.get('N_x', 2.0)}, "
            f"N_y={scaling_cfg.get('N_y', 12.0)}, "
            f"N_z={scaling_cfg.get('N_z', 2.0)})"
        )
        logging.info(
            f"   åŸŸé‚Šç•Œ: x={domain_bounds['x']}, "
            f"y={domain_bounds['y']}, z={domain_bounds['z']}"
        )
    
    elif physics_type == 'ns_2d':
        # === æ¨™æº– NS 2D æ±‚è§£å™¨ ===
        physics = NSEquations2D(
            viscosity=physics_cfg.get('nu', 1e-3),
            density=physics_cfg.get('rho', 1.0)
        )
        logging.info("âœ… ä½¿ç”¨æ¨™æº– NS 2D æ±‚è§£å™¨")
    
    else:
        raise ValueError(
            f"Unsupported physics type: '{physics_type}'. "
            f"Supported types: 'vs_pinn_channel_flow', 'ns_2d'"
        )
    
    # ç§»è‡³ç›®æ¨™è¨­å‚™ï¼ˆåƒ…å° nn.Module å­é¡æœ‰æ•ˆï¼‰
    if isinstance(physics, nn.Module):
        physics = physics.to(device)
        logging.info(f"   Physics module moved to device: {device}")
    else:
        logging.info(f"   Physics object created (device handling in forward pass)")
    
    return physics


# ============================================================================
# å„ªåŒ–å™¨èˆ‡èª¿åº¦å™¨å‰µå»º
# ============================================================================

def create_optimizer(
    model: nn.Module,
    config: Dict[str, Any]
) -> Tuple[torch.optim.Optimizer, Optional[object]]:
    """
    å»ºç«‹å„ªåŒ–å™¨èˆ‡å­¸ç¿’ç‡èª¿åº¦å™¨
    
    Args:
        model: å¾…å„ªåŒ–çš„æ¨¡å‹
        config: å®Œæ•´é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«ï¼š
            - training.optimizer: å„ªåŒ–å™¨é…ç½®ï¼ˆtype, lr, weight_decayï¼‰
            - training.scheduler: [å¯é¸] èª¿åº¦å™¨é…ç½®ï¼ˆtype, warmup_epochs, min_lr, ...ï¼‰
            - training.epochs: [å¯é¸] è¨“ç·´ç¸½è¼ªæ•¸ï¼ˆç”¨æ–¼ Cosine èª¿åº¦å™¨ï¼‰
    
    Returns:
        (optimizer, scheduler): å„ªåŒ–å™¨èˆ‡èª¿åº¦å™¨ï¼ˆè‹¥æœªé…ç½®å‰‡ç‚º Noneï¼‰
    
    Raises:
        ImportError: è‹¥è«‹æ±‚çš„å„ªåŒ–å™¨ï¼ˆå¦‚ SOAPï¼‰æœªå®‰è£
        ValueError: è‹¥é…ç½®åƒæ•¸ç„¡æ•ˆ
    
    Supported Optimizers:
        - 'adam': torch.optim.Adamï¼ˆé»˜èªï¼‰
        - 'soap': SOAPï¼ˆéœ€å®‰è£ torch_optimizerï¼‰
    
    Supported Schedulers:
        - 'warmup_cosine': Warmup + CosineAnnealing
        - 'cosine_warm_restarts': CosineAnnealingWarmRestartsï¼ˆé€±æœŸæ€§é‡å•Ÿï¼‰
        - 'cosine': CosineAnnealingLR
        - 'exponential': ExponentialLR
        - 'none': ç„¡èª¿åº¦å™¨
    
    Examples:
        >>> config = {
        ...     'training': {
        ...         'optimizer': {'type': 'adam', 'lr': 1e-3, 'weight_decay': 0.0},
        ...         'scheduler': {'type': 'warmup_cosine', 'warmup_epochs': 10},
        ...         'epochs': 1000
        ...     }
        ... }
        >>> optimizer, scheduler = create_optimizer(model, config)
    """
    train_cfg = config.get('training', {})
    
    # === å„ªåŒ–å™¨é…ç½® ===
    optimizer_cfg = train_cfg.get('optimizer', {})
    lr = optimizer_cfg.get('lr', train_cfg.get('lr', 1e-3))
    weight_decay = optimizer_cfg.get('weight_decay',
                                      train_cfg.get('weight_decay', 0.0))
    optimizer_name = optimizer_cfg.get('type',
                                        train_cfg.get('optimizer', 'adam')).lower()
    
    if optimizer_name == 'soap':
        # å‹•æ…‹å°å…¥ SOAPï¼ˆé¿å…å¼·åˆ¶ä¾è³´ï¼‰
        try:
            from torch_optimizer import SOAP  # type: ignore
        except ImportError as exc:
            raise ImportError(
                "Requested optimizer 'SOAP' but torch_optimizer is not installed. "
                "Install with: pip install torch-optimizer"
            ) from exc
        
        soap_kwargs = optimizer_cfg.get('soap', train_cfg.get('soap', {}))
        optimizer = SOAP(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            **soap_kwargs
        )
        logging.info("âœ… Using SOAP optimizer")
    
    elif optimizer_name == 'adam':
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
        logging.info("âœ… Using Adam optimizer")
    
    else:
        raise ValueError(
            f"Unsupported optimizer type: '{optimizer_name}'. "
            f"Supported types: 'adam', 'soap'"
        )
    
    # === å­¸ç¿’ç‡èª¿åº¦å™¨é…ç½® ===
    scheduler_cfg = train_cfg.get('scheduler', {})
    scheduler_type = scheduler_cfg.get('type',
                                        train_cfg.get('lr_scheduler', 'none'))
    max_epochs = train_cfg.get('epochs', train_cfg.get('max_epochs', 1000))
    
    scheduler: Optional[object] = None
    
    if scheduler_type == 'warmup_cosine':
        # å‹•æ…‹å°å…¥ WarmupCosineSchedulerï¼ˆé¿å…å¾ªç’°ä¾è³´ï¼‰
        try:
            from pinnx.train.lr_scheduler import WarmupCosineScheduler  # type: ignore
        except ImportError:
            # å›é€€ï¼šä½¿ç”¨ train.py ä¸­å®šç¾©çš„ç‰ˆæœ¬
            logging.warning(
                "pinnx.train.lr_scheduler not found, "
                "falling back to inline WarmupCosineScheduler"
            )
            # TODO: å°‡ WarmupCosineScheduler æå–ç‚ºç¨ç«‹æ¨¡çµ„
            raise ValueError(
                "WarmupCosineScheduler requires pinnx.train.lr_scheduler module. "
                "Please create it or use alternative scheduler types."
            )
        
        warmup_epochs = scheduler_cfg.get('warmup_epochs', 10)
        min_lr = scheduler_cfg.get('min_lr', 1e-6)
        
        scheduler = WarmupCosineScheduler(
            optimizer,
            warmup_epochs=warmup_epochs,
            max_epochs=max_epochs,
            base_lr=lr,
            min_lr=min_lr
        )
        logging.info(
            f"âœ… Using WarmupCosineScheduler "
            f"(warmup={warmup_epochs}, max_epochs={max_epochs})"
        )
    
    elif scheduler_type == 'cosine_warm_restarts':
        T_0 = scheduler_cfg.get('T_0', 100)
        T_mult = scheduler_cfg.get('T_mult', 1)
        eta_min = scheduler_cfg.get('eta_min', 1e-6)
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=T_0,
            T_mult=T_mult,
            eta_min=eta_min
        )
        logging.info(
            f"âœ… Using CosineAnnealingWarmRestarts "
            f"(T_0={T_0}, T_mult={T_mult})"
        )
    
    elif scheduler_type == 'cosine':
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=max_epochs
        )
        logging.info(f"âœ… Using CosineAnnealingLR (T_max={max_epochs})")
    
    elif scheduler_type == 'exponential':
        gamma = scheduler_cfg.get('gamma', 0.999)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)
        logging.info(f"âœ… Using ExponentialLR (gamma={gamma})")
    
    elif scheduler_type in ['none', None]:
        logging.info("No learning rate scheduler configured")
    
    else:
        raise ValueError(
            f"Unsupported scheduler type: '{scheduler_type}'. "
            f"Supported types: 'warmup_cosine', 'cosine_warm_restarts', "
            f"'cosine', 'exponential', 'none'"
        )
    
    return optimizer, scheduler
