"""
PINNs è¨“ç·´å™¨æ¨¡çµ„

æä¾›å®Œæ•´çš„è¨“ç·´å¾ªç’°ç®¡ç†ï¼ŒåŒ…å«ï¼š
- å–®æ­¥è¨“ç·´èˆ‡æ¢¯åº¦è¨ˆç®—
- é©—è­‰æŒ‡æ¨™è¨ˆç®—
- æª¢æŸ¥é»ä¿å­˜èˆ‡æ—©åœ
- å­¸ç¿’ç‡èˆ‡æ¬Šé‡èª¿åº¦
"""

import logging
import os
import time
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List

import numpy as np
import torch
import torch.nn as nn
from torch.amp.grad_scaler import GradScaler  # æ˜ç¢ºå°å…¥ GradScaler

from pinnx.losses.residuals import NSResidualLoss, BoundaryConditionLoss
from pinnx.losses.priors import PriorLossManager
from pinnx.losses.weighting import GradNormWeighter, CausalWeighter, AdaptiveWeightScheduler
from pinnx.train.loop import TrainingLoopManager, apply_point_weights_to_loss
from pinnx.utils.normalization import InputNormalizer, NormalizationConfig, DataNormalizer
from pinnx.evals.metrics import relative_L2


class Trainer:
    """
    PINNs è¨“ç·´å™¨
    
    ç®¡ç†å®Œæ•´çš„è¨“ç·´å¾ªç’°ï¼ŒåŒ…å«ï¼š
    - å„ªåŒ–å™¨èˆ‡å­¸ç¿’ç‡èª¿åº¦
    - æå¤±å‡½æ•¸èˆ‡å‹•æ…‹æ¬Šé‡èª¿æ•´
    - æª¢æŸ¥é»ä¿å­˜èˆ‡è¼‰å…¥
    - é©—è­‰èˆ‡æ—©åœæ©Ÿåˆ¶
    - è‡ªé©æ‡‰æ¡æ¨£ï¼ˆå¯é¸ï¼‰
    
    Attributes:
        model (nn.Module): PINN æ¨¡å‹
        physics (Any): ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆæ”¯æ´ NSEquations2D æˆ– VS-PINNï¼‰
        losses (Dict[str, nn.Module]): æå¤±å‡½æ•¸å­—å…¸
        config (Dict[str, Any]): å®Œæ•´è¨“ç·´é…ç½®
        device (torch.device): è¨ˆç®—è¨­å‚™
        
        optimizer (torch.optim.Optimizer): å„ªåŒ–å™¨
        lr_scheduler: å­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆå¯é¸ï¼‰
        weight_scheduler: æ¬Šé‡èª¿åº¦å™¨ï¼ˆå¯é¸ï¼‰
        input_normalizer (InputNormalizer): è¼¸å…¥æ¨™æº–åŒ–å™¨ï¼ˆå¯é¸ï¼‰
        
        epoch (int): ç•¶å‰è¨“ç·´ epoch
        history (Dict[str, List]): è¨“ç·´æ­·å²è¨˜éŒ„
    """
    
    def __init__(
        self,
        model: nn.Module,
        physics: Any,  # æ”¯æ´ NSEquations2D æˆ– VS-PINN
        losses: Dict[str, nn.Module],
        config: Dict[str, Any],
        device: torch.device,
        weighters: Optional[Dict[str, Any]] = None,
        input_normalizer: Optional[InputNormalizer] = None,
        channel_data_cache: Optional[Dict[str, Any]] = None,
        training_data: Optional[Dict[str, torch.Tensor]] = None,
    ):
        """
        åˆå§‹åŒ–è¨“ç·´å™¨
        
        Args:
            model: PINN æ¨¡å‹
            physics: ç‰©ç†æ–¹ç¨‹æ¨¡çµ„
            losses: æå¤±å‡½æ•¸å­—å…¸
            config: å®Œæ•´è¨“ç·´é…ç½®
            device: è¨ˆç®—è¨­å‚™
            weighters: æå¤±æ¬Šé‡å™¨å­—å…¸ï¼ˆå¯é¸ï¼‰
            input_normalizer: è¼¸å…¥æ¨™æº–åŒ–å™¨ï¼ˆå¯é¸ï¼‰
            channel_data_cache: é€šé“æµè³‡æ–™å¿«å–ï¼ˆå¯é¸ï¼‰
            training_data: è¨“ç·´è³‡æ–™ï¼ˆç”¨æ–¼è‡ªå‹•è¨ˆç®—æ¨™æº–åŒ–çµ±è¨ˆé‡ï¼Œå¯é¸ï¼‰
        """
        self.model = model
        self.physics = physics
        self.losses = losses
        self.config = config
        self.device = device
        self.weighters = weighters or {}
        self.input_normalizer = input_normalizer
        self.channel_data_cache = channel_data_cache or {}
        
        # è¨“ç·´é…ç½®æå–
        self.train_cfg = config['training']
        self.loss_cfg = config.get('losses', {})
        self.log_cfg = config.get('logging', {})
        self.physics_type = config.get('physics', {}).get('type', '')
        self.is_vs_cfg = self.physics_type == 'vs_pinn_channel_flow'
        
        # â­ Phase 5: æª¢æ¸¬æ¨¡å‹å¯¦éš›è¼¸å…¥ç¶­åº¦ï¼ˆæ”¯æ´ 2D/3D æ··åˆé…ç½®ï¼‰
        self.model_input_dim = self._detect_model_input_dim(model, config)
        logging.info(f"ğŸ” æª¢æ¸¬åˆ°æ¨¡å‹è¼¸å…¥ç¶­åº¦: {self.model_input_dim}D")
        
        # âœ… TASK-008: åˆå§‹åŒ–è¼¸å‡ºè®Šé‡æ¨™æº–åŒ–å™¨ï¼ˆå‚³é training_data ä»¥æ”¯æ´è‡ªå‹•è¨ˆç®—çµ±è¨ˆé‡ï¼‰
        self.data_normalizer = DataNormalizer.from_config(config, training_data=training_data)
        logging.info(f"ğŸ“ DataNormalizer åˆå§‹åŒ–: {self.data_normalizer}")
        
        # è¨“ç·´ç‹€æ…‹
        self.epoch = 0
        self.global_step = 0
        self.history = {
            'train_loss': [],
            'pde_loss': [],
            'data_loss': [],
            'val_loss': [],
            'lr': [],
        }
        
        # é©—è­‰ç›¸é—œ
        self.validation_data = None
        self.best_val_loss = float('inf')
        self.best_epoch = -1
        self.patience_counter = 0
        self.best_model_state: Optional[Dict[str, torch.Tensor]] = None
        
        # æª¢æŸ¥é»é…ç½®
        self.checkpoint_dir = Path(config.get('output', {}).get('checkpoint_dir', 'checkpoints'))
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_interval = self.train_cfg.get('checkpoint_interval', 100)
        
        # è¨“ç·´è³‡æ–™ï¼ˆå¾…å¤–éƒ¨è¨­ç½®ï¼‰
        self.training_data: Dict[str, torch.Tensor] = {}
        
        # åˆå§‹åŒ–è¨“ç·´çµ„ä»¶
        self._setup_optimizer()
        self._setup_amp()  # â­ P0.2: AMP æ··åˆç²¾åº¦
        self._setup_schedulers()
        self._setup_early_stopping()
        self._setup_adaptive_sampling()
        self._setup_fourier_annealing()
        # RANS æ¬Šé‡é ç†±å·²ç§»é™¤ï¼ˆ2025-10-14ï¼‰
        self._configure_input_transform()
        
        logging.info(f"âœ… Trainer åˆå§‹åŒ–å®Œæˆï¼ˆè¨­å‚™: {device}ï¼‰")
    
    def _detect_model_input_dim(self, model: nn.Module, config: Dict[str, Any]) -> int:
        """
        æª¢æ¸¬æ¨¡å‹çš„å¯¦éš›è¼¸å…¥ç¶­åº¦
        
        å„ªå…ˆç´šï¼š
        1. é…ç½®æ–‡ä»¶ä¸­çš„ model.in_dim
        2. æ¨¡å‹ wrapper çš„ input_min é•·åº¦ï¼ˆManualScalingWrapperï¼‰
        3. åŸºç¤æ¨¡å‹çš„ in_dim å±¬æ€§
        4. å›é€€åˆ°ç‰©ç†é…ç½®çš„åŸŸç¶­åº¦æª¢æ¸¬
        
        Returns:
            int: æ¨¡å‹è¼¸å…¥ç¶­åº¦ï¼ˆ2 æˆ– 3ï¼‰
        """
        # å„ªå…ˆï¼šå¾é…ç½®æ–‡ä»¶è®€å–
        model_cfg = config.get('model', {})
        if 'in_dim' in model_cfg:
            return int(model_cfg['in_dim'])
        
        # æ¬¡è¦ï¼šå¾ ManualScalingWrapper è®€å–
        if hasattr(model, 'input_min'):
            return model.input_min.numel()
        
        # ç¬¬ä¸‰ï¼šå¾åŸºç¤æ¨¡å‹è®€å–
        base_model = model.base_model if hasattr(model, 'base_model') else model
        if hasattr(base_model, 'in_dim'):
            return int(base_model.in_dim)
        
        # å›é€€ï¼šå¾ç‰©ç†é…ç½®æ¨æ–·
        domain_cfg = config.get('physics', {}).get('domain', {})
        if 'z_range' in domain_cfg:
            return 3
        return 2
    
    def _configure_input_transform(self) -> None:
        """Propagate input normalization metadata to the model if needed."""
        if self.input_normalizer is None:
            return
        try:
            self.input_normalizer.to(self.device)
        except AttributeError:
            pass
        
        if hasattr(self.model, 'configure_fourier_input'):
            metadata = self.input_normalizer.get_metadata()
            self.model.configure_fourier_input(metadata)
    
    def _infer_variable_order(
        self,
        out_dim: int,
        context: str = "",
        data_batch: Optional[Dict[str, torch.Tensor]] = None
    ) -> List[str]:
        """
        æ ¹æ“šè¼¸å‡ºç¶­åº¦æ¨æ–·å°æ‡‰çš„ç‰©ç†è®Šé‡é †åºã€‚
        
        å„ªå…ˆç´šï¼š
        1. é…ç½® (model.output_variables / model.variable_names / model.variables)
        2. æ¨¡å‹å±¬æ€§ (variable_names æˆ– get_variable_names())
        3. å¸¸ç”¨å•Ÿç™¼å¼ï¼ˆu,v,w,p,Sï¼‰
        """
        if out_dim <= 0:
            return []
        
        model_cfg = self.config.get('model', {})
        explicit_order = model_cfg.get('output_variables') or \
            model_cfg.get('variable_names') or \
            model_cfg.get('variables')
        if explicit_order:
            explicit = list(explicit_order)
            if len(explicit) >= out_dim:
                return explicit[:out_dim]
        
        attr_order = getattr(self.model, 'variable_names', None)
        if attr_order is None and hasattr(self.model, 'get_variable_names'):
            try:
                attr_order = self.model.get_variable_names()
            except Exception:
                attr_order = None
        if attr_order:
            attr_list = list(attr_order)
            if len(attr_list) >= out_dim:
                return attr_list[:out_dim]
        
        if out_dim == 1:
            return ['u']
        if out_dim == 2:
            return ['u', 'v']
        if out_dim == 3:
            return ['u', 'v', 'p']
        if out_dim == 4:
            return ['u', 'v', 'w', 'p']
        if out_dim == 5:
            return ['u', 'v', 'w', 'p', 'S']
        
        default_order = ['u', 'v', 'w', 'p', 'S']
        if out_dim <= len(default_order):
            return default_order[:out_dim]
        
        return [f'var_{i}' for i in range(out_dim)]
    
    def _setup_optimizer(self):
        """é…ç½®å„ªåŒ–å™¨"""
        # è™•ç† optimizer é…ç½®ç‚ºå­—ä¸²æˆ–å­—å…¸çš„æƒ…æ³
        optimizer_raw = self.train_cfg.get('optimizer', {})
        if isinstance(optimizer_raw, str):
            # ç°¡å–®é…ç½®ï¼šoptimizer: 'adam'
            optimizer_name = optimizer_raw.lower()
            optimizer_cfg = {}
        elif isinstance(optimizer_raw, dict):
            # è¤‡é›œé…ç½®ï¼šoptimizer: {type: 'adam', lr: 0.001}
            optimizer_name = optimizer_raw.get('type', 'adam').lower()
            optimizer_cfg = optimizer_raw
        else:
            # é è¨­ç‚º Adam
            optimizer_name = 'adam'
            optimizer_cfg = {}
        
        # å¾é…ç½®ä¸­æå–åƒæ•¸ï¼ˆå„ªå…ˆä½¿ç”¨ optimizer_cfgï¼Œå¦å‰‡å¾ train_cfgï¼‰
        lr = optimizer_cfg.get('lr', self.train_cfg.get('lr', 1e-3))
        weight_decay = optimizer_cfg.get('weight_decay', self.train_cfg.get('weight_decay', 0.0))
        
        if optimizer_name == 'soap':
            try:
                from pinnx.optim.soap import SOAP  # Import from our implementation
            except ImportError as exc:
                raise ImportError("SOAP å„ªåŒ–å™¨æœªæ‰¾åˆ°ï¼Œè«‹æª¢æŸ¥ pinnx/optim/soap.py") from exc
            
            # æå– SOAP å°ˆç”¨åƒæ•¸
            precondition_frequency = optimizer_cfg.get('precondition_frequency', 2)
            shampoo_beta = optimizer_cfg.get('shampoo_beta', -1)
            betas = optimizer_cfg.get('betas', (0.9, 0.999))
            
            self.optimizer = SOAP(
                self.model.parameters(),
                lr=lr,
                betas=betas,
                weight_decay=weight_decay,
                precondition_frequency=precondition_frequency,
                shampoo_beta=shampoo_beta
            )
            logging.info(f"âœ… ä½¿ç”¨ SOAP å„ªåŒ–å™¨ (lr={lr}, betas={betas}, precond_freq={precondition_frequency})")
        
        elif optimizer_name == 'lbfgs':
            self.optimizer = torch.optim.LBFGS(
                self.model.parameters(),
                lr=lr,
                max_iter=optimizer_cfg.get('max_iter', 20),
                history_size=optimizer_cfg.get('history_size', 100),
                line_search_fn=optimizer_cfg.get('line_search_fn', 'strong_wolfe')
            )
            logging.info(f"âœ… ä½¿ç”¨ L-BFGS å„ªåŒ–å™¨ï¼ˆlr={lr}ï¼‰")
        
        elif optimizer_name == 'adamw':
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=lr,
                weight_decay=weight_decay,
                betas=tuple(optimizer_cfg.get('betas', [0.9, 0.999]))
            )
            logging.info(f"âœ… ä½¿ç”¨ AdamW å„ªåŒ–å™¨ï¼ˆlr={lr}, wd={weight_decay}ï¼‰")
        
        else:  # é è¨­ Adam
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=lr,
                weight_decay=weight_decay
            )
            logging.info(f"âœ… ä½¿ç”¨ Adam å„ªåŒ–å™¨ï¼ˆlr={lr}, wd={weight_decay}ï¼‰")
    
    def _setup_amp(self):
        """
        é…ç½®è‡ªå‹•æ··åˆç²¾åº¦è¨“ç·´ï¼ˆAMPï¼‰
        
        ç­–ç•¥ï¼š
        - Forward Pass: FP32ï¼ˆç‰©ç†æ®˜å·®è¨ˆç®—æ•¸å€¼ç©©å®šï¼‰
        - Backward Pass: FP16ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰
        - åƒ…åœ¨ Adam + CUDA æ™‚å•Ÿç”¨ï¼ˆL-BFGS ä¸å…¼å®¹ï¼‰
        """
        amp_cfg = self.train_cfg.get('amp', {})
        self.use_amp = amp_cfg.get('enabled', False)
        
        # AMP æ”¯æ´æª¢æŸ¥ï¼šAdam + (CUDA æˆ– MPS)
        is_adam = isinstance(self.optimizer, torch.optim.Adam)
        is_cuda = self.device.type == 'cuda'
        is_mps = self.device.type == 'mps'
        
        if self.use_amp and not is_adam:
            logging.warning(
                "âš ï¸ AMP åƒ…æ”¯æ´ Adam å„ªåŒ–å™¨ï¼Œç•¶å‰ä½¿ç”¨ "
                f"{type(self.optimizer).__name__}ï¼Œå·²ç¦ç”¨ AMP"
            )
            self.use_amp = False
        
        # MPS é™åˆ¶ï¼šGradScaler ä¸æ”¯æ´ï¼ˆfloat64 å•é¡Œï¼‰
        if self.use_amp and is_mps:
            logging.warning(
                "âš ï¸ MPS å¾Œç«¯çš„ GradScaler å­˜åœ¨å·²çŸ¥å•é¡Œï¼ˆä¸æ”¯æ´ float64ï¼‰\n"
                "   å»ºè­°ï¼š(1) ä½¿ç”¨ CUDA è¨­å‚™ï¼Œæˆ– (2) é—œé–‰ AMP\n"
                "   å·²è‡ªå‹•ç¦ç”¨ AMP"
            )
            self.use_amp = False
        
        if self.use_amp and not is_cuda:
            logging.warning(
                "âš ï¸ AMP åƒ…åœ¨ CUDA ç’°å¢ƒå®Œå…¨æ”¯æ´ï¼Œç•¶å‰è¨­å‚™ç‚º "
                f"{self.device}ï¼Œå·²ç¦ç”¨ AMP"
            )
            self.use_amp = False
        
        # åˆå§‹åŒ– GradScalerï¼ˆåƒ… CUDAï¼‰
        if self.use_amp:
            self.scaler = GradScaler(
                'cuda',
                init_scale=2.0**16,  # åˆå§‹ç¸®æ”¾å› å­
                growth_factor=2.0,   # æˆé•·å› å­
                backoff_factor=0.5,  # å›é€€å› å­
                growth_interval=2000,  # å¢é•·é–“éš”
                enabled=True
            )
            logging.info(
                "âœ… AMP å·²å•Ÿç”¨ï¼ˆForward: FP32, Backward: FP16ï¼‰\n"
                f"   - å„ªåŒ–å™¨: {type(self.optimizer).__name__}\n"
                f"   - è¨­å‚™: {self.device} (CUDA)\n"
                f"   - GradScaler åˆå§‹ scale: {self.scaler.get_scale():.0f}"
            )
        else:
            # å‰µå»ºç¦ç”¨çš„ scalerï¼ˆçµ±ä¸€æ¥å£ï¼‰
            device_type = 'cuda' if is_cuda else 'cpu'
            self.scaler = GradScaler(device_type, enabled=False)
            if amp_cfg.get('enabled', False):
                logging.info("â„¹ï¸ AMP é…ç½®å·²ç¦ç”¨ï¼ˆä¸ç¬¦åˆå•Ÿç”¨æ¢ä»¶ï¼‰")
    
    def _setup_schedulers(self):
        """é…ç½®å­¸ç¿’ç‡èˆ‡æ¬Šé‡èª¿åº¦å™¨"""
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        scheduler_cfg = self.train_cfg.get('lr_scheduler', {})
        scheduler_type = scheduler_cfg.get('type', None)
        
        if scheduler_type == 'warmup_cosine':
            # Warmup + CosineAnnealing çµ„åˆèª¿åº¦å™¨
            from pinnx.train.schedulers import WarmupCosineScheduler
            warmup_epochs = scheduler_cfg.get('warmup_epochs', 10)
            max_epochs = self.train_cfg.get('epochs', 1000)
            base_lr = self.optimizer.param_groups[0]['lr']
            min_lr = scheduler_cfg.get('min_lr', 1e-6)
            
            self.lr_scheduler = WarmupCosineScheduler(
                self.optimizer,
                warmup_epochs=warmup_epochs,
                max_epochs=max_epochs,
                base_lr=base_lr,
                min_lr=min_lr
            )
            logging.info(f"âœ… ä½¿ç”¨ WarmupCosine èª¿åº¦å™¨ (warmup={warmup_epochs}, max={max_epochs})")
        
        elif scheduler_type == 'cosine_warm_restarts':
            # CosineAnnealing with Warm Restarts
            T_0 = scheduler_cfg.get('T_0', 100)
            T_mult = scheduler_cfg.get('T_mult', 1)
            eta_min = scheduler_cfg.get('eta_min', 1e-6)
            
            self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=T_0,
                T_mult=T_mult,
                eta_min=eta_min
            )
            logging.info(f"âœ… ä½¿ç”¨ CosineAnnealingWarmRestarts (T_0={T_0}, T_mult={T_mult})")
        
        elif scheduler_type == 'cosine':
            # æ¨™æº– CosineAnnealing èª¿åº¦å™¨
            self.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.train_cfg.get('epochs', 1000),
                eta_min=scheduler_cfg.get('min_lr', 1e-6)
            )
            logging.info("âœ… ä½¿ç”¨ Cosine å­¸ç¿’ç‡èª¿åº¦å™¨")
        
        elif scheduler_type == 'exponential':
            # æŒ‡æ•¸è¡°æ¸›èª¿åº¦å™¨
            gamma = scheduler_cfg.get('gamma', 0.999)
            self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
                self.optimizer,
                gamma=gamma
            )
            logging.info(f"âœ… ä½¿ç”¨ Exponential èª¿åº¦å™¨ (gamma={gamma})")
        
        elif scheduler_type == 'step':
            # StepLR èª¿åº¦å™¨
            self.lr_scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=scheduler_cfg.get('step_size', 100),
                gamma=scheduler_cfg.get('gamma', 0.5)
            )
            logging.info("âœ… ä½¿ç”¨ Step å­¸ç¿’ç‡èª¿åº¦å™¨")
        
        elif scheduler_type in ['none', None]:
            # ç„¡èª¿åº¦å™¨ï¼Œä½¿ç”¨å›ºå®šå­¸ç¿’ç‡
            self.lr_scheduler = None
            logging.info("â„¹ï¸ æœªé…ç½®å­¸ç¿’ç‡èª¿åº¦å™¨ï¼Œä½¿ç”¨å›ºå®šå­¸ç¿’ç‡")
        
        else:
            # ä¸æ”¯æ´çš„é¡å‹
            logging.warning(
                f"âš ï¸ æœªçŸ¥çš„èª¿åº¦å™¨é¡å‹ '{scheduler_type}'ï¼Œä½¿ç”¨å›ºå®šå­¸ç¿’ç‡ã€‚"
                f"æ”¯æ´çš„é¡å‹ï¼š'warmup_cosine', 'cosine_warm_restarts', 'cosine', "
                f"'exponential', 'step', 'none'"
            )
            self.lr_scheduler = None
        
        # æ¬Šé‡èª¿åº¦å™¨ï¼ˆæš«æ™‚ä¿ç•™ç‚º Noneï¼Œç”±å¤–éƒ¨ç®¡ç†ï¼‰
        self.weight_scheduler = None
    
    def _setup_early_stopping(self):
        """é…ç½®æ—©åœæ©Ÿåˆ¶"""
        self.early_stopping_cfg = self.train_cfg.get('early_stopping', {})
        self.early_stopping_enabled = self.early_stopping_cfg.get('enabled', False)
        self.patience = self.early_stopping_cfg.get('patience', 50)
        self.min_delta = self.early_stopping_cfg.get('min_delta', 1e-6)
        
        if self.early_stopping_enabled:
            logging.info(f"âœ… æ—©åœæ©Ÿåˆ¶å•Ÿç”¨ï¼ˆpatience={self.patience}, min_delta={self.min_delta}ï¼‰")
    
    def _setup_adaptive_sampling(self):
        """é…ç½®è‡ªé©æ‡‰æ¡æ¨£"""
        adaptive_cfg = self.train_cfg.get('adaptive_sampling', {})
        self.adaptive_sampling_enabled = adaptive_cfg.get('enabled', False)
        
        if self.adaptive_sampling_enabled:
            self.loop_manager = TrainingLoopManager(self.config)
            logging.info("âœ… è‡ªé©æ‡‰æ¡æ¨£å•Ÿç”¨")
        else:
            self.loop_manager = None
    
    def _setup_fourier_annealing(self):
        """é…ç½® Fourier ç‰¹å¾µé€€ç«èª¿åº¦å™¨"""
        from pinnx.train.fourier_annealing import (
            FourierAnnealingScheduler, 
            create_default_annealing,
            create_channel_flow_annealing
        )
        
        # æª¢æŸ¥é…ç½®ä¸­æ˜¯å¦å•Ÿç”¨é€€ç«
        annealing_cfg = self.config.get('fourier_annealing', {})
        if not annealing_cfg.get('enabled', False):
            self.fourier_annealing = None
            return
        
        # æå–é€€ç«ç­–ç•¥
        strategy = annealing_cfg.get('strategy', 'conservative')
        
        # æ ¹æ“šç­–ç•¥å‰µå»ºèª¿åº¦å™¨
        if strategy in ['conservative', 'aggressive', 'fine']:
            # ä½¿ç”¨é è¨­ç­–ç•¥
            stages = create_default_annealing(strategy)
            axes_names = annealing_cfg.get('axes_names', ['x', 'y', 'z'])
            self.fourier_annealing = FourierAnnealingScheduler(stages, axes_names=axes_names)
            logging.info(f"âœ… Fourier é€€ç«å•Ÿç”¨ï¼ˆç­–ç•¥: {strategy}ï¼‰")
        
        elif strategy == 'channel_flow':
            # ä½¿ç”¨é€šé“æµå°ˆç”¨é…ç½®
            per_axis_config = create_channel_flow_annealing()
            # å°‡æ¯è»¸é…ç½®è½‰æ›ç‚ºèª¿åº¦å™¨æ ¼å¼
            # ä½¿ç”¨ x è»¸ä½œç‚ºå…¨å±€éšæ®µï¼Œy/z ä½œç‚ºæ¯è»¸è¦†è“‹
            global_stages = per_axis_config['x']
            per_axis_stages = {'y': per_axis_config['y'], 'z': per_axis_config['z']}
            self.fourier_annealing = FourierAnnealingScheduler(
                global_stages, 
                per_axis_stages=per_axis_stages,
                axes_names=['x', 'y', 'z']
            )
            logging.info("âœ… Fourier é€€ç«å•Ÿç”¨ï¼ˆé€šé“æµå°ˆç”¨é…ç½®ï¼‰")
        
        elif strategy == 'custom':
            # è‡ªå®šç¾©é…ç½®ï¼ˆå¾é…ç½®æ–‡ä»¶è®€å–ï¼‰
            stages_cfg = annealing_cfg.get('stages', [])
            if not stages_cfg:
                logging.warning("âš ï¸ è‡ªå®šç¾©é€€ç«ç­–ç•¥æœªæä¾›éšæ®µé…ç½®ï¼Œç¦ç”¨é€€ç«")
                self.fourier_annealing = None
                return
            
            from pinnx.train.fourier_annealing import AnnealingStage
            stages = [
                AnnealingStage(s['end_ratio'], s['frequencies'], s.get('description', ''))
                for s in stages_cfg
            ]
            axes_names = annealing_cfg.get('axes_names', ['x', 'y', 'z'])
            self.fourier_annealing = FourierAnnealingScheduler(stages, axes_names=axes_names)
            logging.info(f"âœ… Fourier é€€ç«å•Ÿç”¨ï¼ˆè‡ªå®šç¾©é…ç½®ï¼Œ{len(stages)} éšæ®µï¼‰")
        
        else:
            logging.warning(f"âš ï¸ æœªçŸ¥é€€ç«ç­–ç•¥ '{strategy}'ï¼Œç¦ç”¨é€€ç«")
            self.fourier_annealing = None
    
    # RANS æ–¹æ³•å·²ç§»é™¤ï¼ˆ2025-10-14ï¼‰ï¼š
    # - _setup_rans_warmup()
    # - _update_rans_weights()
    
    def step(
        self,
        data_batch: Dict[str, torch.Tensor],
        epoch: int
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå–®æ­¥è¨“ç·´ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        
        åŒ…å«æ ¸å¿ƒæå¤±è¨ˆç®—ï¼š
        - PDE æ®˜å·®ï¼ˆmomentum + continuityï¼‰
        - å£é¢é‚Šç•Œæ¢ä»¶ï¼ˆç„¡æ»‘ç§»ï¼‰
        - è³‡æ–™ç›£ç£æå¤±ï¼ˆsensor pointsï¼‰
        
        Args:
            data_batch: è¨“ç·´è³‡æ–™æ‰¹æ¬¡ï¼ˆåŒ…å« PDEã€é‚Šç•Œã€è³‡æ–™é»ï¼‰
            epoch: ç•¶å‰ epoch æ•¸
        
        Returns:
            åŒ…å«æå¤±å’ŒæŒ‡æ¨™çš„å­—å…¸
        """
        self.optimizer.zero_grad()
        
        # â­ Phase 5: æª¢æŸ¥æ˜¯å¦ç‚º VS-PINN ç‰©ç†ï¼ˆç”¨æ–¼é¸æ“‡å°æ‡‰çš„æ®˜å·®è¨ˆç®—æ–¹æ³•ï¼‰
        # æ³¨æ„ï¼šåº§æ¨™ç¶­åº¦å·²ç”± self.model_input_dim æ§åˆ¶ï¼Œæ­¤ flag åƒ…ç”¨æ–¼ physics API é¸æ“‡
        is_vs_pinn = 'z_pde' in data_batch and hasattr(self.physics, 'compute_momentum_residuals')
        
        # ==================== è¼”åŠ©å‡½æ•¸ ====================
        def prepare_model_coords(
            coord_tensor: torch.Tensor, 
            require_grad: bool = False
        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            """
            æº–å‚™æ¨¡å‹è¼¸å…¥åº§æ¨™ï¼ˆè™•ç†æ¨™æº–åŒ–èˆ‡ç¸®æ”¾ï¼‰
            
            Returns:
                (coords_physical, coords_norm, model_coords):
                - coords_physical: ç‰©ç†åº§æ¨™ï¼ˆä¾› PDE autograd ä½¿ç”¨ï¼‰
                - coords_norm: æ¨™æº–åŒ–å¾Œçš„åº§æ¨™ï¼ˆè‹¥ç„¡ InputNormalizer å‰‡èˆ‡ coords_physical ç›¸åŒï¼‰
                - model_coords: æœ€çµ‚æ¨¡å‹è¼¸å…¥ï¼ˆå¯èƒ½ç¶“é VS-PINN scalingï¼‰
            """
            # 1. ä¿ç•™ç‰©ç†åº§æ¨™ï¼ˆå•Ÿç”¨æ¢¯åº¦è¿½è¹¤ï¼‰
            coords_physical = coord_tensor.clone()
            if require_grad:
                coords_physical.requires_grad_(True)
            
            # 2. è¼¸å…¥æ¨™æº–åŒ–ï¼ˆå¯é¸ï¼‰
            if self.input_normalizer is not None:
                coords_norm = self.input_normalizer.transform(coords_physical)
            else:
                coords_norm = coords_physical
            
            # 3. VS-PINN ç¸®æ”¾ï¼ˆå¯é¸ï¼Œä½œç”¨æ–¼æ¨™æº–åŒ–å¾Œçš„åº§æ¨™ï¼‰
            if is_vs_pinn and hasattr(self.physics, 'scale_coordinates'):
                model_coords = self.physics.scale_coordinates(coords_norm)
            else:
                model_coords = coords_norm
            
            return coords_physical, coords_norm, model_coords
        
        # ==================== 1. PDE æ®˜å·®æå¤± ====================
        # çµ„åˆ PDE é»åº§æ¨™ï¼ˆä½¿ç”¨æ¨¡å‹å¯¦éš›è¼¸å…¥ç¶­åº¦ï¼‰
        if self.model_input_dim == 3:
            coords_pde = torch.cat([data_batch['x_pde'], data_batch['y_pde'], data_batch['z_pde']], dim=1)
        else:
            coords_pde = torch.cat([data_batch['x_pde'], data_batch['y_pde']], dim=1)
        
        # âœ… è§£åŒ…ä¸‰å…ƒçµ„ï¼š(ç‰©ç†åº§æ¨™, æ¨™æº–åŒ–åº§æ¨™, æ¨¡å‹è¼¸å…¥åº§æ¨™)
        coords_pde_physical, coords_pde_norm, model_coords_pde = prepare_model_coords(coords_pde, require_grad=True)
        
        # èª¿è©¦ï¼šæ‰“å°ç¶­åº¦ï¼ˆåƒ…ç¬¬ä¸€å€‹ epochï¼‰
        if epoch == 0 and not hasattr(self, '_debug_printed'):
            logging.info(f"ğŸ” èª¿è©¦è³‡è¨Šï¼šcoords_pde.shape={coords_pde.shape}, model_coords_pde.shape={model_coords_pde.shape}")
            logging.info(f"  - coords_pde_physical æ¢¯åº¦è¿½è¹¤: {coords_pde_physical.requires_grad}")
            self._debug_printed = True
        
        # æ¨¡å‹é æ¸¬ï¼ˆæ¨™æº–åŒ–ç©ºé–“è¼¸å‡ºï¼‰
        u_pred_norm = self.model(model_coords_pde)
        
        # âœ… ç«‹å³åæ¨™æº–åŒ–ç‚ºç‰©ç†é‡ï¼ˆä¾› PDE æ®˜å·®è¨ˆç®—ä½¿ç”¨ï¼‰
        var_order = self._infer_variable_order(u_pred_norm.shape[1], context='pde')
        u_pred_pde_physical_raw = self.data_normalizer.denormalize_batch(u_pred_norm, var_order=var_order)
        # ç¢ºä¿æ˜¯ Tensor é¡å‹ï¼ˆdenormalize_batch ä¿æŒè¼¸å…¥é¡å‹ï¼‰
        u_pred_pde_physical: torch.Tensor = u_pred_pde_physical_raw if isinstance(u_pred_pde_physical_raw, torch.Tensor) else torch.tensor(u_pred_pde_physical_raw, device=self.device)  # type: ignore
        
        # âœ… æå–é€Ÿåº¦å’Œå£“åŠ›åˆ†é‡ï¼ˆç‰©ç†ç©ºé–“ï¼‰
        if is_vs_pinn:
            if u_pred_pde_physical.shape[1] == 3:
                # æ¨¡å‹åªè¼¸å‡º [u, v, p]ï¼Œæ·»åŠ  w=0
                velocity_phys = u_pred_pde_physical[:, :2]
                pressure_phys = u_pred_pde_physical[:, 2:3]
                w_component_phys = torch.zeros_like(pressure_phys)
                predictions_phys = torch.cat([velocity_phys, w_component_phys, pressure_phys], dim=1)
            elif u_pred_pde_physical.shape[1] == 4:
                # æ¨¡å‹è¼¸å‡ºå®Œæ•´ [u, v, w, p]
                predictions_phys = u_pred_pde_physical
                velocity_phys = u_pred_pde_physical[:, :2]
                pressure_phys = u_pred_pde_physical[:, 3:4]
            else:
                raise ValueError(f"VS-PINN æ¨¡å‹è¼¸å‡ºç¶­åº¦éŒ¯èª¤ï¼š{u_pred_pde_physical.shape[1]}ï¼ŒæœŸæœ› 3 æˆ– 4")
        else:
            if u_pred_pde_physical.shape[1] == 3:
                velocity_phys = u_pred_pde_physical[:, :2]
                pressure_phys = u_pred_pde_physical[:, 2:3]
            elif u_pred_pde_physical.shape[1] == 4:
                velocity_phys = u_pred_pde_physical[:, :2]
                pressure_phys = u_pred_pde_physical[:, 3:4]
            else:
                raise ValueError(f"æ¨™æº– PINN è¼¸å‡ºç¶­åº¦éŒ¯èª¤: {u_pred_pde_physical.shape[1]}ï¼ŒæœŸæœ› 3 æˆ– 4")
            predictions_phys = None
        
        # âœ… è¨ˆç®—ç‰©ç†æ®˜å·®ï¼ˆä½¿ç”¨ç‰©ç†åº§æ¨™ + ç‰©ç†é‡ï¼‰
        try:
            if is_vs_pinn and hasattr(self.physics, 'compute_momentum_residuals'):
                # VS-PINN 3D
                residuals_mom = self.physics.compute_momentum_residuals(
                    coords_pde_physical,    # âœ… ç‰©ç†åº§æ¨™ï¼ˆå«æ¢¯åº¦ï¼‰
                    predictions_phys,        # âœ… ç‰©ç†é‡
                    scaled_coords=model_coords_pde  # VS-PINN ä»éœ€ç¸®æ”¾åº§æ¨™
                )
                continuity_residual = self.physics.compute_continuity_residual(
                    coords_pde_physical,    # âœ… ç‰©ç†åº§æ¨™ï¼ˆå«æ¢¯åº¦ï¼‰
                    predictions_phys,        # âœ… ç‰©ç†é‡
                    scaled_coords=model_coords_pde
                )
                residuals = {
                    'momentum_x': residuals_mom['momentum_x'],
                    'momentum_y': residuals_mom['momentum_y'],
                    'momentum_z': residuals_mom['momentum_z'],
                    'continuity': continuity_residual,
                }
            else:
                # æ¨™æº– NS 2Dï¼šéœ€è¦ 2D åº§æ¨™ [x, y] + å®Œæ•´ pred å¼µé‡ [u, v, p, S]
                # âš ï¸ ä¿®å¾©ï¼šåªå‚³éå‰ 2D åº§æ¨™çµ¦ 2D ç‰©ç†æ¨¡çµ„
                coords_pde_2d = coords_pde_physical[:, :2]  # å– [x, y]ï¼Œå¿½ç•¥ z
                
                # æ§‹å»ºå®Œæ•´é æ¸¬å¼µé‡ [u, v, p, S]ï¼ˆNS 2D éœ€è¦ 4 å€‹åˆ†é‡ï¼‰
                source_term_phys = torch.zeros_like(pressure_phys)  # å‡è¨­æºé …ç‚º 0
                pred_full_phys = torch.cat([velocity_phys, pressure_phys, source_term_phys], dim=1)
                
                # èª¿ç”¨ NSEquations2D.residual_unified() æ–¹æ³•
                residuals = self.physics.residual_unified(coords_pde_2d, pred_full_phys)
            
            # æ‡‰ç”¨é»æ¬Šé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            pde_point_weights = data_batch.get('pde_point_weights', None)
            if pde_point_weights is not None:
                momentum_x_loss = apply_point_weights_to_loss(residuals['momentum_x']**2, pde_point_weights)
                momentum_y_loss = apply_point_weights_to_loss(residuals['momentum_y']**2, pde_point_weights)
                momentum_z_loss = apply_point_weights_to_loss(residuals['momentum_z']**2, pde_point_weights) if is_vs_pinn else torch.tensor(0.0, device=self.device)
                continuity_loss = apply_point_weights_to_loss(residuals['continuity']**2, pde_point_weights)
            else:
                momentum_x_loss = torch.mean(residuals['momentum_x']**2)
                momentum_y_loss = torch.mean(residuals['momentum_y']**2)
                momentum_z_loss = torch.mean(residuals['momentum_z']**2) if is_vs_pinn else torch.tensor(0.0, device=self.device)
                continuity_loss = torch.mean(residuals['continuity']**2)
        
        except Exception as e:
            logging.error(f"ğŸš¨ ç‰©ç†æ®˜å·®è¨ˆç®—å¤±æ•—: {e}")
            logging.error(f"coords_pde shape: {coords_pde.shape}, u_pred_norm shape: {u_pred_norm.shape}, u_pred_pde_physical shape: {u_pred_pde_physical.shape}")
            raise
        
        # ==================== 1B. RANS å·²ç§»é™¤ï¼ˆåƒ…ä¿ç•™ç‚º LoFi å ´è¨ºæ–·å·¥å…·ï¼‰====================
        # âš ï¸ RANS ä¸å†ä½œç‚ºæå¤±é …åƒèˆ‡è¨“ç·´ï¼ˆ2025-10-14 è®Šæ›´ï¼‰
        # compute_rans_residuals() ä¿ç•™åœ¨ physics æ¨¡çµ„ä¸­ç”¨æ–¼ï¼š
        # 1. æœªä¾† LoFi å ´è¼¸å…¥ç‰¹å¾µæå–
        # 2. è¨“ç·´éç¨‹ä¸­çš„è¨ºæ–·èˆ‡ç›£æ§
        
        # ==================== 2. å£é¢é‚Šç•Œæ¢ä»¶æå¤± ====================
        # çµ„åˆé‚Šç•Œæ¢ä»¶åº§æ¨™ï¼ˆä½¿ç”¨æ¨¡å‹å¯¦éš›è¼¸å…¥ç¶­åº¦ï¼‰
        if self.model_input_dim == 3:
            coords_bc = torch.cat([data_batch['x_bc'], data_batch['y_bc'], data_batch['z_bc']], dim=1)
        else:
            coords_bc = torch.cat([data_batch['x_bc'], data_batch['y_bc']], dim=1)
        
        # âœ… è§£åŒ…ä¸‰å…ƒçµ„
        coords_bc_physical, coords_bc_norm, model_coords_bc = prepare_model_coords(coords_bc, require_grad=False)
        u_bc_pred_norm = self.model(model_coords_bc)
        
        # âœ… åæ¨™æº–åŒ–ç‚ºç‰©ç†é‡ï¼ˆå£é¢é‚Šç•Œæ¢ä»¶åœ¨ç‰©ç†ç©ºé–“æ‡‰ç‚º 0ï¼‰
        var_order_bc = self._infer_variable_order(u_bc_pred_norm.shape[1], context='bc')
        u_bc_pred_phys_raw = self.data_normalizer.denormalize_batch(u_bc_pred_norm, var_order=var_order_bc)
        u_bc_pred_phys: torch.Tensor = u_bc_pred_phys_raw if isinstance(u_bc_pred_phys_raw, torch.Tensor) else torch.tensor(u_bc_pred_phys_raw, device=self.device)  # type: ignore
        
        # è­˜åˆ¥å£é¢é»ï¼ˆy = Â±1ï¼‰
        y_bc = data_batch['y_bc']
        wall_mask = (torch.abs(y_bc - 1.0) < 1e-3) | (torch.abs(y_bc + 1.0) < 1e-3)
        wall_mask = wall_mask.squeeze()
        
        if wall_mask.sum() > 0:
            # âœ… ä½¿ç”¨ç‰©ç†é‡ï¼ˆå£é¢é€Ÿåº¦æ‡‰ç‚º 0ï¼‰
            u_wall = u_bc_pred_phys[wall_mask, 0]  # u åˆ†é‡ï¼ˆç‰©ç†ç©ºé–“ï¼‰
            v_wall = u_bc_pred_phys[wall_mask, 1]  # v åˆ†é‡ï¼ˆç‰©ç†ç©ºé–“ï¼‰
            wall_loss = torch.mean(u_wall**2 + v_wall**2)
        else:
            wall_loss = torch.tensor(0.0, device=self.device)
            if epoch == 0:
                logging.warning(f"âš ï¸ æœªæª¢æ¸¬åˆ°å£é¢é‚Šç•Œé»ï¼y_bc ç¯„åœ: [{y_bc.min():.6f}, {y_bc.max():.6f}]")
        
        # ==================== 3. è³‡æ–™ç›£ç£æå¤± ====================
        # çµ„åˆæ„Ÿæ¸¬å™¨åº§æ¨™ï¼ˆä½¿ç”¨æ¨¡å‹å¯¦éš›è¼¸å…¥ç¶­åº¦ï¼‰
        if self.model_input_dim == 3:
            coords_sensors = torch.cat([data_batch['x_sensors'], data_batch['y_sensors'], data_batch['z_sensors']], dim=1)
        else:
            coords_sensors = torch.cat([data_batch['x_sensors'], data_batch['y_sensors']], dim=1)
        
        # âœ… è§£åŒ…ä¸‰å…ƒçµ„
        coords_sensors_physical, coords_sensors_norm, model_coords_sensors = prepare_model_coords(coords_sensors, require_grad=False)
        u_sensors_pred_norm = self.model(model_coords_sensors)
        
        # âœ… åæ¨™æº–åŒ–ç‚ºç‰©ç†é‡ï¼ˆç›´æ¥èˆ‡çœŸå¯¦ç‰©ç†é‡æ¯”è¼ƒï¼‰
        var_order_sensors = self._infer_variable_order(
            u_sensors_pred_norm.shape[1],
            context='sensors',
            data_batch=data_batch
        )
        u_sensors_pred_phys_raw = self.data_normalizer.denormalize_batch(u_sensors_pred_norm, var_order=var_order_sensors)
        u_sensors_pred_phys: torch.Tensor = u_sensors_pred_phys_raw if isinstance(u_sensors_pred_phys_raw, torch.Tensor) else torch.tensor(u_sensors_pred_phys_raw, device=self.device)  # type: ignore
        
        # æå–çœŸå¯¦è³‡æ–™ï¼ˆç‰©ç†ç©ºé–“ï¼‰
        u_true = data_batch['u_sensors']
        v_true = data_batch['v_sensors']
        w_true = data_batch.get('w_sensors', None)  # 3D é€šé“æµæ‰æœ‰ w
        p_true = data_batch['p_sensors']
        
        # âœ… ç‰©ç†ç©ºé–“çš„ MSEï¼ˆç›´æ¥æ¯”å°ï¼Œä¸å†æ¨™æº–åŒ–çœŸå¯¦å€¼ï¼‰
        u_loss = torch.mean((u_sensors_pred_phys[:, 0:1] - u_true)**2)
        v_loss = torch.mean((u_sensors_pred_phys[:, 1:2] - v_true)**2)
        
        # â­ Phase 5: æ ¹æ“šè³‡æ–™èˆ‡æ¨¡å‹ç¶­åº¦å‹•æ…‹é¸æ“‡æå¤±è¨ˆç®—
        # æª¢æŸ¥ w åˆ†é‡æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆï¼ˆéç©ºå¼µé‡ï¼‰
        has_w_data = w_true is not None and w_true.numel() > 0
        model_has_w = u_sensors_pred_phys.shape[1] >= 4  # æ¨¡å‹è¼¸å‡ºè‡³å°‘ 4 å€‹åˆ†é‡
        
        if has_w_data and model_has_w:
            # å®Œæ•´ 3D æ¨¡å¼ï¼šu, v, w, p
            w_loss = torch.mean((u_sensors_pred_phys[:, 2:3] - w_true)**2)
            pressure_loss = torch.mean((u_sensors_pred_phys[:, 3:4] - p_true)**2)
            velocity_loss = u_loss + v_loss + w_loss
        elif model_has_w and not has_w_data:
            # æ··åˆæ¨¡å¼ï¼šæ¨¡å‹è¼¸å‡º 4D ä½†è³‡æ–™åƒ…æœ‰ 3Dï¼ˆå¿½ç•¥ w é æ¸¬ï¼‰
            w_loss = torch.tensor(0.0, device=u_loss.device)
            pressure_loss = torch.mean((u_sensors_pred_phys[:, 3:4] - p_true)**2)  # âš ï¸ å£“åŠ›åœ¨ index 3
            velocity_loss = u_loss + v_loss
        else:
            # æ¨™æº– 2D æ¨¡å¼ï¼šu, v, p
            w_loss = torch.tensor(0.0, device=u_loss.device)
            pressure_loss = torch.mean((u_sensors_pred_phys[:, 2:3] - p_true)**2)  # å£“åŠ›åœ¨ index 2
            velocity_loss = u_loss + v_loss
        
        data_loss = velocity_loss + pressure_loss
        
        # çµ±è¨ˆåŸå§‹ PDE æå¤±ï¼ˆæœªåŠ æ¬Šï¼Œä¾¿æ–¼æ—¥å¿—èˆ‡åˆ†æï¼‰
        pde_loss = momentum_x_loss + momentum_y_loss + momentum_z_loss + continuity_loss
        
        # ==================== 4. çµ„åˆæå¤±ï¼ˆå« GradNorm å‹•æ…‹æ¬Šé‡ï¼‰====================
        loss_cfg = self.loss_cfg
        
        # æ”¶é›†å¯ä¾›æ¬Šé‡èª¿æ•´çš„æå¤±é …
        loss_terms: Dict[str, torch.Tensor] = {
            'data': data_loss,
            'momentum_x': momentum_x_loss,
            'momentum_y': momentum_y_loss,
            'continuity': continuity_loss,
            'wall_constraint': wall_loss,
        }
        if is_vs_pinn:
            loss_terms['momentum_z'] = momentum_z_loss
        
        gradnorm_weighter = self.weighters.get('gradnorm') if hasattr(self, 'weighters') else None
        gradnorm_weights: Optional[Dict[str, float]] = None
        gradnorm_ratio: Dict[str, float] = {}
        if gradnorm_weighter is not None:
            available_losses = {
                name: loss_terms[name]
                for name in gradnorm_weighter.loss_names
                if name in loss_terms
            }
            if len(available_losses) >= 2:
                gradnorm_weights = gradnorm_weighter.update_weights(available_losses)
                initial_values = getattr(gradnorm_weighter, 'initial_weight_values', {})
                eps = float(getattr(gradnorm_weighter, 'eps', 1e-12))
                for name, weight_val in gradnorm_weights.items():
                    base = float(initial_values.get(name, 1.0))
                    if abs(base) < eps:
                        base = 1.0
                    gradnorm_ratio[name] = float(weight_val) / base
                if gradnorm_weighter.step_count % gradnorm_weighter.update_frequency == 0:
                    logging.debug(
                        "GradNorm weights @ step %d: %s",
                        gradnorm_weighter.step_count,
                        {k: round(v, 4) for k, v in gradnorm_weights.items()}
                    )
            else:
                logging.debug("GradNorm requires at least two loss terms; skipping update.")
        
        def scaled_weight(name: str, base: float) -> float:
            return float(base) * gradnorm_ratio.get(name, 1.0)

        # åŸºç¤æ¬Šé‡ï¼ˆå¯å¾é…ç½®è®€å–æˆ–ä½¿ç”¨é è¨­ï¼‰
        base_data_weight = loss_cfg.get('data_weight', 100.0)
        base_pde_weight = loss_cfg.get('pde_weight', 1.0)
        base_bc_weight = loss_cfg.get('bc_weight', 10.0)
        
        # â­ æ”¯æ´ç´°ç²’åº¦æ¬Šé‡é…ç½®ï¼ˆå„ªå…ˆç´šï¼šç´°é … > çµ±ä¸€ > é è¨­ï¼‰
        # å„ªå…ˆè®€å–ç´°é …æ¬Šé‡ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å›é€€åˆ°çµ±ä¸€æ¬Šé‡
        w_data = scaled_weight('data', base_data_weight)
        w_momentum_x = scaled_weight('momentum_x', loss_cfg.get('momentum_x_weight', base_pde_weight))
        w_momentum_y = scaled_weight('momentum_y', loss_cfg.get('momentum_y_weight', base_pde_weight))
        w_momentum_z = scaled_weight('momentum_z', loss_cfg.get('momentum_z_weight', base_pde_weight)) if is_vs_pinn else 0.0
        w_continuity = scaled_weight('continuity', loss_cfg.get('continuity_weight', base_pde_weight))
        w_bc = scaled_weight('wall_constraint', loss_cfg.get('wall_constraint_weight', base_bc_weight))
        
        # ğŸ“Š è¨ºæ–·æ—¥èªŒï¼šåœ¨è¨“ç·´é–‹å§‹æ™‚æ‰“å°å¯¦éš›æ‡‰ç”¨çš„æ¬Šé‡
        if epoch == 0 and not hasattr(self, '_weights_logged'):
            logging.info("=" * 60)
            logging.info("ğŸ“Š Loss æ¬Šé‡é…ç½®æ‘˜è¦")
            logging.info("=" * 60)
            logging.info(f"  Data Loss:        {w_data:.2e}")
            logging.info(f"  Momentum X:       {w_momentum_x:.2e}")
            logging.info(f"  Momentum Y:       {w_momentum_y:.2e}")
            if is_vs_pinn:
                logging.info(f"  Momentum Z:       {w_momentum_z:.2e}")
            logging.info(f"  Continuity:       {w_continuity:.2e}")
            logging.info(f"  Wall Constraint:  {w_bc:.2e}")
            logging.info("=" * 60)
            self._weights_logged = True
        
        # ==================== RANS æå¤±å·²ç§»é™¤ï¼ˆ2025-10-14ï¼‰====================
        # RANS æ¬Šé‡é ç†±åŠŸèƒ½å·²åœç”¨ï¼Œç›¸é—œæå¤±é …å·²å¾è¨“ç·´å¾ªç’°ä¸­ç§»é™¤

        weighted_pde_loss = (
            w_momentum_x * momentum_x_loss +
            w_momentum_y * momentum_y_loss +
            w_momentum_z * momentum_z_loss +
            w_continuity * continuity_loss
        )
        weighted_data_loss = w_data * data_loss
        weighted_wall_loss = w_bc * wall_loss
        
        # â­ Phase 6C: å‡å€¼ç´„æŸæå¤±ï¼ˆè‹¥å•Ÿç”¨ï¼‰
        mean_constraint_loss = torch.tensor(0.0, device=data_loss.device)
        mean_constraint_cfg = loss_cfg.get('mean_constraint', {})
        if mean_constraint_cfg.get('enabled', False) and 'mean_constraint' in self.losses:
            # âœ… ä½¿ç”¨å·²åæ¨™æº–åŒ–çš„ PDE é»é æ¸¬ï¼ˆæ›´å…¨åŸŸçš„çµ±è¨ˆé‡ï¼‰
            # u_pred_pde_physical å·²åœ¨ Line 515 åæ¨™æº–åŒ–å®Œæˆ
            
            target_means = mean_constraint_cfg.get('target_means', {})
            field_indices = {'u': 0, 'v': 1, 'w': 2}  # ä¸ç´„æŸå£“åŠ›å ´
            
            mean_constraint_loss_fn = self.losses['mean_constraint']
            mean_constraint_loss = mean_constraint_loss_fn(
                predictions=u_pred_pde_physical,  # âœ… ä½¿ç”¨ç‰©ç†ç©ºé–“çš„é æ¸¬
                target_means=target_means,         # ç‰©ç†ç©ºé–“çš„ç›®æ¨™å‡å€¼
                field_indices=field_indices
            )
            
            w_mean_constraint = mean_constraint_cfg.get('weight', 10.0)
            mean_constraint_loss = w_mean_constraint * mean_constraint_loss
            
            # è¨˜éŒ„ï¼ˆä½é »ç‡ï¼‰
            if epoch == 0 or (epoch > 0 and epoch % 100 == 0):
                logging.info(f"ğŸ“Š å‡å€¼ç´„æŸæå¤± @ Epoch {epoch}: {mean_constraint_loss.item():.6f}")
                # è¨˜éŒ„é æ¸¬å‡å€¼ä»¥ä¾¿è¨ºæ–·
                with torch.no_grad():
                    for field_name, idx in field_indices.items():
                        if idx < u_pred_pde_physical.shape[-1]:
                            pred_mean = u_pred_pde_physical[:, idx].mean().item()
                            target_mean = target_means.get(field_name, 0.0)
                            logging.info(f"   {field_name}: pred={pred_mean:.4f}, target={target_mean:.4f}")
        
        # ç¸½æå¤±ï¼ˆä¸å« RANS é …ï¼‰
        total_loss = (
            weighted_data_loss +
            weighted_pde_loss +
            weighted_wall_loss +
            mean_constraint_loss  # â­ Phase 6C: å‡å€¼ç´„æŸæå¤±
        )
        
        # ==================== 5. åå‘å‚³æ’­èˆ‡å„ªåŒ– ====================
        # â­ P0.2: AMP æ··åˆç²¾åº¦ç­–ç•¥
        # - Forward Pass: å·²åœ¨ä¸Šé¢å®Œæˆï¼ˆFP32ï¼‰
        # - Backward Pass: ä½¿ç”¨ GradScalerï¼ˆFP16 æ¢¯åº¦ç´¯ç©ï¼‰
        
        # ç¸®æ”¾æå¤±ä¸¦åå‘å‚³æ’­
        scaled_loss = self.scaler.scale(total_loss)
        scaled_loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆéœ€å…ˆ unscaleï¼‰
        if self.train_cfg.get('gradient_clip', 0.0) > 0:
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                self.train_cfg['gradient_clip']
            )
        
        # L-BFGS éœ€è¦ closure å‡½æ•¸ï¼Œå…¶ä»–å„ªåŒ–å™¨ç›´æ¥ step()
        if isinstance(self.optimizer, torch.optim.LBFGS):
            # L-BFGS ä¸æ”¯æ´ AMPï¼ˆå·²åœ¨ _setup_amp ä¸­ç¦ç”¨ï¼‰
            def closure():
                return total_loss
            self.optimizer.step(closure)  # type: ignore
        else:
            # Adam: ä½¿ç”¨ scaler.step() è‡ªå‹•è™•ç† unscale + update
            self.scaler.step(self.optimizer)
            self.scaler.update()
        
        # ğŸ“‰ Steps-based èª¿åº¦å™¨æ›´æ–°ï¼ˆæ¯æ­¥èª¿ç”¨ï¼‰
        if self.lr_scheduler is not None and hasattr(self.lr_scheduler, 'current_step'):
            # Steps-based scheduler (å¦‚ StepsBasedWarmupScheduler)
            self.lr_scheduler.step()
        
        # ==================== 6. è¿”å›çµæœ ====================
        result = {
            'total_loss': total_loss.item(),
            'data_loss': data_loss.item(),
            'pde_loss': pde_loss.item(),
            'weighted_data_loss': weighted_data_loss.item(),
            'weighted_pde_loss': weighted_pde_loss.item(),
            'weighted_wall_loss': weighted_wall_loss.item(),
            'momentum_x_loss': momentum_x_loss.item(),
            'momentum_y_loss': momentum_y_loss.item(),
            'momentum_z_loss': momentum_z_loss.item(),
            'continuity_loss': continuity_loss.item(),
            'wall_loss': wall_loss.item(),
            'u_loss': u_loss.item(),
            'v_loss': v_loss.item(),
            'w_loss': w_loss.item(),
            'pressure_loss': pressure_loss.item(),
        }
        
        # RANS æå¤±é …å·²ç§»é™¤ï¼ˆ2025-10-14ï¼‰
        
        if gradnorm_weights is not None:
            result['gradnorm_weights'] = {k: float(v) for k, v in gradnorm_weights.items()}
            result['applied_weights'] = {
                'data': w_data,
                'momentum_x': w_momentum_x,
                'momentum_y': w_momentum_y,
                'momentum_z': w_momentum_z,
                'continuity': w_continuity,
                'wall_constraint': w_bc,
            }
        
        return result
    
    def validate(self) -> Optional[Dict[str, float]]:
        """
        è¨ˆç®—é©—è­‰æŒ‡æ¨™ï¼ˆMSE èˆ‡ relative L2ï¼‰
        
        Returns:
            é©—è­‰æŒ‡æ¨™å­—å…¸ï¼Œè‹¥ç„¡é©—è­‰è³‡æ–™å‰‡è¿”å› None
            - 'mse': å‡æ–¹èª¤å·®
            - 'relative_l2': ç›¸å° L2 èª¤å·®
        """
        # æª¢æŸ¥é©—è­‰è³‡æ–™æ˜¯å¦å­˜åœ¨
        if self.validation_data is None:
            return None
        
        if self.validation_data.get('size', 0) == 0:
            return None
        
        coords = self.validation_data.get('coords')
        targets = self.validation_data.get('targets')
        
        if coords is None or targets is None or coords.numel() == 0 or targets.numel() == 0:
            return None
        
        # ç§»å‹•è‡³è¨­å‚™
        coords = coords.to(self.device)
        targets = targets.to(self.device)
        
        # ä¿å­˜è¨“ç·´ç‹€æ…‹
        training_mode = self.model.training
        self.model.eval()
        
        with torch.no_grad():
            # âœ… ä½¿ç”¨ prepare_model_coords è¼”åŠ©å‡½æ•¸è™•ç†åº§æ¨™ï¼ˆéœ€è¦åœ¨ step() ä¹‹å¤–å®šç¾©æˆ–æ”¹ç‚ºå¯¦ä¾‹æ–¹æ³•ï¼‰
            # ç°¡åŒ–è™•ç†ï¼šç›´æ¥åœ¨æ­¤è™•ç†åº§æ¨™æ¨™æº–åŒ–èˆ‡ç¸®æ”¾
            coords_for_model = coords
            if self.input_normalizer is not None:
                coords_for_model = self.input_normalizer.transform(coords_for_model)
            if self.physics is not None and hasattr(self.physics, 'scale_coordinates'):
                coords_for_model = self.physics.scale_coordinates(coords_for_model)
            
            # æ¨¡å‹é æ¸¬ï¼ˆæ¨™æº–åŒ–ç©ºé–“è¼¸å‡ºï¼‰
            preds_norm = self.model(coords_for_model)
            
            # âœ… åæ¨™æº–åŒ–ç‚ºç‰©ç†é‡ï¼ˆèˆ‡çœŸå¯¦ç‰©ç†é‡æ¯”è¼ƒï¼‰
            var_order_val = self._infer_variable_order(preds_norm.shape[1], context='validation')
            preds_phys_raw = self.data_normalizer.denormalize_batch(preds_norm, var_order=var_order_val)
            preds_phys: torch.Tensor = preds_phys_raw if isinstance(preds_phys_raw, torch.Tensor) else torch.tensor(preds_phys_raw, device=self.device)  # type: ignore
            
            # è™•ç†ç¶­åº¦ä¸åŒ¹é…ï¼ˆåƒ…æ¯”è¼ƒå¯ç”¨çš„å ´åˆ†é‡ï¼‰
            n_pred = preds_phys.shape[1]
            n_targets = targets.shape[1]
            n_common = min(n_pred, n_targets)
            
            if n_pred != n_targets:
                logging.debug(
                    f"[Validation] è¼¸å‡ºç¶­åº¦ä¸åŒ¹é… (pred={n_pred}, target={n_targets})ï¼›"
                    f"æ¯”è¼ƒå‰ {n_common} å€‹åˆ†é‡ã€‚"
                )
            
            preds_final = preds_phys[:, :n_common]
            targets_final = targets[:, :n_common]
            
            # âœ… è¨ˆç®—èª¤å·®æŒ‡æ¨™ï¼ˆç‰©ç†ç©ºé–“ï¼‰
            diff = preds_final - targets_final
            mse = torch.mean(diff**2).item()
            rel_l2 = relative_L2(preds_final, targets_final).mean().item()
        
        # æ¢å¾©è¨“ç·´ç‹€æ…‹
        if training_mode:
            self.model.train()
        
        return {
            'mse': mse,
            'relative_l2': rel_l2
        }
    
    def train(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´è¨“ç·´å¾ªç’°
        
        Returns:
            è¨“ç·´æ­·å²èˆ‡æœ€çµ‚çµæœå­—å…¸
            - 'final_loss': æœ€çµ‚è¨“ç·´æå¤±
            - 'training_time': è¨“ç·´ç¸½æ™‚é–“ï¼ˆç§’ï¼‰
            - 'epochs_completed': å®Œæˆçš„ epoch æ•¸
            - 'best_epoch': æœ€ä½³æ¨¡å‹çš„ epochï¼ˆè‹¥å•Ÿç”¨æ—©åœï¼‰
            - 'best_metric': æœ€ä½³æŒ‡æ¨™å€¼ï¼ˆè‹¥å•Ÿç”¨æ—©åœï¼‰
            - 'history': è¨“ç·´æ­·å²ï¼ˆæ¯ epoch çš„æå¤±ï¼‰
        """
        logging.info("=" * 80)
        logging.info("ğŸš€ é–‹å§‹è¨“ç·´")
        logging.info(f"   æ¨¡å‹: {self.model.__class__.__name__}")
        logging.info(f"   å„ªåŒ–å™¨: {self.optimizer.__class__.__name__}")
        logging.info(f"   æœ€å¤§ Epochs: {self.train_cfg.get('epochs', 'N/A')}")
        logging.info(f"   æ—©åœ: {'å•Ÿç”¨' if self.early_stopping_enabled else 'ç¦ç”¨'}")
        logging.info("=" * 80)
        
        # è¨“ç·´é…ç½®
        max_epochs = self.train_cfg.get('epochs', self.train_cfg.get('max_epochs', 1000))
        log_freq = self.train_cfg.get('log_interval', self.log_cfg.get('log_freq', 50))
        checkpoint_freq = self.train_cfg.get('checkpoint_freq', 500)
        validation_freq = self.train_cfg.get('validation_freq', self.train_cfg.get('checkpoint_interval', 100))
        
        # è¨“ç·´æ­·å²è¨˜éŒ„
        history = {
            'total_loss': [],
            'val_loss': [],
            'epoch': []
        }
        
        # æ™‚é–“è¨˜éŒ„
        start_time = time.time()
        last_val_metrics: Optional[Dict[str, float]] = None
        
        # åˆå§‹åŒ–æå¤±å­—å…¸ï¼ˆé˜²æ­¢ epoch=0 æ™‚æœªå®šç¾©ï¼‰
        loss_dict = {'total_loss': 0.0, 'residual_loss': 0.0, 'bc_loss': 0.0, 'data_loss': 0.0}
        
        # ç¢ºå®šè¨“ç·´èµ·å§‹ epochï¼ˆæ”¯æ´å¾ checkpoint æ¢å¾©ï¼‰
        start_epoch = self.epoch  # è‹¥å¾ checkpoint æ¢å¾©ï¼Œself.epoch æœƒè¢« load_checkpoint() è¨­å®š
        if start_epoch > 0:
            logging.info(f"ğŸ”„ å¾ epoch {start_epoch} æ¢å¾©è¨“ç·´")
        
        # è¨“ç·´å¾ªç’°
        for epoch in range(start_epoch, max_epochs):
            self.epoch = epoch
            
            # ğŸ”§ è‡ªé©æ‡‰æ¡æ¨£ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.loop_manager is not None:
                # æ›´æ–°è¨“ç·´æ‰¹æ¬¡
                self.training_data = self.loop_manager.update_training_batch(self.training_data, epoch)
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ¡æ¨£ï¼ˆå‚³é loss_dict è€Œéæ®˜å·®ï¼‰
                if epoch > 0 and self.loop_manager.should_resample_collocation_points(
                    epoch, 
                    history['total_loss'][-1] if history['total_loss'] else float('inf'),
                    None  # residuals åƒæ•¸è¨­ç‚º None
                ):
                    try:
                        # æå–åŸŸé‚Šç•Œ
                        domain_bounds = {
                            'x': (self.config['domain']['x_min'], self.config['domain']['x_max']),
                            'y': (self.config['domain']['y_min'], self.config['domain']['y_max'])
                        }
                        if 'z_min' in self.config['domain']:
                            domain_bounds['z'] = (self.config['domain']['z_min'], self.config['domain']['z_max'])
                        if 't_min' in self.config['domain']:
                            domain_bounds['t'] = (self.config['domain']['t_min'], self.config['domain']['t_max'])
                        
                        new_points, metrics = self.loop_manager.resample_collocation_points(
                            self.model, self.physics, domain_bounds, epoch, str(self.device)
                        )
                        logging.info(f"ğŸ”„ é‡æ¡æ¨£ {len(new_points)} å€‹é…é»ï¼ˆepoch {epoch}ï¼‰")
                        logging.debug(f"   æŒ‡æ¨™: {metrics}")
                    except Exception as e:
                        logging.warning(f"âš ï¸ é‡æ¡æ¨£å¤±æ•—ï¼ˆepoch {epoch}ï¼‰: {e}")
            
            # ğŸ¯ Fourier é€€ç«æ›´æ–°ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if self.fourier_annealing is not None:
                try:
                    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ Fourier features æ¨¡çµ„
                    fourier_module = None
                    
                    # å˜—è©¦å¾æ¨¡å‹ä¸­æ‰¾åˆ° Fourier features
                    if hasattr(self.model, 'fourier_features'):
                        fourier_module = self.model.fourier_features
                    elif hasattr(self.model, 'encoder') and hasattr(self.model.encoder, 'fourier_features'):
                        fourier_module = self.model.encoder.fourier_features
                    
                    if fourier_module is not None:
                        # ç²å–æ›´æ–°å‰çš„ç‹€æ…‹ï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
                        old_info = self.fourier_annealing.get_info()
                        
                        # åŸ·è¡Œæ›´æ–°
                        self.fourier_annealing.update_fourier_features(
                            fourier_module, 
                            current_epoch=epoch, 
                            total_epochs=max_epochs
                        )
                        
                        # ç²å–æ›´æ–°å¾Œçš„ç‹€æ…‹
                        new_info = self.fourier_annealing.get_info()
                        
                        # æª¢æŸ¥æ˜¯å¦ç™¼ç”Ÿéšæ®µåˆ‡æ›ï¼ˆæ¯”è¼ƒ stage_indexï¼‰
                        if old_info['stage_index'] != new_info['stage_index']:
                            logging.info(f"ğŸ¯ Fourier é€€ç«éšæ®µåˆ‡æ›ï¼š{new_info['stage_description']}")
                            logging.info(f"   ç•¶å‰é »ç‡: {new_info['active_frequencies']}")
                            logging.info(f"   è¼¸å‡ºç¶­åº¦: {fourier_module.out_dim}")
                    
                except AttributeError as e:
                    # æ¨¡å‹ä¸æ”¯æŒ Fourier é€€ç«ï¼Œè­¦å‘Šä¸€æ¬¡å¾Œç¦ç”¨
                    if epoch == 0:
                        logging.warning(f"âš ï¸ æ¨¡å‹ä¸æ”¯æŒ Fourier é€€ç«ï¼š{e}ï¼Œå·²è‡ªå‹•ç¦ç”¨")
                    self.fourier_annealing = None
                except Exception as e:
                    logging.error(f"âŒ Fourier é€€ç«æ›´æ–°å¤±æ•—ï¼ˆepoch {epoch}ï¼‰: {e}")
            
            # âœ… åŸ·è¡Œè¨“ç·´æ­¥é©Ÿï¼ˆå‚³é training_data å’Œ epochï¼‰
            loss_dict = self.step(self.training_data, epoch)
            
            # âœ… é©—è­‰æŒ‡æ¨™è¨ˆç®—
            if validation_freq > 0 and epoch % validation_freq == 0:
                val_metrics = self.validate()
                if val_metrics is not None:
                    last_val_metrics = val_metrics
                    loss_dict['val_loss'] = val_metrics['relative_l2']
                    loss_dict['val_mse'] = val_metrics['mse']
            
            # è¨˜éŒ„æ­·å²
            history['total_loss'].append(loss_dict['total_loss'])
            history['epoch'].append(epoch)
            if 'val_loss' in loss_dict:
                history['val_loss'].append(loss_dict['val_loss'])
            
            # ğŸš€ èª²ç¨‹è¨“ç·´ï¼šè™•ç†éšæ®µåˆ‡æ›
            if '_curriculum_transition' in loss_dict and loss_dict['_curriculum_transition'] > 0.5:
                new_lr = loss_dict.get('_curriculum_lr', self.train_cfg.get('lr', 1e-3))
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = new_lr
                logging.info(f"ğŸ“‰ èª²ç¨‹è¨“ç·´ï¼šå­¸ç¿’ç‡æ›´æ–°ç‚º {new_lr:.6f}")
                
                # ä¿å­˜éšæ®µæª¢æŸ¥é»ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                if self.log_cfg.get('save_stage_checkpoints', False):
                    stage_name = loss_dict.get('_curriculum_stage', f'stage_{epoch}')
                    self.save_checkpoint(epoch, loss_dict, is_best=False)
                    logging.info(f"ğŸ’¾ éšæ®µæª¢æŸ¥é»å·²ä¿å­˜: {stage_name}")
            
            # ğŸ“‰ æ›´æ–°å­¸ç¿’ç‡èª¿åº¦å™¨ï¼ˆéèª²ç¨‹è¨“ç·´æ¨¡å¼ï¼‰
            if self.lr_scheduler is not None and not hasattr(self, 'curriculum_weighter'):
                self.lr_scheduler.step()
            
            # ğŸ“Š æ—¥èªŒè¼¸å‡º
            if epoch % log_freq == 0:
                self.log_epoch(epoch, loss_dict)
            
            # ğŸ’¾ æª¢æŸ¥é»ä¿å­˜
            if checkpoint_freq > 0 and epoch % checkpoint_freq == 0 and epoch > 0:
                self.save_checkpoint(epoch, loss_dict)
                logging.info(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜ï¼ˆepoch {epoch}ï¼‰")
            
            # ğŸ›‘ æ—©åœæª¢æŸ¥
            if self.early_stopping_enabled:
                # é¸æ“‡ç›£æ§æŒ‡æ¨™
                metric_name = self.early_stopping_cfg.get('monitor', 'total_loss')
                if metric_name == 'val_loss' and 'val_loss' in loss_dict:
                    current_metric = loss_dict['val_loss']
                elif metric_name in loss_dict:
                    current_metric = loss_dict[metric_name]
                else:
                    current_metric = loss_dict['total_loss']
                
                # æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢
                if self.check_early_stopping(current_metric):
                    logging.info(f"ğŸ›‘ æ—©åœè§¸ç™¼æ–¼ epoch {epoch}")
                    logging.info(f"   æœ€ä½³æŒ‡æ¨™: {self.best_val_loss:.6f}ï¼ˆepoch {self.best_epoch}ï¼‰")
                    
                    # æ¢å¾©æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                    if self.early_stopping_cfg.get('restore_best_weights', True) and self.best_model_state is not None:
                        self.model.load_state_dict(self.best_model_state)
                        logging.info(f"âœ… å·²æ¢å¾©æœ€ä½³æ¨¡å‹ï¼ˆepoch {self.best_epoch}ï¼‰")
                    
                    break
            
            # å¿«é€Ÿæ”¶æ–‚æª¢æŸ¥
            if loss_dict['total_loss'] < 1e-6:
                logging.info(f"âœ… å¿«é€Ÿæ”¶æ–‚æ–¼ epoch {epoch}ï¼ˆloss < 1e-6ï¼‰")
                break
        
        # è¨“ç·´çµæŸï¼ˆè™•ç† epoch è®Šæ•¸ä½œç”¨åŸŸï¼‰
        final_epoch = epoch if 'epoch' in locals() else max_epochs - 1
        final_loss = loss_dict['total_loss']
        
        total_time = time.time() - start_time
        logging.info("=" * 80)
        logging.info(f"âœ… è¨“ç·´å®Œæˆ")
        logging.info(f"   ç¸½æ™‚é–“: {total_time:.1f}s")
        logging.info(f"   å®Œæˆ Epochs: {final_epoch + 1}")
        logging.info(f"   æœ€çµ‚æå¤±: {final_loss:.6f}")
        if self.early_stopping_enabled and self.best_epoch >= 0:
            logging.info(f"   æœ€ä½³ Epoch: {self.best_epoch}")
            logging.info(f"   æœ€ä½³æŒ‡æ¨™: {self.best_val_loss:.6f}")
        logging.info("=" * 80)
        
        # ä¿å­˜æœ€çµ‚æª¢æŸ¥é»
        final_checkpoint = self.save_checkpoint(final_epoch + 1, loss_dict, is_best=False)
        logging.info(f"ğŸ’¾ æœ€çµ‚æ¨¡å‹å·²ä¿å­˜")
        
        # è¿”å›è¨“ç·´çµæœ
        return {
            'final_loss': final_loss,
            'training_time': total_time,
            'epochs_completed': final_epoch + 1,
            'best_epoch': self.best_epoch if self.early_stopping_enabled else final_epoch,
            'best_metric': self.best_val_loss if self.early_stopping_enabled else final_loss,
            'history': history,
            'checkpoint_path': final_checkpoint
        }
    
    def save_checkpoint(
        self,
        epoch: int,
        metrics: Optional[Dict[str, float]] = None,
        is_best: bool = False
    ):
        """
        ä¿å­˜æª¢æŸ¥é»
        
        Args:
            epoch: ç•¶å‰ epoch
            metrics: è©•ä¼°æŒ‡æ¨™ï¼ˆå¯é¸ï¼‰
            is_best: æ˜¯å¦ç‚ºæœ€ä½³æ¨¡å‹
        """
        checkpoint_path = self.checkpoint_dir / f"epoch_{epoch}.pth"
        
        checkpoint_data = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'history': self.history,
            'config': self.config,
        }
        
        # ğŸ†• ä¿å­˜ physics çš„ state_dictï¼ˆVS-PINN ç¸®æ”¾åƒæ•¸ç­‰ï¼‰
        if self.physics is not None and hasattr(self.physics, 'state_dict'):
            checkpoint_data['physics_state_dict'] = self.physics.state_dict()
            logging.debug(f"ğŸ’¾ Physics state saved: {list(self.physics.state_dict().keys())}")
        
        # âœ… TASK-008: ä¿å­˜æ¨™æº–åŒ– metadata
        checkpoint_data['normalization'] = self.data_normalizer.get_metadata()
        logging.debug(f"ğŸ’¾ Normalization metadata saved: type={self.data_normalizer.norm_type}")
        
        # â­ P0.2: ä¿å­˜ GradScaler ç‹€æ…‹ï¼ˆAMPï¼‰
        if self.use_amp and hasattr(self, 'scaler'):
            checkpoint_data['scaler_state_dict'] = self.scaler.state_dict()
            logging.debug(f"ğŸ’¾ GradScaler state saved: scale={self.scaler.get_scale():.0f}")
        
        if metrics:
            checkpoint_data['metrics'] = metrics
        
        if self.lr_scheduler:
            checkpoint_data['lr_scheduler_state_dict'] = self.lr_scheduler.state_dict()
        
        torch.save(checkpoint_data, checkpoint_path)
        logging.info(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_path}")
        
        if is_best:
            best_path = self.checkpoint_dir / "best_model.pth"
            torch.save(checkpoint_data, best_path)
            logging.info(f"â­ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """
        è¼‰å…¥æª¢æŸ¥é»
        
        Args:
            checkpoint_path: æª¢æŸ¥é»è·¯å¾‘
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epoch = checkpoint['epoch']
        self.history = checkpoint.get('history', self.history)
        
        # æ¢å¾© physics çš„ state_dictï¼ˆVS-PINN ç¸®æ”¾åƒæ•¸ç­‰ï¼‰
        if self.physics is not None:
            if 'physics_state_dict' not in checkpoint:
                raise KeyError("checkpoint is missing required 'physics_state_dict'")
            if not hasattr(self.physics, 'load_state_dict'):
                raise TypeError("physics module does not support load_state_dict()")
            self.physics.load_state_dict(checkpoint['physics_state_dict'])
            logging.info(f"âœ… Physics state restored: {list(checkpoint['physics_state_dict'].keys())}")
        
        # æ¢å¾©æ¨™æº–åŒ–å™¨
        if 'normalization' not in checkpoint:
            raise KeyError("checkpoint is missing required 'normalization' metadata")
        self.data_normalizer = DataNormalizer.from_metadata(checkpoint['normalization'])
        logging.info(f"âœ… DataNormalizer restored: {self.data_normalizer}")
        
        # æ¢å¾© GradScaler ç‹€æ…‹ï¼ˆAMPï¼‰
        if self.use_amp:
            if not hasattr(self, 'scaler'):
                raise AttributeError("AMP enabled but trainer lacks GradScaler instance")
            if 'scaler_state_dict' not in checkpoint:
                raise KeyError("checkpoint is missing required 'scaler_state_dict' for AMP recovery")
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            logging.info(f"âœ… GradScaler state restored: scale={self.scaler.get_scale():.0f}")
        
        if self.lr_scheduler and 'lr_scheduler_state_dict' in checkpoint:
            self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
        
        logging.info(f"âœ… æª¢æŸ¥é»å·²è¼‰å…¥: {checkpoint_path}ï¼ˆepoch={self.epoch}ï¼‰")
    
    def check_early_stopping(self, val_loss: float) -> bool:
        """
        æª¢æŸ¥æ˜¯å¦æ‡‰è©²æ—©åœ
        
        Args:
            val_loss: é©—è­‰æå¤±
        
        Returns:
            æ˜¯å¦æ‡‰è©²åœæ­¢è¨“ç·´
        """
        if not self.early_stopping_enabled:
            return False
        
        if val_loss < self.best_val_loss - self.min_delta:
            self.best_val_loss = val_loss
            self.best_epoch = self.epoch
            self.patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ç‹€æ…‹ï¼ˆå¦‚æœé…ç½®å•Ÿç”¨ï¼‰
            if self.early_stopping_cfg.get('restore_best_weights', True):
                self.best_model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
            
            # ğŸ†• ç«‹å³ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°ç£ç¢Ÿï¼ˆé˜²æ­¢è¨“ç·´ä¸­æ–·å°è‡´éºå¤±ï¼‰
            metrics = {'val_loss': val_loss, 'best_epoch': self.best_epoch}
            self.save_checkpoint(self.epoch, metrics, is_best=True)
            
            logging.info(f"ğŸ¯ æ–°æœ€ä½³æŒ‡æ¨™: {self.best_val_loss:.6f}ï¼ˆepoch {self.best_epoch}ï¼‰")
            return False
        else:
            self.patience_counter += 1
            if self.patience_counter >= self.patience:
                logging.info(f"ğŸ›‘ æ—©åœè§¸ç™¼ï¼ˆpatience={self.patience}ï¼‰")
                return True
            return False
    
    def get_current_lr(self) -> float:
        """ç²å–ç•¶å‰å­¸ç¿’ç‡"""
        return self.optimizer.param_groups[0]['lr']
    
    def log_epoch(self, epoch: int, metrics: Dict[str, float]):
        """
        è¨˜éŒ„ epoch è¨“ç·´è³‡è¨Š
        
        Args:
            epoch: ç•¶å‰ epoch
            metrics: è¨“ç·´æŒ‡æ¨™
        """
        log_str = f"Epoch {epoch}/{self.train_cfg.get('epochs', '?')}"
        
        for key, value in metrics.items():
            # è·³éå­—å…¸é¡å‹çš„å€¼ï¼ˆå¦‚ gradnorm_weights, applied_weightsï¼‰
            if isinstance(value, dict):
                continue
            # è·³ééæ•¸å€¼é¡å‹ï¼ˆå¦‚å­—ä¸²ã€åˆ—è¡¨ç­‰ï¼‰
            if not isinstance(value, (int, float)):
                continue
            log_str += f" | {key}: {value:.6f}"
        
        log_str += f" | lr: {self.get_current_lr():.2e}"
        
        logging.info(log_str)
        
        # è¨˜éŒ„åˆ°æ­·å²
        for key, value in metrics.items():
            if key not in self.history:
                self.history[key] = []
            self.history[key].append(value)
        
        self.history['lr'].append(self.get_current_lr())
