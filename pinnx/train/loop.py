"""
è¨“ç·´å¾ªç’°è¼”åŠ©æ¨¡çµ„
æä¾›è¨“ç·´éç¨‹ä¸­çš„è‡ªé©æ‡‰æ©Ÿåˆ¶èˆ‡ç›£æ§åŠŸèƒ½
"""

import logging
from typing import Dict, Any, Optional, Tuple, Callable
import torch
import torch.nn as nn
import numpy as np

from pinnx.train.adaptive_collocation import AdaptiveCollocationSampler

logger = logging.getLogger(__name__)


class TrainingLoopManager:
    """
    è¨“ç·´å¾ªç’°ç®¡ç†å™¨
    
    è·è²¬ï¼š
    1. ç®¡ç†è‡ªé©æ‡‰æ®˜å·®é»æ¡æ¨£å™¨
    2. å”èª¿å‹•æ…‹æ¬Šé‡èª¿æ•´
    3. ç›£æ§è¨“ç·´é€²åº¦èˆ‡è¨ºæ–·
    4. è™•ç†è¨“ç·´æ•¸æ“šæ›´æ–°ï¼ˆæ®˜å·®é»ã€ç›£ç£é»è¿½åŠ ç­‰ï¼‰
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: å®Œæ•´è¨“ç·´é…ç½®
        """
        self.config = config
        
        # è‡ªé©æ‡‰æ®˜å·®é»æ¡æ¨£å™¨ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        adaptive_cfg = config.get('sampling', {}).get('adaptive_collocation', {})
        if adaptive_cfg.get('enabled', False):
            self.adaptive_sampler = AdaptiveCollocationSampler(adaptive_cfg)
            logger.info("âœ… è‡ªé©æ‡‰æ®˜å·®é»æ¡æ¨£å™¨å·²å•Ÿç”¨")
        else:
            self.adaptive_sampler = None
            logger.info("âš ï¸  è‡ªé©æ‡‰æ®˜å·®é»æ¡æ¨£å™¨å·²ç¦ç”¨")
        
        # æ–°é»æ¬Šé‡è¡°æ¸›é…ç½®
        new_point_cfg = adaptive_cfg.get('new_point_weighting', {})
        self.new_point_weight_enabled = new_point_cfg.get('enabled', True)
        self.new_point_initial_weight = new_point_cfg.get('initial_weight', 2.5)
        self.new_point_decay_epochs = new_point_cfg.get('decay_epochs', 500)
        self.new_point_final_weight = new_point_cfg.get('final_weight', 1.0)
        
        # å…§éƒ¨ç‹€æ…‹
        self.current_pde_points = None
        self.new_point_mask = None  # æ¨™è¨˜å“ªäº›é»æ˜¯æ–°åŠ å…¥çš„
        self.last_resample_epoch = 0
        self.resample_count = 0
        
        # ç›£æ§çµ±è¨ˆ
        self.epoch_stats = []
    
    def setup_initial_points(self, pde_points: torch.Tensor):
        """
        è¨­ç½®åˆå§‹æ®˜å·®é»ï¼ˆè¨“ç·´é–‹å§‹æ™‚èª¿ç”¨ä¸€æ¬¡ï¼‰
        
        Args:
            pde_points: åˆå§‹ PDE æ®˜å·®é» [N_pde, dim]
        """
        self.current_pde_points = pde_points.clone()
        self.new_point_mask = torch.zeros(len(pde_points), dtype=torch.bool)
        logger.info(f"âœ… åˆå§‹åŒ–æ®˜å·®é»: {len(pde_points)} å€‹é»")
    
    def should_resample_collocation_points(self,
                                          epoch: int,
                                          current_loss: float,
                                          residuals: Optional[torch.Tensor] = None) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦æ‡‰è©²é‡æ–°æ¡æ¨£æ®˜å·®é»
        
        Args:
            epoch: ç•¶å‰ epoch
            current_loss: ç•¶å‰ç¸½æå¤±
            residuals: ç•¶å‰ PDE æ®˜å·® [N_pde, n_equations]
            
        Returns:
            æ˜¯å¦è§¸ç™¼é‡æ¡æ¨£
        """
        if self.adaptive_sampler is None:
            return False
        
        return self.adaptive_sampler.should_trigger(epoch, current_loss, residuals)
    
    def resample_collocation_points(self,
                                   model: nn.Module,
                                   physics_module: Any,
                                   domain_bounds: Dict[str, Tuple[float, float]],
                                   epoch: int,
                                   device: str = 'cpu') -> Tuple[torch.Tensor, Dict]:
        """
        åŸ·è¡Œæ®˜å·®é»é‡æ¡æ¨£
        
        Args:
            model: PINNs æ¨¡å‹
            physics_module: ç‰©ç†æ–¹ç¨‹æ¨¡çµ„ï¼ˆç”¨æ–¼è¨ˆç®—æ®˜å·®ï¼‰
            domain_bounds: åŸŸé‚Šç•Œ
            epoch: ç•¶å‰ epoch
            device: è¨ˆç®—è¨­å‚™
            
        Returns:
            (new_pde_points, metrics)
        """
        if self.adaptive_sampler is None:
            raise RuntimeError("è‡ªé©æ‡‰æ¡æ¨£å™¨æœªå•Ÿç”¨")
        
        if self.current_pde_points is None:
            raise RuntimeError("å°šæœªåˆå§‹åŒ–æ®˜å·®é»ï¼Œè«‹å…ˆèª¿ç”¨ setup_initial_points()")
        
        logger.info("=" * 80)
        logger.info(f"ğŸ”„ è§¸ç™¼æ®˜å·®é»é‡æ¡æ¨£ (Epoch {epoch})")
        logger.info("=" * 80)
        
        # å®šç¾©æ®˜å·®è¨ˆç®—å‡½æ•¸ï¼ˆé–‰åŒ…ï¼‰
        def residual_fn(points: torch.Tensor) -> torch.Tensor:
            """
            è¨ˆç®—çµ¦å®šé»çš„ PDE æ®˜å·®
            
            Args:
                points: è¼¸å…¥é» [N, dim]
                
            Returns:
                residuals: PDE æ®˜å·® [N, n_equations]
            """
            with torch.no_grad():
                points = points.to(device)
                points.requires_grad_(True)
                
                # æ¨¡å‹é æ¸¬
                u_pred = model(points)
                
                # æ ¹æ“šç¶­åº¦åˆ¤æ–·æ˜¯ 2D é‚„æ˜¯ 3D
                if points.shape[1] == 2:  # 2D (x, y)
                    coords = points
                    velocity = u_pred[:, :2]  # u, v
                    pressure = u_pred[:, 2:3]  # p
                elif points.shape[1] == 3:  # 3D (x, y, z)
                    coords = points
                    velocity = u_pred[:, :3]  # u, v, w
                    pressure = u_pred[:, 3:4]  # p
                else:
                    raise ValueError(f"ä¸æ”¯æ´çš„é»ç¶­åº¦: {points.shape[1]}")
                
                # è¨ˆç®— PDE æ®˜å·®
                residual_dict = physics_module.residual(coords, velocity, pressure)
                
                # çµ„åˆæ‰€æœ‰æ®˜å·®é …åˆ°å–®ä¸€å¼µé‡
                # 2D: [momentum_x, momentum_y, continuity]
                # 3D: [momentum_x, momentum_y, momentum_z, continuity]
                residual_list = []
                for key in sorted(residual_dict.keys()):
                    residual_list.append(residual_dict[key])
                
                residuals = torch.stack(residual_list, dim=1)  # [N, n_equations]
                
                return residuals
        
        # åŸ·è¡Œé‡æ¡æ¨£
        new_points, metrics = self.adaptive_sampler.resample_collocation_points(
            current_points=self.current_pde_points,
            domain_bounds=domain_bounds,
            residual_fn=residual_fn,
            n_keep=None,  # è‡ªå‹•è¨ˆç®—
            device=device
        )
        
        # æ›´æ–°å…§éƒ¨ç‹€æ…‹
        n_old = len(self.current_pde_points)
        n_new = len(new_points)
        n_replaced = metrics['n_replaced']
        
        # å‰µå»ºæ–°é»æ©ç¢¼ï¼ˆæ¨™è¨˜æ–°åŠ å…¥çš„é»ï¼‰
        new_mask = torch.zeros(n_new, dtype=torch.bool)
        new_mask[-n_replaced:] = True  # æœ€å¾Œ n_replaced å€‹é»æ˜¯æ–°çš„
        
        self.current_pde_points = new_points
        self.new_point_mask = new_mask
        self.last_resample_epoch = epoch
        self.resample_count += 1
        
        logger.info(f"âœ… æ®˜å·®é»é‡æ¡æ¨£å®Œæˆ:")
        logger.info(f"   èˆŠé»æ•¸: {n_old}")
        logger.info(f"   æ–°é»æ•¸: {n_new}")
        logger.info(f"   ä¿ç•™: {metrics['n_kept']}, æ›¿æ›: {n_replaced}")
        logger.info(f"   SVD ç§©: {metrics.get('svd_rank', 'N/A')}")
        logger.info(f"   èƒ½é‡ä¿ç•™: {metrics.get('svd_energy_ratio', 'N/A'):.4f}" if 'svd_energy_ratio' in metrics else "")
        logger.info("=" * 80)
        
        return new_points, metrics
    
    def get_point_weights(self, epoch: int) -> Optional[torch.Tensor]:
        """
        ç²å–ç•¶å‰æ®˜å·®é»çš„æ¬Šé‡ï¼ˆæ–°é»æ¬Šé‡è¡°æ¸›ï¼‰
        
        Args:
            epoch: ç•¶å‰ epoch
            
        Returns:
            weights: é»æ¬Šé‡ [N_pde,]ï¼Œå¦‚æœæœªå•Ÿç”¨å‰‡è¿”å› None
        """
        if not self.new_point_weight_enabled or self.new_point_mask is None:
            return None
        
        # è¨ˆç®—è¡°æ¸›å› å­ï¼ˆç·šæ€§è¡°æ¸›ï¼‰
        epochs_since_resample = epoch - self.last_resample_epoch
        if epochs_since_resample >= self.new_point_decay_epochs:
            # å®Œå…¨è¡°æ¸›åˆ° final_weight
            decay_factor = 0.0
        else:
            # ç·šæ€§è¡°æ¸›
            decay_factor = 1.0 - (epochs_since_resample / self.new_point_decay_epochs)
        
        # è¨ˆç®—æ¬Šé‡
        weights = torch.ones(len(self.new_point_mask))
        
        # æ–°é»æ¬Šé‡ = final + (initial - final) * decay_factor
        new_point_weight = (
            self.new_point_final_weight +
            (self.new_point_initial_weight - self.new_point_final_weight) * decay_factor
        )
        
        weights[self.new_point_mask] = new_point_weight
        
        if epochs_since_resample == 0 or epochs_since_resample % 100 == 0:
            logger.debug(f"æ–°é»æ¬Šé‡: {new_point_weight:.3f} "
                        f"(è¡°æ¸›é€²åº¦: {(1-decay_factor)*100:.1f}%)")
        
        return weights
    
    def update_training_batch(self, 
                             data_batch: Dict[str, torch.Tensor],
                             epoch: int) -> Dict[str, torch.Tensor]:
        """
        æ›´æ–°è¨“ç·´æ‰¹æ¬¡ï¼ˆåœ¨æ¯å€‹ epoch é–‹å§‹æ™‚èª¿ç”¨ï¼‰
        
        æ›´æ–°å…§å®¹ï¼š
        1. å¦‚æœæœ‰æ–°çš„æ®˜å·®é»ï¼Œæ›´æ–° x_pde, y_pde (å’Œ z_pde)
        2. è¨ˆç®—ä¸¦é™„åŠ é»æ¬Šé‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        
        Args:
            data_batch: åŸå§‹è¨“ç·´æ‰¹æ¬¡
            epoch: ç•¶å‰ epoch
            
        Returns:
            updated_batch: æ›´æ–°å¾Œçš„è¨“ç·´æ‰¹æ¬¡
        """
        updated_batch = data_batch.copy()
        
        # å¦‚æœæœ‰ç•¶å‰æ®˜å·®é»ï¼Œæ›´æ–°åˆ°æ‰¹æ¬¡ä¸­
        if self.current_pde_points is not None:
            dim = self.current_pde_points.shape[1]
            
            if dim == 2:  # 2D
                updated_batch['x_pde'] = self.current_pde_points[:, 0:1]
                updated_batch['y_pde'] = self.current_pde_points[:, 1:2]
            elif dim == 3:  # 3D
                updated_batch['x_pde'] = self.current_pde_points[:, 0:1]
                updated_batch['y_pde'] = self.current_pde_points[:, 1:2]
                updated_batch['z_pde'] = self.current_pde_points[:, 2:3]
            else:
                logger.warning(f"ä¸æ”¯æ´çš„é»ç¶­åº¦: {dim}")
        
        # é™„åŠ é»æ¬Šé‡
        point_weights = self.get_point_weights(epoch)
        if point_weights is not None:
            updated_batch['pde_point_weights'] = point_weights
        
        return updated_batch
    
    def collect_epoch_stats(self, epoch: int, loss_dict: Dict[str, float]):
        """
        æ”¶é›†æ¯å€‹ epoch çš„çµ±è¨ˆä¿¡æ¯
        
        Args:
            epoch: ç•¶å‰ epoch
            loss_dict: æå¤±å­—å…¸
        """
        stats = {
            'epoch': epoch,
            'total_loss': loss_dict.get('total_loss', 0.0),
            'n_pde_points': len(self.current_pde_points) if self.current_pde_points is not None else 0,
            'n_new_points': self.new_point_mask.sum().item() if self.new_point_mask is not None else 0,
            'resample_count': self.resample_count,
        }
        
        self.epoch_stats.append(stats)
    
    def get_summary(self) -> Dict[str, Any]:
        """
        ç²å–è¨“ç·´å¾ªç’°ç®¡ç†å™¨çš„æ‘˜è¦ä¿¡æ¯
        
        Returns:
            summary: æ‘˜è¦å­—å…¸
        """
        return {
            'adaptive_sampler_enabled': self.adaptive_sampler is not None,
            'resample_count': self.resample_count,
            'last_resample_epoch': self.last_resample_epoch,
            'current_n_pde_points': len(self.current_pde_points) if self.current_pde_points is not None else 0,
            'n_new_points': self.new_point_mask.sum().item() if self.new_point_mask is not None else 0,
            'adaptive_sampler_stats': self.adaptive_sampler.get_statistics() if self.adaptive_sampler else {},
        }


def create_training_loop_manager(config: Dict[str, Any]) -> TrainingLoopManager:
    """
    å‰µå»ºè¨“ç·´å¾ªç’°ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        config: å®Œæ•´è¨“ç·´é…ç½®
        
    Returns:
        manager: è¨“ç·´å¾ªç’°ç®¡ç†å™¨å¯¦ä¾‹
    """
    return TrainingLoopManager(config)


def apply_point_weights_to_loss(loss: torch.Tensor,
                                point_weights: Optional[torch.Tensor]) -> torch.Tensor:
    """
    æ‡‰ç”¨é»æ¬Šé‡åˆ°æå¤±ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
    
    Args:
        loss: åŸå§‹æå¤± [N_pde,] æˆ–æ¨™é‡
        point_weights: é»æ¬Šé‡ [N_pde,]
        
    Returns:
        weighted_loss: åŠ æ¬Šæå¤±ï¼ˆæ¨™é‡ï¼‰
    """
    if point_weights is None:
        # ç„¡æ¬Šé‡ï¼Œç›´æ¥å¹³å‡
        return torch.mean(loss)
    
    # ç¢ºä¿å½¢ç‹€åŒ¹é…
    if loss.dim() == 0:
        # æå¤±å·²ç¶“æ˜¯æ¨™é‡ï¼Œç„¡æ³•æ‡‰ç”¨é»æ¬Šé‡
        logger.warning("æå¤±å·²æ˜¯æ¨™é‡ï¼Œç„¡æ³•æ‡‰ç”¨é»æ¬Šé‡")
        return loss
    
    if loss.shape[0] != point_weights.shape[0]:
        logger.warning(f"æå¤±å½¢ç‹€ {loss.shape} èˆ‡æ¬Šé‡å½¢ç‹€ {point_weights.shape} ä¸åŒ¹é…ï¼Œå›é€€åˆ°å‡å‹»æ¬Šé‡")
        return torch.mean(loss)
    
    # åŠ æ¬Šå¹³å‡
    weighted_loss = torch.sum(loss * point_weights) / torch.sum(point_weights)
    
    return weighted_loss


# ========================================
# ç›£ç£é»è¿½åŠ é‚è¼¯ï¼ˆé ç•™æ¥å£ï¼‰
# ========================================

class SupervisedPointAugmentor:
    """
    ç›£ç£é»è¿½åŠ å™¨
    
    åŠŸèƒ½ï¼š
    - åœ¨è¨“ç·´éç¨‹ä¸­å‹•æ…‹è¿½åŠ ç›£ç£é»
    - æ”¯æ´å›ºå®šé€±æœŸèˆ‡æ‰‹å‹•æŒ‡å®š Epoch è§¸ç™¼
    - ç›®å‰æä¾›å‡å‹»æ¡æ¨£è¿½åŠ ç­–ç•¥ï¼ˆä¹‹å¾Œå¯æ“´å±•ç‚ºæ®˜å·®å°å‘ï¼‰
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.enabled = config.get('enabled', False)
        self.start_epoch = int(config.get('start_epoch', 0))
        self.interval = max(1, int(config.get('interval', 100)))
        self.max_rounds = config.get('max_rounds')
        self.manual_epochs = set(config.get('epochs', []))
        self.strategy = config.get('strategy', 'uniform')
        self.jitter = float(config.get('jitter', 0.0))
        self.default_dim = config.get('dim')
        
        self.last_augment_epoch: Optional[int] = None
        self._pending_epoch: Optional[int] = None
        self.augment_count = 0
        self.bounds = self._parse_bounds(config.get('bounds'))
        
        if self.enabled:
            logger.info(
                "âœ… ç›£ç£é»è¿½åŠ å™¨å•Ÿç”¨ | strategy=%s start_epoch=%d interval=%d max_rounds=%s",
                self.strategy,
                self.start_epoch,
                self.interval,
                str(self.max_rounds) if self.max_rounds is not None else "âˆ"
            )
    
    def _parse_bounds(self, bounds_cfg: Optional[Any]) -> list:
        """è§£æé‚Šç•Œé…ç½®ç‚º [(min, max), ...]"""
        bounds: list = []
        if bounds_cfg is None:
            return bounds
        
        if isinstance(bounds_cfg, dict):
            # æŒ‰å¸¸è¦‹é †åºè®€å–
            for key in ['x', 'y', 'z', 't']:
                if key in bounds_cfg:
                    rng = bounds_cfg[key]
                    if isinstance(rng, (list, tuple)) and len(rng) == 2:
                        bounds.append((float(rng[0]), float(rng[1])))
        elif isinstance(bounds_cfg, (list, tuple)):
            for rng in bounds_cfg:
                if isinstance(rng, (list, tuple)) and len(rng) == 2:
                    bounds.append((float(rng[0]), float(rng[1])))
        
        return bounds
    
    def _resolve_bounds(self, dim: int) -> list:
        """æ ¹æ“šéœ€æ±‚ç¶­åº¦å–å¾—é‚Šç•Œï¼ˆä¸è¶³æ™‚ä½¿ç”¨ [-1, 1] è£œé½Šï¼‰"""
        if dim <= 0:
            raise ValueError("ç¶­åº¦éœ€å¤§æ–¼ 0 æ‰èƒ½ç”Ÿæˆç›£ç£é»")
        
        if not self.bounds:
            return [(-1.0, 1.0)] * dim
        
        if len(self.bounds) >= dim:
            return self.bounds[:dim]
        
        # é‚Šç•Œä¸è¶³ï¼Œä½¿ç”¨æœ€å¾Œä¸€å€‹æˆ–é è¨­å€é–“è£œé½Š
        padding = [self.bounds[-1]] if self.bounds else [(-1.0, 1.0)]
        while len(self.bounds) + len(padding) < dim:
            padding.append(padding[-1])
        
        return self.bounds + padding[: dim - len(self.bounds)]
    
    def _can_augment(self) -> bool:
        if not self.enabled:
            return False
        if self.max_rounds is not None and self.augment_count >= self.max_rounds:
            return False
        return True
    
    def should_augment(self, epoch: int) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è¿½åŠ ç›£ç£é»"""
        if not self._can_augment():
            return False
        if epoch < self.start_epoch:
            return False
        if epoch in self.manual_epochs:
            self._pending_epoch = epoch
            return True
        
        if self.last_augment_epoch is None:
            due_epoch = self.start_epoch
        else:
            due_epoch = self.last_augment_epoch + self.interval
        
        if epoch >= due_epoch:
            self._pending_epoch = epoch
            return True
        
        return False
    
    def augment_supervised_points(self, 
                                  current_points: torch.Tensor,
                                  model: nn.Module,
                                  n_augment: int) -> torch.Tensor:
        """
        è¿½åŠ ç›£ç£é»
        
        Args:
            current_points: ç•¶å‰ç›£ç£é»
            model: PINNs æ¨¡å‹ï¼ˆä¿ç•™æ¥å£ï¼Œæœªä½¿ç”¨ï¼‰
            n_augment: è¿½åŠ æ•¸é‡
            
        Returns:
            augmented_points: è¿½åŠ å¾Œçš„ç›£ç£é»
        """
        if not self._can_augment() or n_augment <= 0:
            return current_points
        
        dim = current_points.shape[1] if current_points.numel() > 0 else self.default_dim
        if dim is None:
            raise ValueError("ç„¡æ³•æ¨æ–·ç›£ç£é»ç¶­åº¦ï¼Œè«‹åœ¨é…ç½®ä¸­è¨­å®š dim æˆ–æä¾›åˆå§‹é»")
        
        bounds = self._resolve_bounds(dim)
        device = current_points.device
        dtype = current_points.dtype
        
        samples = []
        for low, high in bounds:
            rand = torch.rand(n_augment, 1, device=device, dtype=dtype)
            samples.append(rand * (high - low) + low)
        
        new_points = torch.cat(samples, dim=1)
        
        if self.jitter > 0.0 and current_points.numel() > 0:
            noise = torch.randn_like(new_points) * self.jitter
            new_points = new_points + noise
            for dim_idx, (low, high) in enumerate(bounds):
                new_points[:, dim_idx].clamp_(min=low, max=high)
        
        augmented = torch.cat([current_points, new_points], dim=0)
        
        self.augment_count += 1
        if self._pending_epoch is not None:
            self.last_augment_epoch = self._pending_epoch
            self._pending_epoch = None
        
        logger.info("ğŸ“ˆ å·²è¿½åŠ  %d å€‹ç›£ç£é» (ç¸½æ•¸: %d)", n_augment, augmented.shape[0])
        
        return augmented


if __name__ == "__main__":
    # å–®å…ƒæ¸¬è©¦
    print("ğŸ§ª æ¸¬è©¦è¨“ç·´å¾ªç’°ç®¡ç†å™¨...")
    
    # æ¨¡æ“¬é…ç½®
    config = {
        'sampling': {
            'adaptive_collocation': {
                'enabled': True,
                'trigger': {
                    'method': 'epoch_interval',
                    'epoch_interval': 1000,
                },
                'resampling_strategy': 'incremental_replace',
                'incremental_replace': {
                    'keep_ratio': 0.7,
                    'replace_ratio': 0.3,
                },
                'residual_qr': {
                    'enabled': True,
                    'candidate_pool_size': 1000,
                },
                'new_point_weighting': {
                    'enabled': True,
                    'initial_weight': 2.5,
                    'decay_epochs': 500,
                    'final_weight': 1.0,
                }
            }
        }
    }
    
    manager = TrainingLoopManager(config)
    print(f"âœ… ç®¡ç†å™¨å‰µå»ºæˆåŠŸ")
    print(f"   è‡ªé©æ‡‰æ¡æ¨£å™¨: {'å•Ÿç”¨' if manager.adaptive_sampler else 'ç¦ç”¨'}")
    
    # æ¸¬è©¦åˆå§‹åŒ–é»
    initial_points = torch.rand(100, 2)
    manager.setup_initial_points(initial_points)
    print(f"âœ… åˆå§‹åŒ– {len(initial_points)} å€‹é»")
    
    # æ¸¬è©¦è§¸ç™¼æª¢æŸ¥
    for epoch in range(0, 3000, 500):
        triggered = manager.should_resample_collocation_points(
            epoch, current_loss=0.1 * np.exp(-epoch/1000)
        )
        if triggered:
            print(f"âœ… Epoch {epoch}: è§¸ç™¼é‡æ¡æ¨£")
    
    # æ¸¬è©¦é»æ¬Šé‡
    print("\næ¸¬è©¦æ–°é»æ¬Šé‡è¡°æ¸›...")
    manager.last_resample_epoch = 1000
    manager.new_point_mask = torch.zeros(100, dtype=torch.bool)
    manager.new_point_mask[-30:] = True  # æœ€å¾Œ 30 å€‹æ˜¯æ–°é»
    
    for epoch in [1000, 1250, 1500, 1750, 2000]:
        weights = manager.get_point_weights(epoch)
        if weights is not None:
            print(f"  Epoch {epoch}: æ–°é»æ¬Šé‡={weights[manager.new_point_mask].mean():.3f}, "
                  f"èˆŠé»æ¬Šé‡={weights[~manager.new_point_mask].mean():.3f}")
    
    # æ¸¬è©¦æ‘˜è¦
    print("\nè¨“ç·´å¾ªç’°ç®¡ç†å™¨æ‘˜è¦:")
    summary = manager.get_summary()
    for key, value in summary.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… è¨“ç·´å¾ªç’°ç®¡ç†å™¨æ¸¬è©¦å®Œæˆï¼")
