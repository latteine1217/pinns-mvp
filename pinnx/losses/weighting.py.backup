"""
å‹•æ…‹æ¬Šé‡å¹³è¡¡æ¨¡çµ„

å¯¦ç¾å¤šç¨®è‡ªé©æ‡‰æ¬Šé‡ç­–ç•¥ï¼Œç”¨æ–¼å¹³è¡¡ PINNs è¨“ç·´ä¸­çš„å¤šé …æå¤±å‡½æ•¸ã€‚
é€™æ˜¯å¯¦ç¾ç©©å®šé«˜æ•ˆè¨“ç·´çš„é—œéµçµ„ä»¶ï¼Œç‰¹åˆ¥é©ç”¨æ–¼é€†å•é¡Œèˆ‡é«˜ Reynolds æ•¸æµå ´ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- GradNorm æ¢¯åº¦ç¯„æ•¸å¹³è¡¡
- NTK æ¬Šé‡ç­–ç•¥ 
- æ™‚é–“å› æœæ¬Šé‡ (Causal Weighting)
- è‡ªé©æ‡‰æ¬Šé‡èª¿åº¦
- å¤šé …æå¤±å‡½æ•¸å‹•æ…‹å¹³è¡¡

æ ¸å¿ƒç®—æ³•ï¼š
1. GradNorm: é€šéæ¢¯åº¦ç¯„æ•¸å¹³è¡¡ä¸åŒæå¤±é …
2. Causal Weights: æ™‚é–“åºåˆ—è¨“ç·´ä¸­çš„å› æœç´„æŸ
3. NTK Weighting: åŸºæ–¼ç¥ç¶“æ­£åˆ‡æ ¸çš„æ¬Šé‡ç­–ç•¥
4. Adaptive Scheduling: è¨“ç·´éç¨‹ä¸­çš„è‡ªé©æ‡‰èª¿æ•´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Callable, Union
import numpy as np
import math
from collections import defaultdict


class GradNormWeighter:
    """
    GradNorm å‹•æ…‹æ¬Šé‡å¹³è¡¡å™¨
    
    åŸºæ–¼æ¢¯åº¦ç¯„æ•¸çš„è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´ï¼Œç¢ºä¿ä¸åŒæå¤±é …å°æ¨¡å‹åƒæ•¸çš„å½±éŸ¿å¹³è¡¡ã€‚
    ç‰¹åˆ¥é©ç”¨æ–¼ PINNs ä¸­ç‰©ç†æ®˜å·®ã€è³‡æ–™ä¸€è‡´æ€§ã€é‚Šç•Œæ¢ä»¶ç­‰å¤šé …æå¤±çš„å¹³è¡¡ã€‚
    
    åƒè€ƒ: GradNorm: Gradient Normalization for Adaptive Loss Balancing (ICML 2018)
    """
    
    def __init__(self,
                 model: nn.Module,
                 loss_names: List[str],
                 alpha: float = 0.12,
                 update_frequency: int = 1000,
                 initial_weights: Optional[Dict[str, float]] = None,
                 target_gradient_ratio: float = 1.0,
                 target_ratios: Optional[List[float]] = None,
                 device: Optional[str] = None):
        """
        Args:
            model: PINN æ¨¡å‹
            loss_names: æå¤±é …åç¨±åˆ—è¡¨ ['data', 'residual', 'boundary', 'prior']
            alpha: æ¢¯åº¦å¹³è¡¡çš„æ›´æ–°ç‡ (0.12 ç‚ºè«–æ–‡å»ºè­°å€¼)
            update_frequency: æ¬Šé‡æ›´æ–°é »ç‡ (æ¯å¤šå°‘æ­¥æ›´æ–°ä¸€æ¬¡)
            initial_weights: åˆå§‹æ¬Šé‡å­—å…¸
            target_gradient_ratio: ç›®æ¨™æ¢¯åº¦æ¯”ä¾‹
            target_ratios: ç›®æ¨™æ¯”ä¾‹åˆ—è¡¨ï¼ˆå¯é¸ï¼Œç”¨æ–¼æ¸¬è©¦ç›¸å®¹æ€§ï¼‰
            device: è¨ˆç®—è¨­å‚™ (None ç‚ºè‡ªå‹•æª¢æ¸¬)
        """
        self.model = model
        self.loss_names = loss_names
        self.alpha = alpha
        self.update_frequency = update_frequency
        self.target_gradient_ratio = target_gradient_ratio
        self.target_ratios = target_ratios  # å­˜å„²ç›®æ¨™æ¯”ä¾‹ï¼ˆç”¨æ–¼ç›¸å®¹æ€§ï¼‰
        
        # è‡ªå‹•æª¢æ¸¬è¨­å‚™
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
        
        # åˆå§‹åŒ–æ¬Šé‡
        if initial_weights is None:
            initial_weights = {name: 1.0 for name in loss_names}
        
        self.weights = {name: torch.tensor(initial_weights.get(name, 1.0), 
                                         device=device, requires_grad=False)
                       for name in loss_names}
        
        # è¨˜éŒ„æ¢¯åº¦æ­·å²ç”¨æ–¼ç©©å®šåŒ–
        self.gradient_history = defaultdict(list)
        self.step_count = 0
        self.initial_losses = None
        
    def compute_gradients(self, losses: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        è¨ˆç®—æ¯å€‹æå¤±é …å°æ¨¡å‹åƒæ•¸çš„æ¢¯åº¦ç¯„æ•¸
        
        Args:
            losses: æå¤±é …å­—å…¸
            
        Returns:
            æ¯å€‹æå¤±é …çš„æ¢¯åº¦ç¯„æ•¸
        """
        gradients = {}
        
        for name, loss in losses.items():
            if name not in self.loss_names:
                continue
                
            # è¨ˆç®—è©²æå¤±é …å°æ¨¡å‹åƒæ•¸çš„æ¢¯åº¦
            grads = torch.autograd.grad(
                outputs=loss,
                inputs=list(self.model.parameters()),
                grad_outputs=torch.ones_like(loss),
                retain_graph=True,
                create_graph=False,
                allow_unused=True
            )
            
            # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
            grad_norm = 0.0
            for grad in grads:
                if grad is not None:
                    grad_norm += (grad.detach() ** 2).sum()
            
            gradients[name] = torch.sqrt(grad_norm + 1e-12)
            
        return gradients
    
    def update_weights(self, 
                      losses: Dict[str, torch.Tensor], 
                      total_loss: Optional[torch.Tensor] = None) -> Dict[str, float]:
        """
        æ›´æ–°å‹•æ…‹æ¬Šé‡ï¼ˆèˆ‡æ¸¬è©¦ç›¸å®¹çš„æ¥å£ï¼‰
        
        Args:
            losses: ç•¶å‰æå¤±é …å­—å…¸
            total_loss: ç¸½æå¤±ï¼ˆå¯é¸ï¼Œç”¨æ–¼è§¸ç™¼æ¢¯åº¦è¨ˆç®—ï¼‰
            
        Returns:
            æ›´æ–°å¾Œçš„æ¬Šé‡å­—å…¸
        """
        self.step_count += 1
        
        # è¨˜éŒ„åˆå§‹æå¤±å€¼ç”¨æ–¼ç›¸å°æ¯”è¼ƒ
        if self.initial_losses is None:
            self.initial_losses = {name: loss.detach().item() 
                                 for name, loss in losses.items() 
                                 if name in self.loss_names}
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¬Šé‡
        if self.step_count % self.update_frequency != 0:
            return {name: weight.item() for name, weight in self.weights.items()}
        
        # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
        gradients = self.compute_gradients(losses)
        
        if len(gradients) < 2:  # è‡³å°‘éœ€è¦å…©å€‹æå¤±é …æ‰é€²è¡Œå¹³è¡¡
            return {name: weight.item() for name, weight in self.weights.items()}
        
        # è¨ˆç®—ç›¸å°æå¤±ç‡ (ç›¸å°æ–¼åˆå§‹å€¼çš„è®ŠåŒ–)
        relative_losses = {}
        for name in self.loss_names:
            if name in losses and name in self.initial_losses:
                current_loss = losses[name].detach().item()
                initial_loss = self.initial_losses[name]
                relative_losses[name] = current_loss / (initial_loss + 1e-12)
        
        # è¨ˆç®—å¹³å‡ç›¸å°æå¤±ç‡
        avg_relative_loss = np.mean(list(relative_losses.values()))
        
        # æ›´æ–°æ¬Šé‡
        total_grad = sum(gradients.values()) + 1e-12
        
        for name in self.loss_names:
            if name not in gradients:
                continue
                
            # è¨ˆç®—ç›®æ¨™æ¢¯åº¦ç¯„æ•¸
            target_grad = total_grad * self.target_gradient_ratio / len(self.loss_names)
            
            # è¨ˆç®—ç•¶å‰æ¢¯åº¦èˆ‡ç›®æ¨™çš„æ¯”å€¼
            current_grad = gradients[name]
            gradient_ratio = current_grad / (target_grad + 1e-12)
            
            # è€ƒæ…®ç›¸å°æå¤±ç‡çš„å½±éŸ¿
            if name in relative_losses:
                loss_ratio = relative_losses[name] / (avg_relative_loss + 1e-12)
                adjustment_factor = gradient_ratio * loss_ratio
            else:
                adjustment_factor = gradient_ratio
            
            # ä½¿ç”¨æŒ‡æ•¸ç§»å‹•å¹³å‡æ›´æ–°æ¬Šé‡
            new_weight = self.weights[name] * (adjustment_factor ** (-self.alpha))
            
            # é™åˆ¶æ¬Šé‡ç¯„åœé¿å…æ•¸å€¼ä¸ç©©å®š
            new_weight = torch.clamp(new_weight, 0.01, 100.0)
            self.weights[name] = new_weight
            
            # è¨˜éŒ„æ¢¯åº¦æ­·å²
            self.gradient_history[name].append(current_grad.item())
            if len(self.gradient_history[name]) > 100:  # ä¿æŒæ­·å²é•·åº¦
                self.gradient_history[name].pop(0)
        
        return {name: weight.item() for name, weight in self.weights.items()}
    
    def get_weights(self) -> Dict[str, float]:
        """ç²å–ç•¶å‰æ¬Šé‡"""
        return {name: weight.item() for name, weight in self.weights.items()}
    
    def reset_weights(self):
        """é‡ç½®æ¬Šé‡ç‚ºåˆå§‹å€¼"""
        for name in self.loss_names:
            self.weights[name] = torch.tensor(1.0, device=self.device)
        self.step_count = 0
        self.initial_losses = None
        self.gradient_history.clear()


class CausalWeighter:
    """
    æ™‚é–“å› æœæ¬Šé‡å™¨
    
    åœ¨éå®šå¸¸å•é¡Œä¸­ï¼Œç¢ºä¿æ¨¡å‹å„ªå…ˆæ“¬åˆæ—©æœŸæ™‚é–“çš„è§£ï¼Œç„¶å¾Œé€æ­¥æ¨é€²åˆ°å¾ŒæœŸã€‚
    é€™æœ‰åŠ©æ–¼é¿å…æ™‚é–“åºåˆ—è¨“ç·´ä¸­çš„æ¢¯åº¦æ¶ˆå¤±å’Œç´¯ç©èª¤å·®å•é¡Œã€‚
    """
    
    def __init__(self,
                 causality_strength: float = 1.0,
                 time_window_size: int = 10,
                 decay_rate: float = 0.1,
                 time_window: int = None,
                 temporal_decay: float = None):
        """
        Args:
            causality_strength: å› æœç´„æŸå¼·åº¦
            time_window_size: æ™‚é–“çª—å£å¤§å°
            decay_rate: æ¬Šé‡è¡°æ¸›ç‡
            time_window: æ™‚é–“çª—å£ï¼ˆåˆ¥åï¼Œç”¨æ–¼æ¸¬è©¦ç›¸å®¹æ€§ï¼‰
            temporal_decay: æ™‚é–“è¡°æ¸›åƒæ•¸ï¼ˆåˆ¥åï¼Œç”¨æ–¼æ¸¬è©¦ç›¸å®¹æ€§ï¼‰
        """
        self.causality_strength = causality_strength
        # å„ªå…ˆä½¿ç”¨æ¸¬è©¦æœŸå¾…çš„åƒæ•¸åç¨±
        self.time_window_size = time_window if time_window is not None else time_window_size
        self.decay_rate = temporal_decay if temporal_decay is not None else decay_rate
        self.accumulated_errors = []
        
    def compute_causal_weights(self, 
                             time_losses: List[torch.Tensor],
                             time_points: Optional[List[float]] = None) -> List[float]:
        """
        è¨ˆç®—æ™‚é–“å› æœæ¬Šé‡
        
        Args:
            time_losses: æŒ‰æ™‚é–“é †åºçš„æå¤±åˆ—è¡¨
            time_points: å°æ‡‰çš„æ™‚é–“é»ï¼ˆå¯é¸ï¼‰
            
        Returns:
            æ™‚é–“å› æœæ¬Šé‡åˆ—è¡¨
        """
        if len(time_losses) <= 1:
            return [1.0] * len(time_losses)
        
        # è¨ˆç®—ç´¯ç©èª¤å·®
        accumulated_error = 0.0
        weights = []
        
        for i, loss in enumerate(time_losses):
            # ç•¶å‰æ™‚é–“æ­¥çš„åŸºç¤æ¬Šé‡
            base_weight = 1.0
            
            # æ ¹æ“šç´¯ç©èª¤å·®èª¿æ•´æ¬Šé‡
            if i == 0:
                # ç¬¬ä¸€å€‹æ™‚é–“æ­¥ç¸½æ˜¯å…¨æ¬Šé‡
                weight = base_weight
            else:
                # åŸºæ–¼å‰é¢ç´¯ç©èª¤å·®è¨ˆç®—æ¬Šé‡
                causal_factor = math.exp(-self.causality_strength * accumulated_error)
                weight = base_weight * causal_factor
            
            weights.append(weight)
            
            # æ›´æ–°ç´¯ç©èª¤å·®ï¼ˆä½¿ç”¨ detach é¿å…æ¢¯åº¦è¨ˆç®—ï¼‰
            accumulated_error += loss.detach().item()
        
        # æ­£è¦åŒ–æ¬Šé‡
        total_weight = sum(weights)
        if total_weight > 0:
            weights = [w / total_weight * len(weights) for w in weights]
        
        return weights
    
    def apply_temporal_decay(self, 
                           weights: List[float], 
                           current_epoch: int) -> List[float]:
        """
        æ‡‰ç”¨æ™‚é–“è¡°æ¸›ç­–ç•¥
        
        éš¨è‘—è¨“ç·´é€²è¡Œï¼Œé€æ¼¸æ¸›å°‘å› æœç´„æŸå¼·åº¦ï¼Œå…è¨±æ¨¡å‹æ›´å¥½åœ°æ“¬åˆå¾ŒæœŸæ™‚é–“
        """
        decay_factor = math.exp(-self.decay_rate * current_epoch / 1000.0)
        causality_factor = self.causality_strength * decay_factor
        
        # é‡æ–°è¨ˆç®—è¡°æ¸›å¾Œçš„æ¬Šé‡
        adjusted_weights = []
        for i, w in enumerate(weights):
            if i == 0:
                adjusted_weights.append(w)
            else:
                # å¾ŒæœŸæ™‚é–“æ­¥å—åˆ°è¼ƒå°‘çš„å› æœç´„æŸ
                causal_adjustment = 1.0 + (causality_factor - 1.0) * math.exp(-i * 0.1)
                adjusted_weights.append(w * causal_adjustment)
        
        return adjusted_weights


class NTKWeighter:
    """
    ç¥ç¶“æ­£åˆ‡æ ¸ (NTK) æ¬Šé‡å™¨
    
    åŸºæ–¼ç¥ç¶“æ­£åˆ‡æ ¸ç†è«–çš„æ¬Šé‡ç­–ç•¥ï¼Œé€šéåˆ†æä¸åŒæå¤±é …å°æ‡‰çš„æ ¸å‡½æ•¸
    ä¾†å‹•æ…‹èª¿æ•´æ¬Šé‡ï¼Œç¢ºä¿è¨“ç·´éç¨‹ä¸­çš„ç†è«–æ”¶æ–‚æ€§ã€‚
    """
    
    def __init__(self,
                 model: nn.Module,
                 loss_names: List[str] = None,
                 sample_size: int = 100,
                 update_frequency: int = 2000,
                 update_freq: Optional[int] = None,
                 reg_param: float = 1e-6):
        """
        Args:
            model: PINN æ¨¡å‹
            loss_names: æå¤±é …åç¨±åˆ—è¡¨
            sample_size: NTK è¨ˆç®—çš„æ¡æ¨£é»æ•¸é‡
            update_frequency: æ¬Šé‡æ›´æ–°é »ç‡
            update_freq: æ¬Šé‡æ›´æ–°é »ç‡ï¼ˆåˆ¥åï¼Œç”¨æ–¼ç›¸å®¹æ€§ï¼‰
            reg_param: æ­£å‰‡åŒ–åƒæ•¸
        """
        self.model = model
        self.loss_names = loss_names or ['data', 'pde']  # é è¨­æå¤±åç¨±
        self.sample_size = sample_size
        # å„ªå…ˆä½¿ç”¨ update_freq åƒæ•¸ï¼ˆæ¸¬è©¦ç›¸å®¹æ€§ï¼‰
        self.update_frequency = update_freq if update_freq is not None else update_frequency
        self.reg_param = reg_param
        self.step_count = 0
        self.ntk_weights = {name: 1.0 for name in self.loss_names}
        
    def compute_ntk_eigenvalues(self, 
                              inputs: torch.Tensor,
                              loss_type: str) -> torch.Tensor:
        """
        è¨ˆç®— NTK ç‰¹å¾µå€¼
        
        Args:
            inputs: è¼¸å…¥æ¡æ¨£é»
            loss_type: æå¤±é¡å‹
            
        Returns:
            NTK çŸ©é™£çš„ç‰¹å¾µå€¼
        """
        # ç°¡åŒ–çš„ NTK ä¼°è¨ˆï¼ˆå®Œæ•´å¯¦ç¾éœ€è¦æ›´è¤‡é›œçš„æ ¸è¨ˆç®—ï¼‰
        batch_size = min(self.sample_size, inputs.shape[0])
        sample_inputs = inputs[:batch_size]
        
        # è¨ˆç®—é›…å¯æ¯”çŸ©é™£
        outputs = self.model(sample_inputs)
        jacobians = []
        
        for i in range(outputs.shape[1]):  # å°æ¯å€‹è¼¸å‡ºç¶­åº¦
            grads = torch.autograd.grad(
                outputs[:, i].sum(),
                self.model.parameters(),
                retain_graph=True,
                create_graph=False
            )
            jacobian = torch.cat([g.view(-1) for g in grads])
            jacobians.append(jacobian)
        
        J = torch.stack(jacobians, dim=0)  # [output_dim, param_dim]
        
        # è¨ˆç®— NTK çŸ©é™£ K = J @ J^T
        K = J @ J.T
        
        # è¨ˆç®—ç‰¹å¾µå€¼
        eigenvals = torch.linalg.eigvals(K).real
        
        return eigenvals
    
    def update_ntk_weights(self, 
                         data_inputs: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        åŸºæ–¼ NTK åˆ†ææ›´æ–°æ¬Šé‡
        
        Args:
            data_inputs: ä¸åŒæå¤±é …å°æ‡‰çš„è¼¸å…¥æ•¸æ“š
            
        Returns:
            æ›´æ–°å¾Œçš„ NTK æ¬Šé‡
        """
        self.step_count += 1
        
        if self.step_count % self.update_frequency != 0:
            return self.ntk_weights.copy()
    
    def update_weights(self, 
                      losses: Dict[str, torch.Tensor],
                      x_train: torch.Tensor,
                      step: int = 0) -> Dict[str, float]:
        """
        æ›´æ–°æ¬Šé‡ï¼ˆæ¸¬è©¦ç›¸å®¹æ€§æ–¹æ³•ï¼‰
        
        Args:
            losses: æå¤±å­—å…¸
            x_train: è¨“ç·´è¼¸å…¥æ•¸æ“š
            step: ç•¶å‰æ­¥æ•¸
            
        Returns:
            æ›´æ–°å¾Œçš„æ¬Šé‡å­—å…¸
        """
        # æ§‹å»º data_inputs æ ¼å¼
        data_inputs = {}
        for name in losses.keys():
            if name in self.loss_names:
                data_inputs[name] = x_train
        
        # è¨­ç½®æ­¥æ•¸ä¸¦èª¿ç”¨åŸæ–¹æ³•
        self.step_count = step
        return self.update_ntk_weights(data_inputs)
        
        ntk_metrics = {}
        
        for loss_name, inputs in data_inputs.items():
            if loss_name not in self.loss_names:
                continue
                
            try:
                eigenvals = self.compute_ntk_eigenvalues(inputs, loss_name)
                
                # è¨ˆç®— NTK åº¦é‡ï¼ˆæ¢ä»¶æ•¸çš„å€’æ•¸ä½œç‚ºæ¬Šé‡æŒ‡æ¨™ï¼‰
                max_eigenval = torch.max(eigenvals)
                min_eigenval = torch.max(eigenvals[eigenvals > 1e-12])
                condition_number = max_eigenval / (min_eigenval + 1e-12)
                
                # æ¢ä»¶æ•¸è¶Šå°ï¼Œæ¬Šé‡è¶Šå¤§ï¼ˆæ›´å¥½çš„æ”¶æ–‚æ€§ï¼‰
                ntk_metrics[loss_name] = 1.0 / (condition_number + 1.0)
                
            except Exception as e:
                # å¦‚æœè¨ˆç®—å¤±æ•—ï¼Œä¿æŒç•¶å‰æ¬Šé‡
                ntk_metrics[loss_name] = self.ntk_weights[loss_name]
        
        # æ­£è¦åŒ–æ¬Šé‡
        if ntk_metrics:
            total_metric = sum(ntk_metrics.values())
            for name in self.loss_names:
                if name in ntk_metrics:
                    self.ntk_weights[name] = ntk_metrics[name] / (total_metric + 1e-12)
        
        return self.ntk_weights.copy()


class AdaptiveWeightScheduler:
    """
    è‡ªé©æ‡‰æ¬Šé‡èª¿åº¦å™¨
    
    çµåˆå¤šç¨®æ¬Šé‡ç­–ç•¥ï¼Œæ ¹æ“šè¨“ç·´éšæ®µå‹•æ…‹èª¿æ•´æ¬Šé‡çµ„åˆã€‚
    æ”¯æ´è¨“ç·´åˆæœŸã€ä¸­æœŸã€å¾ŒæœŸçš„ä¸åŒæ¬Šé‡ç­–ç•¥ã€‚
    """
    
    def __init__(self,
                 loss_names: List[str],
                 phases: Dict[str, Dict] = None,
                 adaptation_method: str = 'exponential',
                 adaptation_rate: float = 0.1):
        """
        Args:
            loss_names: æå¤±é …åç¨±åˆ—è¡¨
            phases: è¨“ç·´éšæ®µé…ç½®å­—å…¸
            adaptation_method: é©æ‡‰æ–¹æ³• ('exponential', 'linear', 'cosine')
            adaptation_rate: é©æ‡‰ç‡
        """
        self.loss_names = loss_names
        self.adaptation_method = adaptation_method
        self.adaptation_rate = adaptation_rate
        
        # é è¨­ä¸‰éšæ®µè¨“ç·´ç­–ç•¥
        if phases is None:
            phases = {
                'warmup': {
                    'duration_ratio': 0.1,      # å‰ 10% çš„è¨“ç·´æ­¥æ•¸
                    'primary_losses': ['data'],  # ä¸»è¦é—œæ³¨è³‡æ–™æ“¬åˆ
                    'weight_ratios': {'data': 2.0, 'residual': 0.5, 'boundary': 1.0}
                },
                'main': {
                    'duration_ratio': 0.7,      # ä¸­é–“ 70% çš„è¨“ç·´æ­¥æ•¸
                    'primary_losses': ['residual', 'boundary'],  # ä¸»è¦é—œæ³¨ç‰©ç†ä¸€è‡´æ€§
                    'weight_ratios': {'data': 1.0, 'residual': 2.0, 'boundary': 1.5}
                },
                'refinement': {
                    'duration_ratio': 0.2,      # æœ€å¾Œ 20% çš„è¨“ç·´æ­¥æ•¸
                    'primary_losses': ['data', 'residual'],  # å¹³è¡¡æ“¬åˆ
                    'weight_ratios': {'data': 1.5, 'residual': 1.5, 'boundary': 1.0}
                }
            }
        
        self.phases = phases
        self.current_phase = 'warmup'
        
    def get_current_phase(self, current_step: int, total_steps: int) -> str:
        """ç¢ºå®šç•¶å‰è¨“ç·´éšæ®µ"""
        progress = current_step / total_steps
        
        warmup_end = self.phases['warmup']['duration_ratio']
        main_end = warmup_end + self.phases['main']['duration_ratio']
        
        if progress <= warmup_end:
            return 'warmup'
        elif progress <= main_end:
            return 'main'
        else:
            return 'refinement'
    
    def get_phase_weights(self, 
                         current_step: int, 
                         total_steps: int) -> Dict[str, float]:
        """
        ç²å–ç•¶å‰éšæ®µçš„æ¬Šé‡é…ç½®
        
        Args:
            current_step: ç•¶å‰è¨“ç·´æ­¥æ•¸
            total_steps: ç¸½è¨“ç·´æ­¥æ•¸
            
        Returns:
            éšæ®µæ¬Šé‡å­—å…¸
        """
        phase = self.get_current_phase(current_step, total_steps)
        self.current_phase = phase
        
        phase_config = self.phases[phase]
        weight_ratios = phase_config['weight_ratios']
        
        # ç”Ÿæˆæ¨™æº–åŒ–æ¬Šé‡
        weights = {}
        for name in self.loss_names:
            weights[name] = weight_ratios.get(name, 1.0)
        
        return weights
    
    def update_weights(self, 
                      losses: Dict[str, torch.Tensor], 
                      step: int,
                      total_steps: int = 10000) -> Dict[str, float]:
        """
        æ ¹æ“šç•¶å‰è¨“ç·´æ­¥æ•¸å’Œæå¤±å€¼å‹•æ…‹æ›´æ–°æ¬Šé‡
        
        Args:
            losses: ç•¶å‰æå¤±å€¼å­—å…¸
            step: ç•¶å‰è¨“ç·´æ­¥æ•¸  
            total_steps: ç¸½è¨“ç·´æ­¥æ•¸
            
        Returns:
            æ›´æ–°å¾Œçš„æ¬Šé‡å­—å…¸
        """
        # ç²å–ç•¶å‰éšæ®µçš„åŸºç¤æ¬Šé‡
        base_weights = self.get_phase_weights(step, total_steps)
        
        # æ ¹æ“šæå¤±å¤§å°å‹•æ…‹èª¿æ•´ï¼ˆå¯é¸çš„è‡ªé©æ‡‰èª¿æ•´ï¼‰
        if self.adaptation_method == 'exponential':
            # æå¤±å¤§çš„é …ç²å¾—æ›´é«˜æ¬Šé‡
            loss_magnitudes = {}
            for name, loss in losses.items():
                if name in self.loss_names:
                    loss_magnitudes[name] = float(loss.detach())
            
            # æ­£è¦åŒ–ä¸¦æ‡‰ç”¨åˆ°åŸºç¤æ¬Šé‡
            if loss_magnitudes:
                max_loss = max(loss_magnitudes.values()) + 1e-12
                adaptive_weights = {}
                for name in self.loss_names:
                    if name in loss_magnitudes:
                        # æå¤±è¶Šå¤§ï¼Œæ¬Šé‡é©æ‡‰æ€§èª¿æ•´è¶Šå¼·
                        loss_ratio = loss_magnitudes[name] / max_loss
                        adaptation_factor = 1.0 + self.adaptation_rate * loss_ratio
                        adaptive_weights[name] = base_weights[name] * adaptation_factor
                    else:
                        adaptive_weights[name] = base_weights[name]
                
                return adaptive_weights
        
        # å¦‚æœç„¡æ³•è¨ˆç®—é©æ‡‰æ€§æ¬Šé‡ï¼Œè¿”å›åŸºç¤æ¬Šé‡
        return base_weights
    
    def combine_weights(self,
                       phase_weights: Dict[str, float],
                       dynamic_weights: Dict[str, float],
                       combination_ratio: float = 0.7) -> Dict[str, float]:
        """
        çµ„åˆéšæ®µæ¬Šé‡èˆ‡å‹•æ…‹æ¬Šé‡
        
        Args:
            phase_weights: éšæ®µæ¬Šé‡
            dynamic_weights: å‹•æ…‹æ¬Šé‡ï¼ˆå¦‚ GradNormï¼‰
            combination_ratio: çµ„åˆæ¯”ä¾‹ï¼ˆéšæ®µæ¬Šé‡çš„æ¯”é‡ï¼‰
            
        Returns:
            çµ„åˆå¾Œçš„æœ€çµ‚æ¬Šé‡
        """
        final_weights = {}
        
        for name in self.loss_names:
            phase_w = phase_weights.get(name, 1.0)
            dynamic_w = dynamic_weights.get(name, 1.0)
            
            # åŠ æ¬Šçµ„åˆ
            final_w = combination_ratio * phase_w + (1 - combination_ratio) * dynamic_w
            final_weights[name] = final_w
        
        return final_weights


class MultiWeightManager:
    """
    å¤šæ¬Šé‡ç­–ç•¥ç®¡ç†å™¨
    
    æ•´åˆ GradNormã€Causalã€NTK ç­‰å¤šç¨®æ¬Šé‡ç­–ç•¥ï¼Œæä¾›çµ±ä¸€çš„æ¬Šé‡ç®¡ç†æ¥å£ã€‚
    """
    
    def __init__(self,
                 objectives_or_model = None,  # å…¼å®¹æ¸¬è©¦æœŸå¾…çš„ç¬¬ä¸€å€‹åƒæ•¸
                 loss_names: List[str] = None,
                 strategies: List[str] = ['gradnorm', 'adaptive'],
                 strategy_weights: Optional[Dict[str, float]] = None,
                 method: str = 'weighted_sum',
                 preference_weights: List[float] = None):
        """
        Args:
            objectives_or_model: ç›®æ¨™åˆ—è¡¨ï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰æˆ– PINN æ¨¡å‹ï¼ˆæ­£å¸¸æ¨¡å¼ï¼‰
            loss_names: æå¤±é …åç¨±åˆ—è¡¨  
            strategies: æ¬Šé‡ç­–ç•¥åˆ—è¡¨
            strategy_weights: ç­–ç•¥æ¬Šé‡å­—å…¸
            method: å¤šç›®æ¨™æ–¹æ³• ('weighted_sum', 'pareto')
            preference_weights: åå¥½æ¬Šé‡åˆ—è¡¨
        """
        # åˆ¤æ–·æ˜¯æ¸¬è©¦æ¨¡å¼é‚„æ˜¯æ­£å¸¸æ¨¡å¼
        if isinstance(objectives_or_model, list):
            # æ¸¬è©¦æ¨¡å¼ï¼šobjectives_or_model æ˜¯ç›®æ¨™åˆ—è¡¨
            self.objectives = objectives_or_model
            self.loss_names = objectives_or_model
            self.model = None
            self.method = method
            self.preference_weights = preference_weights or [1.0/len(objectives_or_model)] * len(objectives_or_model)
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šobjectives_or_model æ˜¯æ¨¡å‹
            self.model = objectives_or_model
            self.loss_names = loss_names or ['data', 'residual']
            self.objectives = self.loss_names
            self.method = 'weighted_sum'
            self.preference_weights = [1.0/len(self.loss_names)] * len(self.loss_names)
        self.strategies = strategies
        
        if strategy_weights is None:
            strategy_weights = {strategy: 1.0/len(strategies) for strategy in strategies}
        self.strategy_weights = strategy_weights
        
        # åˆå§‹åŒ–å„æ¬Šé‡å™¨
        self.weighters = {}
        
        if 'gradnorm' in strategies:
            self.weighters['gradnorm'] = GradNormWeighter(model, loss_names)
            
        if 'causal' in strategies:
            self.weighters['causal'] = CausalWeighter()
            
        if 'ntk' in strategies:
            self.weighters['ntk'] = NTKWeighter(model, loss_names)
            
        if 'adaptive' in strategies:
            self.weighters['adaptive'] = AdaptiveWeightScheduler(loss_names)
    
    def update_weights(self,
                      losses: Dict[str, torch.Tensor],
                      current_step: int = 0,
                      total_steps: int = 100000,
                      time_losses: Optional[List[torch.Tensor]] = None,
                      data_inputs: Optional[Dict[str, torch.Tensor]] = None) -> Dict[str, float]:
        """
        æ›´æ–°ç¶œåˆæ¬Šé‡
        
        Args:
            losses: ç•¶å‰æå¤±å­—å…¸
            current_step: ç•¶å‰æ­¥æ•¸
            total_steps: ç¸½æ­¥æ•¸
            time_losses: æ™‚é–“åºåˆ—æå¤±ï¼ˆç”¨æ–¼ causalï¼‰
            data_inputs: è¼¸å…¥æ•¸æ“šï¼ˆç”¨æ–¼ NTKï¼‰
            
        Returns:
            æœ€çµ‚æ¬Šé‡å­—å…¸
        """
        strategy_results = {}
        
        # è¨ˆç®—å„ç­–ç•¥çš„æ¬Šé‡
        if 'gradnorm' in self.weighters:
            strategy_results['gradnorm'] = self.weighters['gradnorm'].update_weights(losses)
        
        if 'adaptive' in self.weighters:
            strategy_results['adaptive'] = self.weighters['adaptive'].get_phase_weights(
                current_step, total_steps)
        
        if 'causal' in self.weighters and time_losses:
            causal_weights = self.weighters['causal'].compute_causal_weights(time_losses)
            # å°‡ causal æ¬Šé‡è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼ˆå‡è¨­æŒ‰é †åºå°æ‡‰ï¼‰
            if len(causal_weights) == len(self.loss_names):
                strategy_results['causal'] = dict(zip(self.loss_names, causal_weights))
        
        if 'ntk' in self.weighters and data_inputs:
            strategy_results['ntk'] = self.weighters['ntk'].update_ntk_weights(data_inputs)
        
        # çµ„åˆæ‰€æœ‰ç­–ç•¥çš„çµæœ
        final_weights = {}
        
        for name in self.loss_names:
            combined_weight = 0.0
            total_strategy_weight = 0.0
            
            for strategy, results in strategy_results.items():
                if strategy in self.strategy_weights and name in results:
                    strategy_w = self.strategy_weights[strategy]
                    result_w = results[name]
                    combined_weight += strategy_w * result_w
                    total_strategy_weight += strategy_w
            
            # æ­£è¦åŒ–
            if total_strategy_weight > 0:
                final_weights[name] = combined_weight / total_strategy_weight
            else:
                final_weights[name] = 1.0
        
        return final_weights
    
    def get_weights(self) -> Dict[str, float]:
        """ç²å–ç•¶å‰æ¬Šé‡ï¼ˆç„¡æ›´æ–°ï¼‰"""
        weights = {}
        for name in self.loss_names:
            weights[name] = 1.0  # é è¨­æ¬Šé‡
            
        # ç²å–æœ€æ–°çš„ gradnorm æ¬Šé‡
        if 'gradnorm' in self.weighters:
            gradnorm_weights = self.weighters['gradnorm'].get_weights()
            for name in self.loss_names:
                if name in gradnorm_weights:
                    weights[name] = gradnorm_weights[name]
        
        return weights


# ç‚ºæ¸¬è©¦æ–‡ä»¶æä¾›çš„ç›¸å®¹æ€§é¡åˆ¥å
GradNorm = GradNormWeighter  # åˆ¥å
NTKWeighting = NTKWeighter   # åˆ¥å
AdaptiveWeighting = AdaptiveWeightScheduler  # åˆ¥å
CausalWeighting = CausalWeighter  # åˆ¥å
MultiObjectiveWeighting = MultiWeightManager  # åˆ¥å


# ä¾¿æ·å‡½æ•¸
def create_weight_manager(model: nn.Module,
                         loss_names: List[str],
                         config: Optional[Dict] = None) -> MultiWeightManager:
    """
    å‰µå»ºæ¬Šé‡ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        model: PINN æ¨¡å‹
        loss_names: æå¤±é …åç¨±åˆ—è¡¨
        config: é…ç½®å­—å…¸
        
    Returns:
        é…ç½®å¥½çš„æ¬Šé‡ç®¡ç†å™¨
    """
    if config is None:
        config = {
            'strategies': ['gradnorm', 'adaptive'],
            'gradnorm_alpha': 0.12,
            'gradnorm_update_freq': 1000,
            'adaptive_phases': None  # ä½¿ç”¨é è¨­éšæ®µ
        }
    
    strategies = config.get('strategies', ['gradnorm', 'adaptive'])
    
    return MultiWeightManager(
        model=model,
        loss_names=loss_names,
        strategies=strategies
    )


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    print("ğŸ§ª æ¸¬è©¦å‹•æ…‹æ¬Šé‡æ¨¡çµ„...")
    
    # å‰µå»ºç°¡å–®æ¸¬è©¦æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(2, 3)
        def forward(self, x):
            return self.linear(x)
    
    model = TestModel()
    loss_names = ['data', 'residual', 'boundary', 'prior']
    
    # æ¸¬è©¦ GradNorm
    print("æ¸¬è©¦ GradNorm...")
    gradnorm = GradNormWeighter(model, loss_names)
    
    # æ¨¡æ“¬æå¤±
    test_losses = {
        'data': torch.tensor(2.0, requires_grad=True),
        'residual': torch.tensor(0.5, requires_grad=True),
        'boundary': torch.tensor(1.0, requires_grad=True),
        'prior': torch.tensor(0.8, requires_grad=True)
    }
    
    weights = gradnorm.update_weights(test_losses)
    print(f"GradNorm æ¬Šé‡: {weights}")
    
    # æ¸¬è©¦ Causal Weighter
    print("\næ¸¬è©¦ Causal Weighter...")
    causal = CausalWeighter()
    time_losses = [torch.tensor(1.0), torch.tensor(2.0), torch.tensor(1.5)]
    causal_weights = causal.compute_causal_weights(time_losses)
    print(f"Causal æ¬Šé‡: {causal_weights}")
    
    # æ¸¬è©¦å¤šæ¬Šé‡ç®¡ç†å™¨
    print("\næ¸¬è©¦ MultiWeightManager...")
    manager = create_weight_manager(model, loss_names)
    final_weights = manager.update_weights(test_losses, current_step=1000)
    print(f"æœ€çµ‚æ¬Šé‡: {final_weights}")
    
    print("âœ… å‹•æ…‹æ¬Šé‡æ¨¡çµ„æ¸¬è©¦å®Œæˆï¼")


# ç‚ºæ¸¬è©¦ç›¸å®¹æ€§æä¾›åˆ¥å
GradNorm = GradNormWeighter  # æ¸¬è©¦æœŸå¾…çš„é¡åˆ¥åç¨±
NTKWeighting = NTKWeighter  # NTK æ¬Šé‡ç­–ç•¥åˆ¥å  
AdaptiveWeighting = AdaptiveWeightScheduler  # è‡ªé©æ‡‰æ¬Šé‡åˆ¥å
CausalWeighting = CausalWeighter  # å› æœæ¬Šé‡åˆ¥å
MultiObjectiveWeighting = MultiWeightManager  # å¤šç›®æ¨™æ¬Šé‡åˆ¥å