"""
å‹•æ…‹æ¬Šé‡å¹³è¡¡æ¨¡çµ„

å¯¦ç¾å¤šç¨®è‡ªé©æ‡‰æ¬Šé‡ç­–ç•¥ï¼Œç”¨æ–¼å¹³è¡¡ PINNs è¨“ç·´ä¸­çš„å¤šé …æå¤±å‡½æ•¸ã€‚
é€™æ˜¯å¯¦ç¾ç©©å®šé«˜æ•ˆè¨“ç·´çš„é—œéµçµ„ä»¶ï¼Œç‰¹åˆ¥é©ç”¨æ–¼é€†å•é¡Œèˆ‡é«˜ Reynolds æ•¸æµå ´ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- GradNorm æ¢¯åº¦ç¯„æ•¸å¹³è¡¡
- NTK æ¬Šé‡ç­–ç•¥ 
- æ™‚é–“å› æœæ¬Šé‡ (Causal Weighting)
- è‡ªé©æ‡‰æ¬Šé‡èª¿åº¦
- å¤šé …æå¤±å‡½æ•¸å‹•æ…‹å¹³è¡¡

æ ¸å¿ƒç®—æ³•ï¼š
1. GradNorm: é€šéæ¢¯åº¦ç¯„æ•¸å¹³è¡¡ä¸åŒæå¤±é …
2. Causal Weights: æ™‚é–“åºåˆ—è¨“ç·´ä¸­çš„å› æœç´„æŸ
3. NTK Weighting: åŸºæ–¼ç¥ç¶“æ­£åˆ‡æ ¸çš„æ¬Šé‡ç­–ç•¥
4. Adaptive Scheduling: è¨“ç·´éç¨‹ä¸­çš„è‡ªé©æ‡‰èª¿æ•´
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Any, Dict, List, Optional, Tuple, Callable, Union
import numpy as np
import math
from collections import defaultdict

_EPS = 1e-12


class GradNormWeighter:
    """
    GradNorm å‹•æ…‹æ¬Šé‡å¹³è¡¡å™¨
    
    åŸºæ–¼æ¢¯åº¦ç¯„æ•¸çš„è‡ªé©æ‡‰æ¬Šé‡èª¿æ•´ï¼Œç¢ºä¿ä¸åŒæå¤±é …å°æ¨¡å‹åƒæ•¸çš„å½±éŸ¿å¹³è¡¡ã€‚
    ç‰¹åˆ¥é©ç”¨æ–¼ PINNs ä¸­ç‰©ç†æ®˜å·®ã€è³‡æ–™ä¸€è‡´æ€§ã€é‚Šç•Œæ¢ä»¶ç­‰å¤šé …æå¤±çš„å¹³è¡¡ã€‚
    
    åƒè€ƒ: GradNorm: Gradient Normalization for Adaptive Loss Balancing (ICML 2018)
    """
    
    def __init__(self,
                 model: nn.Module,
                 loss_names: List[str],
                 alpha: float = 0.12,
                 update_frequency: int = 1000,
                 initial_weights: Optional[Dict[str, float]] = None,
                 target_gradient_ratio: float = 1.0,
                 target_ratios: Optional[List[float]] = None,
                 device: Optional[str] = None,
                 min_weight: float = 0.1,
                 max_weight: float = 10.0,
                 max_ratio: float = 50.0):
        """
        Args:
            model: PINN æ¨¡å‹
            loss_names: æå¤±é …åç¨±åˆ—è¡¨ ['data', 'residual', 'boundary', 'prior']
            alpha: æ¢¯åº¦å¹³è¡¡çš„æ›´æ–°ç‡ (0.12 ç‚ºè«–æ–‡å»ºè­°å€¼)
            update_frequency: æ¬Šé‡æ›´æ–°é »ç‡ (æ¯å¤šå°‘æ­¥æ›´æ–°ä¸€æ¬¡)
            initial_weights: åˆå§‹æ¬Šé‡å­—å…¸
            target_gradient_ratio: ç›®æ¨™æ¢¯åº¦æ¯”ä¾‹
            target_ratios: ç›®æ¨™æ¯”ä¾‹åˆ—è¡¨ï¼ˆå¯é¸ï¼Œç”¨æ–¼æ¸¬è©¦ç›¸å®¹æ€§ï¼‰
            device: è¨ˆç®—è¨­å‚™ (None ç‚ºè‡ªå‹•æª¢æ¸¬)
        """
        self.model = model
        self.loss_names = loss_names
        self.alpha = alpha
        self.update_frequency = update_frequency
        self.target_gradient_ratio = target_gradient_ratio
        self.target_ratios = target_ratios  # å­˜å„²ç›®æ¨™æ¯”ä¾‹ï¼ˆç”¨æ–¼ç›¸å®¹æ€§ï¼‰
        self.min_weight = float(min_weight)
        self.max_weight = float(max_weight)
        self.max_ratio = float(max(1.0, max_ratio))
        self.eps = _EPS
        
        # è‡ªå‹•æª¢æ¸¬è¨­å‚™
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            if isinstance(device, torch.device):
                self.device = device
            elif isinstance(device, str):
                self.device = torch.device(device)
            else:
                raise TypeError(f"Unsupported device type: {type(device)}")
        
        # åˆå§‹åŒ–æ¬Šé‡
        if initial_weights is None:
            initial_weights = {name: 1.0 for name in loss_names}
        
        self.initial_weight_values = {
            name: float(initial_weights.get(name, 1.0)) for name in loss_names
        }
        self.initial_weight_sum = float(sum(self.initial_weight_values.values()))
        self.weights = {}
        for name in loss_names:
            base_weight = torch.tensor(
                self.initial_weight_values[name],
                device=self.device,
                dtype=torch.float32,
                requires_grad=False
            )
            clamped_weight = torch.clamp(base_weight, self.min_weight, self.max_weight)
            self.weights[name] = clamped_weight
        
        # é å…ˆè¨ˆç®—ç›®æ¨™åˆ†ä½ˆ
        if target_ratios is not None:
            if len(target_ratios) != len(loss_names):
                raise ValueError(
                    "target_ratios length must match loss_names length "
                    f"({len(target_ratios)} != {len(loss_names)})"
                )
            ratios = torch.as_tensor(
                target_ratios, dtype=torch.float32, device=self.device
            )
            ratios = torch.clamp(ratios, min=self.eps)
            normalized = (ratios / ratios.mean()).cpu().tolist()
            self.target_distribution = {
                name: float(normalized[idx]) for idx, name in enumerate(loss_names)
            }
        else:
            self.target_distribution = {name: 1.0 for name in loss_names}
        
        # è¨˜éŒ„æ¢¯åº¦æ­·å²ç”¨æ–¼ç©©å®šåŒ–
        self.gradient_history = defaultdict(list)
        self.step_count = 0
        self.initial_losses = None
        
    def compute_gradients(self, losses: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        è¨ˆç®—æ¯å€‹æå¤±é …å°æ¨¡å‹åƒæ•¸çš„æ¢¯åº¦ç¯„æ•¸
        
        Args:
            losses: æå¤±é …å­—å…¸
            
        Returns:
            æ¯å€‹æå¤±é …çš„æ¢¯åº¦ç¯„æ•¸
        """
        gradients = {}
        
        for name, loss in losses.items():
            if name not in self.loss_names:
                continue
            
            # æª¢æŸ¥æå¤±æ˜¯å¦éœ€è¦æ¢¯åº¦å’Œæ˜¯å¦ç‚ºéé›¶
            if not loss.requires_grad or abs(float(loss.detach())) < self.eps:
                gradients[name] = torch.tensor(self.eps, device=self.device)
                continue
                
            try:
                weight_tensor = self.weights.get(name, None)
                if isinstance(weight_tensor, torch.Tensor):
                    weight_tensor = weight_tensor.detach()
                elif weight_tensor is None:
                    weight_tensor = torch.tensor(1.0, device=self.device)
                else:
                    weight_tensor = torch.tensor(
                        float(weight_tensor), device=self.device
                    )
                
                weighted_loss = loss * weight_tensor
                
                # è¨ˆç®—è©²æå¤±é …å°æ¨¡å‹åƒæ•¸çš„æ¢¯åº¦
                grads = torch.autograd.grad(
                    outputs=weighted_loss,
                    inputs=list(self.model.parameters()),
                    grad_outputs=torch.ones_like(weighted_loss),
                    retain_graph=True,
                    create_graph=False,
                    allow_unused=True
                )
                
                # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
                grad_norm = torch.tensor(0.0, device=self.device)
                for grad in grads:
                    if grad is not None:
                        grad_norm += (grad.detach() ** 2).sum()
                
                gradients[name] = torch.sqrt(grad_norm + self.eps)
                
            except Exception as e:
                # å¦‚æœæ¢¯åº¦è¨ˆç®—å¤±æ•—ï¼Œä½¿ç”¨å°å€¼
                gradients[name] = torch.tensor(self.eps, device=self.device)
                print(f"Warning: Gradient computation failed for {name}: {e}")
            
        return gradients
    
    def update_weights(self, 
                      losses: Dict[str, torch.Tensor], 
                      total_loss: Optional[torch.Tensor] = None) -> Dict[str, float]:
        """
        æ›´æ–°å‹•æ…‹æ¬Šé‡ï¼ˆèˆ‡æ¸¬è©¦ç›¸å®¹çš„æ¥å£ï¼‰
        
        Args:
            losses: ç•¶å‰æå¤±é …å­—å…¸
            total_loss: ç¸½æå¤±ï¼ˆå¯é¸ï¼Œç”¨æ–¼è§¸ç™¼æ¢¯åº¦è¨ˆç®—ï¼‰
            
        Returns:
            æ›´æ–°å¾Œçš„æ¬Šé‡å­—å…¸
        """
        self.step_count += 1
        
        # è¨˜éŒ„åˆå§‹æå¤±å€¼ç”¨æ–¼ç›¸å°æ¯”è¼ƒ
        if self.initial_losses is None:
            self.initial_losses = {name: loss.detach().item() 
                                 for name, loss in losses.items() 
                                 if name in self.loss_names}
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ¬Šé‡
        if self.step_count % self.update_frequency != 0:
            return self.get_weights()
        
        # è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
        gradients = self.compute_gradients(losses)
        
        usable_gradients = [
            gradients[name] for name in self.loss_names if name in gradients
        ]
        if len(usable_gradients) < 2:  # è‡³å°‘éœ€è¦å…©å€‹æå¤±é …æ‰é€²è¡Œå¹³è¡¡
            return self.get_weights()
        
        grad_values = torch.stack(usable_gradients)
        total_grad = grad_values.sum()
        avg_grad = grad_values.mean()
        
        # è¨ˆç®—ç›¸å°æå¤±ç‡ (ç›¸å°æ–¼åˆå§‹å€¼çš„è®ŠåŒ–)
        relative_losses = {}
        for name in self.loss_names:
            if name in losses and name in self.initial_losses:
                current_loss = losses[name].detach().item()
                initial_loss = self.initial_losses[name]
                relative_losses[name] = current_loss / (initial_loss + self.eps)
        
        # è¨ˆç®—å¹³å‡ç›¸å°æå¤±ç‡
        if relative_losses:
            avg_relative_loss = float(np.mean(list(relative_losses.values())))
            if not np.isfinite(avg_relative_loss) or avg_relative_loss < self.eps:
                avg_relative_loss = 1.0
        else:
            avg_relative_loss = 1.0
        
        # æ›´æ–°æ¬Šé‡
        for name in self.loss_names:
            if name not in gradients:
                continue
                
            # è¨ˆç®—ç›®æ¨™æ¢¯åº¦ç¯„æ•¸
            distribution_scale = self.target_distribution.get(name, 1.0)
            target_grad = avg_grad * distribution_scale * self.target_gradient_ratio
            target_grad = torch.clamp(target_grad, min=self.eps)
            
            # è¨ˆç®—ç•¶å‰æ¢¯åº¦èˆ‡ç›®æ¨™çš„æ¯”å€¼
            current_grad = gradients[name]
            gradient_ratio = current_grad / (target_grad + self.eps)
            gradient_ratio = torch.clamp(gradient_ratio, min=self.eps)
            
            # è€ƒæ…®ç›¸å°æå¤±ç‡çš„å½±éŸ¿
            if name in relative_losses:
                loss_ratio = relative_losses[name] / (avg_relative_loss + self.eps)
                loss_ratio = max(loss_ratio, self.eps)
                adjustment_factor = gradient_ratio * loss_ratio
            else:
                adjustment_factor = gradient_ratio
            
            # ä½¿ç”¨æŒ‡æ•¸ç§»å‹•å¹³å‡æ›´æ–°æ¬Šé‡ï¼Œä½†é™åˆ¶èª¿æ•´å¹…åº¦
            weight_adjustment = torch.clamp(
                adjustment_factor.pow(-self.alpha), 0.5, 2.0
            )
            new_weight = torch.clamp(
                self.weights[name] * weight_adjustment,
                self.min_weight,
                self.max_weight
            )
            
            self.weights[name] = new_weight.detach()
            
            # è¨˜éŒ„æ¢¯åº¦æ­·å²
            self.gradient_history[name].append(current_grad.item())
            if len(self.gradient_history[name]) > 100:  # ä¿æŒæ­·å²é•·åº¦
                self.gradient_history[name].pop(0)
        
        self._normalize_weights()
        
        return self.get_weights()
    
    def get_weights(self) -> Dict[str, float]:
        """ç²å–ç•¶å‰æ¬Šé‡"""
        return {name: weight.item() for name, weight in self.weights.items()}
    
    def reset_weights(self):
        """é‡ç½®æ¬Šé‡ç‚ºåˆå§‹å€¼"""
        for name in self.loss_names:
            self.weights[name] = torch.clamp(
                torch.tensor(
                    self.initial_weight_values.get(name, 1.0),
                    device=self.device,
                    dtype=torch.float32
                ),
                min=self.min_weight,
                max=self.max_weight
            )
        self._normalize_weights()
        self.step_count = 0
        self.initial_losses = None
        self.gradient_history.clear()

    def _normalize_weights(self) -> None:
        """Normalize weights to keep total constant and ratios bounded."""
        if not self.loss_names:
            return
        
        target_sum = torch.tensor(
            max(self.initial_weight_sum, self.eps),
            device=self.device,
            dtype=torch.float32
        )
        
        for _ in range(3):
            weights_tensor = torch.stack([self.weights[name] for name in self.loss_names])
            total = weights_tensor.sum()
            if not torch.isfinite(total) or total.abs() <= self.eps:
                break
            scale = target_sum / total
            updated = []
            for name in self.loss_names:
                scaled = self.weights[name] * scale
                updated_weight = torch.clamp(scaled, self.min_weight, self.max_weight)
                self.weights[name] = updated_weight
                updated.append(updated_weight)
            new_total = torch.stack(updated).sum()
            if torch.abs(new_total - target_sum) / target_sum < 1e-6:
                break
        
        # Enforce ratio constraint
        weights_tensor = torch.stack([self.weights[name] for name in self.loss_names])
        max_w = torch.max(weights_tensor)
        min_w = torch.clamp(torch.min(weights_tensor), min=self.min_weight)
        ratio = max_w / (min_w + self.eps)
        if ratio > self.max_ratio:
            geometric_mean = torch.exp(torch.log(weights_tensor + self.eps).mean())
            span = math.sqrt(self.max_ratio)
            lower = torch.tensor(
                max(self.min_weight, geometric_mean / span),
                device=self.device,
                dtype=torch.float32
            )
            upper = torch.tensor(
                min(self.max_weight, geometric_mean * span),
                device=self.device,
                dtype=torch.float32
            )
            for name in self.loss_names:
                self.weights[name] = torch.clamp(self.weights[name], lower, upper)
            
            # Re-normalize after clamping (iterate to respect bounds)
            for _ in range(3):
                weights_tensor = torch.stack([self.weights[name] for name in self.loss_names])
                total = weights_tensor.sum()
                if not torch.isfinite(total) or total.abs() <= self.eps:
                    break
                scale = target_sum / total
                updated = []
                for name in self.loss_names:
                    scaled = self.weights[name] * scale
                    updated_weight = torch.clamp(scaled, self.min_weight, self.max_weight)
                    self.weights[name] = updated_weight
                    updated.append(updated_weight)
                new_total = torch.stack(updated).sum()
                if torch.abs(new_total - target_sum) / target_sum < 1e-6:
                    break


class CausalWeighter:
    """
    æ™‚é–“å› æœæ¬Šé‡å™¨
    
    åœ¨éå®šå¸¸å•é¡Œä¸­ï¼Œç¢ºä¿æ¨¡å‹å„ªå…ˆæ“¬åˆæ—©æœŸæ™‚é–“çš„è§£ï¼Œç„¶å¾Œé€æ­¥æ¨é€²åˆ°å¾ŒæœŸã€‚
    é€™æœ‰åŠ©æ–¼é¿å…æ™‚é–“åºåˆ—è¨“ç·´ä¸­çš„æ¢¯åº¦æ¶ˆå¤±å’Œç´¯ç©èª¤å·®å•é¡Œã€‚
    """
    
    def __init__(self,
                 causality_strength: float = 1.0,
                 time_window_size: int = 10,
                 decay_rate: float = 0.1,
                 time_window: int = None,
                 temporal_decay: float = None):
        """
        Args:
            causality_strength: å› æœç´„æŸå¼·åº¦
            time_window_size: æ™‚é–“çª—å£å¤§å°
            decay_rate: æ¬Šé‡è¡°æ¸›ç‡
            time_window: æ™‚é–“çª—å£ï¼ˆåˆ¥åï¼Œç”¨æ–¼æ¸¬è©¦ç›¸å®¹æ€§ï¼‰
            temporal_decay: æ™‚é–“è¡°æ¸›åƒæ•¸ï¼ˆåˆ¥åï¼Œç”¨æ–¼æ¸¬è©¦ç›¸å®¹æ€§ï¼‰
        """
        self.causality_strength = causality_strength
        # å„ªå…ˆä½¿ç”¨æ¸¬è©¦æœŸå¾…çš„åƒæ•¸åç¨±
        self.time_window_size = time_window if time_window is not None else time_window_size
        self.decay_rate = temporal_decay if temporal_decay is not None else decay_rate
        self.accumulated_errors = []
        
    def compute_causal_weights(self, 
                             time_losses: List[torch.Tensor],
                             time_points: Optional[List[float]] = None) -> List[float]:
        """
        è¨ˆç®—æ™‚é–“å› æœæ¬Šé‡
        
        Args:
            time_losses: æŒ‰æ™‚é–“é †åºçš„æå¤±åˆ—è¡¨
            time_points: å°æ‡‰çš„æ™‚é–“é»ï¼ˆå¯é¸ï¼‰
            
        Returns:
            æ™‚é–“å› æœæ¬Šé‡åˆ—è¡¨
        """
        if len(time_losses) <= 1:
            return [1.0] * len(time_losses)
        
        # è¨ˆç®—ç´¯ç©èª¤å·®
        accumulated_error = 0.0
        weights = []
        
        for i, loss in enumerate(time_losses):
            # ç•¶å‰æ™‚é–“æ­¥çš„åŸºç¤æ¬Šé‡
            base_weight = 1.0
            
            # æ ¹æ“šç´¯ç©èª¤å·®èª¿æ•´æ¬Šé‡
            if i == 0:
                # ç¬¬ä¸€å€‹æ™‚é–“æ­¥ç¸½æ˜¯å…¨æ¬Šé‡
                weight = base_weight
            else:
                # åŸºæ–¼å‰é¢ç´¯ç©èª¤å·®è¨ˆç®—æ¬Šé‡
                causal_factor = math.exp(-self.causality_strength * accumulated_error)
                weight = base_weight * causal_factor
            
            weights.append(weight)
            
            # æ›´æ–°ç´¯ç©èª¤å·®ï¼ˆä½¿ç”¨ detach é¿å…æ¢¯åº¦è¨ˆç®—ï¼‰
            accumulated_error += loss.detach().item()
        
        # æ­£è¦åŒ–æ¬Šé‡
        total_weight = sum(weights)
        if total_weight > 0:
            weights = [w / total_weight * len(weights) for w in weights]
        
        return weights
    
    def apply_temporal_decay(self, 
                           weights: List[float], 
                           current_epoch: int) -> List[float]:
        """
        æ‡‰ç”¨æ™‚é–“è¡°æ¸›ç­–ç•¥
        
        éš¨è‘—è¨“ç·´é€²è¡Œï¼Œé€æ¼¸æ¸›å°‘å› æœç´„æŸå¼·åº¦ï¼Œå…è¨±æ¨¡å‹æ›´å¥½åœ°æ“¬åˆå¾ŒæœŸæ™‚é–“
        """
        decay_factor = math.exp(-self.decay_rate * current_epoch / 1000.0)
        causality_factor = self.causality_strength * decay_factor
        
        # é‡æ–°è¨ˆç®—è¡°æ¸›å¾Œçš„æ¬Šé‡
        adjusted_weights = []
        for i, w in enumerate(weights):
            if i == 0:
                adjusted_weights.append(w)
            else:
                # å¾ŒæœŸæ™‚é–“æ­¥å—åˆ°è¼ƒå°‘çš„å› æœç´„æŸ
                causal_adjustment = 1.0 + (causality_factor - 1.0) * math.exp(-i * 0.1)
                adjusted_weights.append(w * causal_adjustment)
        
        return adjusted_weights
    
class NTKWeighter:
    """
    ç¥ç¶“æ­£åˆ‡æ ¸ (NTK) æ¬Šé‡å™¨
    
    åŸºæ–¼ç¥ç¶“æ­£åˆ‡æ ¸ç†è«–çš„æ¬Šé‡ç­–ç•¥ï¼Œé€šéåˆ†æä¸åŒæå¤±é …å°æ‡‰çš„æ ¸å‡½æ•¸
    ä¾†å‹•æ…‹èª¿æ•´æ¬Šé‡ï¼Œç¢ºä¿è¨“ç·´éç¨‹ä¸­çš„ç†è«–æ”¶æ–‚æ€§ã€‚
    """
    
    def __init__(self,
                 model: nn.Module,
                 loss_names: List[str] = None,
                 sample_size: int = 100,
                 update_frequency: int = 2000,
                 update_freq: Optional[int] = None,
                 reg_param: float = 1e-6):
        """
        Args:
            model: PINN æ¨¡å‹
            loss_names: æå¤±é …åç¨±åˆ—è¡¨
            sample_size: NTK è¨ˆç®—çš„æ¡æ¨£é»æ•¸é‡
            update_frequency: æ¬Šé‡æ›´æ–°é »ç‡
            update_freq: æ¬Šé‡æ›´æ–°é »ç‡ï¼ˆåˆ¥åï¼Œç”¨æ–¼ç›¸å®¹æ€§ï¼‰
            reg_param: æ­£å‰‡åŒ–åƒæ•¸
        """
        self.model = model
        self.loss_names = loss_names or ['data', 'pde']  # é è¨­æå¤±åç¨±
        self.sample_size = sample_size
        # å„ªå…ˆä½¿ç”¨ update_freq åƒæ•¸ï¼ˆæ¸¬è©¦ç›¸å®¹æ€§ï¼‰
        self.update_frequency = update_freq if update_freq is not None else update_frequency
        self.reg_param = reg_param
        self.step_count = 0
        self.ntk_weights = {name: 1.0 for name in self.loss_names}
        
    def compute_ntk_eigenvalues(self, 
                              inputs: torch.Tensor,
                              loss_type: str) -> torch.Tensor:
        """
        è¨ˆç®— NTK ç‰¹å¾µå€¼
        
        Args:
            inputs: è¼¸å…¥æ¡æ¨£é»
            loss_type: æå¤±é¡å‹
            
        Returns:
            NTK çŸ©é™£çš„ç‰¹å¾µå€¼
        """
        # ç°¡åŒ–çš„ NTK ä¼°è¨ˆï¼ˆå®Œæ•´å¯¦ç¾éœ€è¦æ›´è¤‡é›œçš„æ ¸è¨ˆç®—ï¼‰
        batch_size = min(self.sample_size, inputs.shape[0])
        sample_inputs = inputs[:batch_size]
        
        # è¨ˆç®—é›…å¯æ¯”çŸ©é™£
        outputs = self.model(sample_inputs)
        jacobians = []
        
        for i in range(outputs.shape[1]):  # å°æ¯å€‹è¼¸å‡ºç¶­åº¦
            grads = torch.autograd.grad(
                outputs[:, i].sum(),
                self.model.parameters(),
                retain_graph=True,
                create_graph=False
            )
            jacobian = torch.cat([g.view(-1) for g in grads])
            jacobians.append(jacobian)
        
        J = torch.stack(jacobians, dim=0)  # [output_dim, param_dim]
        
        # è¨ˆç®— NTK çŸ©é™£ K = J @ J^T
        K = J @ J.T
        
        # è¨ˆç®—ç‰¹å¾µå€¼
        eigenvals = torch.linalg.eigvals(K).real
        
        return eigenvals
    
    def update_ntk_weights(self, 
                         data_inputs: Dict[str, torch.Tensor]) -> Dict[str, float]:
        """
        åŸºæ–¼ NTK åˆ†ææ›´æ–°æ¬Šé‡
        
        Args:
            data_inputs: ä¸åŒæå¤±é …å°æ‡‰çš„è¼¸å…¥æ•¸æ“š
            
        Returns:
            æ›´æ–°å¾Œçš„ NTK æ¬Šé‡
        """
        self.step_count += 1
        
        if self.step_count % self.update_frequency != 0:
            return self.ntk_weights.copy()
    
    def update_weights(self, 
                      losses: Dict[str, torch.Tensor],
                      x_train: torch.Tensor,
                      step: int = 0) -> Dict[str, float]:
        """
        æ›´æ–°æ¬Šé‡ï¼ˆæ¸¬è©¦ç›¸å®¹æ€§æ–¹æ³•ï¼‰
        
        Args:
            losses: æå¤±å­—å…¸
            x_train: è¨“ç·´è¼¸å…¥æ•¸æ“š
            step: ç•¶å‰æ­¥æ•¸
            
        Returns:
            æ›´æ–°å¾Œçš„æ¬Šé‡å­—å…¸
        """
        # æ§‹å»º data_inputs æ ¼å¼
        data_inputs = {}
        for name in losses.keys():
            if name in self.loss_names:
                data_inputs[name] = x_train
        
        # è¨­ç½®æ­¥æ•¸ä¸¦èª¿ç”¨åŸæ–¹æ³•
        self.step_count = step
        return self.update_ntk_weights(data_inputs)
        
        ntk_metrics = {}
        
        for loss_name, inputs in data_inputs.items():
            if loss_name not in self.loss_names:
                continue
                
            try:
                eigenvals = self.compute_ntk_eigenvalues(inputs, loss_name)
                
                # è¨ˆç®— NTK åº¦é‡ï¼ˆæ¢ä»¶æ•¸çš„å€’æ•¸ä½œç‚ºæ¬Šé‡æŒ‡æ¨™ï¼‰
                max_eigenval = torch.max(eigenvals)
                min_eigenval = torch.max(eigenvals[eigenvals > 1e-12])
                condition_number = max_eigenval / (min_eigenval + 1e-12)
                
                # æ¢ä»¶æ•¸è¶Šå°ï¼Œæ¬Šé‡è¶Šå¤§ï¼ˆæ›´å¥½çš„æ”¶æ–‚æ€§ï¼‰
                ntk_metrics[loss_name] = 1.0 / (condition_number + 1.0)
                
            except Exception as e:
                # å¦‚æœè¨ˆç®—å¤±æ•—ï¼Œä¿æŒç•¶å‰æ¬Šé‡
                ntk_metrics[loss_name] = self.ntk_weights[loss_name]
        
        # æ­£è¦åŒ–æ¬Šé‡
        if ntk_metrics:
            total_metric = sum(ntk_metrics.values())
            for name in self.loss_names:
                if name in ntk_metrics:
                    self.ntk_weights[name] = ntk_metrics[name] / (total_metric + 1e-12)
        
        return self.ntk_weights.copy()


class AdaptiveWeightScheduler:
    """
    è‡ªé©æ‡‰æ¬Šé‡èª¿åº¦å™¨
    
    çµåˆå¤šç¨®æ¬Šé‡ç­–ç•¥ï¼Œæ ¹æ“šè¨“ç·´éšæ®µå‹•æ…‹èª¿æ•´æ¬Šé‡çµ„åˆã€‚
    æ”¯æ´è¨“ç·´åˆæœŸã€ä¸­æœŸã€å¾ŒæœŸçš„ä¸åŒæ¬Šé‡ç­–ç•¥ã€‚
    """
    
    def __init__(self,
                 loss_names: List[str],
                 phases: Dict[str, Dict] = None,
                 adaptation_method: str = 'exponential',
                 adaptation_rate: float = 0.1):
        """
        Args:
            loss_names: æå¤±é …åç¨±åˆ—è¡¨
            phases: è¨“ç·´éšæ®µé…ç½®å­—å…¸
            adaptation_method: é©æ‡‰æ–¹æ³• ('exponential', 'linear', 'cosine')
            adaptation_rate: é©æ‡‰ç‡
        """
        self.loss_names = loss_names
        self.adaptation_method = adaptation_method
        self.adaptation_rate = adaptation_rate
        
        # é è¨­ä¸‰éšæ®µè¨“ç·´ç­–ç•¥
        if phases is None:
            phases = {
                'warmup': {
                    'duration_ratio': 0.1,      # å‰ 10% çš„è¨“ç·´æ­¥æ•¸
                    'primary_losses': ['data'],  # ä¸»è¦é—œæ³¨è³‡æ–™æ“¬åˆ
                    'weight_ratios': {'data': 2.0, 'residual': 0.5, 'boundary': 1.0}
                },
                'main': {
                    'duration_ratio': 0.7,      # ä¸­é–“ 70% çš„è¨“ç·´æ­¥æ•¸
                    'primary_losses': ['residual', 'boundary'],  # ä¸»è¦é—œæ³¨ç‰©ç†ä¸€è‡´æ€§
                    'weight_ratios': {'data': 1.0, 'residual': 2.0, 'boundary': 1.5}
                },
                'refinement': {
                    'duration_ratio': 0.2,      # æœ€å¾Œ 20% çš„è¨“ç·´æ­¥æ•¸
                    'primary_losses': ['data', 'residual'],  # å¹³è¡¡æ“¬åˆ
                    'weight_ratios': {'data': 1.5, 'residual': 1.5, 'boundary': 1.0}
                }
            }
        
        self.phases = phases
        self.current_phase = 'warmup'
        
    def get_current_phase(self, current_step: int, total_steps: int) -> str:
        """ç¢ºå®šç•¶å‰è¨“ç·´éšæ®µ"""
        progress = current_step / total_steps
        
        warmup_end = self.phases['warmup']['duration_ratio']
        main_end = warmup_end + self.phases['main']['duration_ratio']
        
        if progress <= warmup_end:
            return 'warmup'
        elif progress <= main_end:
            return 'main'
        else:
            return 'refinement'
    
    def get_phase_weights(self, 
                         current_step: int, 
                         total_steps: int) -> Dict[str, float]:
        """
        ç²å–ç•¶å‰éšæ®µçš„æ¬Šé‡é…ç½®
        
        Args:
            current_step: ç•¶å‰è¨“ç·´æ­¥æ•¸
            total_steps: ç¸½è¨“ç·´æ­¥æ•¸
            
        Returns:
            éšæ®µæ¬Šé‡å­—å…¸
        """
        phase = self.get_current_phase(current_step, total_steps)
        self.current_phase = phase
        
        phase_config = self.phases[phase]
        weight_ratios = phase_config['weight_ratios']
        
        # ç”Ÿæˆæ¨™æº–åŒ–æ¬Šé‡
        weights = {}
        for name in self.loss_names:
            weights[name] = weight_ratios.get(name, 1.0)
        
        return weights
    
    def update_weights(self, 
                      losses: Dict[str, torch.Tensor], 
                      step: int,
                      total_steps: int = 10000) -> Dict[str, float]:
        """
        æ ¹æ“šç•¶å‰è¨“ç·´æ­¥æ•¸å’Œæå¤±å€¼å‹•æ…‹æ›´æ–°æ¬Šé‡
        
        Args:
            losses: ç•¶å‰æå¤±å€¼å­—å…¸
            step: ç•¶å‰è¨“ç·´æ­¥æ•¸  
            total_steps: ç¸½è¨“ç·´æ­¥æ•¸
            
        Returns:
            æ›´æ–°å¾Œçš„æ¬Šé‡å­—å…¸
        """
        # ç²å–ç•¶å‰éšæ®µçš„åŸºç¤æ¬Šé‡
        base_weights = self.get_phase_weights(step, total_steps)
        
        # æ ¹æ“šæå¤±å¤§å°å‹•æ…‹èª¿æ•´ï¼ˆå¯é¸çš„è‡ªé©æ‡‰èª¿æ•´ï¼‰
        if self.adaptation_method == 'exponential':
            # æå¤±å¤§çš„é …ç²å¾—æ›´é«˜æ¬Šé‡
            loss_magnitudes = {}
            for name, loss in losses.items():
                if name in self.loss_names:
                    loss_magnitudes[name] = float(loss.detach())
            
            # æ­£è¦åŒ–ä¸¦æ‡‰ç”¨åˆ°åŸºç¤æ¬Šé‡
            if loss_magnitudes:
                max_loss = max(loss_magnitudes.values()) + 1e-12
                adaptive_weights = {}
                for name in self.loss_names:
                    if name in loss_magnitudes:
                        # æå¤±è¶Šå¤§ï¼Œæ¬Šé‡é©æ‡‰æ€§èª¿æ•´è¶Šå¼·
                        loss_ratio = loss_magnitudes[name] / max_loss
                        adaptation_factor = 1.0 + self.adaptation_rate * loss_ratio
                        adaptive_weights[name] = base_weights[name] * adaptation_factor
                    else:
                        adaptive_weights[name] = base_weights[name]
                
                return adaptive_weights
        
        # å¦‚æœç„¡æ³•è¨ˆç®—é©æ‡‰æ€§æ¬Šé‡ï¼Œè¿”å›åŸºç¤æ¬Šé‡
        return base_weights
    
    def get_weights(self) -> Dict[str, float]:
        """
        ç²å–ç•¶å‰æ¬Šé‡ï¼ˆé è¨­å‡ç­‰æ¬Šé‡ï¼‰
        
        Returns:
            ç•¶å‰æ¬Šé‡å­—å…¸
        """
        return {name: 1.0 for name in self.loss_names}
    
    def combine_weights(self,
                       phase_weights: Dict[str, float],
                       dynamic_weights: Dict[str, float],
                       combination_ratio: float = 0.7) -> Dict[str, float]:
        """
        çµ„åˆéšæ®µæ¬Šé‡èˆ‡å‹•æ…‹æ¬Šé‡
        
        Args:
            phase_weights: éšæ®µæ¬Šé‡
            dynamic_weights: å‹•æ…‹æ¬Šé‡ï¼ˆå¦‚ GradNormï¼‰
            combination_ratio: çµ„åˆæ¯”ä¾‹ï¼ˆéšæ®µæ¬Šé‡çš„æ¯”é‡ï¼‰
            
        Returns:
            çµ„åˆå¾Œçš„æœ€çµ‚æ¬Šé‡
        """
        final_weights = {}
        
        for name in self.loss_names:
            phase_w = phase_weights.get(name, 1.0)
            dynamic_w = dynamic_weights.get(name, 1.0)
            
            # åŠ æ¬Šçµ„åˆ
            final_w = combination_ratio * phase_w + (1 - combination_ratio) * dynamic_w
            final_weights[name] = final_w
        
        return final_weights


class MultiWeightManager:
    """
    å¤šæ¬Šé‡ç­–ç•¥ç®¡ç†å™¨
    
    æ•´åˆ GradNormã€Causalã€NTK ç­‰å¤šç¨®æ¬Šé‡ç­–ç•¥ï¼Œæä¾›çµ±ä¸€çš„æ¬Šé‡ç®¡ç†æ¥å£ã€‚
    """
    
    def __init__(self,
                 objectives_or_model = None,  # å…¼å®¹æ¸¬è©¦æœŸå¾…çš„ç¬¬ä¸€å€‹åƒæ•¸
                 loss_names: List[str] = None,
                 strategies: List[str] = ['gradnorm', 'adaptive'],
                 strategy_weights: Optional[Dict[str, float]] = None,
                 method: str = 'weighted_sum',
                 preference_weights: List[float] = None,
                 config: Optional[Dict[str, Any]] = None):
        """
        Args:
            objectives_or_model: ç›®æ¨™åˆ—è¡¨ï¼ˆæ¸¬è©¦æ¨¡å¼ï¼‰æˆ– PINN æ¨¡å‹ï¼ˆæ­£å¸¸æ¨¡å¼ï¼‰
            loss_names: æå¤±é …åç¨±åˆ—è¡¨  
            strategies: æ¬Šé‡ç­–ç•¥åˆ—è¡¨
            strategy_weights: ç­–ç•¥æ¬Šé‡å­—å…¸
            method: å¤šç›®æ¨™æ–¹æ³• ('weighted_sum', 'pareto')
            preference_weights: åå¥½æ¬Šé‡åˆ—è¡¨
            config: ç­–ç•¥é…ç½®
        """
        self.config = config or {}
        if strategies is None:
            strategies = self.config.get('strategies', ['gradnorm', 'adaptive'])
        # åˆ¤æ–·æ˜¯æ¸¬è©¦æ¨¡å¼é‚„æ˜¯æ­£å¸¸æ¨¡å¼
        if isinstance(objectives_or_model, list):
            # æ¸¬è©¦æ¨¡å¼ï¼šobjectives_or_model æ˜¯ç›®æ¨™åˆ—è¡¨
            self.objectives = objectives_or_model
            self.loss_names = objectives_or_model
            self.model = None
            self.method = self.config.get('method', method)
            base_pref = preference_weights or self.config.get(
                'preference_weights',
                [1.0/len(objectives_or_model)] * len(objectives_or_model)
            )
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šobjectives_or_model æ˜¯æ¨¡å‹
            self.model = objectives_or_model
            self.loss_names = loss_names or ['data', 'residual']
            self.objectives = self.loss_names
            self.method = self.config.get('method', method if method else 'weighted_sum')
            base_pref = preference_weights or self.config.get(
                'preference_weights',
                [1.0/len(self.loss_names)] * len(self.loss_names)
            )
        self.strategies = strategies

        # æ­£è¦åŒ–åå¥½æ¬Šé‡é•·åº¦
        self.preference_weights = [float(w) for w in base_pref]
        if len(self.preference_weights) < len(self.objectives):
            missing = len(self.objectives) - len(self.preference_weights)
            fill_value = 1.0 / max(1, len(self.objectives))
            self.preference_weights.extend([fill_value] * missing)
        elif len(self.preference_weights) > len(self.objectives):
            self.preference_weights = self.preference_weights[:len(self.objectives)]

        if strategy_weights is None:
            strategy_weights = self.config.get('strategy_weights')
        if strategy_weights is None:
            strategy_weights = {strategy: 1.0/len(strategies) for strategy in strategies}
        self.strategy_weights = strategy_weights
        
        # åˆå§‹åŒ–å„æ¬Šé‡å™¨
        self.weighters = {}
        
        if 'gradnorm' in strategies:
            if self.model is not None:
                gradnorm_kwargs = self._build_gradnorm_kwargs()
                self.weighters['gradnorm'] = GradNormWeighter(
                    self.model,
                    self.loss_names,
                    **gradnorm_kwargs
                )
            else:
                import logging
                logging.warning("GradNorm requires model reference, skipping in test mode")
                self.weighters['gradnorm'] = None
            
        if 'causal' in strategies:
            self.weighters['causal'] = CausalWeighter()
            
        if 'ntk' in strategies:
            if self.model is not None:
                self.weighters['ntk'] = NTKWeighter(self.model, self.loss_names)
            else:
                import logging
                logging.warning("NTK weighting requires model reference, skipping in test mode")
                self.weighters['ntk'] = None
            
        if 'adaptive' in strategies:
            if self.loss_names is not None:
                self.weighters['adaptive'] = AdaptiveWeightScheduler(self.loss_names)
            else:
                import logging
                logging.warning("Adaptive weighting requires loss_names, skipping in test mode")
                self.weighters['adaptive'] = None
    
    def _build_gradnorm_kwargs(self) -> Dict[str, Any]:
        """æå– GradNorm åˆå§‹åŒ–åƒæ•¸"""
        valid_keys = {
            'alpha',
            'update_frequency',
            'initial_weights',
            'target_gradient_ratio',
            'target_ratios',
            'device',
            'min_weight',
            'max_weight'
        }
        gradnorm_kwargs: Dict[str, Any] = {}
        
        gradnorm_cfg = self.config.get('gradnorm', {})
        if isinstance(gradnorm_cfg, dict):
            for key in valid_keys:
                if key in gradnorm_cfg:
                    gradnorm_kwargs[key] = gradnorm_cfg[key]
            # æ”¯æ´ç°¡å¯«åç¨±
            if 'update_freq' in gradnorm_cfg and 'update_frequency' not in gradnorm_kwargs:
                gradnorm_kwargs['update_frequency'] = gradnorm_cfg['update_freq']
        
        return gradnorm_kwargs
    
    def _update_objective_mode(self, losses: Optional[Dict[str, torch.Tensor]]) -> Dict[str, float]:
        """è™•ç†ç´”å¤šç›®æ¨™æ¬Šé‡æ›´æ–°ï¼ˆç„¡æ¨¡å‹åƒèˆ‡çš„æƒ…æ³ï¼‰"""
        if not self.objectives:
            return {}

        eps = 1e-12
        weights = {}

        for idx, name in enumerate(self.objectives):
            pref = self.preference_weights[idx]
            value = losses.get(name) if losses else None
            if isinstance(value, torch.Tensor):
                magnitude = float(value.detach().abs().item())
            elif value is not None:
                magnitude = float(abs(value))
            else:
                magnitude = 1.0

            if self.method == 'pareto':
                weight = pref * (magnitude + eps)
            else:
                weight = pref / (magnitude + eps)
            weights[name] = max(weight, eps)

        total_weight = sum(weights.values()) or 1.0
        for name in weights:
            weights[name] /= total_weight

        return weights

    def update_weights(self,
                      losses: Dict[str, torch.Tensor],
                      current_step: int = 0,
                      total_steps: int = 100000,
                      time_losses: Optional[List[torch.Tensor]] = None,
                      data_inputs: Optional[Dict[str, torch.Tensor]] = None) -> Dict[str, float]:
        """
        æ›´æ–°ç¶œåˆæ¬Šé‡

        Args:
            losses: ç•¶å‰æå¤±å­—å…¸
            current_step: ç•¶å‰æ­¥æ•¸
            total_steps: ç¸½æ­¥æ•¸
            time_losses: æ™‚é–“åºåˆ—æå¤±ï¼ˆç”¨æ–¼ causalï¼‰
            data_inputs: è¼¸å…¥æ•¸æ“šï¼ˆç”¨æ–¼ NTKï¼‰
            
        Returns:
            æœ€çµ‚æ¬Šé‡å­—å…¸
        """
        if self.model is None:
            return self._update_objective_mode(losses)

        strategy_results = {}
        
        # è¨ˆç®—å„ç­–ç•¥çš„æ¬Šé‡
        if 'gradnorm' in self.weighters and self.weighters['gradnorm'] is not None:
            strategy_results['gradnorm'] = self.weighters['gradnorm'].update_weights(losses)
        
        if 'adaptive' in self.weighters and self.weighters['adaptive'] is not None:
            strategy_results['adaptive'] = self.weighters['adaptive'].get_phase_weights(
                current_step, total_steps)
        
        if 'causal' in self.weighters and time_losses:
            causal_weights = self.weighters['causal'].compute_causal_weights(time_losses)
            # å°‡ causal æ¬Šé‡è½‰æ›ç‚ºå­—å…¸æ ¼å¼ï¼ˆå‡è¨­æŒ‰é †åºå°æ‡‰ï¼‰
            if len(causal_weights) == len(self.loss_names):
                strategy_results['causal'] = dict(zip(self.loss_names, causal_weights))
        
        if 'ntk' in self.weighters and data_inputs:
            strategy_results['ntk'] = self.weighters['ntk'].update_ntk_weights(data_inputs)
        
        # çµ„åˆæ‰€æœ‰ç­–ç•¥çš„çµæœ
        final_weights = {}
        
        for name in self.loss_names:
            combined_weight = 0.0
            total_strategy_weight = 0.0
            
            for strategy, results in strategy_results.items():
                if strategy in self.strategy_weights and name in results:
                    strategy_w = self.strategy_weights[strategy]
                    result_w = results[name]
                    combined_weight += strategy_w * result_w
                    total_strategy_weight += strategy_w
            
            # æ­£è¦åŒ–
            if total_strategy_weight > 0:
                final_weights[name] = combined_weight / total_strategy_weight
            else:
                final_weights[name] = 1.0
        
        return final_weights
    
    def get_weights(self) -> Dict[str, float]:
        """ç²å–ç•¶å‰æ¬Šé‡ï¼ˆç„¡æ›´æ–°ï¼‰"""
        weights = {}
        for name in self.loss_names:
            weights[name] = 1.0  # é è¨­æ¬Šé‡
            
        # ç²å–æœ€æ–°çš„ gradnorm æ¬Šé‡
        if 'gradnorm' in self.weighters:
            gradnorm_weights = self.weighters['gradnorm'].get_weights()
            for name in self.loss_names:
                if name in gradnorm_weights:
                    weights[name] = gradnorm_weights[name]
        
        return weights


# ä¾¿æ·å‡½æ•¸
def create_weight_manager(model: nn.Module,
                         loss_names: List[str],
                         config: Optional[Dict[str, Any]] = None) -> MultiWeightManager:
    """
    å‰µå»ºæ¬Šé‡ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        model: PINN æ¨¡å‹
        loss_names: æå¤±é …åç¨±åˆ—è¡¨
        config: é…ç½®å­—å…¸
        
    Returns:
        é…ç½®å¥½çš„æ¬Šé‡ç®¡ç†å™¨
    """
    if config is None:
        config = {
            'strategies': ['gradnorm', 'adaptive'],
            'gradnorm': {
                'alpha': 0.12,
                'update_frequency': 1000
            },
            'adaptive_phases': None  # ä½¿ç”¨é è¨­éšæ®µ
        }
    
    strategies = config.get('strategies', ['gradnorm', 'adaptive'])
    strategy_weights = config.get('strategy_weights')
    method = config.get('method', 'weighted_sum')
    preference_weights = config.get('preference_weights')
    
    return MultiWeightManager(
        model=model,
        loss_names=loss_names,
        strategies=strategies,
        strategy_weights=strategy_weights,
        method=method,
        preference_weights=preference_weights,
        config=config
    )


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    print("ğŸ§ª æ¸¬è©¦å‹•æ…‹æ¬Šé‡æ¨¡çµ„...")
    
    # å‰µå»ºç°¡å–®æ¸¬è©¦æ¨¡å‹
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(2, 3)
        def forward(self, x):
            return self.linear(x)
    
    model = TestModel()
    loss_names = ['data', 'residual', 'boundary', 'prior']
    
    # æ¸¬è©¦ GradNorm
    print("æ¸¬è©¦ GradNorm...")
    gradnorm = GradNormWeighter(model, loss_names)
    
    # æ¨¡æ“¬æå¤±
    test_losses = {
        'data': torch.tensor(2.0, requires_grad=True),
        'residual': torch.tensor(0.5, requires_grad=True),
        'boundary': torch.tensor(1.0, requires_grad=True),
        'prior': torch.tensor(0.8, requires_grad=True)
    }
    
    weights = gradnorm.update_weights(test_losses)
    print(f"GradNorm æ¬Šé‡: {weights}")
    
    # æ¸¬è©¦ Causal Weighter
    print("\næ¸¬è©¦ Causal Weighter...")
    causal = CausalWeighter()
    time_losses = [torch.tensor(1.0), torch.tensor(2.0), torch.tensor(1.5)]
    causal_weights = causal.compute_causal_weights(time_losses)
    print(f"Causal æ¬Šé‡: {causal_weights}")
    
    # æ¸¬è©¦å¤šæ¬Šé‡ç®¡ç†å™¨
    print("\næ¸¬è©¦ MultiWeightManager...")
    manager = create_weight_manager(model, loss_names)
    final_weights = manager.update_weights(test_losses, current_step=1000)
    print(f"æœ€çµ‚æ¬Šé‡: {final_weights}")
    
    print("âœ… å‹•æ…‹æ¬Šé‡æ¨¡çµ„æ¸¬è©¦å®Œæˆï¼")
