#!/usr/bin/env python3
"""
æ¸¬è©¦ SIREN åˆå§‹åŒ–æ˜¯å¦æ­£ç¢ºæ‡‰ç”¨
"""

import sys
from pathlib import Path
import torch
import numpy as np

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from pinnx.models import PINNNet, init_siren_weights

def test_standalone_siren_initialization():
    """æ¸¬è©¦ SIREN æ¬Šé‡åˆå§‹åŒ–"""
    print("=" * 80)
    print("ğŸ§ª æ¸¬è©¦ SIREN æ¬Šé‡åˆå§‹åŒ–")
    print("=" * 80)
    
    # å‰µå»º Sine æ¿€æ´»çš„æ¨¡å‹
    model = PINNNet(
        in_dim=2,
        out_dim=3,
        width=256,
        depth=6,
        activation='sine',
        sine_omega_0=1.0,
        use_fourier=True,
        fourier_m=48,
        fourier_sigma=3.0
    )
    
    print(f"\nâœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")
    print(f"   - æ¿€æ´»å‡½æ•¸: sine")
    print(f"   - Ï‰â‚€: 1.0")
    print(f"   - ç¶²è·¯å¯¬åº¦: 256")
    print(f"   - ç¶²è·¯æ·±åº¦: 6")
    
    # æª¢æŸ¥åˆå§‹åŒ–å‰çš„æ¬Šé‡åˆ†ä½ˆ
    print("\nğŸ“Š æ‡‰ç”¨ SIREN åˆå§‹åŒ–å‰çš„æ¬Šé‡åˆ†ä½ˆ:")
    first_layer_weight = model.hidden_layers[0].linear.weight.data  # type: ignore
    w_min: float = first_layer_weight.min().item()  # type: ignore
    w_max: float = first_layer_weight.max().item()  # type: ignore
    w_std: float = first_layer_weight.std().item()  # type: ignore
    print(f"   ç¬¬ä¸€å±¤æ¬Šé‡ç¯„åœ: [{w_min:.6f}, {w_max:.6f}]")
    print(f"   ç¬¬ä¸€å±¤æ¬Šé‡æ¨™æº–å·®: {w_std:.6f}")
    
    # æ‡‰ç”¨ SIREN åˆå§‹åŒ–
    print("\nğŸ”§ æ‡‰ç”¨ SIREN åˆå§‹åŒ–...")
    init_siren_weights(model)
    
    # æª¢æŸ¥åˆå§‹åŒ–å¾Œçš„æ¬Šé‡åˆ†ä½ˆ
    print("\nğŸ“Š æ‡‰ç”¨ SIREN åˆå§‹åŒ–å¾Œçš„æ¬Šé‡åˆ†ä½ˆ:")
    first_layer_after = model.hidden_layers[0].linear.weight.data  # type: ignore
    n_in = first_layer_after.shape[1]  # type: ignore
    expected_bound = 1.0 / n_in
    
    print(f"   ç¬¬ä¸€å±¤æ¬Šé‡ç¯„åœ: [{first_layer_after.min().item():.6f}, {first_layer_after.max().item():.6f}]")  # type: ignore
    print(f"   ç¬¬ä¸€å±¤æ¬Šé‡æ¨™æº–å·®: {first_layer_after.std().item():.6f}")  # type: ignore
    print(f"   ç†è«–é‚Šç•Œ: Â±{expected_bound:.6f}")
    
    # é©—è­‰æ¬Šé‡æ˜¯å¦åœ¨é æœŸç¯„åœå…§
    within_bounds = (first_layer_after >= -expected_bound * 1.01) & (first_layer_after <= expected_bound * 1.01)  # type: ignore
    print(f"   âœ… {within_bounds.float().mean().item() * 100:.1f}% æ¬Šé‡åœ¨é æœŸç¯„åœå…§")  # type: ignore
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    print("\nğŸ”¬ æ¸¬è©¦å‰å‘å‚³æ’­:")
    x_test = torch.randn(100, 2)
    with torch.no_grad():
        output = model(x_test)
    
    print(f"   è¼¸å…¥å½¢ç‹€: {x_test.shape}")
    print(f"   è¼¸å‡ºå½¢ç‹€: {output.shape}")
    print(f"   è¼¸å‡ºç¯„åœ: [{output.min():.6f}, {output.max():.6f}]")
    print(f"   è¼¸å‡ºæ˜¯å¦åŒ…å« NaN: {'âŒ æ˜¯' if torch.isnan(output).any() else 'âœ… å¦'}")
    print(f"   è¼¸å‡ºæ˜¯å¦åŒ…å« Inf: {'âŒ æ˜¯' if torch.isinf(output).any() else 'âœ… å¦'}")
    
    # æ¸¬è©¦æ¢¯åº¦è¨ˆç®—
    print("\nğŸ”¬ æ¸¬è©¦æ¢¯åº¦è¨ˆç®—:")
    x_test.requires_grad_(True)
    output = model(x_test)
    loss = output.sum()
    loss.backward()
    
    grad = x_test.grad
    if grad is not None:
        print(f"   æ¢¯åº¦æ˜¯å¦åŒ…å« NaN: {'âŒ æ˜¯' if torch.isnan(grad).any() else 'âœ… å¦'}")
        print(f"   æ¢¯åº¦æ˜¯å¦åŒ…å« Inf: {'âŒ æ˜¯' if torch.isinf(grad).any() else 'âœ… å¦'}")
        print(f"   æ¢¯åº¦ç¯„åœ: [{grad.min().item():.6f}, {grad.max().item():.6f}]")
    else:
        print("   âš ï¸  æ¢¯åº¦ç‚º None")
    
    print("\n" + "=" * 80)
    print("âœ… SIREN åˆå§‹åŒ–æ¸¬è©¦å®Œæˆ")
    print("=" * 80)

if __name__ == "__main__":
    test_standalone_siren_initialization()
