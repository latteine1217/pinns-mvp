"""
æ€§èƒ½å„ªåŒ–åŠŸèƒ½æ¸¬è©¦
================

æ¸¬è©¦æ¢¯åº¦æª¢æŸ¥é»èˆ‡å…¶ä»–æ€§èƒ½å„ªåŒ–åŠŸèƒ½çš„æ­£ç¢ºæ€§èˆ‡æ•ˆèƒ½ã€‚

æ¸¬è©¦é …ç›®ï¼š
1. æ¢¯åº¦æª¢æŸ¥é»æ•¸å€¼æ­£ç¢ºæ€§ï¼ˆèˆ‡æ¨™æº–æ–¹æ³•å°æ¯”ï¼‰
2. æ¢¯åº¦æª¢æŸ¥é»è¨˜æ†¶é«”ç¯€çœé©—è­‰
3. æ¢¯åº¦æª¢æŸ¥é»é€Ÿåº¦å½±éŸ¿æ¸¬é‡
4. é«˜éšå°æ•¸è¨ˆç®—æ­£ç¢ºæ€§

ä½œè€…ï¼šPINNs-MVP åœ˜éšŠ
æ—¥æœŸï¼š2025-10-15
"""

import pytest
import torch
import torch.nn as nn
from typing import Dict, Tuple
import time
import gc

from pinnx.physics.vs_pinn_channel_flow import (
    VSPINNChannelFlow,
    compute_gradient_3d,
    compute_gradient_3d_checkpointed
)


class TestGradientCheckpointing:
    """æ¢¯åº¦æª¢æŸ¥é»åŠŸèƒ½æ¸¬è©¦å¥—ä»¶"""
    
    @pytest.fixture
    def device(self):
        """è¨­å‚™é…ç½®"""
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    @pytest.fixture
    def test_data(self, device):
        """ç”Ÿæˆæ¸¬è©¦è³‡æ–™"""
        batch_size = 128
        coords = torch.randn(batch_size, 3, device=device, requires_grad=True)
        return coords
    
    @pytest.fixture
    def physics_standard(self, device):
        """æ¨™æº–ç‰©ç†æ¨¡çµ„ï¼ˆé—œé–‰æª¢æŸ¥é»ï¼‰"""
        return VSPINNChannelFlow(
            use_gradient_checkpointing=False
        ).to(device)
    
    @pytest.fixture
    def physics_checkpointed(self, device):
        """å•Ÿç”¨æª¢æŸ¥é»çš„ç‰©ç†æ¨¡çµ„"""
        return VSPINNChannelFlow(
            use_gradient_checkpointing=True
        ).to(device)
    
    def test_gradient_numerical_correctness(self, test_data, device):
        """
        æ¸¬è©¦ 1ï¼šæ•¸å€¼æ­£ç¢ºæ€§é©—è­‰
        
        é©—è­‰æ¢¯åº¦æª¢æŸ¥é»èˆ‡æ¨™æº–æ–¹æ³•è¨ˆç®—çµæœå®Œå…¨ä¸€è‡´ï¼ˆå®¹å¿åº¦ 1e-6ï¼‰
        """
        # å‰µå»ºæ¨™é‡å ´
        field = torch.sin(test_data[:, 0:1]) * torch.cos(test_data[:, 1:2])
        
        # æ¨™æº–æ¢¯åº¦è¨ˆç®—
        grad_standard_x = compute_gradient_3d(field, test_data, component=0)
        grad_standard_y = compute_gradient_3d(field, test_data, component=1)
        grad_standard_z = compute_gradient_3d(field, test_data, component=2)
        
        # æª¢æŸ¥é»æ¢¯åº¦è¨ˆç®—
        grad_checkpoint_x = compute_gradient_3d_checkpointed(field, test_data, component=0)
        grad_checkpoint_y = compute_gradient_3d_checkpointed(field, test_data, component=1)
        grad_checkpoint_z = compute_gradient_3d_checkpointed(field, test_data, component=2)
        
        # æ•¸å€¼ä¸€è‡´æ€§æª¢æŸ¥ï¼ˆçµ•å°èª¤å·® + ç›¸å°èª¤å·®ï¼‰
        atol = 1e-6
        rtol = 1e-5
        
        assert torch.allclose(grad_standard_x, grad_checkpoint_x, atol=atol, rtol=rtol), \
            f"X æ¢¯åº¦ä¸ä¸€è‡´ï¼šæœ€å¤§èª¤å·® {(grad_standard_x - grad_checkpoint_x).abs().max().item()}"
        
        assert torch.allclose(grad_standard_y, grad_checkpoint_y, atol=atol, rtol=rtol), \
            f"Y æ¢¯åº¦ä¸ä¸€è‡´ï¼šæœ€å¤§èª¤å·® {(grad_standard_y - grad_checkpoint_y).abs().max().item()}"
        
        assert torch.allclose(grad_standard_z, grad_checkpoint_z, atol=atol, rtol=rtol), \
            f"Z æ¢¯åº¦ä¸ä¸€è‡´ï¼šæœ€å¤§èª¤å·® {(grad_standard_z - grad_checkpoint_z).abs().max().item()}"
        
        print("âœ… æ¢¯åº¦æ•¸å€¼æ­£ç¢ºæ€§é©—è­‰é€šéï¼ˆèª¤å·® < 1e-6ï¼‰")
    
    def test_second_order_gradient_correctness(self, test_data, device):
        """
        æ¸¬è©¦ 2ï¼šäºŒéšå°æ•¸æ­£ç¢ºæ€§
        
        é©—è­‰ Laplacian è¨ˆç®—çš„æ•¸å€¼ä¸€è‡´æ€§
        """
        # å‰µå»ºæ¨™é‡å ´ï¼ˆå¯è§£ææ±‚å°ï¼‰
        field = torch.sin(test_data[:, 0:1]) * torch.cos(test_data[:, 1:2]) * torch.exp(test_data[:, 2:3])
        
        # æ¨™æº–æ–¹æ³•ï¼šä¸€éšå°æ•¸
        grad_x = compute_gradient_3d(field, test_data, component=0)
        grad_xx_standard = compute_gradient_3d(grad_x, test_data, component=0)
        
        # æª¢æŸ¥é»æ–¹æ³•ï¼šä¸€éšå°æ•¸
        grad_x_cp = compute_gradient_3d_checkpointed(field, test_data, component=0)
        grad_xx_checkpoint = compute_gradient_3d_checkpointed(grad_x_cp, test_data, component=0)
        
        # äºŒéšå°æ•¸ä¸€è‡´æ€§æª¢æŸ¥
        atol = 1e-5  # äºŒéšå°æ•¸å…è¨±ç¨å¤§èª¤å·®
        rtol = 1e-4
        
        assert torch.allclose(grad_xx_standard, grad_xx_checkpoint, atol=atol, rtol=rtol), \
            f"äºŒéšå°æ•¸ä¸ä¸€è‡´ï¼šæœ€å¤§èª¤å·® {(grad_xx_standard - grad_xx_checkpoint).abs().max().item()}"
        
        print("âœ… äºŒéšå°æ•¸æ­£ç¢ºæ€§é©—è­‰é€šéï¼ˆèª¤å·® < 1e-5ï¼‰")
    
    def test_physics_module_integration(self, physics_standard, physics_checkpointed, test_data):
        """
        æ¸¬è©¦ 3ï¼šç‰©ç†æ¨¡çµ„æ•´åˆæ¸¬è©¦
        
        é©—è­‰ VSPINNChannelFlow çš„æ¢¯åº¦è¨ˆç®—è·¯ç”±æ­£ç¢º
        """
        # å‰µå»ºæ¨™é‡å ´ï¼ˆéœ€è¦è¨ˆç®—åœ–ï¼‰
        field = torch.sin(test_data[:, 0:1]) * torch.cos(test_data[:, 1:2])
        
        # æ¨™æº–æ¨¡çµ„è¨ˆç®—
        grads_standard = physics_standard.compute_gradients(
            field, test_data, order=1
        )
        
        # æª¢æŸ¥é»æ¨¡çµ„è¨ˆç®—
        grads_checkpoint = physics_checkpointed.compute_gradients(
            field, test_data, order=1
        )
        
        # é€åˆ†é‡æ¯”è¼ƒ
        for key in ['x', 'y', 'z']:
            assert torch.allclose(
                grads_standard[key], 
                grads_checkpoint[key], 
                atol=1e-6, 
                rtol=1e-5
            ), f"ç‰©ç†æ¨¡çµ„ {key} æ¢¯åº¦ä¸ä¸€è‡´"
        
        print("âœ… ç‰©ç†æ¨¡çµ„æ•´åˆæ¸¬è©¦é€šé")
    
    @pytest.mark.skipif(not torch.cuda.is_available(), reason="éœ€è¦ GPU æ¸¬é‡è¨˜æ†¶é«”")
    def test_memory_saving(self, physics_standard, physics_checkpointed, device):
        """
        æ¸¬è©¦ 4ï¼šè¨˜æ†¶é«”ç¯€çœé©—è­‰ï¼ˆåƒ… GPUï¼‰
        
        æ¸¬é‡æ¨™æº–æ–¹æ³•èˆ‡æª¢æŸ¥é»æ–¹æ³•çš„è¨˜æ†¶é«”å·®ç•°
        """
        batch_size = 2048  # å¤§æ‰¹æ¬¡ä»¥çªé¡¯è¨˜æ†¶é«”å·®ç•°
        coords = torch.randn(batch_size, 3, device=device, requires_grad=True)
        
        # æ¨¡æ“¬è¤‡é›œè¨ˆç®—ï¼ˆå¤šæ¬¡æ¢¯åº¦èª¿ç”¨ï¼‰
        def complex_gradient_computation(physics, coords):
            field = torch.sin(coords[:, 0:1]) * torch.cos(coords[:, 1:2])
            grads = physics.compute_gradients(field, coords, order=1)
            # è¨ˆç®— Laplacianï¼ˆé€é …åŠ ç¸½ï¼‰
            laplacian_terms = [
                physics.compute_gradients(grads[key], coords, order=1)[key] 
                for key in ['x', 'y', 'z']
            ]
            # æ­£ç¢ºçš„ Tensor åŠ ç¸½æ–¹å¼
            laplacian = torch.stack(laplacian_terms, dim=0).sum(dim=0)
            loss = laplacian.mean()
            loss.backward()
            return loss
        
        # æ¸…ç©º GPU å¿«å–
        torch.cuda.empty_cache()
        gc.collect()
        
        # æ¸¬é‡æ¨™æº–æ–¹æ³•è¨˜æ†¶é«”
        torch.cuda.reset_peak_memory_stats()
        _ = complex_gradient_computation(physics_standard, coords.clone().detach().requires_grad_(True))
        mem_standard = torch.cuda.max_memory_allocated() / 1024**2  # MB
        
        # æ¸…ç©º GPU å¿«å–
        torch.cuda.empty_cache()
        gc.collect()
        
        # æ¸¬é‡æª¢æŸ¥é»æ–¹æ³•è¨˜æ†¶é«”
        torch.cuda.reset_peak_memory_stats()
        _ = complex_gradient_computation(physics_checkpointed, coords.clone().detach().requires_grad_(True))
        mem_checkpoint = torch.cuda.max_memory_allocated() / 1024**2  # MB
        
        # è¨ˆç®—ç¯€çœæ¯”ä¾‹
        memory_reduction = (mem_standard - mem_checkpoint) / mem_standard
        
        print(f"ğŸ“Š è¨˜æ†¶é«”ä½¿ç”¨ï¼š")
        print(f"  æ¨™æº–æ–¹æ³•ï¼š{mem_standard:.2f} MB")
        print(f"  æª¢æŸ¥é»æ–¹æ³•ï¼š{mem_checkpoint:.2f} MB")
        print(f"  ç¯€çœæ¯”ä¾‹ï¼š{memory_reduction * 100:.1f}%")
        
        # é©—æ”¶æ¨™æº–ï¼šè‡³å°‘ç¯€çœ 20%ï¼ˆç›®æ¨™ 30-50%ï¼‰
        assert memory_reduction >= 0.20, \
            f"è¨˜æ†¶é«”ç¯€çœä¸è¶³ï¼šåƒ… {memory_reduction * 100:.1f}%ï¼ˆç›®æ¨™ â‰¥20%ï¼‰"
        
        print("âœ… è¨˜æ†¶é«”ç¯€çœé©—è­‰é€šéï¼ˆâ‰¥20%ï¼‰")
    
    @pytest.mark.skipif(not torch.cuda.is_available(), reason="éœ€è¦ GPU æ¸¬é‡é€Ÿåº¦")
    def test_speed_overhead(self, physics_standard, physics_checkpointed, device):
        """
        æ¸¬è©¦ 5ï¼šé€Ÿåº¦å½±éŸ¿æ¸¬é‡ï¼ˆåƒ… GPUï¼‰
        
        æ¸¬é‡æª¢æŸ¥é»æ–¹æ³•çš„é€Ÿåº¦é–‹éŠ·ï¼ˆç›®æ¨™ <15%ï¼‰
        """
        batch_size = 1024
        coords = torch.randn(batch_size, 3, device=device, requires_grad=True)
        num_iterations = 50
        
        def benchmark_forward_backward(physics, coords, iterations):
            """å‰å‘+åå‘å‚³æ’­åŸºæº–æ¸¬è©¦"""
            timings = []
            for _ in range(iterations):
                coords_copy = coords.clone().detach().requires_grad_(True)
                field = torch.sin(coords_copy[:, 0:1])
                
                # è¨˜éŒ„æ™‚é–“
                torch.cuda.synchronize()
                start = time.perf_counter()
                
                grads = physics.compute_gradients(field, coords_copy, order=1)
                # æ­£ç¢ºçš„ Tensor åŠ ç¸½æ–¹å¼
                loss = torch.stack(list(grads.values()), dim=0).sum(dim=0).mean()
                loss.backward()
                
                torch.cuda.synchronize()
                end = time.perf_counter()
                
                timings.append(end - start)
            
            return sum(timings) / len(timings)
        
        # é ç†±
        _ = benchmark_forward_backward(physics_standard, coords, 5)
        _ = benchmark_forward_backward(physics_checkpointed, coords, 5)
        
        # æ­£å¼æ¸¬è©¦
        time_standard = benchmark_forward_backward(physics_standard, coords, num_iterations)
        time_checkpoint = benchmark_forward_backward(physics_checkpointed, coords, num_iterations)
        
        # è¨ˆç®—é€Ÿåº¦é–‹éŠ·
        speed_overhead = (time_checkpoint - time_standard) / time_standard
        
        print(f"â±ï¸ é€Ÿåº¦æ¸¬è©¦ï¼š")
        print(f"  æ¨™æº–æ–¹æ³•ï¼š{time_standard * 1000:.2f} ms")
        print(f"  æª¢æŸ¥é»æ–¹æ³•ï¼š{time_checkpoint * 1000:.2f} ms")
        print(f"  é€Ÿåº¦é–‹éŠ·ï¼š{speed_overhead * 100:.1f}%")
        
        # é©—æ”¶æ¨™æº–ï¼šé€Ÿåº¦æ…¢ â‰¤20%ï¼ˆç›®æ¨™ 10-15%ï¼‰
        assert speed_overhead <= 0.20, \
            f"é€Ÿåº¦é–‹éŠ·éå¤§ï¼š{speed_overhead * 100:.1f}%ï¼ˆç›®æ¨™ â‰¤20%ï¼‰"
        
        print("âœ… é€Ÿåº¦å½±éŸ¿é©—è­‰é€šéï¼ˆé–‹éŠ· â‰¤20%ï¼‰")
    
    def test_backward_gradient_correctness(self, test_data, device):
        """
        æ¸¬è©¦ 6ï¼šåå‘å‚³æ’­æ¢¯åº¦æ­£ç¢ºæ€§
        
        é©—è­‰æª¢æŸ¥é»ä¸å½±éŸ¿åƒæ•¸æ¢¯åº¦è¨ˆç®—
        """
        # å‰µå»ºç°¡å–®æ¨¡å‹
        model = nn.Linear(3, 1).to(device)
        
        # æ¨™æº–æ–¹æ³•æ¢¯åº¦
        coords_1 = test_data.clone().detach().requires_grad_(True)
        field_1 = model(coords_1)
        grad_1 = compute_gradient_3d(field_1, coords_1, component=0)
        loss_1 = grad_1.mean()
        loss_1.backward()
        assert model.weight.grad is not None, "æ¨™æº–æ–¹æ³•æœªç”¢ç”Ÿæ¢¯åº¦"
        param_grad_1 = model.weight.grad.clone()
        model.zero_grad()
        
        # æª¢æŸ¥é»æ–¹æ³•æ¢¯åº¦
        coords_2 = test_data.clone().detach().requires_grad_(True)
        field_2 = model(coords_2)
        grad_2 = compute_gradient_3d_checkpointed(field_2, coords_2, component=0)
        loss_2 = grad_2.mean()
        loss_2.backward()
        assert model.weight.grad is not None, "æª¢æŸ¥é»æ–¹æ³•æœªç”¢ç”Ÿæ¢¯åº¦"
        param_grad_2 = model.weight.grad.clone()
        
        # åƒæ•¸æ¢¯åº¦ä¸€è‡´æ€§æª¢æŸ¥
        assert torch.allclose(param_grad_1, param_grad_2, atol=1e-6, rtol=1e-5), \
            f"åƒæ•¸æ¢¯åº¦ä¸ä¸€è‡´ï¼šæœ€å¤§èª¤å·® {(param_grad_1 - param_grad_2).abs().max().item()}"
        
        print("âœ… åå‘å‚³æ’­æ¢¯åº¦æ­£ç¢ºæ€§é©—è­‰é€šé")


class TestConfigurationLoading:
    """é…ç½®è¼‰å…¥æ¸¬è©¦"""
    
    def test_gradient_checkpoint_config_parsing(self):
        """
        æ¸¬è©¦ 7ï¼šé…ç½®æª”æ¡ˆè§£æ
        
        é©—è­‰ use_gradient_checkpointing åƒæ•¸æ­£ç¢ºå‚³é
        """
        # æ¸¬è©¦å•Ÿç”¨æª¢æŸ¥é»
        physics_enabled = VSPINNChannelFlow(use_gradient_checkpointing=True)
        assert physics_enabled.use_gradient_checkpointing is True
        
        # æ¸¬è©¦é—œé–‰æª¢æŸ¥é»
        physics_disabled = VSPINNChannelFlow(use_gradient_checkpointing=False)
        assert physics_disabled.use_gradient_checkpointing is False
        
        # æ¸¬è©¦é»˜èªå€¼ï¼ˆæ‡‰å•Ÿç”¨ï¼‰
        physics_default = VSPINNChannelFlow()
        assert physics_default.use_gradient_checkpointing is True
        
        print("âœ… é…ç½®åƒæ•¸è§£ææ¸¬è©¦é€šé")


if __name__ == "__main__":
    """
    ç›´æ¥åŸ·è¡Œæ¸¬è©¦
    
    ä½¿ç”¨æ–¹å¼ï¼š
        python tests/test_performance_optimizations.py
    """
    pytest.main([__file__, "-v", "-s"])
