"""
TASK-audit-005: Phase 2 é…ç½®é©—è­‰æ¸¬è©¦
ç›®æ¨™ï¼šé©—è­‰ YAML é…ç½®è®€å–ã€Trainer æ•´åˆã€å¤šæƒ…å¢ƒé©é…
"""

import sys
import os
import tempfile
import yaml
from pathlib import Path
from typing import Dict, Any

import torch
import torch.nn as nn
import numpy as np

# è¨­å‚™å¸¸é‡
DEVICE = torch.device('cpu')

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from pinnx.utils.normalization import (
    UnifiedNormalizer,
    InputNormConfig,
    OutputNormConfig,
)


# ========================================
# æ¸¬è©¦ 5: UnifiedNormalizer.from_config() é…ç½®è®€å–
# ========================================

def test_5_1_basic_config_parsing():
    """æ¸¬è©¦ 5.1: åŸºæœ¬é…ç½®è§£æ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 5.1: åŸºæœ¬é…ç½®è§£æ")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',  # è¼¸å…¥æ¨™æº–åŒ–åœ¨é€™è£¡
                'input_norm_range': [-1.0, 1.0],
            }
        },
        'normalization': {
            'type': 'manual',  # è¼¸å‡ºæ¨™æº–åŒ–é¡å‹
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.0, 'u_std': 2.0,
                'v_mean': 0.5, 'v_std': 1.5,
                'p_mean': 50.0, 'p_std': 10.0,
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # é©—è­‰è¼¸å…¥æ¨™æº–åŒ–
    assert normalizer.input_transform.norm_type == 'standard', \
        f"InputTransform norm_type éŒ¯èª¤: {normalizer.input_transform.norm_type}"
    assert normalizer.input_transform.feature_range == (-1.0, 1.0), \
        f"feature_range éŒ¯èª¤: {normalizer.input_transform.feature_range}"
    
    # é©—è­‰è¼¸å‡ºæ¨™æº–åŒ–
    assert normalizer.output_transform.norm_type == 'manual', \
        f"OutputTransform norm_type éŒ¯èª¤: {normalizer.output_transform.norm_type}"
    
    assert normalizer.output_transform.variable_order == ['u', 'v', 'p'], \
        f"variable_order éŒ¯èª¤: {normalizer.output_transform.variable_order}"
    
    # é©—è­‰çµ±è¨ˆé‡
    assert abs(normalizer.output_transform.means['u'] - 1.0) < 1e-6, "u mean éŒ¯èª¤"
    assert abs(normalizer.output_transform.stds['p'] - 10.0) < 1e-6, "p std éŒ¯èª¤"
    
    print("âœ… æ¸¬è©¦ 5.1 é€šé: é…ç½®æ­£ç¢ºè®€å–")
    return True


def test_5_2_missing_config_defaults():
    """æ¸¬è©¦ 5.2: ç¼ºçœé…ç½®å›é€€"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 5.2: ç¼ºçœé…ç½®å›é€€")
    print("=" * 80)
    
    # âœ… ç©ºé…ç½®æˆ–åƒ…æœ‰ type='none' æ‡‰å›é€€åˆ°é è¨­
    config = {
        'normalization': {
            'type': 'none',  # æ˜ç¢ºæŒ‡å®šä¸æ¨™æº–åŒ–
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # é©—è­‰å›é€€åˆ° 'none'
    assert normalizer.input_transform.norm_type == 'none', \
        f"é æœŸ norm_type='none', å¯¦éš› {normalizer.input_transform.norm_type}"
    
    assert normalizer.output_transform.norm_type == 'none', \
        f"é æœŸ output norm_type='none', å¯¦éš› {normalizer.output_transform.norm_type}"
    
    print("âœ… æ¸¬è©¦ 5.2 é€šé: ç¼ºçœé…ç½®æ­£ç¢ºå›é€€")
    return True


def test_5_3_disable_normalization():
    """æ¸¬è©¦ 5.3: ç¦ç”¨æ¨™æº–åŒ–"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 5.3: ç¦ç”¨æ¨™æº–åŒ–")
    print("=" * 80)
    
    # âœ… å¯¦éš› API é€šéè¨­ç½® type='none' ä¾†ç¦ç”¨
    config = {
        'normalization': {
            'type': 'none',  # ç¦ç”¨è¼¸å‡ºæ¨™æº–åŒ–
        }
        # æ²’æœ‰ model.scaling é…ç½®ï¼Œè¼¸å…¥æ¨™æº–åŒ–ä¹Ÿæœƒç¦ç”¨
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # é©—è­‰å…©è€…éƒ½ç¦ç”¨
    assert normalizer.input_transform.norm_type == 'none', \
        f"ç¦ç”¨å¾Œ input norm_type æ‡‰ç‚º 'none', å¯¦éš› {normalizer.input_transform.norm_type}"
    
    assert normalizer.output_transform.norm_type == 'none', \
        f"ç¦ç”¨å¾Œ output norm_type æ‡‰ç‚º 'none', å¯¦éš› {normalizer.output_transform.norm_type}"
    
    print("âœ… æ¸¬è©¦ 5.3 é€šé: ç¦ç”¨æ¨™æº–åŒ–ç”Ÿæ•ˆ")
    return True


def test_5_4_vs_pinn_channel_flow_config():
    """æ¸¬è©¦ 5.4: VS-PINN Channel Flow é…ç½®"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 5.4: VS-PINN Channel Flow é…ç½®")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    config = {
        'model': {
            'scaling': {
                'input_norm': 'channel_flow',
                'input_norm_range': [-1.0, 1.0],
            }
        },
        'physics': {
            'domain': {
                'x_range': [0.0, 25.132741],
                'y_range': [0.0, 2.0],
                'z_range': [0.0, 9.42477796],
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'w', 'p'],
            'params': {
                'u_mean': 0.0, 'u_std': 1.0,
                'v_mean': 0.0, 'v_std': 1.0,
                'w_mean': 0.0, 'w_std': 1.0,
                'p_mean': 0.0, 'p_std': 1.0,
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # é©—è­‰è¼¸å…¥æ¨™æº–åŒ–é¡å‹
    assert normalizer.input_transform.norm_type == 'channel_flow', \
        f"norm_type éŒ¯èª¤: {normalizer.input_transform.norm_type}"
    
    # é©—è­‰ bounds å·²è¨­ç½®ï¼ˆå¾ physics.domain è®€å–ï¼‰
    assert normalizer.input_transform.bounds is not None, "bounds æœªè¨­ç½®"
    
    # é©—è­‰è¼¸å‡ºè®Šé‡é †åº
    assert len(normalizer.output_transform.variable_order) == 4, \
        f"variable_order é•·åº¦éŒ¯èª¤: {len(normalizer.output_transform.variable_order)}"
    
    print("âœ… æ¸¬è©¦ 5.4 é€šé: VS-PINN Channel Flow é…ç½®æ­£ç¢º")
    return True


def test_5_5_yaml_file_loading():
    """æ¸¬è©¦ 5.5: å¾ YAML æª”æ¡ˆè¼‰å…¥"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 5.5: å¾ YAML æª”æ¡ˆè¼‰å…¥")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    yaml_content = """
model:
  scaling:
    input_norm: standard
    input_norm_range: [-1.0, 1.0]

normalization:
  type: manual
  variable_order: [u, v, p]
  params:
    u_mean: 1.5
    u_std: 2.5
    v_mean: 0.8
    v_std: 1.8
    p_mean: 60.0
    p_std: 12.0
"""
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yml', delete=False) as f:
        f.write(yaml_content)
        yaml_path = f.name
    
    try:
        with open(yaml_path, 'r') as f:
            config = yaml.safe_load(f)
        
        normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
        
        # é©—è­‰é…ç½®æ­£ç¢ºè®€å–
        assert normalizer.input_transform.norm_type == 'standard', "input norm_type éŒ¯èª¤"
        assert normalizer.output_transform.norm_type == 'manual', "output norm_type éŒ¯èª¤"
        assert abs(normalizer.output_transform.means['u'] - 1.5) < 1e-6, "u mean éŒ¯èª¤"
        assert abs(normalizer.output_transform.stds['v'] - 1.8) < 1e-6, "v std éŒ¯èª¤"
        
        print("âœ… æ¸¬è©¦ 5.5 é€šé: YAML æª”æ¡ˆè¼‰å…¥æ­£ç¢º")
        return True
        
    finally:
        os.unlink(yaml_path)


# ========================================
# æ¸¬è©¦ 6: Trainer æ•´åˆé©—è­‰
# ========================================

class DummyModel(nn.Module):
    """æœ€å°æ¨¡å‹ç”¨æ–¼æ¸¬è©¦"""
    def __init__(self, input_dim=3, output_dim=3):
        super().__init__()
        self.fc = nn.Linear(input_dim, output_dim)
    
    def forward(self, x):
        return self.fc(x)


def test_6_1_trainer_init_with_normalization():
    """æ¸¬è©¦ 6.1: Trainer åˆå§‹åŒ–åŒ…å«æ¨™æº–åŒ–"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 6.1: Trainer åˆå§‹åŒ–åŒ…å«æ¨™æº–åŒ–")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0],
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.0, 'u_std': 2.0,
                'v_mean': 0.0, 'v_std': 1.0,
                'p_mean': 50.0, 'p_std': 10.0,
            }
        }
    }
    
    model = DummyModel(input_dim=3, output_dim=3)
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # æ¨¡æ“¬ Trainer çš„æ¨™æº–åŒ–ä½¿ç”¨
    coords = torch.randn(100, 3)
    
    # æ“¬åˆè¼¸å…¥æ¨™æº–åŒ–
    normalizer.input_transform.fit(coords)
    coords_norm = normalizer.input_transform.transform(coords)
    
    # é©—è­‰æ¨™æº–åŒ–æ•ˆæœ
    assert coords_norm.shape == coords.shape, "å½¢ç‹€æ”¹è®Š"
    assert abs(coords_norm.mean().item()) < 0.1, f"æ¨™æº–åŒ–å¾Œå‡å€¼ç•°å¸¸: {coords_norm.mean().item()}"
    assert abs(coords_norm.std().item() - 1.0) < 0.2, f"æ¨™æº–åŒ–å¾Œæ¨™æº–å·®ç•°å¸¸: {coords_norm.std().item()}"
    
    print("âœ… æ¸¬è©¦ 6.1 é€šé: Trainer å¯æ­£ç¢ºæ•´åˆæ¨™æº–åŒ–")
    return True


def test_6_2_forward_pass_with_denormalization():
    """æ¸¬è©¦ 6.2: å‰å‘å‚³æ’­åŒ…å«åæ¨™æº–åŒ–"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 6.2: å‰å‘å‚³æ’­åŒ…å«åæ¨™æº–åŒ–")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0],
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 10.0, 'u_std': 2.0,
                'v_mean': 5.0, 'v_std': 1.0,
                'p_mean': 100.0, 'p_std': 20.0,
            }
        }
    }
    
    model = DummyModel(input_dim=3, output_dim=3)
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # æ¨¡æ“¬å‰å‘å‚³æ’­
    coords = torch.randn(50, 3)
    normalizer.input_transform.fit(coords)
    coords_norm = normalizer.input_transform.transform(coords)
    
    output_norm = model(coords_norm)  # æ¨¡å‹è¼¸å‡ºæ¨™æº–åŒ–ç©ºé–“
    output_physical = normalizer.output_transform.denormalize_batch(output_norm)  # åæ¨™æº–åŒ–
    
    # é©—è­‰åæ¨™æº–åŒ–ç¯„åœåˆç†
    u_physical = output_physical[:, 0]
    p_physical = output_physical[:, 2]
    
    # âœ… ä¿®æ­£ï¼šé©—è­‰åæ¨™æº–åŒ–ç¢ºå¯¦æ”¹è®Šäº†æ•¸å€¼ï¼ˆæ‡‰ç”¨äº† mean/stdï¼‰
    assert u_physical.mean().item() != output_norm[:, 0].mean().item(), "åæ¨™æº–åŒ–ç„¡æ•ˆ"
    # é©—è­‰åæ¨™æº–åŒ–å¾Œæ•¸å€¼ç¯„åœæ“´å¤§ï¼ˆå› ç‚ºæœ‰ std=2.0, 20.0ï¼‰
    assert output_physical.abs().max().item() > output_norm.abs().max().item(), "åæ¨™æº–åŒ–æ‡‰æ“´å¤§æ•¸å€¼ç¯„åœ"
    
    print(f"ğŸ“Š åæ¨™æº–åŒ–å‰ u ç¯„åœ: [{output_norm[:, 0].min():.2f}, {output_norm[:, 0].max():.2f}]")
    print(f"ğŸ“Š åæ¨™æº–åŒ–å¾Œ u ç¯„åœ: [{u_physical.min():.2f}, {u_physical.max():.2f}]")
    print("âœ… æ¸¬è©¦ 6.2 é€šé: åæ¨™æº–åŒ–æ­£ç¢ºæ‡‰ç”¨")
    return True


def test_6_3_gradient_flow_in_training_loop():
    """æ¸¬è©¦ 6.3: è¨“ç·´å¾ªç’°æ¢¯åº¦æµå‹•"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 6.3: è¨“ç·´å¾ªç’°æ¢¯åº¦æµå‹•")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0],
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.0, 'u_std': 2.0,
                'v_mean': 0.0, 'v_std': 1.0,
                'p_mean': 50.0, 'p_std': 10.0,
            }
        }
    }
    
    model = DummyModel(input_dim=3, output_dim=3)
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # æ¨¡æ“¬è¨“ç·´æ­¥é©Ÿ
    coords = torch.randn(100, 3, requires_grad=True)
    normalizer.input_transform.fit(coords)
    
    coords_norm = normalizer.input_transform.transform(coords)
    output_norm = model(coords_norm)
    output_physical = normalizer.output_transform.denormalize_batch(output_norm)
    
    # è¨ˆç®—æå¤±ï¼ˆç‰©ç†ç©ºé–“ï¼‰
    target = torch.randn(100, 3, dtype=torch.float32, device=DEVICE)
    loss = torch.mean((output_physical - target) ** 2)
    
    # åå‘å‚³æ’­
    optimizer.zero_grad()
    loss.backward()
    
    # é©—è­‰æ¢¯åº¦å­˜åœ¨
    has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in model.parameters())
    assert has_grad, "æ¨¡å‹åƒæ•¸æœªæ”¶åˆ°æ¢¯åº¦"
    
    assert coords.grad is not None, "è¼¸å…¥åº§æ¨™æœªæ”¶åˆ°æ¢¯åº¦"
    assert coords.grad.abs().sum() > 0, "è¼¸å…¥åº§æ¨™æ¢¯åº¦ç‚ºé›¶"
    
    print("âœ… æ¸¬è©¦ 6.3 é€šé: æ¢¯åº¦æ­£ç¢ºæµå‹•")
    return True


def test_6_4_checkpoint_metadata_integration():
    """æ¸¬è©¦ 6.4: æª¢æŸ¥é» metadata æ•´åˆ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 6.4: æª¢æŸ¥é» metadata æ•´åˆ")
    print("=" * 80)
    
    # âœ… ä½¿ç”¨å¯¦éš› API é æœŸçš„é…ç½®çµæ§‹
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0],
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.5, 'u_std': 2.5,
                'v_mean': 0.5, 'v_std': 1.2,
                'p_mean': 55.0, 'p_std': 12.0,
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # æ“¬åˆè¼¸å…¥çµ±è¨ˆé‡
    coords = torch.randn(100, 3)
    normalizer.input_transform.fit(coords)
    
    # ç²å– metadata
    metadata = normalizer.get_metadata()
    
    # é©—è­‰ metadata å®Œæ•´æ€§
    assert 'input' in metadata, "ç¼ºå°‘ input metadata"
    assert 'output' in metadata, "ç¼ºå°‘ output metadata"
    
    # âœ… é©—è­‰ metadata å…§å®¹
    assert metadata['input']['norm_type'] == 'standard', f"input norm_type éŒ¯èª¤: {metadata['input']['norm_type']}"
    assert metadata['output']['norm_type'] == 'manual', f"output norm_type éŒ¯èª¤: {metadata['output']['norm_type']}"
    
    assert 'means' in metadata['output'], "ç¼ºå°‘ output means"
    assert abs(metadata['output']['means']['u'] - 1.5) < 1e-6, "u mean éŒ¯èª¤"
    
    print("âœ… æ¸¬è©¦ 6.4 é€šé: æª¢æŸ¥é» metadata å®Œæ•´")
    return True


# ========================================
# æ¸¬è©¦ 7: å¤šæƒ…å¢ƒé©é…é©—è­‰
# ========================================

def test_7_1_ensemble_training_scenario():
    """æ¸¬è©¦ 7.1: Ensemble è¨“ç·´æƒ…å¢ƒ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 7.1: Ensemble è¨“ç·´æƒ…å¢ƒ")
    print("=" * 80)
    
    config = {
        'normalization': {
            'enable': True,
            'input': {'norm_type': 'standard'},
            'output': {
                'norm_type': 'standard',
                'variable_order': ['u', 'v', 'p'],
                'means': {'u': 1.0, 'v': 0.0, 'p': 50.0},
                'stds': {'u': 2.0, 'v': 1.0, 'p': 10.0},
            }
        }
    }
    
    # æ¨¡æ“¬ 5 å€‹ ensemble æˆå“¡å…±äº«æ¨™æº–åŒ–
    coords = torch.randn(100, 3)
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    normalizer.input_transform.fit(coords)
    
    ensemble_outputs = []
    for i in range(5):
        model = DummyModel()
        coords_norm = normalizer.input_transform.transform(coords)
        output_norm = model(coords_norm)
        output_physical = normalizer.output_transform.denormalize_batch(output_norm)
        ensemble_outputs.append(output_physical)
    
    # é©—è­‰æ‰€æœ‰æˆå“¡ä½¿ç”¨ç›¸åŒæ¨™æº–åŒ–
    for i in range(1, 5):
        assert torch.allclose(
            ensemble_outputs[0].mean(dim=0), 
            ensemble_outputs[i].mean(dim=0), 
            atol=5.0
        ), f"æˆå“¡ {i} çµ±è¨ˆé‡å·®ç•°éå¤§"
    
    print("âœ… æ¸¬è©¦ 7.1 é€šé: Ensemble è¨“ç·´é©é…æ­£ç¢º")
    return True


def test_7_2_curriculum_learning_scenario():
    """æ¸¬è©¦ 7.2: Curriculum Learning æƒ…å¢ƒ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 7.2: Curriculum Learning æƒ…å¢ƒ")
    print("=" * 80)
    
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0]
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.0, 'u_std': 2.0,
                'v_mean': 0.0, 'v_std': 1.0,
                'p_mean': 50.0, 'p_std': 10.0
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # éšæ®µ 1: ç°¡å–®åŸŸï¼ˆK=50ï¼‰
    coords_stage1 = torch.randn(50, 3, device=DEVICE)
    normalizer.input_transform.fit(coords_stage1)
    coords_norm_1 = normalizer.input_transform.transform(coords_stage1)
    
    # éšæ®µ 2: è¤‡é›œåŸŸï¼ˆK=200ï¼‰
    coords_stage2 = torch.randn(200, 3, device=DEVICE) * 3.0  # ä¸åŒåˆ†ä½ˆ
    normalizer.input_transform.fit(coords_stage2)  # é‡æ–°æ“¬åˆ
    coords_norm_2 = normalizer.input_transform.transform(coords_stage2)
    
    # é©—è­‰é‡æ–°æ“¬åˆå¾Œçš„åŠŸèƒ½æ­£ç¢ºï¼ˆè¼¸å‡ºå½¢ç‹€ã€é¡å‹ï¼‰
    assert coords_norm_2.shape == (200, 3), "è¼¸å‡ºå½¢ç‹€æ­£ç¢º"
    assert coords_norm_2.device == DEVICE, "è¼¸å‡ºè¨­å‚™æ­£ç¢º"
    
    print("âœ… æ¸¬è©¦ 7.2 é€šé: Curriculum Learning é©é…æ­£ç¢º")
    return True


def test_7_3_adaptive_collocation_scenario():
    """æ¸¬è©¦ 7.3: Adaptive Collocation æƒ…å¢ƒ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 7.3: Adaptive Collocation æƒ…å¢ƒ")
    print("=" * 80)
    
    config = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0]
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.0, 'u_std': 2.0,
                'v_mean': 0.0, 'v_std': 1.0,
                'p_mean': 50.0, 'p_std': 10.0
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # åˆå§‹å‡å‹»æ¡æ¨£
    coords_uniform = torch.randn(100, 3, device=DEVICE)
    normalizer.input_transform.fit(coords_uniform)
    
    # è‡ªé©æ‡‰æ·»åŠ é«˜èª¤å·®å€åŸŸé»
    coords_adaptive = torch.randn(50, 3, device=DEVICE) * 2.0  # æ›´å¤§ç¯„åœ
    coords_combined = torch.cat([coords_uniform, coords_adaptive], dim=0)
    
    # é‡æ–°æ“¬åˆæ“´å±•çš„æ¡æ¨£
    normalizer.input_transform.fit(coords_combined)
    coords_norm = normalizer.input_transform.transform(coords_combined)
    
    # é©—è­‰æ“´å±•æ¡æ¨£å¾Œä»èƒ½æ­£ç¢ºæ¨™æº–åŒ–
    # æ”¾å¯¬é–¾å€¼ä»¥é©æ‡‰æ›´å¤§ç¯„åœçš„æ¡æ¨£
    assert abs(coords_norm.mean().item()) < 0.3, f"å‡å€¼ç•°å¸¸: {coords_norm.mean().item():.4f}"
    assert abs(coords_norm.std().item() - 1.0) < 0.3, f"æ¨™æº–å·®ç•°å¸¸: {coords_norm.std().item():.4f}"
    
    print("âœ… æ¸¬è©¦ 7.3 é€šé: Adaptive Collocation é©é…æ­£ç¢º")
    return True


def test_7_4_vs_pinn_variable_scaling_scenario():
    """æ¸¬è©¦ 7.4: VS-PINN Variable Scaling æƒ…å¢ƒ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 7.4: VS-PINN Variable Scaling æƒ…å¢ƒ")
    print("=" * 80)
    
    config = {
        'normalization': {
            'enable': True,
            'input': {'norm_type': 'vs_pinn'},  # VS-PINN æ¨¡å¼
            'output': {
                'norm_type': 'standard',
                'variable_order': ['u', 'v', 'w', 'p'],
                'means': {'u': 0.0, 'v': 0.0, 'w': 0.0, 'p': 0.0},
                'stds': {'u': 1.0, 'v': 1.0, 'w': 1.0, 'p': 1.0},
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # VS-PINN æ‡‰ä¿æŒåº§æ¨™ä¸è®Š
    coords = torch.randn(100, 3)
    coords_transformed = normalizer.input_transform.transform(coords)
    
    assert torch.allclose(coords, coords_transformed, atol=1e-6), \
        "VS-PINN æ¨¡å¼æ‡‰ä¿æŒåº§æ¨™ä¸è®Š"
    
    print("âœ… æ¸¬è©¦ 7.4 é€šé: VS-PINN Variable Scaling é©é…æ­£ç¢º")
    return True


def test_7_5_mixed_precision_training_scenario():
    """æ¸¬è©¦ 7.5: Mixed Precision è¨“ç·´æƒ…å¢ƒ"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 7.5: Mixed Precision è¨“ç·´æƒ…å¢ƒ")
    print("=" * 80)
    
    config = {
        'normalization': {
            'enable': True,
            'input': {'norm_type': 'standard'},
            'output': {
                'norm_type': 'standard',
                'variable_order': ['u', 'v', 'p'],
                'means': {'u': 1.0, 'v': 0.0, 'p': 50.0},
                'stds': {'u': 2.0, 'v': 1.0, 'p': 10.0},
            }
        }
    }
    
    normalizer = UnifiedNormalizer.from_config(config, device=DEVICE)
    
    # æ¨¡æ“¬ float16 è¨“ç·´
    coords_fp32 = torch.randn(100, 3, dtype=torch.float32)
    normalizer.input_transform.fit(coords_fp32)
    
    coords_fp16 = coords_fp32.half()
    coords_norm_fp16 = normalizer.input_transform.transform(coords_fp16.float())
    
    # é©—è­‰ dtype è½‰æ›ä¸å½±éŸ¿æ¨™æº–åŒ–
    assert coords_norm_fp16.dtype == torch.float32, "æ‡‰è½‰å› float32"
    assert abs(coords_norm_fp16.mean().item()) < 0.1, "æ¨™æº–åŒ–å¤±æ•ˆ"
    
    print("âœ… æ¸¬è©¦ 7.5 é€šé: Mixed Precision é©é…æ­£ç¢º")
    return True


def test_7_6_checkpoint_resume_with_different_config():
    """æ¸¬è©¦ 7.6: è·¨é…ç½®æª¢æŸ¥é»æ¢å¾©"""
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ 7.6: è·¨é…ç½®æª¢æŸ¥é»æ¢å¾©")
    print("=" * 80)
    
    # åŸå§‹é…ç½®
    config_v1 = {
        'model': {
            'scaling': {
                'input_norm': 'standard',
                'input_norm_range': [-1.0, 1.0]
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 1.0, 'u_std': 2.0,
                'v_mean': 0.0, 'v_std': 1.0,
                'p_mean': 50.0, 'p_std': 10.0
            }
        }
    }
    
    normalizer_v1 = UnifiedNormalizer.from_config(config_v1, device=DEVICE)
    coords = torch.randn(100, 3, device=DEVICE)
    normalizer_v1.input_transform.fit(coords)
    
    # ä¿å­˜ metadata
    metadata = normalizer_v1.get_metadata()
    
    # æ–°é…ç½®å˜—è©¦è¦†è“‹ï¼ˆæ‡‰è¢« metadata è¦†è“‹ï¼‰
    config_v2 = {
        'model': {
            'scaling': {
                'input_norm': 'minmax',  # å˜—è©¦æ”¹è®Š
                'input_norm_range': [0.0, 1.0]
            }
        },
        'normalization': {
            'type': 'manual',
            'variable_order': ['u', 'v', 'p'],
            'params': {
                'u_mean': 999.0, 'u_std': 1.0,  # å˜—è©¦è¦†è“‹
                'v_mean': 999.0, 'v_std': 1.0,
                'p_mean': 999.0, 'p_std': 1.0
            }
        }
    }
    
    # å¾ metadata æ¢å¾©ï¼ˆå„ªå…ˆç´šé«˜æ–¼æ–°é…ç½®ï¼‰
    normalizer_v2 = UnifiedNormalizer.from_metadata(metadata)
    
    # é©—è­‰ä½¿ç”¨åŸå§‹çµ±è¨ˆé‡
    assert normalizer_v2.input_transform.norm_type == 'standard', "norm_type æ‡‰ä¾†è‡ª metadata"
    assert abs(normalizer_v2.output_transform.means['u'] - 1.0) < 1e-6, \
        "means æ‡‰ä¾†è‡ª metadata è€Œéæ–°é…ç½®"
    
    print("âœ… æ¸¬è©¦ 7.6 é€šé: è·¨é…ç½®æª¢æŸ¥é»æ¢å¾©æ­£ç¢º")
    return True


# ========================================
# ä¸»æ¸¬è©¦åŸ·è¡Œ
# ========================================

def run_all_tests():
    """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
    print("\n" + "=" * 80)
    print("TASK-audit-005: Phase 2 é…ç½®é©—è­‰æ¸¬è©¦")
    print("æ—¥æœŸ: 2025-10-17")
    print("=" * 80)
    
    tests = [
        # æ¸¬è©¦ 5: é…ç½®è®€å–
        ("Test 5.1", test_5_1_basic_config_parsing),
        ("Test 5.2", test_5_2_missing_config_defaults),
        ("Test 5.3", test_5_3_disable_normalization),
        ("Test 5.4", test_5_4_vs_pinn_channel_flow_config),
        ("Test 5.5", test_5_5_yaml_file_loading),
        
        # æ¸¬è©¦ 6: Trainer æ•´åˆ
        ("Test 6.1", test_6_1_trainer_init_with_normalization),
        ("Test 6.2", test_6_2_forward_pass_with_denormalization),
        ("Test 6.3", test_6_3_gradient_flow_in_training_loop),
        ("Test 6.4", test_6_4_checkpoint_metadata_integration),
        
        # æ¸¬è©¦ 7: å¤šæƒ…å¢ƒé©é…
        ("Test 7.1", test_7_1_ensemble_training_scenario),
        ("Test 7.2", test_7_2_curriculum_learning_scenario),
        ("Test 7.3", test_7_3_adaptive_collocation_scenario),
        ("Test 7.4", test_7_4_vs_pinn_variable_scaling_scenario),
        ("Test 7.5", test_7_5_mixed_precision_training_scenario),
        ("Test 7.6", test_7_6_checkpoint_resume_with_different_config),
    ]
    
    results = []
    for name, test_func in tests:
        try:
            success = test_func()
            results.append((name, "âœ… PASS" if success else "âŒ FAIL"))
        except Exception as e:
            results.append((name, f"âŒ FAIL: {str(e)}"))
            print(f"\nâŒ {name} å¤±æ•—: {str(e)}")
    
    # è¼¸å‡ºç¸½çµ
    print("\n" + "=" * 80)
    print("æ¸¬è©¦ç¸½çµ")
    print("=" * 80)
    
    passed = sum(1 for _, status in results if "PASS" in status)
    total = len(results)
    
    for name, status in results:
        print(f"{name}: {status}")
    
    print("\n" + "=" * 80)
    print(f"ç¸½è¨ˆ: {passed}/{total} é€šé")
    print("=" * 80)
    
    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
