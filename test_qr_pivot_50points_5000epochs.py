#!/usr/bin/env python3
"""
Task-014: 3D PINNsé‡å»ºç²¾åº¦å„ªåŒ– - QR Pivoting 50é» + 5000 Epochs è¶…é•·è¨“ç·´ç‰ˆ
===============================================================================

çµ‚æ¥µæŒ‘æˆ°ç­–ç•¥:
1. ğŸ¯ ä½¿ç”¨QR Pivotingç®—æ³•é¸æ“‡50å€‹æœ€å„ªæ„Ÿæ¸¬é»
2. ğŸš€ è¶…é•·5000 epochsè¨“ç·´ä»¥å……åˆ†å­¸ç¿’ç¨€ç–æ•¸æ“š
3. ğŸ¯ ä¿æŒæˆåŠŸçš„Vå ´æ¬Šé‡å¹³è¡¡ç­–ç•¥
4. ğŸ¯ æ·»åŠ å­¸ç¿’ç‡è¡°æ¸›èˆ‡æ—©åœæ©Ÿåˆ¶
5. ğŸ¯ å¢å¼·çš„æ­£å‰‡åŒ–ç­–ç•¥é˜²æ­¢éæ“¬åˆ

ç›®æ¨™: é©—è­‰æ¥µå°‘é»æ•¸(50)ä¸‹çš„é‡å»ºæ¥µé™èƒ½åŠ›
"""

import numpy as np
import torch
import h5py
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
from scipy.linalg import qr
warnings.filterwarnings("ignore")

def load_and_process_jhtdb_3d():
    """è¼‰å…¥ä¸¦è™•ç†JHTDB 3Dæ•¸æ“š"""
    print("ğŸ“ è¼‰å…¥JHTDB 3Dæ•¸æ“š...")
    
    cache_file = Path("data/jhtdb/channel_34e525c703a89036170603d28e552870.h5")
    if not cache_file.exists():
        raise FileNotFoundError(f"æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {cache_file}")
    
    with h5py.File(cache_file, 'r') as f:
        # è®€å–3Dç¶²æ ¼æ•¸æ“š [64,32,32]
        u_grid = np.array(f['u'])
        v_grid = np.array(f['v'])
        w_grid = np.array(f['w'])  
        p_grid = np.array(f['p'])
        
        print(f"   åŸå§‹ç¶²æ ¼: {u_grid.shape}")
        
        # å‰µå»º3Dåæ¨™ç¶²æ ¼
        nz, ny, nx = u_grid.shape
        
        z_coords = np.linspace(0, 2*np.pi, nz)
        y_coords = np.linspace(-1, 1, ny)
        x_coords = np.linspace(0, np.pi, nx)
        t_coord = 0.0
        
        T, Z, Y, X = np.meshgrid([t_coord], z_coords, y_coords, x_coords, indexing='ij')
        
        coords = np.column_stack([
            T.flatten(),  # t
            X.flatten(),  # x (å±•å‘)
            Y.flatten(),  # y (å£é¢æ³•å‘)
            Z.flatten()   # z (æµå‘)
        ])
        
        values = np.column_stack([
            u_grid.flatten(),
            v_grid.flatten(), 
            w_grid.flatten(),
            p_grid.flatten()
        ])
        
        print(f"   åæ¨™ç¶­åº¦: {coords.shape}")
        print(f"   ç‹€æ…‹ç¶­åº¦: {values.shape}")
        
        for i, name in enumerate(['U', 'V', 'W', 'P']):
            print(f"   {name}: mean={values[:,i].mean():.6f}, std={values[:,i].std():.6f}")
        
        return coords, values

def qr_pivot_sensor_selection(coords, values, n_sensors=50):
    """ä½¿ç”¨QR Pivotingé¸æ“‡æœ€å„ªæ„Ÿæ¸¬é»"""
    print(f"\nğŸ¯ ä½¿ç”¨QR Pivotingé¸æ“‡{n_sensors}å€‹æœ€å„ªæ„Ÿæ¸¬é»...")
    
    # æº–å‚™æ•¸æ“šçŸ©é™£ï¼šä½¿ç”¨é€Ÿåº¦å ´æ§‹å»ºæ„Ÿæ¸¬çŸ©é™£
    # å–å‰3å€‹å ´ (u,v,w) ä½œç‚ºæ„Ÿæ¸¬ç›®æ¨™ï¼Œå£“åŠ›å ´ä½œç‚ºé‡å»ºç›®æ¨™
    data_matrix = values[:, :3]  # [n_points, 3] (u,v,w)
    
    print(f"   æ•¸æ“šçŸ©é™£: {data_matrix.shape}")
    print(f"   é¸æ“‡é»æ•¸: {n_sensors}")
    
    # å°æ•¸æ“šçŸ©é™£é€²è¡ŒQRåˆ†è§£é¸ä¸»å…ƒ
    try:
        # è½‰ç½®æ•¸æ“šçŸ©é™£ä»¥ä¾¿é¸æ“‡ç©ºé–“é»
        X = data_matrix.T  # [3, n_points]
        Q, R, piv = qr(X, mode='economic', pivoting=True)
        
        # é¸æ“‡å‰n_sensorså€‹ä¸»å…ƒé»
        selected_indices = piv[:n_sensors]
        
        print(f"   âœ… QR PivotingæˆåŠŸé¸æ“‡{len(selected_indices)}å€‹é»")
        
        # è¨ˆç®—é¸æ“‡é»çš„ç©ºé–“åˆ†ä½ˆ
        selected_coords = coords[selected_indices]
        x_range = [selected_coords[:, 1].min(), selected_coords[:, 1].max()]
        y_range = [selected_coords[:, 2].min(), selected_coords[:, 2].max()]
        z_range = [selected_coords[:, 3].min(), selected_coords[:, 3].max()]
        
        print(f"   ğŸ“ é¸æ“‡é»ç©ºé–“åˆ†ä½ˆ:")
        print(f"      X(å±•å‘): [{x_range[0]:.3f}, {x_range[1]:.3f}]")
        print(f"      Y(å£é¢): [{y_range[0]:.3f}, {y_range[1]:.3f}]")
        print(f"      Z(æµå‘): [{z_range[0]:.3f}, {z_range[1]:.3f}]")
        
        # è¨ˆç®—æ¢ä»¶æ•¸ä½œç‚ºå“è³ªæŒ‡æ¨™
        selected_data = data_matrix[selected_indices, :]
        cond_number = np.linalg.cond(selected_data)
        print(f"   ğŸ“Š é¸æ“‡æ•¸æ“šæ¢ä»¶æ•¸: {cond_number:.2e}")
        
        return selected_indices
        
    except Exception as e:
        print(f"   âŒ QR Pivotingå¤±æ•—: {e}")
        print(f"   ğŸ”„ å›é€€åˆ°éš¨æ©Ÿé¸æ“‡...")
        return np.random.choice(len(coords), n_sensors, replace=False)

class UltraLongTrain3DNet(torch.nn.Module):
    """è¶…é•·è¨“ç·´å°ˆç”¨3D PINNsç¶²è·¯"""
    
    def __init__(self, layers=[4, 128, 128, 128, 128, 4]):
        super().__init__()
        
        self.layers = torch.nn.ModuleList()
        for i in range(len(layers) - 1):
            self.layers.append(torch.nn.Linear(layers[i], layers[i+1]))
            
        # Dropouté˜²æ­¢éæ“¬åˆï¼ˆåƒ…åœ¨æ¥µé•·è¨“ç·´æ™‚å•Ÿç”¨ï¼‰
        self.dropout = torch.nn.Dropout(0.1)
        self.training_mode = True
        
        # æˆåŠŸçš„åˆå§‹åŒ–ç­–ç•¥
        for layer in self.layers:
            if isinstance(layer, torch.nn.Linear):
                torch.nn.init.xavier_normal_(layer.weight)
                torch.nn.init.zeros_(layer.bias)
        
        print(f"   ğŸ¯ UltraLongTrain3Dç¶²è·¯: {layers}")
        print(f"   ğŸ“Š åƒæ•¸ç¸½æ•¸: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, x):
        """å‰å‘å‚³æ’­ï¼ˆæ”¯æ´dropoutï¼‰"""
        for i, layer in enumerate(self.layers[:-1]):
            x = torch.tanh(layer(x))
            # åœ¨ä¸­é–“å±¤æ·»åŠ è¼•å¾®dropout
            if self.training_mode and i > 0:
                x = self.dropout(x)
        x = self.layers[-1](x)
        return x

def compute_physics_residual_ultra(model, coords_tensor):
    """è¶…é•·è¨“ç·´çš„ç‰©ç†ç´„æŸè¨ˆç®—"""
    coords_tensor.requires_grad_(True)
    pred = model(coords_tensor)
    
    u = pred[:, 0:1]
    v = pred[:, 1:2]
    w = pred[:, 2:3]
    p = pred[:, 3:4]
    
    # æ¢¯åº¦è¨ˆç®—
    u_grads = torch.autograd.grad(u.sum(), coords_tensor, create_graph=True, retain_graph=True)[0]
    v_grads = torch.autograd.grad(v.sum(), coords_tensor, create_graph=True, retain_graph=True)[0]
    w_grads = torch.autograd.grad(w.sum(), coords_tensor, create_graph=True, retain_graph=True)[0]
    
    # é€£çºŒæ€§æ–¹ç¨‹
    u_x = u_grads[:, 1:2]
    v_y = v_grads[:, 2:3]
    w_z = w_grads[:, 3:4]
    
    continuity = u_x + v_y + w_z
    
    # Vå ´å£é¢ç´„æŸ
    y_coords = coords_tensor[:, 2:3]
    wall_mask = (torch.abs(torch.abs(y_coords) - 1.0) < 0.1).squeeze()
    
    if wall_mask.sum() > 0:
        v_wall = v[wall_mask]
        wall_constraint = torch.mean(v_wall**2)
    else:
        wall_constraint = torch.tensor(0.0)
    
    return continuity, wall_constraint

def train_ultra_long_5000epochs():
    """QR Pivoting 50é» + 5000 epochsè¶…é•·è¨“ç·´"""
    print("ğŸš€ QR Pivoting 50é» + 5000 Epochs è¶…é•·è¨“ç·´...")
    
    # è¼‰å…¥æ•¸æ“š
    coords, values = load_and_process_jhtdb_3d()
    
    # ğŸ¯ ä½¿ç”¨QR Pivotingé¸æ“‡50å€‹æœ€å„ªé»
    selected_indices = qr_pivot_sensor_selection(coords, values, n_sensors=50)
    train_coords = coords[selected_indices]
    train_values = values[selected_indices]
    
    print(f"\nğŸ“ˆ æœ€çµ‚è¨“ç·´é…ç½®:")
    print(f"   è¨“ç·´é»æ•¸: {len(train_coords)} (QR Pivotingé¸æ“‡)")
    print(f"   è¨“ç·´è¼ªæ•¸: 5000 epochs")
    
    # è½‰æ›ç‚ºtensor
    coords_tensor = torch.FloatTensor(train_coords)
    values_tensor = torch.FloatTensor(train_values)
    
    # ç‰©ç†ç´„æŸé»ï¼ˆä½¿ç”¨æ›´å¤šé»ä»¥ä¿æŒç‰©ç†ä¸€è‡´æ€§ï¼‰
    n_physics = 200
    physics_indices = np.random.choice(len(coords), n_physics, replace=False)
    physics_tensor = torch.FloatTensor(coords[physics_indices])
    
    # === å°ºåº¦æ¬Šé‡è¨ˆç®— ===
    field_names = ['U', 'V', 'W', 'P']
    field_stds = [values_tensor[:, i].std().item() for i in range(4)]
    base_std = field_stds[0]
    
    scale_weights = []
    for i, name in enumerate(field_names):
        if field_stds[i] > 1e-8:
            weight = base_std / field_stds[i]
            if name == 'V':
                weight *= 1.5  # Vå ´å¢å¼·
        else:
            weight = 1.0
        scale_weights.append(weight)
        print(f"   {name}å ´ std={field_stds[i]:.6f}, æ¬Šé‡={weight:.4f}")
    
    scale_weights = torch.FloatTensor(scale_weights)
    
    # åˆå§‹åŒ–ç¶²è·¯
    model = UltraLongTrain3DNet()
    
    # ğŸ¯ è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿åº¦ (é‡å°è¶…é•·è¨“ç·´)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # å¤šéšæ®µå­¸ç¿’ç‡è¡°æ¸›
    def lr_scheduler(epoch):
        if epoch < 1000:
            return 1.0  # åˆæœŸä¿æŒåŸå­¸ç¿’ç‡
        elif epoch < 2000:
            return 0.5  # ä¸­æœŸæ¸›åŠ
        elif epoch < 4000:
            return 0.2  # å¾ŒæœŸå¤§å¹…é™ä½
        else:
            return 0.1  # æœ€çµ‚éšæ®µç²¾ç´°èª¿æ•´
    
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_scheduler)
    
    # æ—©åœæ©Ÿåˆ¶
    best_loss = float('inf')
    patience = 500
    patience_counter = 0
    
    losses = []
    v_errors = []  # å°ˆé–€è¿½è¹¤Vå ´èª¤å·®
    
    print("\nğŸ”„ é–‹å§‹è¶…é•·è¨“ç·´ (5000 epochs)...")
    
    for epoch in range(5000):
        optimizer.zero_grad()
        
        # === æ•¸æ“šæå¤±è¨ˆç®— ===
        pred_data = model(coords_tensor)
        
        field_losses = {}
        total_weighted_loss = 0
        
        for i, name in enumerate(field_names):
            field_pred = pred_data[:, i:i+1]
            field_true = values_tensor[:, i:i+1]
            field_mse = torch.nn.MSELoss()(field_pred, field_true)
            
            weighted_loss = scale_weights[i] * field_mse
            field_losses[name] = weighted_loss
            total_weighted_loss += weighted_loss
            
            # Vå ´å°ˆé …è¿½è¹¤
            if name == 'V':
                v_rmse = torch.sqrt(field_mse)
                v_errors.append(v_rmse.item())
        
        data_loss = total_weighted_loss / len(field_names)
        
        # === ç‰©ç†ç´„æŸæå¤± ===
        try:
            continuity, wall_v = compute_physics_residual_ultra(model, physics_tensor)
            continuity_loss = torch.mean(continuity**2)
            wall_loss = wall_v
        except:
            continuity_loss = torch.tensor(0.0)
            wall_loss = torch.tensor(0.0)
        
        # === æ­£å‰‡åŒ–æå¤± (é˜²æ­¢éæ“¬åˆ) ===
        l2_reg = 0
        for param in model.parameters():
            l2_reg += torch.norm(param)
        l2_lambda = 1e-6  # è¼•å¾®L2æ­£å‰‡åŒ–
        
        # === ç¸½æå¤± ===
        lambda_data = 10.0
        lambda_physics = 1.0
        lambda_wall = 2.0
        
        total_loss = (lambda_data * data_loss + 
                     lambda_physics * continuity_loss +
                     lambda_wall * wall_loss +
                     l2_lambda * l2_reg)
        
        total_loss.backward()
        
        # æ¢¯åº¦è£å‰ª (ç©©å®šè¶…é•·è¨“ç·´)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        scheduler.step()
        
        losses.append(total_loss.item())
        
        # æ—©åœæª¢æŸ¥
        if total_loss.item() < best_loss:
            best_loss = total_loss.item()
            patience_counter = 0
        else:
            patience_counter += 1
        
        # è©³ç´°é€²åº¦å ±å‘Š
        if epoch % 500 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            v_current = v_errors[-1] if v_errors else 0
            print(f"Epoch {epoch:4d}: ç¸½æå¤±={total_loss.item():.4f}, "
                  f"æ•¸æ“š={data_loss.item():.4f}, ç‰©ç†={continuity_loss.item():.6f}, "
                  f"Vå ´RMSE={v_current:.6f}, LR={current_lr:.6f}")
        
        # æ—©åœæ¢ä»¶
        if patience_counter >= patience and epoch > 2000:
            print(f"   ğŸ›‘ æ—©åœæ–¼ epoch {epoch} (patience={patience})")
            break
    
    print("âœ… è¶…é•·è¨“ç·´å®Œæˆ!")
    print(f"   ğŸ¯ æœ€ä½³æå¤±: {best_loss:.6f}")
    print(f"   ğŸ¯ å¯¦éš›è¨“ç·´è¼ªæ•¸: {epoch+1}")
    
    # é—œé–‰dropouté€²è¡Œè©•ä¼°
    model.training_mode = False
    
    # è©•ä¼°
    errors, msg = evaluate_ultra_performance(model, coords, values, train_coords, train_values)
    
    return model, losses, errors, v_errors

def evaluate_ultra_performance(model, coords, values, train_coords, train_values):
    """è©•ä¼°è¶…é•·è¨“ç·´ç‰ˆæ€§èƒ½"""
    print("\nğŸ” è©•ä¼°QR Pivoting + 5000 Epochsæ€§èƒ½...")
    
    coords_tensor = torch.FloatTensor(coords)
    
    with torch.no_grad():
        pred_full = model(coords_tensor).numpy()
    
    field_names = ['U', 'V', 'W', 'P']
    print("=== QR Pivoting + è¶…é•·è¨“ç·´é‡å»ºèª¤å·®åˆ†æ ===")
    
    errors = []
    for i, name in enumerate(field_names):
        true_field = values[:, i]
        pred_field = pred_full[:, i]
        
        l2_error = np.sqrt(np.mean((pred_field - true_field)**2))
        rel_error = l2_error / (np.sqrt(np.mean(true_field**2)) + 1e-10) * 100
        
        errors.append(rel_error)
        print(f"{name}å ´: L2={l2_error:.6f}, ç›¸å°èª¤å·®={rel_error:.1f}%")
    
    avg_error = np.mean(errors)
    print(f"\nğŸ“Š QR Pivoting + è¶…é•·è¨“ç·´å¹³å‡ç›¸å°èª¤å·®: {avg_error:.1f}%")
    
    # èˆ‡ç›®æ¨™å°æ¯”
    if avg_error < 30.0:
        print("ğŸ‰ æˆåŠŸé”åˆ°<30%ç›®æ¨™!")
        success_msg = "âœ… QR Pivoting + 5000 epochs æˆåŠŸé”æ¨™"
    else:
        print(f"âš ï¸  å‘ç›®æ¨™åŠªåŠ› (ç›®æ¨™<30%)")
        success_msg = f"ğŸ”„ QR Pivoting + 5000 epochs ({avg_error:.1f}% vs 30%)"
    
    # èˆ‡ä¹‹å‰ç‰ˆæœ¬å°æ¯”
    print(f"\nğŸ“ˆ èˆ‡æ•¸æ“šå¢å¼·ç‰ˆå°æ¯”:")
    data_enhanced_avg = 33.0
    improvement = data_enhanced_avg - avg_error
    print(f"   æ•¸æ“šå¢å¼·ç‰ˆ(2048é»): {data_enhanced_avg:.1f}%")
    print(f"   QR Pivotingç‰ˆ(50é»): {avg_error:.1f}%")
    print(f"   æ”¹å–„: {improvement:.1f}% (ä½¿ç”¨{2048/50:.1f}xæ›´å°‘æ•¸æ“š)")
    
    # è¨“ç·´é»æ€§èƒ½åˆ†æ
    print(f"\nğŸ¯ è¨“ç·´é»é‡å»ºæ€§èƒ½:")
    train_tensor = torch.FloatTensor(train_coords)
    with torch.no_grad():
        pred_train = model(train_tensor).numpy()
    
    train_errors = []
    for i, name in enumerate(field_names):
        true_train = train_values[:, i]
        pred_train_field = pred_train[:, i]
        train_error = np.sqrt(np.mean((pred_train_field - true_train)**2)) / (np.sqrt(np.mean(true_train**2)) + 1e-10) * 100
        train_errors.append(train_error)
        print(f"   {name}å ´è¨“ç·´èª¤å·®: {train_error:.1f}%")
    
    train_avg = np.mean(train_errors)
    generalization = avg_error - train_avg
    print(f"   è¨“ç·´å¹³å‡èª¤å·®: {train_avg:.1f}%")
    print(f"   æ³›åŒ–å·®è·: {generalization:.1f}%")
    
    # Vå ´å°ˆé …åˆ†æ
    v_error = errors[1]
    v_improvement = 214.6 - v_error
    print(f"\nğŸ¯ Vå ´å°ˆé …æˆå°±:")
    print(f"   åŸå§‹Vå ´èª¤å·®: 214.6% â†’ QRç‰ˆ: {v_error:.1f}% (æ”¹å–„ {v_improvement:.1f}%)")
    
    # QR Pivotingæ•ˆæœåˆ†æ
    print(f"\nğŸ§® QR Pivotingé¸é»æ•ˆæœ:")
    print(f"   é¸æ“‡é»æ•¸: 50 (ç¸½æ•¸çš„{50/len(coords)*100:.3f}%)")
    print(f"   æ•¸æ“šæ•ˆç‡: {2048/50:.1f}xé»æ•¸æ¸›å°‘")
    print(f"   è¨ˆç®—æ•ˆç‡: 5000 vs 600 epochs (8.3xè¨“ç·´æ™‚é–“)")
    
    return errors, success_msg

if __name__ == "__main__":
    print("=" * 80)
    print("ğŸ¯ Task-014: QR Pivoting 50é» + 5000 Epochs è¶…é•·è¨“ç·´æŒ‘æˆ°")
    print("=" * 80)
    
    try:
        model, losses, errors, v_errors = train_ultra_long_5000epochs()
        print("\nğŸ‰ è¶…é•·è¨“ç·´æŒ‘æˆ°å®Œæˆ!")
        
        # å¯è¦–åŒ–Vå ´æ”¶æ–‚éç¨‹
        if len(v_errors) > 100:
            plt.figure(figsize=(10, 6))
            plt.plot(v_errors[::10])  # æ¯10å€‹epochå–ä¸€å€‹é»
            plt.title('V Field RMSE Convergence (QR Pivoting + 5000 Epochs)')
            plt.xlabel('Epoch (Ã—10)')
            plt.ylabel('V Field RMSE')
            plt.yscale('log')
            plt.grid(True)
            plt.savefig('qr_pivot_v_convergence.png', dpi=150, bbox_inches='tight')
            print("   ğŸ“Š Vå ´æ”¶æ–‚åœ–å·²ä¿å­˜: qr_pivot_v_convergence.png")
        
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()